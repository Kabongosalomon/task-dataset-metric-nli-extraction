<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Instant Response Few-shot Object Detector via Meta-Strategy and Explicit Localization Inference</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junying</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sibo</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyu</forename><surname>Zhan</surname></persName>
						</author>
						<title level="a" type="main">Instant Response Few-shot Object Detector via Meta-Strategy and Explicit Localization Inference</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Computing methodologies ? Object detection KEYWORDS Few-Shot Object Detection</term>
					<term>Few-Shot Learning</term>
					<term>Instant Response</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Aiming at recognizing and localizing the object of novel categories by a few reference samples, few-shot object detection (FSOD) is a quite challenging task. Previous works often depend on the finetuning process to transfer their model to the novel category and rarely consider the defect of fine-tuning, resulting in many application drawbacks. For example, these methods are far from satisfying in the episode-changeable scenarios due to excessive fine-tuning times, and their performance on low-quality (e.g., low-shot and class-incomplete) support sets degrades severely. To this end, this paper proposes an instant response few-shot object detector (IR-FSOD) that can accurately and directly detect the objects of novel categories without the fine-tuning process. To accomplish the objective, we carefully analyze the defects of individual modules in the Faster R-CNN framework under the FSOD setting and then extend it to IR-FSOD by improving these defects. Specifically, we first propose two simple but effective meta-strategies for the box classifier and RPN module to enable the object detection of novel categories with instant response. Then, we introduce two explicit inferences into the localization module to alleviate its over-fitting to the base categories, including explicit localization score and semi-explicit box regression. Extensive experiments show that the IR-FSOD framework not only achieves few-shot object detection with the instant response but also reaches state-of-the-art performance in precision and recall under various FSOD settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In the past decade, deep-learning-based methods have achieved remarkable success in various computer vision tasks, such as image classification <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13]</ref> and object detection <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26]</ref>. However, these methods are fundamentally dependent on a large amount of annotated data, so their generalization ability toward the open-world tasks is limited. This triggers active research on few-shot learning, which aims to develop models that can be generalized to the unseen categories with only a few support data with annotations.</p><p>By leveraging meta-learning and distance metric learning, remarkable progress has been made in few-shot image classification. However, the works attempting to solve few-shot object detection (FSOD) have encountered setbacks due to the complexity of object detection tasks. Most existing methods, whether meta-learningbased or fine-tuning-based, require a cumbersome fine-tuning process on the support data. Otherwise, they will suffer from seriously degraded or even be invalidated (e.g., the methods mentioned in <ref type="figure">Figure 1)</ref>. Therefore, these methods still have some drawbacks: (a) Due  <ref type="figure">Figure 1</ref>: Comprehensive comparison between different models on MS COCO dataset, including response time for fine-tuning, performance with/without fine-tuning, and the performance under low-shot setting. TFA <ref type="bibr" target="#b33">[34]</ref>, FSIW <ref type="bibr" target="#b38">[39]</ref>, and FSCE <ref type="bibr" target="#b27">[28]</ref> are invalidated before fine-tuning. A-RPN <ref type="bibr" target="#b5">[6]</ref> can be respond instantly the same as ours, but it performs poorly before fine-tuning. On the contrary, IR-FSOD can achieved optimal performance while supporting instant response.</p><p>to the need for independent fine-tuning processes in each episode, they are far from satisfying in the episode-changeable scenario, requiring unacceptable response times. (b) They tend to ignore the instances diverse from the support set due to over-fitting to the support instances, resulting in a low recall. (c) The effectiveness of fine-tuning depends on the quality of support sets, so they perform poorly under settings such as low-shot and class-incomplete.</p><p>To remedy these defects, this paper proposes a novel Instant Response Few-shot Object Detection (IR-FSOD) framework. "Instant response" (IR) refers specifically to the direct object detection for the novel category without preparatory work such as fine-tuning. The IR-FSOD framework is based on Faster R-CNN <ref type="bibr" target="#b25">[26]</ref> and built by addressing its defects in few-shot object detection, including the improvements on the box classifier, RPN module, and the localization module in the R-CNN derived model.</p><p>First of all, the box multi-classifier can only classify the region proposal into the seen category, which directly causes many existing few-shot object detectors <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39]</ref> to be invalidated on the novel category before fine-tuning. A-RPN <ref type="bibr" target="#b5">[6]</ref> and RepMet <ref type="bibr" target="#b15">[16]</ref> attempt to replace the multi-classifier with the comparison-classifier and distance-classifier to recognize the instance of novel categories. However, object detection is a much more complex task, which requires detecting the foreground, localization, and classification. The comparison-classifier and distance-classifier perform poorly in learning the complex feature space for object detection since they can't preserve the class-specific knowledge, so the performance of RepMet and A-RPN lags behind the fine-tuning-based methods.  <ref type="figure">Figure 2</ref>: Comparisons of the motivation, learnability, and generalization ability between three classifiers.</p><p>In <ref type="figure">Figure 2</ref>, we compare the three classifiers mentioned above: (a) The multi-classifier explicitly learns a hyper-plane for each base category on the feature space, which shows the best learnability but is invalidated on the novel category; (b) The comparison-classifier learns a class-agnostic binary classifier on the joint feature space. It can directly recognize the objects of novel categories but still suffers from the bias of base categories since its parameters are trained on the base category data; (c) The distance-classifier is non-parameter and performs classification according to the nearest neighbor rule. Although it can't preserve the knowledge from training, it doesn't suffer from category bias. Based on the observation, we propose a meta-strategy called the dynamic classifier module that uses different classifiers during training and inference to build a box classifier with both generalization and learnability in the IR-FSOD framework. It can significantly improve the few-shot performance while supporting instant response.</p><p>Second, the region proposal network (RPN) suffers a fatal defect in few-shot object detection. As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, the positive regions in the training phase are disjoint with the related regions for the novel category so that these related regions are either ignored or regarded as background in the base training. A-RPN <ref type="bibr" target="#b5">[6]</ref> and meta-rcnn <ref type="bibr" target="#b37">[38]</ref> proposes to generate class-specific region proposals to avoid RPN focusing only on the base class. But the fundamental problem that only the objects belonging to base categories are treated as positive anchors in base training remains unresolved, which leads to the bottleneck of their models before fine-tuning. FSCE <ref type="bibr" target="#b27">[28]</ref> and R-FSOD <ref type="bibr" target="#b6">[7]</ref> also point out the defects of the RPN module, but they only adjust the RPN module in the fine-tuning process, increasing the dependence on the fine-tuning process.</p><p>Different from them, we argue that the RPN module should focus on any potential foreground instances during base training instead of only the annotated instances. Therefore, we propose another meta-strategy that trains the RPN module with semi-supervised algorithms to capture the potential foreground instances. Concretely, the negative anchors during the base training are actually composed of background and the potential object instances not belonging to the base categories, so we remark them as unlabeled data and leverage them by the semi-supervised method.</p><p>Third, the localization module in the R-CNN derived model, composed of the localization score and box regression, is enabled by implicit fitting and lacks logic inference, which suffers from over-fitting to the distribution of the base categories. Therefore, we propose to improve the generalization ability of the localization module by introducing explicit logic inference. Concretely, we first introduce the pixel-wise contrast into the box classifier to explicitly evaluate the localization score, which can generate the confidence relevant to the localization result of the region proposal. For the box regression module, we propose a semi-explicit box regressor to strengthen the logic relation between the region feature and its box regression. These logic inferences are category-agnostic, and thus can maintain generalization to the novel category.</p><p>Extensive experiments on two large and challenging few-shot detection benchmark datasets, i.e., MS COCO <ref type="bibr" target="#b17">[18]</ref> and FSOD dataset <ref type="bibr" target="#b5">[6]</ref>, show that IR-FSOD can reach the state-of-the-art few-shot object detection performance while achieving instant response. Especially under the instant response setting, it promotes the current upper-performance limit <ref type="bibr" target="#b5">[6]</ref> by a large margin. Our main contributions can be summarized as follows:</p><p>(1) We propose two meta-strategies that can enable the object detection of novel categories with instant response. (2) We further introduce two explicit localization inferences into the localization module in the R-CNN derived model to alleviate its over-fitting to the base categories. (3) By applying the improvements, we propose a novel Instant Response Few-shot Object Detection (IR-FSOD) framework, which can achieve state-of-the-art results in both response time, precision, and recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>General Object Detection is a fundamental task in computer vision that has attracted lots of attention. Modern object detectors can be divided into two kinds: one-stage detectors and two-stage detectors. One-stage detectors directly predict categories and locations of objects, e.g., YOLO series <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref>, SSD <ref type="bibr" target="#b19">[20]</ref>, etc. Two-stage detectors, pioneered by R-CNN <ref type="bibr" target="#b10">[11]</ref>, first generate class-agnostic region proposals, then further refine and classify the proposals <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">26]</ref>. These works heavily rely on a huge amount of annotated data and are invalidated on the data with unseen categories, thus they can not be used to solve the FSOD problem.</p><p>Few-shot learning aims to recognize novel classes given limited labeled data. Meta-learning methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b39">40]</ref>, aka "learning to learn", propose to learn a meta-learner that can fast adapt to new tasks with few labeled samples. Distance metric learning methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42]</ref> focus on designing a distance formulation between the samples in an embedding space generated by  deep neural networks. Popular metrics include cosine similarity <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref>, Euclidean distance <ref type="bibr" target="#b26">[27]</ref> and graph distance <ref type="bibr" target="#b8">[9]</ref>. Few-shot Object Detection was proposed to handle the object detection for the novel category by only a few annotated samples. There are mainly two types of methods aiming to address the fewshot object detection problem,i.e., meta-learning-based methods and fine-tuning-based methods: Meta-learning-based methods attempt to build their few-shot detectors by leveraging various meta-learning techniques to extract the class-agnostic knowledge or transfer the knowledge from base categories to novel categories. Despite this goal, these methods still require a fine-tuning process. Otherwise, they are either invalidated <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41]</ref> or lagging behind other methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16]</ref>. FSRW <ref type="bibr" target="#b14">[15]</ref> extracts generic meta-features from base categories, then adjusts them using the re-weighting features for novel categories. Meta R-CNN <ref type="bibr" target="#b40">[41]</ref> and FSIW <ref type="bibr" target="#b38">[39]</ref> propose to use the re-weighting features over RoI features instead of the image feature. MetaDet <ref type="bibr" target="#b34">[35]</ref> and GenDet <ref type="bibr" target="#b18">[19]</ref> propose to estimate the new parameters in the detector for detecting novel category instances. RepMet <ref type="bibr" target="#b15">[16]</ref> incorporates distance metric learning into few-shot detection to help classify the proposals. A-RPN <ref type="bibr" target="#b5">[6]</ref> and meta-rcnn <ref type="bibr" target="#b37">[38]</ref> propose attention-RPN to generate the class-specific region proposal. A-RPN also proposes a multi-relation detector and a contrastive training strategy. Recent DCNet <ref type="bibr" target="#b13">[14]</ref> proposes to fully exploit local information to benefit the detection process and alleviate the scale variation problem by context-aware feature aggregation.</p><p>Fine-tuning-based methods adopt the general object detectors and focus on improving the fine-tuning process on the support data to effectively transfer the category-specific model to the novel category. They are once suffered from poor performance, but recent works set the new state-of-the-art. TFA <ref type="bibr" target="#b33">[34]</ref> simply fine-tunes the last layer of Faster R-CNN <ref type="bibr" target="#b25">[26]</ref> and substantially improves the performance. MPSR <ref type="bibr" target="#b36">[37]</ref> proposes to handle the scale variance issue by multi-scale positive sample refinement, but it needs a manual selection. Recent FSCE <ref type="bibr" target="#b27">[28]</ref> builds a strong baseline upon TFA <ref type="bibr" target="#b33">[34]</ref> and boosts the performance by large margins. It also integrates contrastive learning and achieves impressive performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY 3.1 Problem Definition</head><p>Given a base dataset D with annotated instances of the base (seen) category C , the objective of few-shot object detection is to train a robust model on D which can be generalized on the novel dataset D with instances of the novel (unseen) category C (C ? C = ). For each novel category, there is also a support set S with a few annotated instances, which are only available during testing. In more detail, N-way K-shot object detection means C contains N categories and each support set contains K annotated instances (usually less than 10), i.e., S = {( , ), = 1, ..., } where and denote the support image and the bounding box of the support instances. The novel dataset is also called query set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">IR-FSOD Framework</head><p>The overview of IR-FSOD is shown in <ref type="figure" target="#fig_2">Figure 4</ref>, which is based on Faster R-CNN <ref type="bibr" target="#b25">[26]</ref>. The general process is as follows: Given a query image and a support set, the goal is to detect the objects belonging to the support category in the query image. Firstly, the backbone extracts the feature maps of the query image and all support images. Then the RPN module predicts the region proposals in the query image. After that, the RoI module, including RoI-pooling and RoI-extractor, extracts the feature maps of region proposals and the feature maps of all the support instances, with the same shape. Finally, the box classifier and box regressor further predict the category and the box regression of the region proposals by comparing the region features and the support feature which is the average of all support instance feature maps. In addition to the framework, there is also a non-maximum suppression (NMS) algorithm to select the non-overlap prediction box according to the localization score.</p><p>In particular, we first propose two meta-strategies (Sec. 3.3), i.e., dynamic classifier and semi-supervised RPN (SS-RPN), to enable the object detection of novel categories with instant response. Then, we further present two explicit localization inferences (Sec. 3.4), i.e., explicit localization score and semi-explicit box regression, to alleviate the over-fitting to the base categories. As shown in <ref type="figure" target="#fig_2">Figure  4</ref>, we mark the position of the proposed techniques in yellow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Meta-strategies</head><p>In this section, we focus on the classification module in the framework, including the box classifier and the RPN module. Dynamic Classifier: As analyzed in Sec. 1 and <ref type="figure">Figure 2</ref>, we argue that the three existing commonly used classifiers all have some defects in the few-shot object detection task. Therefore, we propose to adopt a dynamic classifier module. Specifically, the box classifier module uses different classifiers in training and inference to improve both the generalization to novel categories and the learnability for feature space. In addition, since the distance-classifier is non-parameter, it doesn't require re-training when replacing the trained classifier with the distance-classifier during inference. <ref type="table" target="#tab_2">Table 1</ref> shows the ablation study of different classifier combinations on the MS COCO dataset under the instant response setting and 10-shot one-time FSOD evaluation protocol. For a fair comparison, both the multi-classifier and comparison-classifier are single connection layers, and the distance-classifier is the cosine distance between the region feature and the support feature. As shown in <ref type="table" target="#tab_2">Table 1</ref>, the multi-classifier is invalidated on the novel category before re-training. The comparison-classifier performs worse than the distance-classifier since its parameters are still affected by the bias of the base categories. However, the distanceclassifier can significantly benefit from models trained by the multiclassifier or comparison-classifier due to their learnability. Based on the results, the IR-FSOD framework adopts the comparisonclassifier in the base training and replaces it with the distanceclassifier during inference. The details of the two classifiers will be described respectively in Sec. 3.4. Semi-supervised RPN (SS-RPN): Generating region proposals by the class-agnostic detector (e.g., RPN) is a crucial idea in twostage detection models, but it has a fatal defect in few-shot object detection. As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, the positive regions in the training phase are disjoint with the related regions for the novel categories so that these related regions are either ignored or regarded as background in the base training. Thus the RPN module in the FSOD framework is implicitly class-specific to the base categories, which is hard to capture the anchors related to novel categories.</p><p>To address the problem, we propose to adopt semi-supervised algorithms to train the RPN module. Concretely, in the RPN training, all the positive anchors are certain foreground instances. But the negative anchors actually consist of background or potential object instances not belonging to the base categories, so we remark them as unlabeled data. In the IR-FSOD framework, we adopt a simple but effective semi-supervised algorithm, i.e., Pseudo Label.</p><p>In more detail, as shown at the top of <ref type="figure" target="#fig_3">Figure 5</ref>, we first annotate the anchors whose Intersection over Union (IoU) with the ground truth box is less than 0.3 as negative, and the anchors whose IoU is greater than 0.7 as positive, following the standard RPN training process. Then, we annotate the negative anchors with RPN prediction probabilities greater than threshold as the pseudo positive label and compute positive loss, the same as the positive label. To calculate the balance loss, we keep the ratio of the positive anchors, the negative anchors, and the pseudo positive anchors as 1:1:1. The selection of threshold is shown in Sec. 4.5.</p><p>Discussion: In the object detection task, it is unrealistic to achieve both high recall and high precision. The RPN with semisupervised training inevitably leads to more background proposals in the inference. However, we argue that this is worth it because it is possible to eliminate these background proposals by the box classifier in the two-stage detection model. On the contrary, the foreground anchors ignored by the RPN module are irreparable. Experiments also validate that even the naive semi-supervised algorithm (i.e., pseudo label) can significantly boost the object detection performance for the novel category (As shown in <ref type="table" target="#tab_2">Table 1</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semi-explicit</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Explicit Localization Inferences</head><p>In this section, we focus on the localization module in the framework, including the localization score and the box regression. Explicit Localization Score: R-CNN derived model implicitly evaluates the localization score of the region proposal by the classification score from the box classifier. However, the object bounding box usually contains some low-confidence regions, such as the background and the low-discrimination parts of the target object. In contrast, the classification score often only considers the highconfidence region. For example, the classification scores of the two region box in <ref type="figure" target="#fig_5">Figure 6</ref> (a) are almost the same. Thus the classification score is inconsistent with the localization score, i.e., it cannot reflect the localization result. Although this inconsistency can be alleviated by training on a large amount of annotated data, it can not be generalized to the novel category. Thus it is necessary to explore a training-independent alleviating method.</p><p>To tackle the problem, we propose to integrate the pixel-wise contrast between feature maps into the box classification to explicitly evaluate the localization confidence of the region proposal.  Specifically, the RoI module first extracts the feature maps of the region proposal and the support feature with the same shape. Then the core idea is to integrate the features comparison on each pixel of the feature map, which can compare the similarity between the instance distribution in the region proposal and the standard distribution in the support box. Obviously, the higher similarity between the instance distribution can often indicate better localization. We also provide the specific case in the Appendix to illustrate the effectiveness of this approach. As mentioned above, IR-FSOD adopts a dynamic classifier module, thus we design different integration methods for the comparison-classifier and the distance-classifier, respectively. The integration method for the comparison-classifier is shown in the middle of <ref type="figure" target="#fig_3">Figure 5</ref>, which integrates the pixel-wise contrast by a lightweight network. For the distance-classifier, it can integrate the pixel-wise contrast by the distance between the flattened feature maps. Concretely, given a region proposal and a support set of category , it first calculates the cosine distance between global feature vectors and the cosine distance between flattened feature maps, then adopts their weighted sum to integrate the two distances. Finally, the probability of belonging to category is predicted as:</p><formula xml:id="formula_0">( ; ) = [ (1 ? ) ( , ) + ( , )) ]<label>(1)</label></formula><p>( , ) = || || ? || || (2)</p><formula xml:id="formula_1">( ) = 1 1 + ?<label>(3)</label></formula><p>where and represent the vectors obtained from the feature map by global average pooling and flatten function, respectively. and mean the cosine distance and sharp sigmoid function. The selection of and is shown in Sec. 4.5.</p><p>Semi-explicit Box Regression: General object detectors <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26]</ref> often implicitly fit the mapping between the features and the box regression by a lightweight network. These mappings are dependent on the category, thus the trained regressor is hard to generalize to the novel category. To tackle the problem, we propose a semiexplicit box regressor by introducing the category-agnostic logic relation into the regression mapping. It leverages an explicit regression mechanism that: any two pairs of coordinates between the region proposal and the GT box can provide two regression equations, equivalent to a correct box regression, as shown in <ref type="figure" target="#fig_5">Figure 6</ref> (b). Despite the equivalence, this explicit regression is invalidated since the GT box is unavailable during inference. Therefore, we propose to extract sufficient possible coordinate pairs from the comparison between the region proposal and the support box, and then predict the box regression by these coordinate pairs. Specifically, given the feature map of a region proposal and the average support feature map of category (e.g., , ? ? ? ), we first reshape them as list of feature vectors (e.g.,?,?? 2 ? ), then compute the distance matrix between two lists, where , = (?,?).</p><p>Then, we flatten the distance matrix to a distance vector ? 4 and concatenate it with the region proposal feature. Finally, we feed the concatenated feature into a lightweight network to predict the box regression, as shown at the bottom of <ref type="figure" target="#fig_3">Figure 5</ref>. Discussion: In , each index represents a coordinate pair between two feature maps, and the corresponding value indicates the confidence score of the coordinate pair. However, these confidence scores may not be accurate due to the difference between the support instance and the GT instance. Explicitly calculating box regression by these coordinate pairs will suffer from serious errors by inaccurate scores. Thus we still predict the box regression by feeding all confidence scores into a neural network to implicitly synthesize all the equations. This regression method is between the implicit regression in general object detectors and the explicit regression by the regression equation, so we call it semi-explicit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training Strategy:</head><p>Inspired by A-RPN <ref type="bibr" target="#b5">[6]</ref>, we train our model by the 2-way 10-shot contrastive training strategy. For each training image as a query image, we first randomly select a positive category 1 that appears in the image and a negative category 2 that doesn't appear in the image ( 1 , 2 ? C ) and then collect their support sets (S 1 and S 2 ) from D , both containing ten object instances. After the forward process described in 3.2, we train the box comparison-classifier by the positive loss that matches the same category and the negative loss that distinguishes the different categories. For the box regressor, we only calculate the box regression loss of the region proposal belonging to 1 . The training of the semi-supervised RPN is the same as Faster R-CNN <ref type="bibr" target="#b25">[26]</ref> except for the pseudo-label described above. The final loss function is defined as:</p><formula xml:id="formula_3">L = L + L + L<label>(5)</label></formula><p>where L consists of the classification loss and regression loss of proposals, L is the binary cross-entropy loss for box classification, L is the smoothed 1 loss for box regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Experimental Setup</head><p>Dataset: In this paper, we conduct experiments on two large and challenging few-shot detection benchmark datasets, MS COCO <ref type="bibr" target="#b17">[18]</ref> and FSOD dataset <ref type="bibr" target="#b5">[6]</ref>, which contains 800K objects belonging  to 80 categories and 182K objects belonging to 1000 categories respectively. For MS COCO, we set the 20 categories belonging to PASCAL VOC <ref type="bibr" target="#b4">[5]</ref> as the novel categories and the remaining 60 categories as the base categories following the existing works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34]</ref>. We use the train2017 with only annotations of base categories for training and evaluate the detection result of novel categories on the val2017. FSOD dataset is specially designed for few-shot object detection, whose training set and test set only contain disjoint 800 base categories and 200 novel categories. Implement Details: We use the commonly used ResNet-50 <ref type="bibr" target="#b12">[13]</ref> as our backbone. Following the existing works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34]</ref>, the backbone is pre-trained on ImageNet <ref type="bibr" target="#b3">[4]</ref>. The most of network architectures and the hyper-parameters remain the same as Faster R-CNN <ref type="bibr" target="#b25">[26]</ref> except for the box classifier and box regressor, as described in Sec. 3.4. In addition, we halve the number of sampled anchors in RPN and proposals in the RoI head used for loss calculation from (512, 256) for the positive and negative anchors to (128, 128, 128) for the positive, the negative, and the pseudo positive anchors during training. Our model is trained by SGD optimizer on 3 RTX 2080Ti GPUs with a batch size of 9 (3 query images per GPU) for 120,000 iterations. The learning rate is initialized as 0.003 with the weight decay of factor 0.1 at 80,000 ? and 110,000 ? iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation protocol:</head><p>We conduct experiments on the one-time FSOD protocol proposed in <ref type="bibr" target="#b14">[15]</ref> and the meta-testing protocol commonly used in the few-shot learning <ref type="bibr" target="#b31">[32]</ref>. Given the support sets of all novel categories, the one-time FSOD protocol directly evaluates the performance of detecting these novel categories on the complete test set. The meta-testing protocol requires evaluating the average performance of the detector under numerous random episodes. Each episode randomly collects a novel category subset and consists of the corresponding support set and query set. In addition, we also provide the experiments under the class-incomplete setting in the Appendix since it is rarely used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">One-time FSOD evaluation</head><p>MS COCO result: In <ref type="table" target="#tab_3">Table 2</ref>, we compare our IR-FSOD with the previous state-of-the-art methods under the 10-shot setting. For a fair comparison, we also report the backbone used in the models and the time required for the tuning process. Although the competitive DCNet <ref type="bibr" target="#b13">[14]</ref> doesn't adopt FPN, it still uses multi-scale features to enhance its detector. As shown in the table, IR-FSOD achieves new state-of-the-art results in most indicators. Under the instant response setting, it outperforms the latest method <ref type="bibr" target="#b5">[6]</ref> by about 78% on the metric. It's worth noting that IR-FSOD achieves both the highest precision and recall, which is rare in the existing methods. For example, A-RPN <ref type="bibr" target="#b5">[6]</ref> before fine-tuning is competitive to our model on the recall, but its precision ( ) is only 56% of ours; DCNet <ref type="bibr" target="#b13">[14]</ref> is competitive to our model on the precision, but its recall ( 10 ) is only 80% of ours. To sum up, IR-FSOD achieves the overall leadership in response time, precision, and recall.</p><p>Then, we conduct further comparison experiments under different shot settings ( ? {1, 2, 3, 5, 10}). For a fair comparison, we evaluate all the methods over ten random runs. In each run, all the methods adopt the same support set. The support sets are generated from TFA <ref type="bibr" target="#b33">[34]</ref>. Besides IR-FSOD, A-RPN <ref type="bibr" target="#b5">[6]</ref> is the only method that can support instant response, and others require fine-tuning time ranging from ten minutes to several hours per run to respond. As shown in <ref type="table" target="#tab_4">Table 3</ref>, IR-FSOD can outperform the previous works by 0.8%-2.6% under different shot settings. Although A-RPN can also outperform other methods under the low-shot settings while supporting instant response, it becomes significantly behind the method with fine-tuning as the shot number increase. On the contrary, IR-FSOD can always stay ahead, demonstrating its generalized effectiveness under varied few-shot settings. FSOD dataset result: Similar to MS COCO, we evaluate all the methods over ten random runs, and all the methods adopt the same support set in each run. The support sets are generated from the code of TFA <ref type="bibr" target="#b33">[34]</ref>. The average results under the 1/3/5 shot setting are shown in <ref type="table" target="#tab_5">Table 4</ref>. As shown in the table, IR-FSOD achieves state-ofthe-art results on 1/3 shots and comparable results on 5-shots with instant response, demonstrating the strong generalization to the various novel categories. It should be noted that the FSOD dataset has 200 novel classes, i.e., the support set in the 5-shot setting has 1000 object instances. It's usually hard to obtain so many instances in the practice scenario. Therefore, the detection performance under the low-shot setting is more important. In addition, IR-FSOD also performs both high precision and high recall on the FSOD dataset, the same as the results on the MS COCO dataset, indicating that it is not accidental on a particular dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Meta-testing protocol</head><p>In this section, we perform the meta-testing protocol on the MS COCO dataset. For an -way -shot few-shot object detection, we collect 1,000 episodes and evaluate the average object detection  performance with 95% confidence interval. Each episode consists of an -way -shot support set and a query set containing ten images for each category. Since the evaluation is performed on each episode independently, including the fine-tuning process and the inference process, the models with fine-tuning require unacceptable time. For example, DCNet <ref type="bibr" target="#b13">[14]</ref> requires more than a month to perform the whole meta-testing. Therefore, we only compare our IR-FSOD with the models that support instant response <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref>. <ref type="table" target="#tab_6">Table 5</ref> and 6 report the average results with the 95 % confidence interval and the detection time (seconds-per-episode) under different few-shot settings, including ? {5, 10} and ? {5, 10}. As shown in the table, our IR-FSOD achieves a significant lead in both performance and efficiency. Specifically, it outperform A-RPN by 2.3%-3.4% , 5.4%-7.3% 50 , and 1.5%-2.8% 75 . In addition, IR-FSOD runs only half as long as A-RPN, since A-RPN introduces many complexities, such as generating the class-specific proposal for each category and integrating multiple relation modules. In contrast, the approaches in IR-FSOD are simpler but more effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head><p>In this section, we evaluate the effects of the core components in IR-FSOD. All ablation studies are conducted on the COCO dataset under the 10-shot setting and one-time FSOD evaluation protocol. IR-FSOD is built on top of Faster R-CNN <ref type="bibr" target="#b10">[11]</ref>, which is designed for general object detection. Thus we adopt it as the baseline and design the ablation experiments in two stages in <ref type="table" target="#tab_8">Table 7</ref>.</p><p>In the first stage, we evaluate the effect of the proposed dynamic classifier module, which is the essential strategy to transform the general object detector into a few-shot object detector with instant response. Without the dynamic classifier module, the model suffers from low performance or even invalidation due to low learnability or low generalization of the classifier. Just by introducing the dynamic classifier module into the Faster R-CNN, it is already comparable with some methods requiring fine-tuning <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>In the second stage, we evaluate the different combinations of three proposed boosted modules, including the semi-supervised RPN (SS-RPN), the box classifier with the pixel-wise contrast (Cls-PW), and the semi-explicit box regressor (SE-Reg). As shown in <ref type="table" target="#tab_8">Table 7</ref>, their improvements for the model performance are different and are all in line with our expectations. Concretely, (a) The semisupervised RPN mainly achieves the performance improvement on the 50 metric (+1.4%-3.6%), indicating that it successfully captured more potential region proposals belonging to the novel categories; (b) The semi-explicit box regressor can significantly improve the result on the 75 metric (+1.6%-2.0%), which shows that it can generate more accurate high-quality boxes by improving the localization accuracy; (c) The pixel-wise contrast in the box classifier mainly improves the confidence ranking of all the predicted boxes, thus can achieve significant improvement on all evaluation metrics, e.g., 50 (+1.4%-3.6%) and 75 (+1.3%-1.7%).</p><p>Finally, we also perform qualitative ablation studies for the two explicit localization inferences to verify their improvement in the localization process. As shown in <ref type="figure" target="#fig_7">Figure 7 (a)</ref>, the classifier with the pixel-wise contrast can produce the confidence (top) that is basically consistent with the localization result (i.e., the IoU between ground truth). Without the pixel-wise contrast, the confidence (bottom) is irrelevant to localization, resulting in high-confidence but poorly localized predictions. <ref type="figure" target="#fig_7">Figure 7 (b)</ref> shows that the semi-explicit box regressor can significantly improve the localization accuracy and generate more high-quality predicted boxes. We also provide more qualitative ablation studies in the Appendix by more detection cases and the statistics of detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Hyper-parameter Studies:</head><p>In this section, we study the effect and selection of the hyperparameters in IR-FSOD, including the threshold in the semisupervised RPN, as well as the balance weight and the scaling factor in the distance-classifier. For each hyper-parameter, we   first select a candidate set by observation and then evaluate their performance on the COCO dataset under the 10-shot setting and one-time FSOD evaluation protocol. The performances at different values of them in <ref type="table" target="#tab_9">Table 8</ref>. The specific analysis is as follows: Threshold : Semi-supervised learning on image classification usually requires high thresholds to reduce the incorrect pseudo labels. However, in the two-stage detector, we expect RPN to capture all the potential objects as possible and then eliminate the incorrect proposals by the box classifier. Therefore, the model performs better when is lower and reaches the optimal performance at = 0.25. Balance weight : Compared with = 0.0 and = 1.0, the performances with other values are improved significantly, indicating that the global contrast and the pixel-wise contrast are both valuable. It reaches the optimal integration performance at = 1 2 . Scaling factor : In the IR-FSOD, the performance is not affected by the scaling factor since it doesn't affect the confidence ranking of the predicted box. Thus we empirically choose = 20 to adjust the sharpness of the prediction distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In the few-shot object detection field, the existing methods tend to transfer their model to the detection task by leveraging a finetuning process, resulting in many application drawbacks. To tackle the problem, this paper studies in-depth how to get rid of finetuning while maintaining the FSOD performance. Through careful study of each module in a general object detector (i.e., Faster R-CNN), this paper builds an instant response few-shot object detector (IR-FSOD) that can accurately detect the object of novel categories while getting rid of the fine-tuning process. To more solidly validate the proposed analysis, we deliberately avoid introducing excessive extra-complexity when designing the improved components. Despite its simplicity, IR-FSOD can reach state-of-the-art performance in both efficiency, precision, and recall. It is noteworthy that our works are built on only Faster R-CNN without other prior methods, so all the approaches are easily compatible with the existing FSOD methods. We hope our studies can inspire future works to explore more powerful few-shot object detectors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The positive regions for the RPN in the training and testing phase are disjoint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The overview of the IR-FSOD framework. It is built on top of Faster R-CNN. The module without yellow mask is the same as Faster R-CNN. SS-RPN means the proposed semi-supervised RPN. The detail of the semi-supervised RPN, dynamic box classifier, and semi-explicit box regressor are shown in Figure 5, Sec. 3.3, and Sec. 3.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>, FC , ReLU , FC ?, 1x1 Conv, ReLU, GAP, FC ? ? The detail of components in the IR-FSOD. The input of the comparison-classifier and semi-explicit box regressor are the feature map of a region proposal (blue) and the support feature map of a category (red). GAP, FC, and Conv mean the global average pooling, full connection layer, and convolution layer. ? means to concatenate the input features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>The motivations of explicit localization inferences. The yellow box and the red box in (a) mean the bounding box and the local high confidence region respective. (b) shows an explicit box regression method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) The top-5 predicted boxes from the box classifier with (top) / without (bottom) the pixel-wise contrast. (b) The localization result from the semi-explicit box regressor (left) and the general box regressor (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>The qualitative ablation studies for the introduced explicit localization inference, i.e., (a) the pixel-wise contrast and (b) the semi-explicit box regressor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Ablation experimental results for dynamic classifier and semi-supervised rpn on MS COCO under 10-shot setting.</figDesc><table><row><cell cols="2">Dynamic Classifier</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(Training)</cell><cell>(Inference)</cell><cell></cell><cell>50</cell><cell>75</cell></row><row><cell>Multi</cell><cell>Multi</cell><cell></cell><cell>invalidated</cell><cell></cell></row><row><cell>Comparison</cell><cell>Comparison</cell><cell>4.71</cell><cell>8.79</cell><cell>4.41</cell></row><row><cell>Distance</cell><cell>Distance</cell><cell>5.94</cell><cell>15.64</cell><cell>2.82</cell></row><row><cell>Multi</cell><cell>Distance</cell><cell>6.89</cell><cell>15.28</cell><cell>5.44</cell></row><row><cell>Comparison</cell><cell>Distance</cell><cell cols="3">8.69 17.38 7.78</cell></row><row><cell cols="2">+ Semi-supervised RPN</cell><cell cols="3">10.54 20.96 9.08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Few-shot detection results for 20 novel classes on COCO dataset. "IR" means the model is instant response, i.e., without tuning process. RED/BLUE indicate the SOTA/second best. + means the result is estimated by the description in their paper.</figDesc><table><row><cell>Model</cell><cell>Backbone</cell><cell></cell><cell>50</cell><cell cols="2">Average Precision 75</cell><cell></cell><cell></cell><cell>1</cell><cell>10</cell><cell cols="2">Average Recall 100</cell><cell></cell><cell></cell><cell>Tuning time</cell></row><row><cell>A-RPN [6]</cell><cell>Res-50</cell><cell>7.3</cell><cell>13.2</cell><cell>7.1</cell><cell>4.4</cell><cell>8.7</cell><cell>10.7</cell><cell>17.5</cell><cell>32.3</cell><cell>33.2</cell><cell>10.0</cell><cell>34.7</cell><cell>50.4</cell><cell>IR</cell></row><row><cell cols="2">Meta R-CNN [41] Res-50</cell><cell>8.7</cell><cell>19.1</cell><cell>6.6</cell><cell>2.3</cell><cell>7.7</cell><cell>14.0</cell><cell>12.6</cell><cell>17.8</cell><cell>17.9</cell><cell>7.8</cell><cell>15.6</cell><cell>27.2</cell><cell>5 + h</cell></row><row><cell>MPSR [37]</cell><cell>FPN-101</cell><cell>9.8</cell><cell>17.9</cell><cell>9.7</cell><cell>3.3</cell><cell>9.2</cell><cell>16.1</cell><cell>15.7</cell><cell>21.2</cell><cell>21.2</cell><cell>4.6</cell><cell>19.6</cell><cell cols="2">34.3 40 + min</cell></row><row><cell>TFA [34]</cell><cell>FPN-101</cell><cell>9.8</cell><cell>19.7</cell><cell>8.9</cell><cell>2.8</cell><cell>9.2</cell><cell>16.1</cell><cell>14.5</cell><cell>18.6</cell><cell>18.6</cell><cell>5.3</cell><cell>14.8</cell><cell>33.1</cell><cell>16 h</cell></row><row><cell>FSCE [28]</cell><cell>FPN-101</cell><cell>11.9</cell><cell>22.3</cell><cell>11.6</cell><cell>2.9</cell><cell>11.1</cell><cell>17.6</cell><cell>17.0</cell><cell>26.6</cell><cell>26.5</cell><cell>6.7</cell><cell>26.3</cell><cell>42.3</cell><cell>3 h</cell></row><row><cell>A-RPN+FT[6]</cell><cell>Res-50</cell><cell>12.0</cell><cell>22.4</cell><cell>11.8</cell><cell>2.9</cell><cell>12.2</cell><cell>20.7</cell><cell>18.8</cell><cell>26.4</cell><cell>26.4</cell><cell>3.6</cell><cell>23.6</cell><cell>45.6</cell><cell>15 min</cell></row><row><cell>FSIW [39]</cell><cell>Res-50</cell><cell>12.5</cell><cell>27.3</cell><cell>9.8</cell><cell>2.5</cell><cell>13.8</cell><cell>19.9</cell><cell>20.0</cell><cell>25.5</cell><cell>25.7</cell><cell>7.5</cell><cell>27.6</cell><cell>38.9</cell><cell>5 h</cell></row><row><cell>DCNet [14]</cell><cell>Res-101</cell><cell>12.8</cell><cell>23.4</cell><cell>11.2</cell><cell>4.3</cell><cell>13.8</cell><cell>21.0</cell><cell>18.1</cell><cell>26.7</cell><cell>25.6</cell><cell>7.9</cell><cell>24.5</cell><cell cols="2">36.7 40 + min</cell></row><row><cell>IR-FSOD</cell><cell>Res-50</cell><cell>13.1</cell><cell>24.5</cell><cell>12.3</cell><cell>5.9</cell><cell>16.2</cell><cell>22.0</cell><cell>19.1</cell><cell>33.5</cell><cell>35.6</cell><cell>11.4</cell><cell>39.4</cell><cell>54.9</cell><cell>IR</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Few-shot detection results on COCO dataset under different shot settings. * means the result is re-implemented. RED/BLUE indicate the SOTA/second best. The results are averaged over ten random runs.</figDesc><table><row><cell>Model</cell><cell>Backbone</cell><cell></cell><cell>1-shot</cell><cell></cell><cell></cell><cell>2-shot</cell><cell></cell><cell></cell><cell>3-shot</cell><cell></cell><cell></cell><cell>5-shot</cell><cell></cell><cell></cell><cell>10-shot</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>50</cell><cell>75</cell><cell></cell><cell>50</cell><cell>75</cell><cell></cell><cell>50</cell><cell>75</cell><cell></cell><cell>50</cell><cell>75</cell><cell></cell><cell>50</cell><cell>75</cell></row><row><cell>A-RPN* [6]</cell><cell>Res-50</cell><cell>3.6</cell><cell>7.2</cell><cell>3.2</cell><cell>5.1</cell><cell>9.7</cell><cell>4.7</cell><cell>5.6</cell><cell>10.7</cell><cell>5.2</cell><cell>6.3</cell><cell>11.9</cell><cell>5.9</cell><cell>6.7</cell><cell>12.5</cell><cell>5.8</cell></row><row><cell>TFA [34]</cell><cell>FPN-101</cell><cell>1.9</cell><cell>3.8</cell><cell>1.7</cell><cell>3.9</cell><cell>7.8</cell><cell>3.6</cell><cell>5.1</cell><cell>9.9</cell><cell>4.8</cell><cell>7.0</cell><cell>13.3</cell><cell>6.5</cell><cell>9.1</cell><cell>17.1</cell><cell>8.8</cell></row><row><cell>FSIW [39]</cell><cell>Res-50</cell><cell>3.2</cell><cell>8.9</cell><cell>1.4</cell><cell>4.9</cell><cell>13.3</cell><cell>2.3</cell><cell>6.7</cell><cell>18.6</cell><cell>2.9</cell><cell>8.1</cell><cell>20.1</cell><cell>4.4</cell><cell>10.7</cell><cell>25.6</cell><cell>6.5</cell></row><row><cell cols="2">A-RPN+FT*[6] Res-50</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>4.8</cell><cell>9.2</cell><cell>3.9</cell><cell>5.9</cell><cell>11.6</cell><cell>5.7</cell><cell>8.1</cell><cell>15.5</cell><cell>7.4</cell><cell>10.9</cell><cell>20.5</cell><cell>9.4</cell></row><row><cell>FSCE* [28]</cell><cell>FPN-101</cell><cell>2.0</cell><cell>4.9</cell><cell>1.3</cell><cell>4.2</cell><cell>9.5</cell><cell>3.4</cell><cell>5.7</cell><cell>12.0</cell><cell>4.7</cell><cell>7.6</cell><cell>15.6</cell><cell>6.5</cell><cell>11.2</cell><cell>22.3</cell><cell>9.8</cell></row><row><cell>IR-FSOD</cell><cell>Res-50</cell><cell>5.1</cell><cell>10.8</cell><cell>4.3</cell><cell>7.7</cell><cell>15.7</cell><cell>6.6</cell><cell>8.9</cell><cell>17.6</cell><cell>7.9</cell><cell cols="2">10.5 20.8</cell><cell>9.1</cell><cell cols="3">12.0 23.5 10.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Few-shot detection results for 200 novel classes on FSOD dataset. "Time" means the tuning time. "IR" means the model is instant response, i.e., without tuning process. RED/BLUE indicate the SOTA/second best. All results are reimplemented and averaged over ten random runs.</figDesc><table><row><cell cols="3">Shot Method Backbone</cell><cell></cell><cell>50</cell><cell>75</cell><cell>10 Time</cell></row><row><cell></cell><cell cols="2">A-RPN [6] Res-50</cell><cell cols="4">9.79 16.00 10.25 40.33 IR</cell></row><row><cell>1</cell><cell cols="2">TFA [34] FPN-101 FSCE [28] FPN-101</cell><cell>7.43 7.21</cell><cell>12.07 12.54</cell><cell>7.79 6.92</cell><cell>14.42 2 h 15.20 1.5 h</cell></row><row><cell></cell><cell>IR-FSOD</cell><cell>Res-50</cell><cell cols="4">10.66 18.41 16.62 38.47 IR</cell></row><row><cell></cell><cell cols="2">A-RPN [6] Res-50</cell><cell cols="4">14.94 23.68 15.91 50.01 IR</cell></row><row><cell>3</cell><cell cols="6">TFA [34] FPN-101 13.21 21.10 14.16 24.87 3 h FSCE [28] FPN-101 14.96 25.85 14.73 29.21 3 h</cell></row><row><cell></cell><cell>IR-FSOD</cell><cell>Res-50</cell><cell cols="4">16.33 27.59 10.65 48.91 IR</cell></row><row><cell></cell><cell cols="2">A-RPN [6] Res-50</cell><cell cols="4">17.28 27.13 18.49 52.30 IR</cell></row><row><cell>5</cell><cell cols="6">TFA [34] FPN-101 15.85 24.89 17.31 27.74 3.5 h FSCE [28] FPN-101 19.58 33.42 19.79 36.10 3.5 h</cell></row><row><cell></cell><cell>IR-FSOD</cell><cell>Res-50</cell><cell cols="4">19.24 32.08 19.75 52.69 IR</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Meta-testing evaluation with 95% confidence interval on the MS-COCO dataset under 5-way and 1,000 episodes setting. SPE means seconds-per-episode.</figDesc><table><row><cell cols="2">K Method</cell><cell></cell><cell>50</cell><cell>75</cell><cell>SPE</cell></row><row><cell></cell><cell cols="2">DANA [2] 12.60 ?0.29</cell><cell>25.90 ?0.44</cell><cell>11.30 ?0.35</cell><cell>10</cell></row><row><cell>5</cell><cell cols="2">A-RPN [6] 14.27 ?0.27</cell><cell>26.61 ?0.45</cell><cell>13.58 ?0.31</cell><cell>9</cell></row><row><cell></cell><cell>IR-FSOD</cell><cell>16.59 ?0.28</cell><cell>31.97 ?0.47</cell><cell>15.12 ?0.33</cell><cell>5</cell></row><row><cell>10</cell><cell cols="2">A-RPN [6] 15.12 ?0.29 IR-FSOD 17.74 ?0.29</cell><cell>27.74 ?0.47 33.59 ?0.47</cell><cell>14.61 ?0.32 16.56 ?0.34</cell><cell>10 5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">: Meta-testing evaluation with 95% confidence in-</cell></row><row><cell cols="5">terval on the MS-COCO dataset under 10-way and 1,000</cell></row><row><cell cols="4">episodes setting. SPE means seconds-per-episode.</cell><cell></cell></row><row><cell cols="2">K Method</cell><cell>50</cell><cell>75</cell><cell>SPE</cell></row><row><cell>5</cell><cell>A-RPN [6] 11.32 ?0.19 IR-FSOD 14.23 ?0.16</cell><cell>20.84 ?0.28 27.39 ?0.26</cell><cell>10.87 ?0.20 13.11 ?0.18</cell><cell>28 12</cell></row><row><cell>10</cell><cell>A-RPN [6] 12.11 ?0.15 IR-FSOD 15.52 ?0.14</cell><cell>22.05 ?0.27 29.41 ?0.26</cell><cell>11.78 ?0.19 14.55 ?0.16</cell><cell>30 13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Ablation for key components proposed in this paper: results from on the COCO dataset under the 10-shot setting.</figDesc><table><row><cell></cell><cell>Ablation</cell><cell></cell><cell></cell><cell>50</cell><cell>75</cell></row><row><cell>Faster R-CNN (baseline)</cell><cell cols="2">+ Multi-classifier + Comparison-classifier</cell><cell>4.71</cell><cell>invalidated 8.79</cell><cell>4.41</cell></row><row><cell></cell><cell cols="2">+ Distance-classifier</cell><cell>5.94</cell><cell>15.64</cell><cell>2.82</cell></row><row><cell>+ Dyn-cls:</cell><cell cols="5">(Multi (Comparison + Distance) 8.69 17.38 7.78 + Distance) 6.89 15.28 5.44</cell></row><row><cell>+ SS-RPN</cell><cell></cell><cell></cell><cell cols="2">10.54 20.96</cell><cell>9.08</cell></row><row><cell>+ SE-Reg</cell><cell></cell><cell></cell><cell cols="2">10.64 20.59</cell><cell>9.58</cell></row><row><cell>+ Cls-PW</cell><cell></cell><cell></cell><cell cols="2">10.67 20.92</cell><cell>9.47</cell></row><row><cell>+ SS-RPN</cell><cell>+ Cls-PW</cell><cell></cell><cell cols="3">11.65 22.52 10.31</cell></row><row><cell>+ SS-RPN</cell><cell>+ SE-Reg</cell><cell></cell><cell cols="3">11.82 22.92 10.60</cell></row><row><cell>+ Cls-PW</cell><cell>+ SE-Reg</cell><cell></cell><cell cols="3">11.94 22.21 11.19</cell></row><row><cell>+ SS-RPN</cell><cell>+ Cls-PW</cell><cell>+ SE-Reg</cell><cell cols="3">13.05 24.50 12.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Hyper-parameter studies of and in the IR-FSOD: results from on the COCO dataset under the 10-shot setting.</figDesc><table><row><cell></cell><cell>50</cell><cell>75</cell><cell></cell><cell>50</cell><cell>75</cell></row><row><cell>-</cell><cell cols="2">11.94 22.21 11.19</cell><cell>0.0</cell><cell>11.82 22.92 10.60</cell></row><row><cell cols="3">0.75 12.38 22.98 11.73</cell><cell>1/3</cell><cell>12.96 24.39 12.16</cell></row><row><cell cols="3">0.50 12.66 23.97 11.79</cell><cell cols="2">1/2 13.05 24.50 12.33</cell></row><row><cell cols="3">0.25 13.05 24.50 12.33</cell><cell>2/3</cell><cell>12.92 24.20 12.18</cell></row><row><cell cols="3">0.10 12.87 24.05 12.03</cell><cell>1.0</cell><cell>12.01 22.24 11.49</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Yolov4: Optimal speed and accuracy of object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10934</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dual-Awareness Attention for Few-Shot Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-I</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueh-Cheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Cheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Hsiang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Fong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Chin</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hsu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMM.2021.3125195</idno>
		<ptr target="https://doi.org/10.1109/TMM.2021.3125195" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04232</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The Pascal Visual Object Classes (VOC) Challenge. In International Journal of Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="303" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Few-shot object detection with attention-RPN and multi-relation detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4013" to="4022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generalized few-shot object detection without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4527" to="4536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dense relation distillation with context-aware aggregation for few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanzhe</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoxue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshi</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10185" to="10194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Few-shot object detection via feature reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8420" to="8429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Repmet: Representative-based metric learning for classification and few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivan</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Aides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML deep learning workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Zitnick</surname></persName>
		</author>
		<ptr target="https://www.microsoft.com/en-us/research/publication/microsoft-coco-common-objects-in-context/" />
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gendet: Meta learning to generate detectors from few shots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bochao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghui</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing-Hao</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingmin</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<title level="m">Reptile: a scalable metalearning algorithm</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">YOLO9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zemel</forename><surname>Richard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fsce: Few-shot object detection via contrastive proposal encoding</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<editor>Bo Sun, Banghuai Li, Shengcai Cai, Ye Yuan, and Chi Zhang</editor>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7352" to="7362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="403" to="412" />
		</imprint>
	</monogr>
	<note>Tat-Seng Chua, and Schiele Bernt</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.4773</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5265" to="5274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Frustratingly Simple Few-Shot Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9919" to="9928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Meta-learning to detect rare objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9925" to="9934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cooperative Bi-Path Metric for Few-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1524" to="1532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-Scale Positive Sample Refinement for Few-Shot Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="456" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Meta-RCNN: Meta Learning for Few-Shot Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiongwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doyen</forename><surname>Sahoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1679" to="1687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Few-Shot Object Detection and Viewpoint Estimation for Objects in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>Marlet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Target-Guided Adaptive Base Class Reweighting for Few-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianming</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3474085.3475656</idno>
		<ptr target="https://doi.org/10.1145/3474085.3475656" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5335" to="5343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Meta r-cnn: Towards general solver for instance-level low-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anni</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9577" to="9586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Towards Cross-Granularity Few-Shot Learning: Coarse-to-Fine Pseudo-Labeling with Visual-Semantic Meta-Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3005" to="3014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
