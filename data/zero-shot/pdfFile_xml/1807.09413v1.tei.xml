<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3DFeat-Net: Weakly Supervised Local 3D Features for Point Cloud Registration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><forename type="middle">Jian</forename><surname>Yew</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim</forename><forename type="middle">Hee</forename><surname>Lee</surname></persName>
							<email>gimhee.lee@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">3DFeat-Net: Weakly Supervised Local 3D Features for Point Cloud Registration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>point cloud</term>
					<term>registration</term>
					<term>deep learning</term>
					<term>weak supervision</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose the 3DFeat-Net which learns both 3D feature detector and descriptor for point cloud matching using weak supervision. Unlike many existing works, we do not require manual annotation of matching point clusters. Instead, we leverage on alignment and attention mechanisms to learn feature correspondences from GPS/INS tagged 3D point clouds without explicitly specifying them. We create training and benchmark outdoor Lidar datasets, and experiments show that 3DFeat-Net obtains state-of-the-art performance on these gravityaligned datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>3D point cloud registration plays an important role in many real-world applications such as 3D Lidar-based mapping and localization for autonomous robots, and 3D model acquisition for archaeological studies, geo-surveying and architectural inspections etc. Compared to images, point clouds exhibit less variation and can be matched under strong lighting changes, i.e. day and night, or summer and winter ( <ref type="figure" target="#fig_0">Fig. 1)</ref>. A two-step process is commonly used to solve the point cloud registration problem -(1) establishing 3D-3D point correspondences between the source and target point clouds, and (2) finding the optimal rigid transformation between the two point clouds that minimizes the total Euclidean distance between all point correspondences. Unfortunately, the critical step of establishing 3D-3D point correspondences is non-trivial. Even though many handcrafted 3D feature detectors <ref type="bibr" target="#b42">[35,</ref><ref type="bibr" target="#b5">5]</ref> and descriptors <ref type="bibr" target="#b31">[26,</ref><ref type="bibr" target="#b30">25,</ref><ref type="bibr" target="#b16">13,</ref><ref type="bibr" target="#b37">30,</ref><ref type="bibr" target="#b34">28]</ref> have been proposed over the years, the performance of establishing 3D-3D point correspondences remains unsatisfactory. As a result, iterative algorithms, e.g. Iterative Closest Point (ICP) <ref type="bibr" target="#b3">[3]</ref>, that circumvent the need for wide-baseline 3D-3D point correspondences with good initialization and nearest neighbors, are often used. This severely limits usage in applications such as global localization / pose estimation <ref type="bibr" target="#b19">[16]</ref> and loop-closures <ref type="bibr" target="#b7">[7]</ref> that require wide-baseline correspondences.</p><p>Inspired by the success of deep learning for computer vision tasks such as image-based object recognition <ref type="bibr" target="#b25">[21]</ref>, several deep learning based works that learn 3D feature descriptors for finding wide-baseline 3D-3D point matches have been proposed in the recent years. Regardless of the improvements of these deep learning based 3D descriptors over the traditional handcrafted 3D descriptors, none of them proposed a full pipeline that uses deep learning to concurrently learn both the 3D feature detector and descriptor. This is because the existing deep learning approaches are mostly based on supervised learning that requires huge amounts of hand-labeled data for training. It is impossible for anyone to manually identify and label salient 3D features from a point cloud. Hence, most existing approaches focused only on learning the 3D descriptors, while the detection of the 3D features are done with random selection <ref type="bibr" target="#b41">[34,</ref><ref type="bibr" target="#b8">8]</ref>. On the other hand, it is interesting to note the availability of an abundance of GPS/INS tagged 3D point cloud based datasets collected over large environments, e.g. the Oxford RobotCar <ref type="bibr" target="#b23">[19]</ref> and KITTI <ref type="bibr" target="#b10">[9]</ref> datasets etc. This naturally leads us into the question: "Can we design a deep learning framework that concurrently learns the 3D feature detector and descriptor from the GPS/INS tagged 3D point clouds?"</p><p>In view of the difficulty to get datasets of accurately labeled salient 3D features for training the deep networks, we propose a weakly supervised deep learning framework -the 3DFeat-Net to holistically learn a 3D feature detector and descriptor from GPS/INS tagged 3D point clouds. Specifically, our 3DFeat-Net is a Siamese architecture <ref type="bibr" target="#b4">[4]</ref> that learns to recognize whether two given 3D point clouds are taken from the same location. We leverage on the recently proposed PointNet <ref type="bibr" target="#b27">[23,</ref><ref type="bibr" target="#b29">24]</ref> to enable us to directly use the 3D point cloud as input to our network. The output of our 3DFeat-Net is a set of local descriptor vectors. The network is trained by minimizing a Triplet loss <ref type="bibr" target="#b35">[29]</ref> where the positive and "hardest" negative samples are chosen from the similarity measures between all pairs of descriptors <ref type="bibr" target="#b17">[14]</ref> from two input point clouds. Furthermore, we add an attention layer <ref type="bibr" target="#b24">[20]</ref> that learns importance weights that weigh the contribution of each input descriptor towards the Triplet loss. During inference, we use the output from the attention layer to determine the saliency likelihood of an input 3D point. Additionally, we take the output descriptor vector from our network as the descriptor for finding good 3D-3D correspondences. Experimental results from real-world datasets <ref type="bibr" target="#b23">[19,</ref><ref type="bibr" target="#b10">9,</ref><ref type="bibr" target="#b26">22]</ref> validates the feasibility of our 3DFeat-Net.</p><p>Our contributions in this paper can be summarized as follows:</p><p>? Propose a weakly supervised network that holistically learns a 3D feature detector and descriptor using only GPS/INS tagged 3D point clouds. ? Use an attention layer <ref type="bibr" target="#b24">[20]</ref> that allows our network to learn the saliency likelihood of an input 3D point. ? Create training and benchmark datasets from Oxford RobotCar dataset <ref type="bibr" target="#b23">[19]</ref>.</p><p>We have made our source code and dataset available online. <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Existing approaches of the local 3D feature detectors and descriptors are heavily influenced by the widely studied 2D local features methods <ref type="bibr" target="#b20">[17,</ref><ref type="bibr" target="#b2">2]</ref>, and can be broadly categorized into handcrafted <ref type="bibr" target="#b42">[35,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b31">26,</ref><ref type="bibr" target="#b30">25,</ref><ref type="bibr" target="#b16">13,</ref><ref type="bibr" target="#b37">30,</ref><ref type="bibr" target="#b34">28]</ref> and learning approaches <ref type="bibr" target="#b41">[34,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b18">15,</ref><ref type="bibr" target="#b33">27,</ref><ref type="bibr" target="#b6">6</ref>] -i.e. pre-and post-deep learning approaches.</p><p>Handcrafted 3D Features Several handcrafted 3D features are proposed before the popularity of deep learning. The design of these features are largely based on the domain knowledge of 3D point clouds by the researchers. The authors of <ref type="bibr" target="#b42">[35]</ref> and <ref type="bibr" target="#b5">[5]</ref> detects salient keypoints which have large variations in the principal direction <ref type="bibr" target="#b42">[35]</ref>, or unique curvatures <ref type="bibr" target="#b5">[5]</ref>. The similarity between keypoints can then be estimated using descriptors. PFH <ref type="bibr" target="#b31">[26]</ref> and FPFH <ref type="bibr" target="#b30">[25]</ref> consider pairwise combinations of surface normals to describe the curvature around a keypoint. Other 3D descriptors <ref type="bibr" target="#b16">[13,</ref><ref type="bibr" target="#b37">30]</ref> build histograms based on the number of points falling into each spatial bin around the keypoint. A comprehensive evaluation of the common handcrafted 3D detectors and descriptors can be found in <ref type="bibr" target="#b13">[11]</ref>. As we show in our evaluation, many of these handcrafted detectors and descriptors do not work well on real world point clouds, which can be noisy and low density.</p><p>Learned 2D Features Some recent works have applied deep learning to learn detectors and descriptors on images for 2D-2D correspondences. LIFT <ref type="bibr" target="#b39">[32]</ref> learns to distinguish between matching and non-matching pairs with a Siamese CNN, where the matching pairs are obtained from feature matches that survive the Structure from Motion (SfM) pipeline. TILDE <ref type="bibr" target="#b38">[31]</ref> learns to detect keypoints that can be reliably matched over different lighting conditions. These works rely on the matches provided by handcrafted 2D features, e.g. SIFT <ref type="bibr" target="#b20">[17]</ref>, but unfortunately handcrafted 3D features are less robust and do not provide as good a starting point to learn better features. On the other hand, a recently proposed work -DELF <ref type="bibr" target="#b24">[20]</ref> uses a weakly supervised framework to learn salient local 2D image features through an attention mechanism in a landmark classification task. This motivates our work in using an attention mechanism to identify good 3D local keypoints and descriptors for matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learned 3D Features</head><p>The increasing success and popularity of deep learning has recently inspired many learned 3D features. 3DMatch <ref type="bibr" target="#b41">[34]</ref> uses a 3D convolutional network to learn local descriptors for indoor RGB-D matching by training on matching and non-matching pairs. PPFNet <ref type="bibr" target="#b6">[6]</ref> operates on raw points, incorporating point pair features and global context to improve the descriptor representation. Other works such as CGF <ref type="bibr" target="#b18">[15]</ref> and LORAX <ref type="bibr" target="#b8">[8]</ref> utilize deep learning to reduce the dimension of their handcrafted descriptors. Despite the good performance of these works, none of them learns to detect keypoints. The descriptors are either computed on all or random sampled points. On the other hand, <ref type="bibr" target="#b33">[27]</ref> learns to detect keypoints that give good matching performance with handcrafted SHOT <ref type="bibr" target="#b34">[28]</ref> descriptors. This provides the intuition for our work, i.e. a good keypoint is one that gives a good descriptor performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Formulation</head><p>A point cloud P is represented as a set of N 3D points {x i |i = 1, ..., N }. Each point cloud P (m) is cropped to a ball with fixed radius R around its centroid c m . We assume the absolute pose of the point cloud is available during training, e.g. from GPS/INS, but is not sufficiently accurate to infer point-to-point correspondences. We define the distance between two point clouds d(m, n) as the Euclidean distance between their centroids, i.e. d(m, n) = c m ? c n 2 .</p><p>We train our network using a set of triplets containing an anchor, positive and negative point cloud {P (anc) , P (pos) , P (neg) }, similar to typical instance retrieval approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">10]</ref>. We define positive instances as point clouds with distance to the anchor below a threshold, d(anc, pos) &lt; ? p . Similarly, negative instances are point clouds far away from the anchor, i.e. d(anc, neg) &gt; ? n . The thresholds ? p and ? n are chosen such that positive and negative point clouds have large and small overlaps with the anchor point cloud respectively.</p><p>The objective of our network is to learn to find a set of correspondences</p><formula xml:id="formula_0">{(x (m) 1 , x (n) 1 ), (x (m) 2 , x (n) 2 ), ..., (x (m) L , x (n) L )|x (m) i ? P (m) , x (n) j</formula><p>? P (n) } between a subset of points in two point clouds P (m) and P (n) . Our network learning is weakly supervised in two ways. Firstly, only model level annotations in the form of relative poses of the point clouds are provided, and we do not explicitly specify which subset of points to choose for the 3D features. Secondly, the ground truth poses are not accurate enough to infer point-to-point correspondences.  <ref type="bibr" target="#b24">[20]</ref> are predicted by a detector network. A descriptor network then rotates the cluster C k to a canonical configuration using the predicted orientation ? k and computes a descriptor f k ? R d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Our 3DFeat-Net</head><p>We train our network with the triplet loss to minimize the difference between the anchor and positive point clouds, while maximizing the difference between anchor and negative point clouds. To allow the loss to take individual cluster descriptors into account, we use an alignment model <ref type="bibr" target="#b17">[14]</ref> to align each descriptor to its best match before aggregating the loss. Since not all sampled clusters have the same distinctiveness, the predicted attention w k from the detector is used to weigh the contribution of each cluster descriptor in the training loss. These attention weights are learned on arbitrarily sampled clusters during training, and later used to detect distinctive keypoints in the point cloud during inference.</p><p>Clustering The first stage of the network is to sample clusters from the point cloud. To this end, we use the sample and grouping layers in PointNet++ <ref type="bibr" target="#b29">[24]</ref>. The sampling layer samples a set of points {x i1 , x i2 , . . . , x i K } from an input point cloud P . The coordinates of these sampled points and the point cloud are then passed into the grouping layer that outputs K clusters of points. Each cluster C k is a collection of points in a local region of a predefined radius r cluster around the sampled point x i k . These clusters are used as support regions to compute local descriptors, analogous to 2D image patches around detected keypoints in 2D feature matching frameworks. We use the iterative farthest point sampling scheme as in PointNet++ for sampling, but any form of sampling that can sufficiently cover the point cloud (e.g. Random Sphere Cover Set <ref type="bibr" target="#b8">[8]</ref>) is also suitable. Such sampling schemes increases the likelihood that each sampled cluster in an anchor point cloud has a nearby cluster in the positive point cloud.</p><p>Detector Each cluster C k sampled by the clustering step is passed to the detector network that predicts an orientation ? k and an attention score w k . The attention score w k is a positive number that indicates the saliency of the input cluster C k . This design is inspired by typical 2D feature detectors, e.g. SIFT <ref type="bibr" target="#b20">[17]</ref>. The predicted orientation is used to rotate the cluster to a canonical orientation, so that the final descriptor is invariant to the cluster's original orientation. We construct our detector network using point symmetric functions defined in PointNet <ref type="bibr" target="#b27">[23]</ref>, which is defined as</p><formula xml:id="formula_1">f ({x 1 , ..., x n }) = g(h(x 1 ), ..., h(x n )), where h(.)</formula><p>is a shared function that transforms each individual point x i , and g(.) is a symmetric function on all transformed elements. These functions are invariant to point ordering and generates fixed length features given arbitrary sized point clouds. We use a three fully connected layers (64-128-256 nodes) for the implementation of h(.). The symmetric function g(.) is implemented as a max-pooling followed by two fully connected layers (128-64 nodes), before branching into two 1-layer paths for orientation and attention predictions.</p><p>We only predict a single 1D rotation angle ? i , avoiding unnecessary equivariances to retain higher discriminating power. This is reasonable since point clouds are usually aligned to the gravity direction due to the sensor setup (e.g. a Velodyne Lidar mounted upright on a car); for other cases, the gravity vector obtained from an IMU can be used to rotate the point clouds into the upright orientation. Similar to <ref type="bibr" target="#b40">[33]</ref>, we do not regress the angle directly. Instead, we regress two separate values ? k1 and ? k2 that denote the sine and cosine of the angle. We use a 2 normalization layer to add a constraint of ? 2 k1 + ? 2 k2 = 1 to ensure valid sine and cosine values. The final angle can be computed as ? k = arctan2(? k1 , ? k2 ). For the attention weights w i 's, we use the softplus activation as suggested by <ref type="bibr" target="#b24">[20]</ref> to prevent the network from learning negative attention weights.</p><p>Descriptor Our descriptor network takes each cluster C k from the clustering layer and orientation ? k from the detector network as inputs, and generates a descriptor f k ? R d for each cluster. More specifically, ? k is first used to rotate cluster C k into a canonical configuration, before they are passed into another point symmetric function to generate the descriptor. In practice, we find it helpful to aggregate contextual information in the computation of the descriptor. Hence, after applying max-pooling to obtain a cluster feature vector, we concatenate this cluster feature vector with the individual point features to incorporate context. We then apply a single fully connected layer with d nodes before another max-pooling. Finally, we apply another fully connected layer and l 2 normalization to produce a final cluster descriptor f k ? R d for cluster C k . The addition of the contextual information improves the discriminating power of the descriptor.</p><p>Feature Alignment Triplet Loss The output from the descriptor network in the previous stage is a set of features f i for each cluster. We use the alignment objective introduced in <ref type="bibr" target="#b17">[14]</ref> to compare individual descriptors since the supervision is given as model-level annotations. Instead of the dot product similarity measure used in <ref type="bibr" target="#b17">[14]</ref>, we adopt the Euclidean distance measure which is more commonly used for comparing feature descriptors. Specifically, the distance between all pairs of descriptors between the two point clouds P (m) and P (n) with clusters C (m) and C (n) is given by:</p><formula xml:id="formula_2">D m,n = Ci?C (m) w i ? min Cj ?C (n) f i ? f j 2 , w i = w i j?P (m) w j ,<label>(1)</label></formula><p>where w i is the normalized attention weight. Under this formulation, every descriptor from the first point cloud aligns to its closest descriptor in the second one. Intuitively, in a matching point cloud pair, clusters in the first point cloud should have a similar cluster in the second point cloud. For non-matching pairs, the above distance simply aligns a descriptor to one which is most similar to itself, i.e. its hardest negative. This consideration of the hardest negative descriptor in the non-matching image provides the advantage that no explicit mining for hard negatives is required. Our model trains well with randomly sampled negative point clouds. We formulate the triplet loss for each training tuple {P (anc) , P (pos) , P (neg) } as:</p><formula xml:id="formula_3">L triplet = [D anc,pos ? D anc,neg + ?] + ,<label>(2)</label></formula><p>where [z] + = max(z, 0) denotes the hinge loss, and ? denotes the margin which is enforced between the positive and negative pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Inference Pipeline</head><p>The keypoints and descriptors are computed in two separate stages during inference. In the first stage, the attention scores of all points in a given point cloud are computed. We apply non-maximal suppression over a fixed radius r nms around each point, and keep the remaining M points with the highest attention weights. Furthermore, points with low attention are filtered out by rejecting those with attention w i &lt; ? * max(w 1 , w 2 , ..., w N ). The remaining points are our detected keypoints. In the next stage, the descriptor network computes the descriptors only for these keypoints. The separation of the inference into two detector-descriptor stages is computationally and memory efficient since only the detector network needs to process all the points while the descriptor network processes only the clusters that correspond to the selected keypoints. As a result, our network can scale up to larger point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluations and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Benchmark Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Oxford RobotCar</head><p>We use the open-source Oxford RobotCar dataset <ref type="bibr" target="#b23">[19]</ref> for training and evaluation. This dataset consists of a large number of traversals over the same route in central Oxford, UK, at different times over a year. The push-broom 2D scans are accumulated into 3D point clouds using the associated GPS/INS poses. For each traversal, we create a 3D point cloud with 30m radius for every 10m interval whenever good GPS/INS poses are available. Each point cloud is then downsampled using a VoxelGrid filter with a grid size of 0.2m. We split the data into two disjoint sets for training and testing. The first 35 traversals are used for training and the last 5 traversals are used for testing. We obtain a total of 21,875 training and initially 828 testing point cloud sets. We make use of the pairwise relative poses computed from the GPS/INS poses as ground truth to evaluate the performance on the test set. However, the GPS/INS poses may contain errors in the order of several meters. To improve the fidelity of our test set, we refine these poses using ICP <ref type="bibr" target="#b3">[3]</ref>. We set one of the test traversals as reference, and register all test point clouds within 10m to their respective point clouds in the reference. We retain matches with an estimated Root Mean Square Error (RMSE) of &lt; 0.5m, and perform manual visual inspection to filter out bad matches. We get 794/828 good point clouds that give us 3426 pairwise relative poses for testing. Lastly, we randomly rotate each point cloud around the vertical axis to evaluate rotational equivariances and randomly downsample each test point cloud to 16,384 points. KITTI Dataset We evaluate the performance of our trained network on the 11 training sequences of the KITTI Odometry dataset <ref type="bibr" target="#b10">[9]</ref> to understand how well our trained detector and descriptor generalizes to point clouds captured in a different city using a different sensor. The KITTI dataset contains point clouds captured on a Velodyne-64 3D Lidar scaner in Karlsruhe, Germany. We sample the Lidar scans at 10m intervals to obtain 2369 point clouds, and downsample them using a VoxelGrid filter with a grid size of 0.2m. We consider the 2831 point cloud pairs that are captured within 10m range of each other. We use the provided GPS/INS pose as ground truth.</p><p>ETH Dataset Our network is also evaluated on the "challenging dataset for point cloud registration algorithms" <ref type="bibr" target="#b26">[22]</ref>. This dataset is captured on a ground Lidar scanner, and contains largely unstructured vegetation unlike the previous two datasets. Following <ref type="bibr" target="#b8">[8]</ref>, we accumulate point clouds captured in one season of the Gazebo and Wood scenes to build a global point cloud, and register local point clouds captured in the other season to it. We take the liberty to build the global point cloud for both scenes using the summer data since <ref type="bibr" target="#b8">[8]</ref> did not state the season that was used. During pre-procesing, we downsample the point clouds using a VoxelGrid filter of 0.1m grid size. We choose a finer resolution because of the finer features in the vegetation in this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Setup</head><p>We train using a batch size of 6 triplets, and use the ADAM optimizer with a constant learning rate of 1e-5. We use points within a radius of R = 20m from the centroid of the point cloud for training, and sample K = 512 clusters with a radius of r cluster = 2.0m. The thresholds for defining positive and negative point clouds are set to ? p = 5m and ? n = 50m. We randomly downsample each point cloud to 4096 points on the fly during training. We found that training with this random input dropout leads to better generalization behavior as also observed by <ref type="bibr" target="#b29">[24]</ref>. We apply the following data augmentations during training time: random jitter to the individual points, random shifts and small rotations. Random rotations are also applied around the z-axis, i.e. upright axis in order to learn rotation equivariance. Note that our training data is already oriented in the upright direction using its GPS/INS pose. Our network is end-to-end differentiable, but in practice, we find it sometimes hard to train. Hence, we train in two phases to improve stability. We first pretrain the network without the detector for 2 epochs, i.e. the clusters are fed directly into the descriptor network without rotation, and all clusters have equal attention weights. During this phase, we apply all data augmentations except for large 1D rotations. We use these learned weights to initialize the descriptor in the second phase, where we train the entire network and apply all the above data augmentation. Training took 34 hours on a Nvidia Geforce GTX1080Ti.</p><p>For inference, we use the following parameters for both the Oxford and KITTI odometry datasets: r nms = 0.5m, ? = 0.01. For the ETH dataset, we use r nms = 0.3m, ? = 0.005 to boost the number of detected keypoints in the semi-structured environments. We limit the number of keypoints M to 1024 for all cases, except for the global model in the ETH dataset which we use 2048 due to its larger size. Inference took around 0.8s for a point cloud with 16,384 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Baseline Algorithms</head><p>We compare our algorithm with three commonly used handcrafted 3D feature descriptors: Fast Point Feature Histograms (FPFH) <ref type="bibr" target="#b30">[25]</ref> (33 dimensions), Spin-Image (SI) <ref type="bibr" target="#b16">[13]</ref> (153 dimensions), and Unique Shape Context (USC) <ref type="bibr" target="#b37">[30]</ref> (1980 dimensions). We use the implementation provided in the Point Cloud Library (PCL) <ref type="bibr" target="#b14">[12]</ref> for all the handcrafted descriptors. In addition, we include two recent learned 3D descriptors: Compact Geometric Features (CGF) <ref type="bibr" target="#b18">[15]</ref> (32 dimensions) and 3DMatch <ref type="bibr" target="#b41">[34]</ref> (512 dimensions) in our comparisons. Note that our comparisons are done using their provided weights that were pretrained on indoor datasets. This is because we are unable to train CGF and 3DMatch on the weakly supervised Oxford dataset as these networks require strong supervision. We also train a modified PointNet++ (PN++) <ref type="bibr" target="#b29">[24]</ref> in a weakly supervised manner on a retrieval task, which we use as a baseline to show the importance of descriptor alignment in learning local descriptors. We modify PointNet++ as follows: the first set abstraction layer is replaced by our detection and description network. The second and third set abstraction layers remain unchanged. We replace the subsequent fully connected layers with fc1024-dropout-fc512 layers. During inference, we extract descriptors as the output from the first set abstraction layer. We tuned the parameters for all descriptors to the best of our ability, except in Section 5.4, where we used the same cluster radii for all descriptors to ensure all descriptors "see" the same information.</p><p>Since none of the above baseline feature descriptors comes with a feature detector, we use the handcrafted detector from Intrinsic Shape Signatures (ISS) <ref type="bibr" target="#b42">[35]</ref> implemented in PCL. Following <ref type="bibr" target="#b41">[34]</ref>, we also show the performance on randomly sampled keypoints for 3DMatch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Descriptor Matching</head><p>We first evaluate the ability of the descriptors to distinguish between different clusters using the procedure in <ref type="bibr" target="#b41">[34]</ref>. We extract each matching cluster at randomly selected locations from two matching model pairs. Non-matching clusters are extracted from two random point clouds at locations that are at least 20m apart. We extract 30,000 pairs of 3D clusters, equally split between matches and non-matches. As in <ref type="bibr" target="#b41">[34]</ref>, our evaluation metric is the false-positive rate at 95% recall. To ensure all descriptors have access to similar amounts of information, we fixed the cluster radius r cluster to 2.0m for all descriptors in this experiment. <ref type="figure" target="#fig_3">Fig. 3</ref> shows the performance of our descriptor at different dimensionality. We observe that there is diminishing returns above 32 dimensions, and will use a feature dimension of d = 32 for the rest of the experiments.    <ref type="table" target="#tab_1">Table 2</ref> compares our descriptor with the baseline algorithms. Our learned descriptor yields a lower error rate than all other descriptors despite having a similar or smaller dimension. It performs significantly better than the best handcrafted descriptor (FPFH) which uses explicitly computed normals. The other two handcrafted descriptors (USC and SI), as well as the learned CGF consider the number of points falling in each subregion around the keypoint and could not differentiate the sparse point cloud clusters. 3DMatch performed well in differentiating randomly sampled clusters, but requiring a larger feature dimension. Lastly, the modified PointNet++ network did not learn a good descriptor in the weakly supervised setting and gives significantly higher error than our descriptor despite having the same descriptor network structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Keypoint Detection and Feature Description</head><p>We follow the procedure in <ref type="bibr" target="#b18">[15]</ref> to evaluate the joint performance of keypoints and descriptors. For each keypoint descriptor in the first model of the pair, we find its nearest neighbor in the second model via exhaustive search. We then compute the distance between this nearest neighbor and its ground truth location. We obtain a precision plot, as shown in <ref type="figure">Fig. 4</ref>, by varying the distance threshold x for considering a match as correct and plotting the proportion of correct matches. We evaluate on all the baseline descriptors, and also show the performance of our descriptor with the ISS detector, random sampling of points (RS), and points obtained using Random Sphere Cover Set (RSCS) <ref type="bibr" target="#b8">[8]</ref>. We tuned the cluster sizes for all baseline descriptors, as many of them require larger cluster sizes to work well. Nevertheless, our keypoint detector and descriptor combination still yields the best precision for all distance thresholds, and obtains a precision of 15.6% at 1m. We also note that our descriptor underperforms when used with the two random sampling methods or the generic ISS detector, indicating the importance of a dedicated feature detector. <ref type="figure">Fig. 5</ref> shows the attention weights computed by our network, as well as the retrieved keypoints. We also show the keypoints obtained using ISS for comparison. Our network learns to give higher attentions to lower regions of the walls (near the ground), and mostly ignores the ground and the cars (which are transient and not useful for matching). In comparison, ISS detects many non-distinctive keypoints on the ground and cars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of Keypoint Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Geometric Registration</head><p>We test our keypoint detection and description algorithm on the geometric registration problem. We perform nearest neighbor matching on the computed keypoints and descriptors, and use RANSAC on these nearest neighbor matches to estimate a rigid transformation between the two point clouds. The number of RANSAC iterations is automatically adjusted based on 99% confidence but is limited to 10,000. No subsequent refinement, e.g. using ICP is performed. We evaluate the estimated pose against its ground truth using the Relative Translational Error (RTE) and Relative Rotation Error (RRE) as in <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b22">18]</ref>. We consider registration successful when the RTE and RRE are both below a predefined threshold of 2m and 5 ? , and report the average RTE and RRE values for successful cases. We also report the average number of RANSAC iterations. Performance on Oxford RobotCar <ref type="table">Table 3</ref> shows the performance on the Oxford dataset. We observe the following: (1) using a keypoint detector instead of random sampling improves geometric registration performance, even for 3DMatch which is designed for random keypoints, (2) our learned descriptor gives good accuracy even when used with handcrafted descriptors or random sampling, suggesting that it generalizes well to generic point clusters, (3) our detector and descriptor combination gives the highest success rates and lowest errors. This highlights the importance of designing a keypoint detector and descriptor simultaneously, and the applicability of our approach to geometric registration. Some qualitative registration results can be found in <ref type="figure">Fig. 6(a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance on KITTI Dataset</head><p>We evaluate the generalization performance of our network on the KITTI odometry dataset by comparing the geometric registration performance against ISS + FPFH in <ref type="table">Table 4</ref>. We use the same parameters as the Oxford dataset for all algorithms, and did not fine-tune our network in any way. Nevertheless, our 3DFeat-Net outperforms all other algorithms in most measures. It underperforms CGF slightly in terms of RTE, but has a significantly higher success rate and requires far fewer RANSAC iterations. We show some matching results on the KITTI dataset in <ref type="figure">Fig. 6(b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance on ETH Dataset</head><p>We compare our performance against LO-RAX <ref type="bibr" target="#b8">[8]</ref>, which evaluates on 9 models from the Gazebo dataset and 3 models from the Wood dataset. We show our performance on the best performing point clouds for each algorithm in <ref type="table" target="#tab_3">Table 5</ref> since it is not explicitly stated in <ref type="bibr" target="#b8">[8]</ref> which datasets were used. Note that success rate in this experiment refers to a RTE of below 1m to be consistent to <ref type="bibr" target="#b8">[8]</ref>. We also report the success rate over the entire dataset. Detailed results on the entire dataset can be found in the supplementary material. LORAX considers 3 best descriptor matches for each keypoint. These matches are used to compute multiple pose hypotheses which are then refined using ICP for robustness. For our algorithm and baselines, we only consider the best match, compute a single pose hypothesis and do not perform any refinement of the pose. Despite this, our approach outperforms <ref type="bibr" target="#b8">[8]</ref> and most baseline algorithms. It only underperforms USC which uses a much larger dimension (1980D). . Qualitative registration results, using our approach (left) and ISS + FPFH (right). We only show a random subset of matches retained after RANSAC, and exclude the ground in (c) for clarity. We also show the results using ISS + FPFH. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed the 3DFeat-Net model that learns the detection and description of keypoints in Lidar point clouds in a weakly supervised fashion by making use of a triplet loss that takes into account individual descriptor similarities and the saliency of input 3D points. Experimental results showed our learned detector and descriptor compares favorably against previous handcrafted and learned ones on several outdoor gravity aligned datasets. However, we note that our network is unable to train well on overly noisy point clouds, and the use of PointNet limits the max size of the input point cloud. We also do not extract a fully rotational invariant descriptor. We leave these as future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Left: 2D images from the Oxford dataset at different times of the day (top) or seasons of the year (bottom) give mostly wrong matches even after RANSAC. Right: 3D point cloud of the same scene remains largely the same and can be matched easily.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>4. 1</head><label>1</label><figDesc>Network ArchitectureFig. 2 shows the three-branch Siamese architecture of our 3DFeat-Net. Each branch takes an entire point cloud P as input. Point clusters {C 1 , ..., C K } are sampled from the point cloud in a clustering layer. For each cluster C k , an orientation ? k and attention w k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>(a) Network architecture of our 3DFeat-Net. The three-branch Siamese architecture uses a training tuple of an anchor, positive and negative point cloud to compute a triplet loss. (b) and (c) show detailed view of the detector and descriptor networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Matching error at 95% recall for different descriptor dimensionality</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Precision on the Oxford Robotcar dataset using different keypoint and descriptor combination. (Kpt = keypoints, Desc = descriptors) Left: Attention using proposed method (brighter colors indicate higher attention). Middle: Our Keypoints (red dots). Right: ISS keypoints (red dots). Colors in middle and right images indicate different heights above ground.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 (Fig. 6</head><label>66</label><figDesc>c) shows an example of a successful matching by our approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Breakdown of Oxford RobotCar Data for Training and Testing</figDesc><table><row><cell></cell><cell cols="3"># Traversals # Point clouds # Matched pairs</cell></row><row><cell>Train</cell><cell>35</cell><cell>21875</cell><cell>-</cell></row><row><cell>Test</cell><cell>5</cell><cell>828</cell><cell>-</cell></row><row><cell>Test (after registration)</cell><cell>5</cell><cell>794</cell><cell>3426</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Descriptor matching task. Error at 95% recall. Lower is better.</figDesc><table><row><cell cols="7">SI[13] FPFH[25] USC[30] CGF[15] 3DMatch[34] PN++[24] Ours</cell></row><row><cell>Error (%) 68.51</cell><cell>54.13</cell><cell>91.59</cell><cell>69.77</cell><cell>38.49</cell><cell>50.57</cell><cell>36.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Performance on the Oxford RobotCar dataset. Performance on the KITTI odometry dataset.</figDesc><table><row><cell>Method</cell><cell>RTE (m)</cell><cell>RRE ( ? )</cell><cell cols="2">Success Rate Avg # iter</cell></row><row><cell>ISS [35] + FPFH [25]</cell><cell>0.396 ? 0.290</cell><cell>1.60 ? 1.02</cell><cell>92.32%</cell><cell>7171</cell></row><row><cell>ISS [35] + SI [13]</cell><cell>0.415 ? 0.309</cell><cell>1.61 ? 1.12</cell><cell>87.45 %</cell><cell>9888</cell></row><row><cell>ISS [35] + USC [30]</cell><cell>0.324 ? 0.270</cell><cell>1.22 ? 0.95</cell><cell>94.02%</cell><cell>7084</cell></row><row><cell>ISS [35] + CGF [15]</cell><cell>0.431 ? 0.320</cell><cell>1.62 ? 1.10</cell><cell>87.36%</cell><cell>9628</cell></row><row><cell>RS + 3DMatch [34]</cell><cell>0.616 ? 0.407</cell><cell>2.02 ? 1.17</cell><cell>54.64%</cell><cell>9848</cell></row><row><cell>ISS [35] + 3DMatch [34]</cell><cell>0.494 ? 0.366</cell><cell>1.78 ? 1.21</cell><cell>69.06%</cell><cell>9131</cell></row><row><cell>ISS [35] + PN++ [24]</cell><cell>0.511 ? 0.391</cell><cell>1.88 ? 1.20</cell><cell>48.86%</cell><cell>9904</cell></row><row><cell>RS + Our Desc</cell><cell>0.435 ? 0.305</cell><cell>1.64 ? 1.04</cell><cell>90.28%</cell><cell>9941</cell></row><row><cell>RSCS [8] + Our Desc</cell><cell>0.386 ? 0.292</cell><cell>1.46 ? 1.01</cell><cell>92.64%</cell><cell>9913</cell></row><row><cell>ISS [35] + Our Desc</cell><cell>0.314 ? 0.262</cell><cell>1.08 ? 0.83</cell><cell>97.66%</cell><cell>7127</cell></row><row><cell>Our Kpt + Desc</cell><cell cols="2">0.300 ? 0.255 1.07 ? 0.85</cell><cell>98.10%</cell><cell>2940</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Performance on the ETH dataset, the results of FPFH and LORAX are taken from<ref type="bibr" target="#b8">[8]</ref>. The last column indicates the success rate over the entire dataset.</figDesc><table><row><cell>Method</cell><cell>RTE (m)</cell><cell>RRE ( ? )</cell><cell>Success Rate</cell><cell>(All)</cell></row><row><cell>FPFH [25]</cell><cell>0.44 ? 0.2</cell><cell>12.2 ? 4.8</cell><cell>67%</cell><cell>-</cell></row><row><cell>LORAX [8]</cell><cell>0.42 ? 0.27</cell><cell>2.5 ? 1.2</cell><cell>83%</cell><cell>-</cell></row><row><cell>ISS [35] + SI [13]</cell><cell>0.176 ? 0.083</cell><cell>1.97 ? 0.74</cell><cell>100%</cell><cell>93.7%</cell></row><row><cell>ISS [35] + USC [30]</cell><cell>0.130 ? 0.056</cell><cell>1.52 ? 0.30</cell><cell>100%</cell><cell>100%</cell></row><row><cell>ISS [35] + CGF [15]</cell><cell>0.157 ? 0.066</cell><cell>1.47 ? 0.60</cell><cell>100%</cell><cell>92.1%</cell></row><row><cell>RS + 3DMatch [34]</cell><cell>0.292 ? 0.199</cell><cell>4.71 ? 3.16</cell><cell>91.7%</cell><cell>33.3%</cell></row><row><cell cols="2">ISS [35] + 3DMatch [34] 0.401 ? 0.222</cell><cell>5.32 ? 3.25</cell><cell>100%</cell><cell>33.3%</cell></row><row><cell>Our Kpt + Desc</cell><cell>0.156 ? 0.112</cell><cell>1.56 ? 0.66</cell><cell>100%</cell><cell>95.2%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/yewzijian/3DFeatNet</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">3DFeat-Net</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">3DFeat-Net</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Netvlad: Cnn architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5297" to="5307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/CVPR.2016.572</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.572" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Surf: Speeded up robust features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="DOI">10.1007/11744023_32</idno>
		<ptr target="https://doi.org/10.1007/1174402332" />
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<editor>Leonardis, A., Bischof, H., Pinz, A.</editor>
		<meeting><address><addrLine>Berlin Heidelberg; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="404" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A method for registration of 3-d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Besl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Mckay</surname></persName>
		</author>
		<idno type="DOI">10.1109/34.121791</idno>
		<ptr target="https://doi.org/10.1109/34.121791" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="239" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Signature verification using a &quot;siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>S?ckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3d free-form object recognition in range images using local surface patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bhanu</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICPR.2004.1334487</idno>
		<ptr target="https://doi.org/10.1109/ICPR.2004.1334487" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="136" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ppfnet: Global context aware local features for robust 3d point matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dub?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dugas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stumm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07720</idno>
		<title level="m">Segmatch: Segment based loop-closure for 3d point clouds</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3d point cloud registration for localization using a deep neural network auto-encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Elbaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Avraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/CVPR.2017.265</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.265" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/CVPR.2012.6248074</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2012.6248074" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep image retrieval: Learning global representations for image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almaz?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46466-4_15</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46466-415" />
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<editor>Leibe, B., Matas, J., Sebe, N., Welling, M.</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="241" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Comparison of 3d interest point detectors and descriptors for point cloud fusion. ISPRS Annals of the Photogrammetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>H?nsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hellwich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing and Spatial Information Sciences</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">57</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Registration with the point cloud library: A modular framework for aligning in 3-d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Holz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Ichim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics Automation Magazine</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="110" to="124" />
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/MRA.2015.2432331</idno>
		<ptr target="https://doi.org/10.1109/MRA.2015.2432331" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Using spin images for efficient object recognition in cluttered 3d scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<idno type="DOI">10.1109/34.765655</idno>
		<ptr target="https://doi.org/10.1109/34.765655" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="433" to="449" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298932</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2015.7298932" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning compact geometric features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.26</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.26" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="153" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Minimal solutions for the multicamera pose estimation problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fraundorfer</surname></persName>
		</author>
		<idno type="DOI">10.1177/0278364914557969</idno>
		<ptr target="https://doi.org/10.1177/0278364914557969" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research (IJRR). pp</title>
		<imprint>
			<biblScope unit="page" from="837" to="848" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<idno type="DOI">10.1023/B:VISI.0000029664.99615.94</idno>
		<ptr target="https://doi.org/10.1023/B:VISI.0000029664.99615.94" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast and accurate registration of structured point clouds with small overlaps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW.2016.86</idno>
		<ptr target="https://doi.org/10.1109/CVPRW.2016.86" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="643" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">1 year, 1000km: The oxford robotcar dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pascoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Linegar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
		<idno type="DOI">10.1177/0278364916679498</idno>
		<ptr target="https://doi.org/10.1177/0278364916679498" />
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research (IJRR)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Large-scale image retrieval with attentive deep local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.374</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.374" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3476" to="3485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reliable object recognition using sift features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Pavel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Feng</surname></persName>
		</author>
		<idno type="DOI">10.1109/MMSP.2009.5293282</idno>
		<ptr target="https://doi.org/10.1109/MMSP.2009.5293282" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Multimedia Signal Processing (MMSP)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Challenging data sets for point cloud registration algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Colas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research (IJRR)</title>
		<imprint>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1705" to="1711" />
			<date type="published" when="2012-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="77" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/CVPR.2017.16</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.16" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5105" to="5114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast point feature histograms (fpfh) for 3d registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
		<idno type="DOI">10.1109/ROBOT.2009.5152473</idno>
		<ptr target="https://doi.org/10.1109/ROBOT.2009.5152473" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="3212" to="3217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Aligning point cloud views using persistent feature histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3384" to="3391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/IROS.2008.4650967</idno>
		<ptr target="https://doi.org/10.1109/IROS.2008.4650967" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning a descriptor-specific 3d keypoint detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Spezialetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Stefano</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.267</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.267" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2318" to="2326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Shot: Unique signatures of histograms for surface and texture description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Di Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="page" from="251" to="264" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/CVPR.2015.7298682</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2015.7298682" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unique shape context for 3d data description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Di Stefano</surname></persName>
		</author>
		<idno type="DOI">10.1145/1877808.1877821</idno>
		<idno>3DOR &apos;10</idno>
		<ptr target="https://doi.org/10.1145/1877808.1877821" />
	</analytic>
	<monogr>
		<title level="m">ACM Workshop on 3D Object Retrieval</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="57" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">TILDE: A Temporally Invariant Learned DEtector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Verdie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7299165</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2015.7299165" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5279" to="5288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Lift: Learned invariant feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46466-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46466-428" />
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<editor>Leibe, B., Matas, J., Sebe, N., Welling, M.</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="467" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to assign orientations to feature points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Verdie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.19</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.19" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="107" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">3dmatch: Learning local geometric descriptors from rgb-d reconstructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.29</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.29" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="199" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Intrinsic shape signatures: A shape descriptor for 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCVW.2009.5457637</idno>
		<ptr target="https://doi.org/10.1109/ICCVW.2009.5457637" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops, (ICCVW)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
