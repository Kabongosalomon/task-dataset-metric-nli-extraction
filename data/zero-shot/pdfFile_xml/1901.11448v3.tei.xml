<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feature-Critic Networks for Heterogeneous Domain Generalisation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiying</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
						</author>
						<title level="a" type="main">Feature-Critic Networks for Heterogeneous Domain Generalisation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The well known domain shift issue causes model performance to degrade when deployed to a new target domain with different statistics to training. Domain adaptation techniques alleviate this, but need some instances from the target domain to drive adaptation. Domain generalisation is the recently topical problem of learning a model that generalises to unseen domains out of the box, and various approaches aim to train a domaininvariant feature extractor, typically by adding some manually designed losses. In this work, we propose a learning to learn approach, where the auxiliary loss that helps generalisation is itself learned. Beyond conventional domain generalisation, we consider a more challenging setting of heterogeneous domain generalisation, where the unseen domains do not share label space with the seen ones, and the goal is to train a feature representation that is useful off-the-shelf for novel data and novel categories. Experimental evaluation demonstrates that our method outperforms state-of-the-art solutions in both settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A shift in data statistics between training and testing is often unavoidable in real-world applications, and leads to a significant negative impact on the performance of machine learning models in practice. This motivates research into methods to ameliorate the impact of domain shift, including Domain Adaption (DA) <ref type="bibr" target="#b1">(Bousmalis et al., 2016;</ref><ref type="bibr" target="#b6">Ganin &amp; Lempitsky, 2015;</ref><ref type="bibr" target="#b17">Long et al., 2015;</ref> and Domain Generalisation (DG) <ref type="bibr" target="#b20">(Muandet et al., 2013;</ref><ref type="bibr" target="#b8">Ghifary et al., 2015;</ref><ref type="bibr" target="#b13">Li et al., 2018a;</ref><ref type="bibr" target="#b32">Shankar et al., 2018)</ref>.</p><p>Proceedings of the 36 th International Conference on Machine Learning, <ref type="bibr">Long Beach, California, PMLR 97, 2019</ref>. Copyright 2019 by the author(s).</p><p>Unsupervised Domain Adaptation (UDA) <ref type="bibr" target="#b18">(Long et al., 2016;</ref><ref type="bibr" target="#b30">Saito et al., 2017;</ref><ref type="bibr" target="#b33">Shu et al., 2018)</ref> methods operate in the setting where we can access unlabelled testing (target) domain data during training to drive model adaptation and compensate for the domain shift. Domain Generalisation addresses the harder setting, where a model trained on a set of source domains should perform well on a novel target domain with different data statistics, without requiring any access to target domain data during training. That is, the model should be robust enough out-of-the-box to perform well in a new domain, without further parameter updates. Both DA and DG methods almost always assume the label space is consistent across both source and target domains.</p><p>In the case of disjoint label spaces between source and target domain, we term the domain generalisation problem as one of heterogeneous domain generalisation. In this case a feature representation trained on a source domain should generalise to supporting recognition of novel categories in a novel target domain. This problem setting is actually widely encountered. The central example is the ubiquitous computer vision pipeline where a CNN feature extractor pre-trained on ImageNet is re-used for diverse applications. If data, computation, and human expert time is available, the feature can be fine-tuned on the target problem. However, for many practical applications lacking one or more of these requirements, standard practice is to use an ImageNet CNN off-the-shelf as a fixed feature extractor, and train a shallow model such as SVM or KNN for the new problem <ref type="bibr" target="#b3">(Donahue et al., 2014;</ref><ref type="bibr" target="#b25">Razavian et al., 2014)</ref>. This pipeline is an example of the heterogeneous domain generalisation setting, in that a feature is being asked to generalise to supporting recognition of novel categories in data with novel statistics. The ImageNet pre-trained feature is strong enough to do a reasonable job of this already. However, given the ubiquity of this pipeline, providing an improved general purpose feature would be widely beneficial. In this paper we aim to do exactly this by presenting a novel method that explicitly trains a feature to prepare it for domain and label shift. We demonstrate this via performing heterogeneous DG on the Visual Decathalon benchmark <ref type="bibr" target="#b26">(Rebuffi et al., 2017)</ref>. This also provides the largest scale evaluation of DG to date.</p><p>We are inspired by recent meta-learning learning methods that perform episodic training <ref type="bibr" target="#b5">(Finn et al., 2017;</ref><ref type="bibr" target="#b34">Snell et al., 2017;</ref><ref type="bibr" target="#b24">Ravi &amp; Larochelle, 2017)</ref> to simulate the train/test process to improve few-shot learning. In this work, we propose to perform meta-learning to improve feature extractor training, and deliver a better model for both homogeneous and heterogeneous DG problems.</p><p>To realise our idea, we simulate training-to-testing domain shift by splitting our source domains into virtual training and testing (i.e., validation) domains. The source model is decomposed into feature extractor and task networks (i.e., a classifier network in our case). Crucially we then introduce a feature-critic network that learns to criticise the quality of the features produced by the feature network, specifically with regards to their robustness to the simulated domain shift. This feature-critic provides a learned auxiliary loss which provides an additional source of feedback to the feature network (besides the conventional supervised classification loss via the task network), and enables it to produce a more robust feature. The feature, task and critic networks are trained together end-to-end in a meta-learning pipeline. Our evaluation shows good performance in the conventional DG setting using Rotated MNIST <ref type="bibr" target="#b8">(Ghifary et al., 2015;</ref><ref type="bibr" target="#b19">Motiian et al., 2017)</ref> and PACS <ref type="bibr" target="#b12">(Li et al., 2017a)</ref> benchmarks, as well as the heterogeneous DG setting using the larger scale Visual Decathlon (VD) <ref type="bibr" target="#b26">(Rebuffi et al., 2017)</ref> benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Multi-Domain Learning (MDL) MDL addresses training a single model capable of solving multiple datasets (domains). If the data is relatively small and the domains are similar, this sharing can lead to improved performance compared to training a separate model per domain <ref type="bibr" target="#b40">(Yang &amp; Hospedales, 2015)</ref>. On the other hand, for diverse domains with large data, MDL may under-perform a single model per domain; but is nonetheless is of interest due to the simplicity of a single model and its better memory scalability compared to a separate model per domain <ref type="bibr" target="#b26">(Rebuffi et al., 2017;</ref>. We mention MDL here, because DG methods typically train on multiple source domains as per MDL -but furthermore aim to generalise to novel held out domains.</p><p>Domain Generalisation (DG) DG relates to domainadaptation in that we care about performance on a target domain, rather than source domains; however it considers the case where target domain samples are unavailable during training, so the model must generalise directly rather than adapt to the target domain. DG is of related to conventional generalisation: where models learned on a set of training instances generalise to novel testing instances, for example by regularisation. However it operates at a higher level, where we aim to help models trained on a set of training domains generalise to a novel testing domain.</p><p>Most existing DG approaches can be split into three categories: feature-based methods, classifier-based methods, and data augmentation methods. Feature-based methods: These aim to generate a domain-invariant representation. For example where the distance between the empirical distributions of the source and target examples is minimized <ref type="bibr" target="#b14">(Li et al., 2018b;</ref><ref type="bibr" target="#b20">Muandet et al., 2013;</ref><ref type="bibr" target="#b13">Li et al., 2018a)</ref>. Classifier-based methods: Some aim to enhance generalisation by fusing multiple sub-classifiers learned from source domains <ref type="bibr" target="#b4">(Duan et al., 2012;</ref><ref type="bibr" target="#b22">Niu et al., 2015a;</ref>, and others learn an improved classifier regulariser using source samples -notably the recently proposed MetaReg <ref type="bibr" target="#b0">(Balaji et al., 2018)</ref>. Data augmentation methods: CrossGrad <ref type="bibr" target="#b32">(Shankar et al., 2018)</ref> generates provides domain-guided perturbations of input instances, which are then used to train a more robust model. <ref type="bibr" target="#b38">Volpi et al. (2018)</ref> defines an adaptive data augmentation scheme by appending adversarial examples at each iteration. Our Feature-Critic approach falls into the feature-based category, but meta-learns a feature-critic network to train a robust shared feature extractor.</p><p>Few studies have considered the heterogeneous DG setting, where the domains do not share the same label space. We do not expect the classifier to generalise directly to the target domain (impossible due to the change in label space), but we do aim to improve the robustness of a source-domain trained feature in terms of its generalisation to successfully represent a novel problem. Most existing DG methods cannot be applied here. We show how to modify MetaReg <ref type="bibr" target="#b0">(Balaji et al., 2018)</ref> and Reptile <ref type="bibr" target="#b21">(Nichol et al., 2018)</ref> to address this DG setting. The most relevant benchmark is Visual Decathlon (VD) <ref type="bibr" target="#b26">(Rebuffi et al., 2017)</ref>. The VD benchmark was proposed to evaluate multi-domain and lifelong <ref type="bibr" target="#b29">(Rosenfeld &amp; Tsotsos, 2018)</ref> learning. We re-purpose it for DG evaluation. In this case a model trained on the six largest datasets in VD should produce a feature which provides a general and robust enough encoding to allow the four smaller datasets to be classified with a simple shallow classifier.</p><p>Meta-Learning Meta-learning (a.k.a. learning to learn, <ref type="bibr" target="#b31">(Schmidhuber et al., 1997;</ref><ref type="bibr" target="#b37">Thrun &amp; Pratt, 1998)</ref>) has received resurgence in interest recently with applications in few-shot learning <ref type="bibr" target="#b16">(Li et al., 2017b;</ref><ref type="bibr" target="#b34">Snell et al., 2017;</ref><ref type="bibr" target="#b36">Sung et al., 2018)</ref> and beyond <ref type="bibr" target="#b39">(Xu et al., 2018)</ref>. In few-shot metalearning, a common strategy is to simulate the few-shot learning scenario by randomly drawing few-shot train/test episodes from the full training set. We adapt this episodic training strategy by creating virtual training and testing splits of our source domains in each mini-batch.</p><p>A few methods have applied related episodic meta-learning strategies in DG <ref type="bibr" target="#b13">(Li et al., 2018a;</ref><ref type="bibr" target="#b0">Balaji et al., 2018)</ref>. MLDG <ref type="bibr" target="#b13">(Li et al., 2018a)</ref> defined a heuristic gradient descent update rule based on the gradients of the simulated training and testing domains. MetaReg <ref type="bibr" target="#b0">(Balaji et al., 2018)</ref> trains the weights of the classifier's regulariser so as to produce a more general classifier for a fixed feature extractor. In contrast, our Feature-Critic produces a more general feature extractor that can be used with any classifier. This is achieved by simultaneously learning an auxiliary loss function <ref type="bibr" target="#b9">(Gygli et al., 2017;</ref><ref type="bibr" target="#b35">Sung et al., 2017</ref>) (i.e., the critic network) that trains the feature extractor for improved domain invariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>We introduce the proposed method under the heterogeneous DG setting, but it is straightforwardly applicable to conventional (homogeneous) DG as a special case. Assuming that we have N domains (datasets) D = {D 1 , D 2 , . . . , D N }, and each domain contains a set of data-label pairs, i.e.,</p><formula xml:id="formula_0">D i = {X (i) , Y (i) }.</formula><p>We also have the training split of target (testing) domain, D N +1 = {X (N +1) , Y (N +1) }, but we can not access this for feature learning.</p><p>We assume a CNN model split into two parts: feature extractor f ? and classifier g ? . For heterogeneous DG, we have N classifiers, denoted g ?1 , g ?2 , . . . , g ? N , and a universal feature extractor f ? shared for all domains (assuming that images from all domains are resized to the same size). In the homogeneous DG, we only need a single classifier g ? that can be shared across all domains.</p><p>The proposed workflow is: (i) train a multi-domain model g ?f with D, (ii) take the shared feature extractor part f ? and use it as a fixed feature extractor for the target domain, (iii) extract features for target domain's train set D N +1 and train a SVM or KNN classifier, (iv) evaluate on the testing split of the target domain, denoted asD N +1 = {X (N +1) ,? (N +1) }. In the case of homogeneous DG, we can also take the shared classifier f ? and use the full model g ? ? f ? directly for the target domain. The goal is to perform the training in step (i) above so that the feature extractor f ? is robust enough to perform well on any target domain without fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">A Simple Baseline</head><p>A naive deep learning approach called aggregation (AGG) trains a single extractor to minimise the total cross-entropy</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Simulating Domain Shift in Training</head><formula xml:id="formula_1">Input: D = {D 1 , D 2 , . . . , D N } Output: ? begin</formula><p>while not converge or reach max steps do Randomly split D:</p><formula xml:id="formula_2">D trn ? D val = ? D trn ? D val = D for t ? [1, 2, . . . , T ] do Sample mini-batch D tr from each D j ? D trn</formula><p>Optimise feature extractor g ? on D tr using supervised and auxiliary loss h w .</p><formula xml:id="formula_3">Sample mini-batch D val from each D k ? D val Optimise auxiliary loss h ? on D val (CE) loss of all domains. min ?,?1,?2,...,? N Dj ?D dj ?Dj (CE) (g ?j (f ? (x (j) )), y (j) ) (1)</formula><p>Here D j is the jth domain and d j is a mini-batch of it.</p><p>Then we fix f ? and extract features for the training split of target domain D N +1 , i.e., f ? (X (N +1) ). With those extracted features, we can train a classifier using the pairs</p><formula xml:id="formula_4">{f ? (X (N +1) ), Y (N +1) }.</formula><p>Finally we test the model on the</p><formula xml:id="formula_5">testing split of target domainD N +1 = {X (N +1) ,? (N +1) }.</formula><p>This simple baseline surpasses many prior purpose designed DG methods as discussed in <ref type="bibr" target="#b12">Li et al. (2017a)</ref>. The key question is how to improve this naive approach, such that the trained feature extractor f ? produces more robust features that generalise better to unseen target domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Simulating Domain Shift in Training</head><p>Our high-level strategy simulates domain-shift during training as illustrated in Algo. 1 and <ref type="figure" target="#fig_0">Figure 1</ref>. We use the learned feature-critic loss to guide learning on the meta-training set D trn , and optimise the feature-critic itself on the metavalidation set D val . The key idea is that training f ? with h ? on D trn should improve its performance on D val .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Meta-Learning an Auxiliary loss</head><p>We aim to make the model training process behave well, i.e., after each update with mini-batches from D trn , performance should improve for mini-batches from D val . This would happen to some extent without any extra effort, but we aim to enforce it by introducing a learned auxiliary loss function, denoted (Aux) = h ? . The only requirements for h ? are (i) it outputs a non-negative scalar (since it is a loss) and (ii) its input depends on the feature extractor's parameter ?.</p><p>For now, we assume a suitable function h ? (i.e., (Aux) ) exists and discuss how to use it. We discuss design choices for h ? in Sec. 3.5. With the auxiliary loss, the objective function in Eq. 1 becomes,</p><formula xml:id="formula_6">min ?,?j s Dj ?Dtrn dj ?Dj (CE) (g ?j (f ? (x (j) )), y (j) ) + (Aux)<label>(2)</label></formula><p>Taking the gradient of Eq. 2 w.r.t. ? gives two terms: (i) cross-entropy loss and (ii) auxiliary loss (recall that (Aux) 's input must depend on ?, which means ? (Aux) ?? generates nonzero values).</p><p>Consider two alternative updates to ?, with and without the help of (Aux) . We have</p><formula xml:id="formula_7">? (OLD) = ? ? ? ? (CE) ?? and ? (NEW) = ? ? ? ? (CE) ?? ? ? ? (Aux) ?? .</formula><p>Here ? is the step size of ?. If the auxiliary loss (Aux) indeed does a good job of promoting domain invariance of ?, then the latter update ? (NEW) exploiting (Aux) should produce a more effective feature on mini-batches from the validation domain D val .</p><p>Thus we train the auxiliary loss (feature-critic network) to promote this. Specifically, we optimise the parameter ? of feature-critic network as follows:</p><formula xml:id="formula_8">max ? Dj ?Dval dj ?Dj tanh(?(? (NEW) , ? j , x (j) , y (j) ) ??(? (OLD) , ? j , x (j) , y (j) ))<label>(3)</label></formula><p>Here ? is a function that measures the validation domain performance (larger is better), and we discuss how to design it in Sec. 3.4. tanh is a utility function, which converts the reward (performance gain) to utility. It reflects the commonly accepted idea concept diminishing marginal utility, and links ? (NEW) with ? (OLD) . If tanh and the ? (OLD) term are excluded in Eq. 3, it would simply maximise the validation set performance with ? (NEW) . The reason Eq. 3 is better is that ?(? (OLD) , ? j , x (j) , y (j) ) serves as a baseline, making the value range -and thus the gradient -more stable. One can understand the role of tanh here as a smoother version of min/max-margin or a softer version of gradient clipping.</p><p>In summary, optimising the feature-critic h ? as Eq. 3 produces a loss (Aux) that encourages the base network to extract domain agnostic features when applied in Eq. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Measuring Validation Performance: Designing ?</head><p>To measure validation performance, ? can take up to four variables as input: feature extractor parameter ?, classifier parameter ?, data x, and label y, i.e., ?(?, ?, x, y). One simple choice is the negative classification loss, i.e.,</p><formula xml:id="formula_9">?(?, ?, x, y) = ? (CE) (g ? (f ? (x)), y).<label>(4)</label></formula><p>Inserting Eq. 4 into Eq. 3, we have</p><formula xml:id="formula_10">min ? Dj ?Dval dj ?Dj tanh( (CE) (g ?j (f (NEW) ? (x (j) )), y (j) ) ? (CE) (g ?j (f (OLD) ? (x (j) )), y (j) ))<label>(5)</label></formula><p>We can now introduce the meta-loss (Meta) on D val to abbreviate Eq. 5 as min ? (Meta) . Note that the design of ? should reflect the demands of the testing stage. Here we choose to use classification loss because we assume the model will be deployed for a classification task eventually. An alternative choice could be a metric-based loss if we knew the final task was about retrieval. We emphasise that it is not necessary for the objective function used for the training sets (e.g., <ref type="bibr">(CE)</ref> ) to match with the ? function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Designing Feature-Critic h ?</head><p>Finally, we design our feature-critic network h ? (i.e., <ref type="bibr">(Aux)</ref> ). Recall the requirements for such an auxiliary loss: (i) It outputs a non-negative scalar; (ii) Its input depends on ?. We note that MetaReg <ref type="bibr" target="#b0">(Balaji et al., 2018)</ref>, has a regularisation function that plays the similar role to h ? . MetaReg proposed the following form: h ? (?) = i ? i |? i |. However, this introduces the same number of parameters as ?. Doubling the number of model parameters in large modern CNNs is an expensive proposition that increases optimisation difficulty and overfitting risk.</p><p>Therefore rather than designing h ? to take ? directly, we propose a more efficient and effective way to enable h ? to promote the base network's generalisation. Specifically, the auxiliary loss operates on the extracted features f ? (x). Since our auxiliary generalisation-promoting loss operates on the feature representation produced by the base network, we denote it Feature-Critic.</p><p>Denote F = f ? (X (j) ) as the M ? H sized matrix stacking the H-dimensional features from M examples in a minibatch from the jth domain in the virtual training set D trn .</p><formula xml:id="formula_11">h ? := h ? (F ) = h ? (f ? (X (j) )).</formula><p>A key requirement of h ? is that it should be permutation invariant to the rows of F , i.e., it should not make a difference if we feed images indexed <ref type="bibr">[1,</ref><ref type="bibr">2,</ref><ref type="bibr">3]</ref> or [3, 2, 1]. Two available choices are:</p><p>(i) The set embedding <ref type="bibr" target="#b42">(Zaheer et al., 2017)</ref></p><formula xml:id="formula_12">, i.e., h ? (F ) = 1 M M i=1 MLP ? (F i )<label>(6)</label></formula><p>where F i denotes a row of F , and MLP is a multi-layer perceptron.</p><p>(ii) The flattened covariance matrix, i.e.,</p><p>h ? (F ) = MLP ? (Flatten(F T F ))</p><p>Algorithm 2 Full Algorithm Input: D = {D 1 , D 2 , . . . , D N }, ?, and ? Output: ? begin while not converge or reach max steps do Randomly split D:</p><formula xml:id="formula_14">D trn ? D val = ? D trn ? D val = D for t ? [1, 2, . . . , T ] do Meta-train: Sample mini-batch d trn from each D j ? D trn (CE) (d trn ) ? Eq. (1) //Supervised loss (Aux) (d trn ) ? Eqs. (6 or 7). //Auxiliary loss ? (OLD) = ? ? ?? ? (CE) ? (NEW) = ? (OLD) ? ?? ? (Aux) Meta-test: Sample mini-batch d val from each D k ? D val (Meta) (d val , ? (OLD) , ? (NEW) ) ? Eq. (5) //Meta-loss Meta-optimization: ? ? ? ? ?(? ? (CE) + ? ? (Aux) ) //Update feat. ? ? ? ? ?? ? (CE) //Update classifier ? ? ? ? ?? ? (Meta) //Update feature-critic</formula><p>Finally, the MLP's output should be a scalar and we place a softplus activation to make sure its output is non-negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Summary</head><p>Bringing all the components together, we have the full Algo. 2. To summarise, we randomly draw train/validation domains in each iteration and: Perform a putative feature extractor update on ? with and without the auxiliary Feature-Critic loss. Then generate a meta-loss based on whether or not the feature extractor update has improved performance on the validation set. Finally the feature extractor/classifier are updated using the supervised and auxiliary losses, and auxiliary loss itself is updated using the meta-loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our approach, first on the heterogeneous DG problem using the VD benchmark (Section 4.1), and then on the conventional homogeneous DG using Rotated MNIST and PACS (Section 4.2). Our demo code can be viewed on https://github.com/liyiying/ Feature_Critic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Heterogeneous DG experiments with VD</head><p>Dataset The Visual Decathlon dataset, initially proposed for multi-domain learning <ref type="bibr" target="#b26">(Rebuffi et al., 2017)</ref>, also pro-vides a large scale and rigorous benchmark for DG. VD contains ten diverse domains including handwritten characters, pedestrians, traffic signs, etc. The images have been pre-processed to 72 ? 72. To use this benchmark for DG, we aim to train a network on a subset of source domains, and produce a robust feature extractor that provides a good representation for classification in a disjoint subset of target domains. It should do so 'out-of-the-box', without further fine tuning. Specifically, we take the six larger datasets (CIFAR-100, Daimler Ped, GTSRB, Omniglot, SVHN and ImageNet) as source domains and hold out the four smaller datasets (Aircraft, D. Textures, VGG-Flowers and UCF101) as target domains. We use ImageNet pre-trained ResNet-18 <ref type="bibr" target="#b10">(He et al., 2016)</ref> as the base network for all competitors. For computational efficiency, we freeze the first four blocks of ResNet-18 and only update the remaining blocks, as well as the average pooling layer, during DG training. For all methods, the final feature is used to train SVM or KNN for the target task. All methods are evaluated by both average multi-class classification accuracy in the target domains, as well as the VD-Score metric <ref type="bibr" target="#b26">(Rebuffi et al., 2017</ref>) that rewards consistently high performance across all domains.</p><p>Competitors Few competitors can address heterogeneous DG. For these we consider AGG baseline (Eq 1), CrossGrad <ref type="bibr" target="#b32">(Shankar et al., 2018)</ref>, MetaReg <ref type="bibr" target="#b0">(Balaji et al., 2018)</ref>, and Reptile <ref type="bibr" target="#b21">(Nichol et al., 2018)</ref>. MetaReg is originally designed to produce a robust classifier given a fixed feature. We modify MetaReg to support the heterogeneous DG by (i) applying it on the feature extractor instead (as per our Feature-Critic), called MR; (ii) applying it on the final layer of feature extractor, called MR-FL. Meanwhile Reptile is designed for few-shot meta-learning. However after modifying it for multi-domain rather than multi-task meta-learning, we found it effective for heterogeneous DG.</p><p>Feature-Critic Settings We use the set embedding architecture for the critic network (Eq 6), as the covariance architecture requires too many parameters using high dimensional ResNet. During each iteration, we randomly choose four of the six source domains as meta-train, and the remaining two provide the meta-test (validation) domains. We train all components end-to-end using the AMSGrad <ref type="bibr" target="#b28">(Reddi et al., 2018)</ref> (batch-size/per meta-train domain=64, batch-size/per meta-test domain=32, lr=0.0005, weight decay=0.0001) for 30k iterations where the lr decayed in 5K, 12K, 15K, 20K iterations by a factor 5, 10, 50, 100, respectively. Similar to MetaReg <ref type="bibr" target="#b0">(Balaji et al., 2018)</ref>, after the parameters are trained via meta-learning, we fine-tune the network on all source datasets for the final 10k iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We first assume the full training split is available for each target domain. <ref type="table" target="#tab_0">Table 1</ref> shows that: (i) The original ImageNet feature transfers to novel tasks reasonably well, as observed by classic studies <ref type="bibr">(Yosinski et al.,</ref>  2014). (ii) Demonstrating the benefit of simply exploiting large datasets, the AGG baseline's feature, trained on more than 1.39 million images across the six domains, provides strong performance. However, while it has a higher average accuracy than the ImageNet feature, AGG's VD score is lower, reflecting its inconsistent performance. Thus obtaining consistently high scores from multi-domain training is non-trivial. Naively aggregating more diverse source domains into training can both help and hinder performance (for example, depending on if aggregated domains are particularly similar or dissimilar to a given target). Nevertheless, AGG sometimes outperforms prior purpose designed DG methods CrossGrad and MetaReg, with only Reptile producing a feature that outperforms AGG in both accuracy and VD-score metrics. (iii) Overall, our Feature-Critic (FC) method generally provides the best performance across domains and across both types of classifiers evaluated.</p><p>Although the above application scenario of heterogeneous DG is one where compute, memory or human resources rule out feature fine-tuning, another motivating scenario is where the target domain data is too sparse for effective fine-tuning. Thus we next investigate the situation if less target data is available. Specifically, we repeat the evaluation assuming that [10%, 25%, 50%, 100%] of the training split is available for SVM/KNN training. <ref type="table" target="#tab_1">Table 2</ref> reports target domain test accuracies under these settings. We can see that Feature-Critic provides a consistent improvement over the alternatives. Finally, we also consider a genuinely few-shot setting for the target domain. In this case we consider K = 3, 5, 8, 10 labelled examples per class in the target domain, and perform KNN recognition on their test sets. The results in <ref type="table" target="#tab_2">Table 3</ref> show that for simple similarity-based matching in a novel target domain, Feature-Critic also provides the best off-the-shelf feature representation.</p><p>In summary the Feature-Critic meta-training strategy produces a feature extractor that is generally useful for diverse target problems in an off-the-shelf feature + shallow classifier configuration. The results outperform both the standard ImageNet feature and the obvious Data Aggregation extension across a range of operating points in the target domain from the few to many-shot regime. This suggests that Feature-Critic trained feature extractors are of wide potential value in diverse applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Homogeneous DG experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">ROTATED MNIST</head><p>Dataset and Settings Rotated MNIST <ref type="bibr" target="#b8">(Ghifary et al., 2015)</ref> contains six domains with each corresponding to a degree of roll rotation in the classic MNIST dataset. The basic view (M0) is formed by randomly choosing 100 images each of ten classes from the original MNIST and we create 5 rotating domains from M0 with 15 ? rotation each in clockwise direction, denoted by M15, M30, M45, M60, and M75. Following the setting in <ref type="bibr" target="#b32">(Shankar et al., 2018)</ref>, we perform leave-one-domain-out experiments by picking one domain to hold out as the target. We compare AGG baseline, as well as CrossGrad and MetaReg. For a recognition network all competitors use the standard MNIST CNN with two conv and one FC layer as the feature network and another FC layer as the classifier. We note prior studies <ref type="bibr" target="#b8">(Ghifary et al., 2015;</ref><ref type="bibr" target="#b32">Shankar et al., 2018;</ref><ref type="bibr" target="#b2">Deshmukh et al., 2017)</ref> did not release specific selection of digits from within MNIST, so our results do not match the numbers in those papers exactly. However, we repeat all experiments 10 times and report the mean and standard deviation of recognition accuracy.</p><p>For Feature-Critic, we train using the AMSGrad optimizer (lr=0.001, weight decay=0.00005) for 5,000 iterations. For each iteration, one meta-train and one meta-test domain are chosen randomly from the five source domains. We also use this opportunity to compare the two variants of our loss function: Feature-Critic-MLP and Feature-Critic-Flatten.</p><p>Results It can be seen from <ref type="table">Table 4</ref> that AGG is again a strong baseline to beat. Over ten trials of 1000 digit samples, CrossGrad and MetaReg failed to match AGG, with only Reptile matching AGG's performance. Meanwhile, Feature-Critic performs well with both variants of the auxiliary loss network, with the set embedding (Eq. 6) performing slightly better than the covariance matrix embedding (Eq. 7).</p><p>To qualitatively visualise the results we perform PCA projections of the features in the target domain. <ref type="figure" target="#fig_1">Figure 2</ref> shows these projections, taking as an example the M15 domain as held out. Each dot denotes an image and the colour denotes its label. We can see that Feature-Critic (take MLP style as an example) feature extractor provides improved separability in the target domain compared to the AGG baseline.   Further Analysis <ref type="figure" target="#fig_2">Figure 3</ref> reports the loss curves of cross entropy loss, auxiliary loss, and meta-loss for Feature-Critic during training. The cross entropy loss converges to zero, as the network usually can fit the training data perfectly. The auxiliary loss fluctuates up and down for the early stage of training, and finally stabilises to a small value. Because the auxiliary loss function h ? itself is learned, its behaviour changes with its own learning process, which explains the fluctuations, esp. for the early stage. It is more interesting to see the pattern of meta-loss, which is the performance difference of feature extractor parameterised by ? (OLD) and that by ? (NEW) . If we select zero as a threshold, meta-loss has a clear pattern: "above zero" ? "below zero" ? "'being zero'. This pattern is expected because: (i) For the early stage, the auxiliary loss' parameters are randomly initialised, so it knows little about how to help generalise, thus the gradients produced by it are rather random and less likely to help. Thus ? (OLD) -based model outperforms ? (NEW) -based model. (ii) With the updating of ?, h ? improves and begins to make ? (NEW) better than ? <ref type="bibr">(OLD)</ref> . During this period, gradients produced by the auxiliary loss help the model learn to generalise. (iii) For the late stage, meta-loss goes towards zero, which indicates that h ? no longer helps (but it does not hurt either), as all of its knowledge has now been distilled into the feature extractor. The pattern of the three losses also demonstrates that, empirically, the whole algorithm converges, including the learned auxiliary loss.  evaluate both of these settings.</p><p>The ImageNet pre-trained AlexNet <ref type="bibr" target="#b11">(Krizhevsky et al., 2012)</ref> is used as the backbone network. Our competitors include: DICA <ref type="bibr" target="#b20">(Muandet et al., 2013)</ref>, D-MTAE <ref type="bibr" target="#b8">(Ghifary et al., 2015)</ref>, DSN <ref type="bibr" target="#b1">(Bousmalis et al., 2016)</ref>, TF-CNN <ref type="bibr" target="#b12">(Li et al., 2017a)</ref>, MLDG <ref type="bibr" target="#b13">(Li et al., 2018a)</ref>, DANN <ref type="bibr" target="#b7">(Ganin et al., 2016)</ref>, CIDDG <ref type="bibr" target="#b15">(Li et al., 2018c)</ref>, Reptile <ref type="bibr" target="#b21">(Nichol et al., 2018)</ref>, CrossGrad <ref type="bibr" target="#b32">(Shankar et al., 2018)</ref> and MetaReg <ref type="bibr" target="#b0">(Balaji et al., 2018)</ref>. We note that DANN is designed for domain adaptation, and Reptile for few-shot learning. We re-purpose them for DG. DANN, Reptile, CrossGrad, AGG, and MetaReg in <ref type="table">Table 5</ref> are our implementations. The other results are taken from <ref type="bibr" target="#b13">Li et al. (2018a)</ref>, <ref type="bibr" target="#b15">Li et al. (2018c)</ref> and <ref type="bibr" target="#b0">Balaji et al. (2018)</ref>. Among these, MetaReg makes a domain general classifier; MLDG aligns gradients to achieve a more robust optima; CrossGrad synthesises data for a new domain; DANN makes indistinguishable representations across source domains; CIDDG learns discriminative features to match the distributions across domains; Feature-Critic (FC) learns representations that generalise to new domains with a better objective since it trains a supervised loss and explicitly simulates domain shift.</p><p>Our Feature-Critic (set embedding variant) is trained with M-SGD optimizer (batch size/per meta-trian domain=32, batch size/per meta-test domain=16, lr=0.0005, weight de-cay=0.00005, momentum=0.9) for 45K iterations. At each iteration, we randomly choose two of the three source domains as meta-train and the remaining one as meta-test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The comparison with state-of-the-art methods on PACS dataset is shown in <ref type="table">Table 5</ref> and <ref type="table">Table 6</ref>. AGG provides a hard baseline to beat as usual. Nevertheless Feature-Critic performs comparably to the best performing state of the art alternative in both settings of this benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We addressed the domain generalisation problem with a particular focus on the heterogeneous case, by meta-learning a regulariser to help train a feature extractor to be domain invariant. The resulting feature extractor outperforms alternatives for general purpose use as a fixed downstream image encoding. Evaluated on Visual Decathlon -the largest DG evaluation thus far -this suggests that Feature-Critic trained feature extractors could be of wide potential value in diverse applications. Furthermore Feature-Critic also performs favourably compared to state-of-the-art in the homogeneous DG setting. In future work we will apply Feature-Critic to other problems including RL, and explore the impact on fine-tuning target problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of the Feature-Critic learning framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Rotated MNIST. PCA projections of target domain M15 features. Left: AGG. Right: Feature-Critic. Color: Digit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Loss curves of Feature-Critic on Rotated MNIST. Left to right: CE loss, auxiliary loss and meta-loss during DG training. Top: Feature-Critic-MLP. Bottom: Feature-Critic-Flatten.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Recognition accuracy (%) and VD scores on four held out target datasets in Visual Decathlon using ResNet-18 extractor.</figDesc><table><row><cell>Target</cell><cell cols="2">Im.N. PT CrossGrad</cell><cell cols="4">SVM Classifier MR MR-FL Reptile AGG</cell><cell>FC</cell><cell cols="2">Im.N. PT CrossGrad</cell><cell cols="4">KNN Classifier MR MR-FL Reptile AGG</cell><cell>FC</cell></row><row><cell>Aircraft</cell><cell>16.62</cell><cell>19.92</cell><cell>20.91</cell><cell>18.18</cell><cell>19.62</cell><cell cols="2">19.56 20.94</cell><cell>11.46</cell><cell>15.93</cell><cell>12.03</cell><cell>11.46</cell><cell>13.27</cell><cell cols="2">14.03 16.01</cell></row><row><cell>D. Textures</cell><cell>41.70</cell><cell>36.54</cell><cell>32.34</cell><cell>35.69</cell><cell>37.39</cell><cell cols="2">36.49 38.88</cell><cell>39.52</cell><cell>31.98</cell><cell>27.93</cell><cell>39.41</cell><cell>32.80</cell><cell cols="2">32.02 34.92</cell></row><row><cell>VGG-Flowers</cell><cell>51.57</cell><cell>57.84</cell><cell>35.49</cell><cell>53.04</cell><cell>58.26</cell><cell cols="2">58.04 58.53</cell><cell>41.08</cell><cell>48.00</cell><cell>23.63</cell><cell>39.51</cell><cell>45.80</cell><cell cols="2">45.98 47.04</cell></row><row><cell>UCF101</cell><cell>44.93</cell><cell>45.80</cell><cell>47.34</cell><cell>48.10</cell><cell>49.85</cell><cell cols="2">46.98 50.82</cell><cell>35.25</cell><cell>37.95</cell><cell>34.43</cell><cell>35.25</cell><cell>39.06</cell><cell cols="2">38.04 41.87</cell></row><row><cell>Ave.</cell><cell>38.71</cell><cell>40.03</cell><cell>34.02</cell><cell>38.75</cell><cell>41.28</cell><cell cols="2">40.27 42.29</cell><cell>31.83</cell><cell>33.47</cell><cell>24.51</cell><cell>31.41</cell><cell>32.73</cell><cell cols="2">32.52 34.96</cell></row><row><cell>VD-Score</cell><cell>308</cell><cell>280</cell><cell>269</cell><cell>296</cell><cell>324</cell><cell>290</cell><cell>344</cell><cell>215</cell><cell>188</cell><cell>144</cell><cell>215</cell><cell>201</cell><cell>189</cell><cell>236</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>VD recognition accuracy differences (%) against AGG with different proportions of training data available.</figDesc><table><row><cell>Data</cell><cell cols="6">SVM Classifier AGG Im.N. PT CrossGrad MR MR-FL Reptile</cell><cell>FC</cell><cell cols="6">KNN Classifier AGG Im.N. PT CrossGrad MR MR-FL Reptile</cell><cell>FC</cell></row><row><cell cols="2">10% 18.27</cell><cell>+1.74</cell><cell>+0.58</cell><cell>-2.38</cell><cell>-1.50</cell><cell cols="3">+1.83 +1.31 13.93</cell><cell>+1.15</cell><cell>-0.29</cell><cell>-3.08</cell><cell>+0.63</cell><cell>+1.01 +1.39</cell></row><row><cell cols="2">25% 30.10</cell><cell>-5.14</cell><cell>-2.48</cell><cell>-8.44</cell><cell>-2.48</cell><cell>-2.13</cell><cell cols="2">+0.87 23.80</cell><cell>-3.62</cell><cell>-0.06</cell><cell>-9.24</cell><cell>-3.78</cell><cell>-1.32</cell><cell>+1.48</cell></row><row><cell cols="2">50% 34.63</cell><cell>-3.55</cell><cell>-0.89</cell><cell>-7.04</cell><cell>-0.07</cell><cell cols="3">+0.37 +2.12 30.19</cell><cell>-5.02</cell><cell>-0.41</cell><cell>-8.87</cell><cell>-4.17</cell><cell>-0.15</cell><cell>+2.60</cell></row><row><cell cols="2">100% 40.27</cell><cell>-1.56</cell><cell>-0.24</cell><cell>-6.25</cell><cell>-1.52</cell><cell cols="3">+1.01 +2.02 32.52</cell><cell>-0.69</cell><cell>+0.95</cell><cell>-8.01</cell><cell>-1.11</cell><cell>+0.21 +2.44</cell></row><row><cell cols="2">Ave. 30.82</cell><cell>-2.13</cell><cell>-0.76</cell><cell>-6.03</cell><cell>-1.39</cell><cell cols="3">+0.27 +1.58 25.11</cell><cell>-2.05</cell><cell>+0.05</cell><cell>-7.30</cell><cell>-2.11</cell><cell>-0.06</cell><cell>+1.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Recognition accuracy (%) averaged over 10 test runs on VD K-shot learning.</figDesc><table><row><cell>K</cell><cell>Im.N. PT</cell><cell>CrossGrad</cell><cell>MR</cell><cell>MR-FL</cell><cell>Reptile</cell><cell>AGG</cell><cell>Feature-Critic</cell></row><row><cell>3</cell><cell cols="7">13.88 ? 1.82 14.01 ? 1.98 8.86 ? 1.52 13.26 ? 1.62 14.59 ? 2.30 13.75 ? 1.77 15.18 ? 2.26</cell></row><row><cell>5</cell><cell cols="7">17.63 ? 1.55 17.74 ? 1.40 12.01 ? 1.51 17.22 ? 0.84 19.17 ? 0.94 18.00 ? 1.58 19.02 ? 1.57</cell></row><row><cell>8</cell><cell cols="7">21.58 ? 1.12 20.98 ? 1.07 14.37 ? 1.35 21.61 ? 0.92 21.24 ? 1.40 21.36 ? 1.05 22.39 ? 1.00</cell></row><row><cell>10</cell><cell cols="7">23.40 ? 0.99 22.84 ? 0.89 15.61 ? 0.88 22.80 ? 0.61 23.42 ? 1.12 22.56 ? 0.84 24.23 ? 1.00</cell></row><row><cell>Ave.</cell><cell>19.12</cell><cell>18.89</cell><cell>12.71</cell><cell>18.72</cell><cell>19.61</cell><cell>18.92</cell><cell>20.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>4.2.2. EVALUATION ON PACS DATASETDataset and Settings PACS<ref type="bibr" target="#b12">(Li et al., 2017a</ref>) is a recent object recognition benchmark for domain generalisation. PACS contains 9991 images of size 224 ? 224 from four different domains -Photo, Art painting, Cartoon and Sketch. It has 7 categories across these domains: dog, elephant, giraffe, guitar, house, horse and person. We follow the standard protocol and perform leave-one-domain-out evaluation. Beyond this there have been two splits of PACS used in the literature. PACS was defined with a train/validation/test split within each domain. In<ref type="bibr" target="#b13">Li et al. (2018a)</ref> models are trained on the train split alone with the validation split used for early stopping. In<ref type="bibr" target="#b0">Balaji et al. (2018)</ref> the combined train+validation splits were used to train the models, resulting in slightly higher performance due to more data.</figDesc><table /><note>For direct comparison with previously published results we</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .Table 5 .Table 6 .</head><label>456</label><figDesc>Recognition accuracy (%) averaged over 10 train+test runs on Rotated MNIST. ? 0.69 85.70 ? 0.31 87.78 ? 0.30 86.42 ? 0.24 89.23 ? 0.25 87.04 ? 0.31 M15 98.92 ? 0.53 98.87 ? 0.41 99.44 ? 0.22 98.61 ? 0.27 99.68 ? 0.24 99.53 ? 0.27 M30 98.60 ? 0.51 98.32 ? 0.44 98.42 ? 0.24 99.19 ? 0.19 99.20 ? 0.20 99.41 ? 0.18 M45 98.39 ? 0.29 98.58 ? 0.28 98.80 ? 0.20 98.22 ? Cross-domain recognition accuracy (%) on PACS using train split (Li et al., 2017b) for training. Target DICA D-MTAE DSN TF-CNN MLDG DANN CIDDG Reptile CrossGrad MetaReg AGG FC A Cross-domain recognition accuracy (%) on PACS using train+validation split<ref type="bibr" target="#b0">(Balaji et al., 2018)</ref> for training.</figDesc><table><row><cell></cell><cell>Target</cell><cell></cell><cell>CrossGrad</cell><cell cols="2">MetaReg</cell><cell>Reptile</cell><cell>AGG</cell><cell>Feature-Critic-MLP Feature-Critic-Flatten</cell></row><row><cell></cell><cell>M0</cell><cell cols="6">86.03 0.24</cell><cell>99.24 ? 0.18</cell><cell>99.52 ? 0.24</cell></row><row><cell></cell><cell>M60</cell><cell cols="6">98.68 ? 0.28 98.93 ? 0.32 99.03 ? 0.28 99.48 ? 0.19</cell><cell>99.53 ? 0.23</cell><cell>99.23 ? 0.16</cell></row><row><cell></cell><cell>M75</cell><cell cols="6">88.94 ? 0.47 89.44 ? 0.37 87.42 ? 0.33 88.92 ? 0.43</cell><cell>91.44 ? 0.34</cell><cell>91.52 ? 0.26</cell></row><row><cell></cell><cell>Ave.</cell><cell></cell><cell>94.93</cell><cell cols="2">94.97</cell><cell>95.15</cell><cell>95.14</cell><cell>96.39</cell><cell>96.04</cell></row><row><cell cols="7">Target CrossGrad MetaReg Reptile AGG Feature-Critic</cell></row><row><cell>A</cell><cell cols="2">64.84</cell><cell>69.82</cell><cell>64.35</cell><cell>63.77</cell><cell>64.89</cell></row><row><cell>C</cell><cell cols="2">67.69</cell><cell>70.35</cell><cell>70.09</cell><cell>66.77</cell><cell>71.72</cell></row><row><cell>P</cell><cell cols="2">88.48</cell><cell>91.07</cell><cell>88.78</cell><cell>88.62</cell><cell>89.94</cell></row><row><cell>S</cell><cell cols="2">57.52</cell><cell>59.26</cell><cell>59.91</cell><cell>57.27</cell><cell>61.85</cell></row><row><cell>Ave.</cell><cell cols="2">69.63</cell><cell>72.62</cell><cell>70.78</cell><cell>69.11</cell><cell>72.10</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was produced while the first author was visiting the University of Edinburgh. This work was supported by National Natural Science Foundation of China (Grant No. 61751208), China Scholarship Council, EPSRC grant EP/R026173/1, and NVIDIA Corporation GPU donation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Metareg: towards domain generalization using meta-regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">References</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiclass domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Deshmukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Limited Labeled Data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain transfer multiple kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="465" to="479" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Domain generalization for object recognition with multitask autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep value networks learn to evaluate and iteratively refine structured outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Imagenet classification with deep convolutional neural networks. In NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to generalize: Meta-learning for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Domain generalization with adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep domain generalization via conditional invariant adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning to learn quickly for few shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meta-Sgd</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unified deep supervised domain adaptation and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Motiian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Piccirilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Domain generalization via invariant feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schlkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-view domain generalization for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visual recognition by learning from web data: A weakly supervised domain generalization approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Cnn features off-the-shelf: An astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlsson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning multiple visual domains with residual adapters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient parametrization of multi-domain deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On the convergence of adam and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Incremental learning through deep adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Asymmetric tritraining for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Shifting inductive bias with success-story algorithm, adaptive levin search, and incremental self-improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wiering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generalizing across domains via cross-gradient training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Piratla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jyothi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sarawagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A dirt-t approach to unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Narui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Prototypical networks for few shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning to learn: Meta-critic networks for sample efficient learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<idno>arXiv 1706.09529</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning to compare: relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning to Learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pratt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Kluwer Academic Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Generalizing to unseen domains via adversarial data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Meta-gradient reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A unified perspective on multidomain and multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<title level="m">How transferable are features in deep neural networks? In NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
