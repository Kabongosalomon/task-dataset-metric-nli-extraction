<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Convolutional Neural Networks for Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
							<email>mathias.niepert@neclab.eu</email>
							<affiliation key="aff0">
								<orgName type="institution">NEC Labs Europe</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
							<email>mohamed.ahmed@neclab.eu</email>
							<affiliation key="aff0">
								<orgName type="institution">NEC Labs Europe</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NEC Labs Europe</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov@neclab</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NEC Labs Europe</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NEC Labs Europe</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Convolutional Neural Networks for Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Numerous important problems can be framed as learning from graph data. We propose a framework for learning convolutional neural networks for arbitrary graphs. These graphs may be undirected, directed, and with both discrete and continuous node and edge attributes. Analogous to image-based convolutional networks that operate on locally connected regions of the input, we present a general approach to extracting locally connected regions from graphs. Using established benchmark data sets, we demonstrate that the learned feature representations are competitive with state of the art graph kernels and that their computation is highly efficient.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With this paper we aim to bring convolutional neural networks to bear on a large class of graph-based learning problems. We consider the following two problems.</p><p>1. Given a collection of graphs, learn a function that can be used for classification and regression problems on unseen graphs. The nodes of any two graphs are not necessarily in correspondence. For instance, each graph of the collection could model a chemical compound and the output could be a function mapping unseen compounds to their level of activity against cancer cells.</p><p>2. Given a large graph, learn graph representations that can be used to infer unseen graph properties such as node types and missing edges.</p><p>We propose a framework for learning representations for classes of directed and undirected graphs. The graphs may   <ref type="figure">Figure 1</ref>. A CNN with a receptive field of size 3x3. The field is moved over an image from left to right and top to bottom using a particular stride (here: 1) and zero-padding (here: none) (a). The values read by the receptive fields are transformed into a linear layer and fed to a convolutional architecture (b). The node sequence for which the receptive fields are created and the shapes of the receptive fields are fully determined by the hyper-parameters.</p><p>have nodes and edges with multiple discrete and continuous attributes and may have multiple types of edges. Similar to convolutional neural network for images, we construct locally connected neighborhoods from the input graphs. These neighborhoods are generated efficiently and serve as the receptive fields of a convolutional architecture, allowing the framework to learn effective graph representations.</p><p>The proposed approach builds on concepts from convolutional neural networks (CNNs) <ref type="bibr" target="#b15">(Fukushima, 1980;</ref><ref type="bibr" target="#b1">Atlas et al., 1988;</ref><ref type="bibr" target="#b26">LeCun et al., 1998;</ref><ref type="bibr" target="#b9">2015)</ref> for images and extends them to arbitrary graphs. <ref type="figure">Figure 1</ref> illustrates the locally connected receptive fields of a CNN for images. An image can be represented as a square grid graph whose nodes represent pixels. Now, a CNN can be seen as traversing a node sequence (nodes 1-4 in <ref type="figure">Figure 1(a)</ref>) and generating fixed-size neighborhood graphs (the 3x3 grids in <ref type="figure">Figure 1(b)</ref>) for each of the nodes. The neighborhood graphs serve as the receptive fields to read feature values from the pixel nodes. Due to the implicit spatial order of the pixels, the sequence of nodes for which neighborhood graphs are created, from left to right and top to bottom, is uniquely determined. The same holds for NLP problems where each sentence (and its parse-tree) determines arXiv:1605.05273v4 <ref type="bibr">[cs.</ref>LG] 8 Jun 2016 a sequence of words. However, for numerous graph collections a problem-specific ordering (spatial, temporal, or otherwise) is missing and the nodes of the graphs are not in correspondence. In these instances, one has to solve two problems: (i) Determining the node sequences for which neighborhood graphs are created and (ii) computing a normalization of neighborhood graphs, that is, a unique mapping from a graph representation into a vector space representation. The proposed approach, termed PATCHY-SAN, addresses these two problems for arbitrary graphs. For each input graph, it first determines nodes (and their order) for which neighborhood graphs are created. For each of these nodes, a neighborhood consisting of exactly k nodes is extracted and normalized, that is, it is uniquely mapped to a space with a fixed linear order. The normalized neighborhood serves as the receptive field for a node under consideration. Finally, feature learning components such as convolutional and dense layers are combined with the normalized neighborhood graphs as the CNN's receptive fields. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the PATCHY-SAN architecture which has several advantages over existing approaches: First, it is highly efficient, naively parallelizable, and applicable to large graphs. Second, for a number of applications, ranging from computational biology to social network analysis, it is important to visualize learned network motifs <ref type="bibr" target="#b32">(Milo et al., 2002)</ref>. PATCHY-SAN supports feature visualizations providing insights into the structural properties of graphs. Third, instead of crafting yet another graph kernel, PATCHY-SAN learns application dependent features without the need to feature engineering. Our theoretical contributions are the definition of the normalization problem on graphs and its complexity; a method for comparing graph labeling approaches for a collection of graphs; and a result that shows that PATCHY-SAN generalizes CNNs on images. Using standard benchmark data sets, we demonstrate that the learned CNNs for graphs are both efficient and effective compared to state of the art graph kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Graph kernels allow kernel-based learning approaches such as SVMs to work directly on graphs <ref type="bibr" target="#b40">(Vishwanathan et al., 2010)</ref>. Kernels on graphs were originally defined as similarity functions on the nodes of a single graph <ref type="bibr" target="#b23">(Kondor &amp; Lafferty, 2002)</ref>. Two representative classes of kernels are the skew spectrum kernel <ref type="bibr" target="#b22">(Kondor &amp; Borgwardt, 2008)</ref> and kernels based on graphlets <ref type="bibr" target="#b24">(Kondor et al., 2009;</ref><ref type="bibr" target="#b37">Shervashidze et al., 2009</ref>). The latter is related to our work, as it builds kernels based on fixed-sized subgraphs. These subgraphs, which are often called motifs or graphlets, reflect functional network properties <ref type="bibr" target="#b32">(Milo et al., 2002;</ref><ref type="bibr" target="#b0">Alon, 2007</ref>  <ref type="bibr" target="#b44">(Yanardag &amp; Vishwanathan, 2015)</ref> and graph invariant kernels <ref type="bibr" target="#b35">(Orsini et al., 2015)</ref> compare graphs based on the existence or count of small substructures such as shortest paths <ref type="bibr" target="#b6">(Borgwardt &amp; Kriegel, 2005)</ref>, graphlets, subtrees, and other graph invariants <ref type="bibr" target="#b17">(Haussler, 1999;</ref><ref type="bibr" target="#b35">Orsini et al., 2015)</ref>. In contrast, PATCHY-SAN learns substructures from graph data and is not limited to a predefined set of motifs. Moreover, while all graph kernels have a training complexity at least quadratic in the number of graphs <ref type="bibr" target="#b38">(Shervashidze et al., 2011)</ref>, which is prohibitive for large-scale problems, PATCHY-SAN scales linearly with the number of graphs.</p><p>Graph neural networks (GNNs) <ref type="bibr" target="#b36">(Scarselli et al., 2009</ref>) are a recurrent neural network architecture defined on graphs. GNNs apply recurrent neural networks for walks on the graph structure, propagating node representations until a fixed point is reached. The resulting node representations are then used as features in classification and regression problems. GNNs support only discrete labels and perform as many backpropagation operations as there are edges and nodes in the graph per learning iteration. Gated Graph Sequence Neural Networks modify GNNs to use gated recurrent units and to output sequences <ref type="bibr" target="#b29">(Li et al., 2015)</ref>.</p><p>Recent work extended CNNs to topologies that differ from the low-dimensional grid structure <ref type="bibr" target="#b7">(Bruna et al., 2014;</ref><ref type="bibr" target="#b18">Henaff et al., 2015)</ref>. All of these methods, however, assume one global graph structure, that is, a correspondence of the vertices across input examples. <ref type="bibr" target="#b13">(Duvenaud et al., 2015)</ref> perform convolutional type operations on graphs, developing a differentiable variant of one specific graph feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head><p>We provide a brief introduction to the required background in convolutional networks and graph theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Convolutional Neural Networks</head><p>CNNs were inspired by earlier work that showed that the visual cortex in animals contains complex arrangements of cells, responsible for detecting light in small local regions of the visual field <ref type="bibr" target="#b19">(Hubel &amp; Wiesel, 1968)</ref>. CNNs were developed in the 1980s and have been applied to image, speech, text, and drug discovery problems <ref type="bibr" target="#b1">(Atlas et al., 1988;</ref><ref type="bibr" target="#b25">LeCun et al., 1989;</ref><ref type="bibr" target="#b26">1998;</ref><ref type="bibr" target="#b9">2015;</ref><ref type="bibr" target="#b42">Wallach et al., 2015)</ref>. A predecessor to CNNs was the Neocognitron <ref type="bibr" target="#b15">(Fukushima, 1980)</ref>. A typical CNN is composed of convolutional and dense layers. The purpose of the first convolutional layer is the extraction of common patterns found within local regions of the input images. CNNs convolve learned filters over the input image, computing the inner product at every image location in the image and outputting the result as tensors whose depth is the number of filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Graphs</head><p>A graph G is a pair (V, E) with V = {v 1 , ..., v n } the set of vertices and E ? V ? V the set of edges. Let n be the number of vertices and m the number of edges. Each graph can be represented by an adjacency matrix A of size n ? n, where A i,j = 1 if there is an edge from vertex v i to vertex v j , and A i,j = 0 otherwise. In this case, we say that vertex v i has position i in A. Moreover, if A i,j = 1 we say v i and v j are adjacent. Node and edge attributes are features that attain one value for each node and edge of a graph. We use the term attribute value instead of label to avoid confusion with the graph-theoretical concept of a labeling. A walk is a sequence of nodes in a graph, in which consecutive nodes are connected by an edge. A path is a walk with distinct nodes. We write d(u, v) to denote the distance between u and v, that is, the length of the shortest path between u and v. N 1 (v) is the 1-neighborhood of a node, that is, all nodes that are adjacent to v.</p><p>Labeling and Node Partitions. PATCHY-SAN utilizes graph labelings to impose an order on nodes. A graph labeling is a function : V ? S from the set of vertices V to an ordered set S such as the real numbers and integers. A graph labeling procedure computes a graph labeling for an input graph. When it is clear from the context, we use labeling to refer to both, the graph labeling and the procedure to compute it. A ranking (or coloring) is a function r : V ? {1, ..., |V |}. Every labeling induces a ranking r with r(u) &lt; r(v) if and only if (u) &gt; (v). If the labeling of graph G is injective, it determines a total order of G's vertices and a unique adjacency matrix</p><formula xml:id="formula_0">A (G) of G where vertex v has position r(v) in A (G). Moreover, every graph labeling induces a partition {V 1 , ..., V n } on V with u, v ? V i if and only if (u) = (v).</formula><p>Examples of graph labeling procedures are node degree and other measures of centrality commonly used in the analysis of networks. For instance, the betweeness centrality of a vertex v computes the fractions of shortest paths that pass through v. The Weisfeiler-Lehman algorithm <ref type="bibr" target="#b43">(Weisfeiler &amp; Lehman, 1968;</ref><ref type="bibr" target="#b12">Douglas, 2011</ref>) is a procedure for partitioning the vertices of a graph. It is also known as color refinement and naive vertex classification. Color refinement has attracted considerable interest in the ML community since it can be applied to speed-up inference in graphical models <ref type="bibr" target="#b20">(Kersting et al., 2009;</ref> and as a method to compute graph kernels <ref type="bibr" target="#b38">(Shervashidze et al., 2011)</ref>. PATCHY-SAN applies these labeling procedures, among others (degree, page-rank, eigenvector centrality, etc.), to impose an order on the nodes of graphs, replacing application-dependent orders (temporal, spatial, etc.) where missing.</p><p>Isomorphism and Canonicalization. The computational problem of deciding whether two graphs are isomorphic surfaces in several application domains. The graph isomorphism (GI) problem is in NP but not known to be in P or NP-hard. Under several mild restrictions, GI is known to be in P. For instance, GI is in P for graphs of bounded degree <ref type="bibr" target="#b30">(Luks, 1982)</ref>. A canonicalization of a graph G is a graph G with a fixed vertex order which is isomorphic to G and which represents its entire isomorphism class. In practice, the graph canonicalization tool NAUTY has shown remarkable performance <ref type="bibr" target="#b31">(McKay &amp; Piperno, 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Learning CNNs for Arbitrary Graphs</head><p>When CNNs are applied to images, a receptive field (a square grid) is moved over each image with a particular step size. The receptive field reads the pixels' feature values, for each channel once, and a patch of values is created for each channel. Since the pixels of an image have an implicit arrangement -a spatial order -the receptive fields are always moved from left to right and top to bottom. Moreover, the spatial order uniquely determines the nodes of each receptive field and the way these nodes are mapped to a vector space representation (see <ref type="figure">Figure 1(b)</ref>). Consequently, the values read from two pixels using two different locations of the receptive field are assigned to the same relative position if and only if the pixels' structural roles (their spatial position within the receptive field) are identical.</p><p>To show the connection between CNNs and PATCHY-SAN, we frame CNNs on images as identifying a sequence of nodes in the square grid graph representing the image and building a normalized neighborhood graph -a receptive Algorithm 1 SELNODESEQ: Select Node Sequence 1: input: graph labeling procedure , graph G = (V, E), stride s, width w, receptive field size k 2: Vsort = top w elements of V according to 3: i = 1, j = 1 4: while j &lt; w do 5: if i ? |Vsort| then 6: f = RECEPTIVEFIELD(Vsort[i]) 7: else 8: f = ZERORECEPTIVEFIELD() 9: apply f to each input channel 10:</p><formula xml:id="formula_1">i = i + s, j = j + 1</formula><p>field -for each node in the identified sequence. For graph collections where an application-dependent node order is missing and where the nodes of any two graphs are not yet aligned, we need to determine for each graph (i) the sequences of nodes for which we create neighborhoods, and (ii) a unique mapping from the graph representation to a vector representation such that nodes with similar structural roles in the neighborhood graphs are positioned similarly in the vector representation.</p><p>We address these problems by leveraging graph labeling procedures that assigns nodes from two different graphs to a similar relative position in their respective adjacency matrices if their structural roles within the graphs are similar. Given a collection of graphs, PATCHY-SAN (SELECT-ASSEMBLE-NORMALIZE) applies the following steps to each graph: (1) Select a fixed-length sequence of nodes from the graph;</p><p>(2) assemble a fixed-size neighborhood for each node in the selected sequence;</p><p>(3) normalize the extracted neighborhood graph; and (4) learn neighborhood representations with convolutional neural networks from the resulting sequence of patches.</p><p>In the following, we describe methods that address the above-mentioned challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Node Sequence Selection</head><p>Node sequence selection is the process of identifying, for each input graph, a sequence of nodes for which receptive fields are created. Algorithm 1 lists one such procedure. First, the vertices of the input graph are sorted with respect to a given graph labeling. Second, the resulting node sequence is traversed using a given stride s and for each visited node, Algorithm 3 is executed to construct a receptive field, until exactly w receptive fields have been created. The stride s determines the distance, relative to the selected node sequence, between two consecutive nodes for which a receptive field is created. If the number of nodes is smaller than w, the algorithm creates all-zero receptive fields for padding purposes.</p><p>Several alternative methods for vertex sequence selection are possible. For instance, a depth-first traversal of the in- put graph guided by the values of the graph labeling. We leave these ideas to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Neighborhood Assembly</head><p>For each of the nodes identified in the previous step, a receptive field has to be constructed. Algorithm 3 first calls Algorithm 2 to assembles a local neighborhood for the input node. The nodes of the neighborhood are the candidates for the receptive field. Algorithm 2 lists the neighborhood assembly steps. Given as inputs a node v and the size of the receptive field k, the procedure performs a breadth-first search, exploring vertices with an increasing distance from v, and adds these vertices to a set N . If the number of collected nodes is smaller than k, the 1-neighborhood of the vertices most recently added to N are collected, and so on, until at least k vertices are in N , or until there are no more neighbors to add. Note that at this time, the size of N is possibly different to k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Graph Normalization</head><p>The receptive field for a node is constructed by normalizing the neighborhood assembled in the previous step. Illustrated in <ref type="figure">Figure 3</ref>, the normalization imposes an order on the nodes of the neighborhood graph so as to map from the unordered graph space to a vector space with a linear order. The basic idea is to leverage graph labeling procedures that assigns nodes of two different graphs to a similar relative position in the respective adjacency matrices if and only if their structural roles within the graphs are similar.</p><p>To formalize this intuition, we define the optimal graph normalization problem which aims to find a labeling that is optimal relative to a given collection of graphs.</p><p>Problem 1 (Optimal graph normalization). Let G be a collection of unlabeled graphs with k nodes, let be an injective graph labeling procedure, let d G be a distance measure on graphs with k nodes, and let d A be a distance measure on k ? k matrices. Find? such that</p><formula xml:id="formula_2">= arg min E G d A A (G), A (G ) ? d G (G, G ) .</formula><p>The problem amounts to finding a graph labeling procedure , such that, for any two graphs drawn uniformly at Learning Convolutional Neural Networks for Graphs  <ref type="figure">Figure 3</ref>. The normalization is performed for each of the graphs induced on the neighborhood of a root node v (the red node; node colors indicate distance to the root node). A graph labeling is used to rank the nodes and to create the normalized receptive fields, one of size k (here: k = 9) for node attributes and one of size k ? k for edge attributes. Normalization also includes cropping of excess nodes and padding with dummy nodes. Each vertex (edge) attribute corresponds to an input channel with the respective receptive field.</p><p>Algorithm 3 RECEPTIVEFIELD: Create Receptive Field 1: input: vertex v, graph labeling , receptive field size k 2: N = NEIGHASSEMB(v, k) 3: Gnorm = NORMALIZEGRAPH(N, v, , k) 4: return Gnorm random from G, the expected difference between the distance of the graphs in vector space (with respect to the adjacency matrices based on ) and the distance of the graphs in graph space is minimized. The optimal graph normalization problem is a generalization of the classical graph canonicalization problem. A canonical labeling algorithm, however, is optimal only for isomorphic graphs and might perform poorly for graphs that are similar but not isomorphic. In contrast, the smaller the expectation of the optimal normalization problem, the better the labeling aligns nodes with similar structural roles. Note that the similarity is determined by d G .</p><p>We have the following result concerning the complexity of the optimal normalization problem. Theorem 1. Optimal graph normalization is NP-hard.</p><p>Proof: By reduction from subgraph isomorphism.</p><p>PATCHY-SAN does not solve the above optimization problem. Instead, it may compare different graph labeling methods and choose the one that performs best relative to a given collection of graphs. Theorem 2. Let G be a collection of graphs and let (G 1 , G 1 ), ..., (G N , G N ) be a sequence of pairs of graphs sampled independently and uniformly at random from G.</p><formula xml:id="formula_3">Let? := N i=1 d A A (G i ), A (G i ) /N and ? := E G d A A (G), A (G ) ? d G (G, G ) . If d A ? d G , then E G [? 1 ] &lt; E G [? 2 ] if and only if ? 1 &lt; ? 2 .</formula><p>Theorem 2 enables us to compare different labeling procedures in an unsupervised manner via a comparison of the corresponding estimators. Under the assumption d A ? d G , the smaller the estimate? the smaller the absolute difference. Therefore, we can simply choose the labeling for which? is minimal. The assumption d A ? d G holds, for instance, for the edit distance on graphs and the Ham- ming distance on adjacency matrices. Finally, note that all of the above results can be extended to directed graphs.</p><p>The graph normalization problem and the application of appropriate graph labeling procedures for the normalization of local graph structures is at the core of the proposed approach. Within the PATCHY-SAN framework, we normalize the neighborhood graphs of a vertex v. The labeling of the vertices is therefore constrained by the graph distance to v: for any two vertices u, w, if u is closer to v than w, then v is always ranked higher than w. This definition ensures that v has always rank 1, and that the closer a vertex is to v in G, the higher it is ranked in the vector space representation.</p><p>Since most labeling methods are not injective, it is necessary to break ties between same-label nodes. To do so, we use NAUTY <ref type="bibr" target="#b31">(McKay &amp; Piperno, 2014)</ref>. NAUTY accepts prior node partitions as input and breaks remaining ties by choosing the lexicographically maximal adjacency matrix. It is known that graph isomorphism is in PTIME for graphs of bounded degree <ref type="bibr" target="#b30">(Luks, 1982)</ref>. Due to the constant size k of the neighborhood graphs, the algorithm runs in time polynomial in the size of the original graph and, on average, in time linear in k <ref type="bibr" target="#b2">(Babai et al., 1980)</ref>. Our experiments verify that computing a canonical labeling of the graph neigborhoods adds a negligible overhead.</p><p>Algorithm 4 lists the normalization procedure. If the size of the input set U is larger than k, it first applies the ranking based on to select the top k nodes and recomputes a ranking on the smaller set of nodes. If the size of U is smaller than k, it adds disconnected dummy nodes. Finally, it induces the subgraph on the vertices N and canonicalizes the graph taking the ranking r as prior coloring.</p><p>We can relate PATCHY-SAN to CNNs for images as follows.</p><p>Theorem 3. Given a sequence of pixels taken from an image. Applying PATCHY-SAN with receptive field size (2m ? 1) 2 , stride s, no zero padding, and 1-WL normalization to the sequence is identical (up to a fixed permutation of the receptive field) to the first layer of a CNN with receptive field size 2m ? 1, stride s, and no zero padding.</p><p>Proof: It is possible to show that if an input graph is a square grid, then the 1-WL normalized receptive field constructed for a vertex is always a square grid graph with a unique vertex order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Convolutional Architecture</head><p>PATCHY-SAN is able to process both vertex and edge attributes (discrete and continuous). Let a v be the number of vertex attributes and let a e be the number of edge attributes. For each input graph G, it applies normalized receptive fields for vertices and edges which results in one (w, k, a v ) and one (w, k, k, a e ) tensor. These can be reshaped to a (wk, a v ) and a (wk 2 , a e ) tensors. Note that a v and a e are the number of input channels. We can now apply a 1-dimensional convolutional layer with stride and receptive field size k to the first and k 2 to the second tensor. The rest of the architecture can be chosen arbitrarily. We may use merge layers to combine convolutional layers representing nodes and edges, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Complexity and Implementation</head><p>PATCHY-SAN's algorithm for creating receptive fields is highly efficient and naively parallelizable because the fields are generated independently. We can show the following asymptotic worst-case result.</p><p>Theorem 4. Let N be the number of graphs, let k be the receptive field size, w the width, <ref type="figure">and O(f (n, m)</ref>) the complexity of computing a given labeling for a graph with n vertices and m edges. PATCHY-SAN has a worst-case complexity of O(N w(f (n, m) + n log(n) + exp(k))) for computing the receptive fields for N graphs.</p><p>Proof: Node sequence selection requires the labeling of each input graph and the retrieval of the k highest ranked nodes. For the creation of normalized graph patches, most computational effort is spent applying the labeling procedure to a neighborhood whose size may be larger than  k. Let d be the maximum degree of the input graph G, and U the neighborhood returned by Algorithm 2. We have |U | ? (k ? 2)d ? n. The term exp(k) comes from the worst-case complexity of the graph canonicalization algorithm NAUTY on a k node graph <ref type="bibr" target="#b33">(Miyazaki, 1997)</ref>.</p><p>For instance, for the Weisfeiler-Lehman algorithm, which has a complexity of O((n + m) log(n)) <ref type="bibr" target="#b5">(Berkholz et al., 2013)</ref>, and constants w n and k n, the complexity of PATCHY-SAN is linear in N and quasi-linear in m and n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We conduct three types of experiments: a runtime analysis, a qualitative analysis of the learned features, and a comparison to graph kernels on benchmark data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Runtime Analysis</head><p>We assess the efficiency of PATCHY-SAN by applying it to real-world graphs. The objective is to compare the rates at which receptive fields are generated to the rate at which state of the art CNNs perform learning. All input graphs are part of the collection of the Python module GRAPH-TOOL 1 . For a given graph, we used PATCHY-SAN to compute a receptive field for all nodes using the 1-dimensional Weisfeiler-Lehman <ref type="bibr" target="#b12">(Douglas, 2011)</ref> (1-WL) algorithm for the normalization. torus is a periodic lattice with 10, 000 nodes; random is a random undirected graph with 10, 000 nodes and a degree distribution P (k) ? 1/k and k max = 3; power is a network representing the topology of a power grid in the US; polbooks is a co-purchasing network of books about US politics published during the 2004 presidential election; preferential is a preferential attachment network model where newly added vertices have degree 3; astro-ph is a coauthorship network between authors of preprints posted on the astrophysics arxiv <ref type="bibr" target="#b34">(Newman, 2001)</ref>; email-enron is a communication network generated from about half a million sent emails <ref type="bibr" target="#b28">(Leskovec et al., 2009)</ref>. All experiments were run on commodity hardware with 64G RAM and a single 2.8 GHz CPU.  <ref type="figure" target="#fig_3">Figure 4</ref> depicts the receptive fields per second rates for each input graph. For receptive field size k = 5 and k = 10 PATCHY-SAN creates fields at a rate of more than 1000/s except for email-enron with a rate of 600/s and 320/s, respectively. For k = 50, the largest tested size, fields are created at a rate of at least 100/s. A CNN with 2 convolutional and 2 dense layers learns at a rate of about 200-400 training examples per second on the same machine. Hence, the speed at which receptive fields are generated is sufficient to saturate a downstream CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Feature Visualization</head><p>The visualization experiments' aim is to qualitatively investigate whether popular models such as the restricted Boltzman machine (RBM) <ref type="bibr" target="#b14">(Freund &amp; Haussler, 1992)</ref> can be combined with PATCHY-SAN for unsupervised feature learning. For every input graph, we have generated receptive fields for all nodes and used these as input to an RBM. The RBM had 100 hidden nodes and was trained for 30 epochs with contrastive divergence and a learning rate of 0.01. We visualize the features learned by a single-layer RBM for 1-dimensional Weisfeiler-Lehman (1-WL) normalized receptive fields of size 9. Note that the features learned by the RBM correspond to reoccurring receptive field patterns. <ref type="figure" target="#fig_6">Figure 5</ref> depicts some of the features and samples drawn from it for four different graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Graph Classification</head><p>Graph classification is the problem of assigning graphs to one of several categories.</p><p>Data Sets. We use 6 standard benchmark data sets to compare run-time and classification accuracy with state of the art graph kernels: MUTAG, PCT, NCI1, NCI109, PRO-TEIN, and D&amp;D. MUTAG <ref type="bibr" target="#b10">(Debnath et al., 1991)</ref> is a data set of 188 nitro compounds where classes indicate whether the compound has a mutagenic effect on a bacterium. PTC consists of 344 chemical compounds where classes indicate carcinogenicity for male and female rats <ref type="bibr" target="#b39">(Toivonen et al., 2003)</ref>. NCI1 and NCI109 are chemical compounds screened for activity against non-small cell lung cancer and ovarian cancer cell lines <ref type="bibr" target="#b41">(Wale &amp; Karypis, 2006)</ref>. PRO-TEINS is a graph collection where nodes are secondary structure elements and edges indicate neighborhood in the amino-acid sequence or in 3D space. Graphs are classified as enzyme or non-enzyme. D&amp;D is a data set of 1178 protein structures <ref type="bibr" target="#b11">(Dobson &amp; Doig, 2003)</ref> classified into enzymes and non-enzymes.</p><p>Experimental Set-up. We compared PATCHY-SAN with the shortest-path kernel (SP) <ref type="bibr" target="#b6">(Borgwardt &amp; Kriegel, 2005)</ref>, the random walk kernel (RW) <ref type="bibr" target="#b16">(Gaertner et al., 2003)</ref>, the graphlet count kernel (GK) , and the Weisfeiler-Lehman subtree kernel (WL) <ref type="bibr" target="#b38">(Shervashidze et al., 2011)</ref>. Similar to previous work <ref type="bibr" target="#b44">(Yanardag &amp; Vishwanathan, 2015)</ref>, we set the height parameter of WL to 2, the size of the graphlets for GK to 7, and chose the decay factor for RW from {10 ?6 , 10 ?5 , ..., 10 ?1 }. We performed 10-fold cross-validation with LIB-SVM <ref type="bibr" target="#b8">(Chang &amp; Lin, 2011)</ref>, using 9 folds for training and 1 for testing, and repeated the experiments 10 times. We report average prediction accuracies and standard deviations.</p><p>For PATCHY-SAN (referred to as PSCN), we used 1dimensional WL normalization, a width w equal to the average number of nodes (see <ref type="table">Table 1</ref>), and receptive field sizes of k = 5 and k = 10. For the experiments we only used node attributes. In addition, we ran experiments for k = 10 where we combined receptive fields for nodes and edges using a merge layer (k = 10 E ). To make a fair com- parison, we used a single network architecture with two convolutional layers, one dense hidden layer, and a softmax layer for all experiments. The first convolutional layer had 16 output channels (feature maps). The second conv layer has 8 output channels, a stride of s = 1, and a field size of 10. The convolutional layers have rectified linear units. The dense layer has 128 rectified linear units with a dropout rate of 0.5. Dropout and the relatively small number of neurons are needed to avoid overfitting on the smaller data sets. The only hyperparameter we optimized is the number of epochs and the batch size for the mini-batch gradient decent algorithm RMSPROP. All of the above was implemented with the THEANO <ref type="bibr" target="#b4">(Bergstra et al., 2010)</ref> wrapper KERAS <ref type="bibr" target="#b9">(Chollet, 2015)</ref>. We also applied a logistic regression (PSLR) classifier on the patches for k = 10.</p><p>Moreover, we ran experiments with the same set-up 2 on larger social graph data sets (up to 12000 graphs each, with an average of 400 nodes), and compared PATCHY-SAN with previously reported results for the graphlet count (GK) and the deep graphlet count kernel (DGK) <ref type="bibr" target="#b44">(Yanardag &amp; Vishwanathan, 2015)</ref>. We used the normalized node degree as attribute for PATCHY-SAN, highlighting one of its advantages: it can easily incorporate continuous features.</p><p>Results. <ref type="table">Table 1</ref> lists the results of the experiments. We omit the results for NCI109 as they are almost identical to NCI1. Despite using a one-fits-all CNN architecture, the CNNs accuracy is highly competitive with existing graph 2 Due to the larger size of the data sets, we removed dropout. kernels. In most cases, a receptive field size of 10 results in the best classification accuracy. The relatively high variance can be explained with the small size of the benchmark data sets and the fact that the CNNs hyperparameters (with the exception of epochs and batch size) were not tuned to individual data sets. Similar to the experience on image and text data, we expect PATCHY-SAN to perform even better for large data sets. Moreover, PATCHY-SAN is between 2 and 8 times more efficient than the most efficient graph kernel (WL). We expect the performance advantage to be much more pronounced for data sets with a large number of graphs. Results for betweeness centrality normalization are similar with the exception of the runtime which increases by about 10%. Logistic regression applied to PATCHY-SAN's receptive fields performs worse, indicating that PATCHY-SAN works especially well in conjunction with CNNs which learn non-linear feature combinations and which share weights across receptive fields.</p><p>PATCHY-SAN is also highly competitive on the social graph data. It significantly outperforms the other two kernels on four of the six data sets and achieves ties on the rest. <ref type="table">Table 2</ref> lists the results of the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion and Future Work</head><p>We proposed a framework for learning graph representations that are especially beneficial in conjunction with CNNs. It combines two complementary procedures: (a) selecting a sequence of nodes that covers large parts of the graph and (b) generating local normalized neighborhood representations for each of the nodes in the sequence. Experiments show that the approach is competitive with state of the art graph kernels.</p><p>Directions for future work include the use of alternative neural network architectures such as RNNs; combining different receptive field sizes; pretraining with RBMs and autoencoders; and statistical relational models based on the ideas of the approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 2</head><label>2</label><figDesc>NEIGHASSEMB: Neighborhood Assembly 1: input: vertex v, receptive field size k 2: output: set of neighborhood nodes N for v 3: N = [v] 4: L = [v] 5: while |N | &lt; k and |L| &gt; 0 do 6: L = v?L N1(v) 7: N = N ? L 8: return the set of vertices N</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 4</head><label>4</label><figDesc>NORMALIZEGRAPH: Graph Normalization 1: input: subset of vertices U from original graph G, vertex v, graph labeling , receptive field size k 2: output: receptive field for v 3: compute ranking r of U using , subject to?u, w ? U : d(u, v) &lt; d(w, v) ? r(u) &lt; r(w) 4: if |U | &gt; kthen 5: N = top k vertices in U according to r 6: compute ranking r of N using , subject to ?u, w ? N : d(u, v) &lt; d(w, v) ? r(u) &lt; r(w) 7: else if |V | &lt; k then 8: N = U and k ? |U | dummy nodes 9: else 10: N = U 11: construct the subgraph G[N ] for the vertices N 12: canonicalize G[N ], respecting the prior coloring r 13: return G[N ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Receptive fields per second rates on different graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Visualization of RBM features learned with 1-dimensional WL normalized receptive fields of size 9 for a torus (periodic lattice, top left), a preferential attachment graph(Barab?si &amp; Albert 1999, bottom left), a co-purchasing network of political books (top right), and a random graph (bottom right). Instances of these graphs with about 100 nodes are depicted on the left. A visual representation of the feature's weights (the darker a pixel, the stronger the corresponding weight) and 3 graphs sampled from the RBMs by setting all but the hidden node corresponding to the feature to zero. Yellow nodes have position 1 in the adjacency matrices. (Best seen in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Proceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&amp;CP volume 48. Copyright 2016 by the author(s).</figDesc><table><row><cell>1 2</cell><cell>1 2</cell><cell>1 2</cell><cell></cell><cell cols="2">1 2</cell></row><row><cell>3 4</cell><cell>3 4</cell><cell>3 4</cell><cell></cell><cell cols="2">3 4</cell></row><row><cell></cell><cell></cell><cell>...</cell><cell></cell><cell>...</cell><cell></cell></row><row><cell></cell><cell>...</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>...</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>). However, due to the combinatorial complexity of subgraph enumeration, graphlet kernels are restricted to ... ...</figDesc><table><row><cell>convolutional architecture</cell></row><row><cell>graph normalization</cell></row><row><cell>neighborhood graph construction</cell></row><row><cell>node sequence selection</cell></row><row><cell>Figure 2. An illustration of the proposed architecture. A node</cell></row><row><cell>sequence is selected from a graph via a graph labeling procedure.</cell></row><row><cell>For some nodes in the sequence, a local neighborhood graph is as-</cell></row><row><cell>sembled and normalized. The normalized neighborhoods are used</cell></row><row><cell>as receptive fields and combined with existing CNN components.</cell></row><row><cell>subgraphs with few nodes. An effective class of graph kernels are the Weisfeiler-Lehman (WL) kernels (Sher-vashidze et al., 2011). WL kernels, however, only sup-port discrete features and use memory linear in the num-ber of training examples at test time. PATCHY-SAN uses WL as one possible labeling procedure to compute re-ceptive fields. Deep graph kernels</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Properties of the data sets and accuracy and timing results (in seconds) for PATCHY-SAN and 4 state of the art graph kernels. M10k 31.82 ? 0.08 32.22 ? 0.10 41.32 ? 0.42 Comparison of accuracy results on social graphs[45].</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">Learning Convolutional Neural Networks for Graphs</cell></row><row><cell>Data set</cell><cell>MUTAG</cell><cell></cell><cell></cell><cell>PCT</cell><cell>NCI1</cell><cell>PROTEIN</cell><cell>D &amp; D</cell></row><row><cell>Max Avg Graphs</cell><cell>28 17.93 188</cell><cell></cell><cell></cell><cell>109 25.56 344</cell><cell>111 29.87 4110</cell><cell>620 39.06 1113</cell><cell>5748 284.32 1178</cell></row><row><cell>SP [7] RW [17] GK [38] WL [39]</cell><cell cols="6">85.79 ? 2.51 83.68 ? 1.66 81.58 ? 2.11 80.72 ? 3.00 (5s) 56.97 ? 2.01 (30s) 80.22 ? 0.51 (375s) 72.92 ? 0.56 (143s) 77.95 ? 0.70 (609s) 58.53 ? 2.55 73.00 ? 0.51 75.07 ? 0.54 &gt; 3 days 57.26 ? 1.30 &gt; 3 days 74.22 ? 0.42 &gt; 3 days 57.32 ? 1.13 62.28 ? 0.29 71.67 ? 0.55 78.45 ? 0.26</cell></row><row><cell cols="3">PSCN k=5 PSCN k=10 88.95 ? 4.37 (3s) 91.58 ? 5.86 (2s)</cell><cell cols="2">59.43 ? 3.14 (4s) 62.29 ? 5.68 (6s)</cell><cell>72.80 ? 2.06 (59s) 76.34 ? 1.68 (76s)</cell><cell>74.10 ? 1.72 (22s) 75.00 ? 2.51 (30s)</cell><cell>74.58 ? 2.85 (121s) 76.27 ? 2.64 (154s)</cell></row><row><cell cols="3">PSCN k=10 E 92.63 ? 4.21 (3s)</cell><cell cols="2">60.00 ? 4.82 (6s)</cell><cell>78.59 ? 1.89 (76s)</cell><cell>75.89 ? 2.76 (30s)</cell><cell>77.12 ? 2.41 (154s)</cell></row><row><cell>PSLR k=10</cell><cell>87.37 ? 7.88</cell><cell></cell><cell cols="2">58.57 ? 5.46</cell><cell>70.00 ? 1.98</cell><cell>71.79 ? 3.71</cell><cell>68.39 ? 5.56</cell></row><row><cell>Data set</cell><cell>GK [38]</cell><cell cols="2">DGK [45]</cell><cell>PSCN k=10</cell><cell></cell></row><row><cell cols="5">COLLAB 72.84 ? 0.28 73.09 ? 0.25 72.60 ? 2.15 IMDB-B 65.87 ? 0.98 66.96 ? 0.56 71.00 ? 2.29 IMDB-M 43.89 ? 0.38 44.55 ? 0.52 45.23 ? 2.84 RE-B 77.34 ? 0.18 78.04 ? 0.39 86.30 ? 1.58 RE-M5k 41.01 ? 0.17 41.27 ? 0.18 49.10 ? 0.70 RE-</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://graph-tool.skewed.de/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Many thanks to the anonymous ICML reviewers who provided tremendously helpful comments. The research leading to these results has received funding from the European Union's Horizon 2020 innovation action program under grant agreement No 653449-TYPES.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Network motifs: theory and experimental approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Genetics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="450" to="461" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An artificial neural network for spatio-temporal bipolar patterns: Application to phoneme classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Les</forename><forename type="middle">E</forename><surname>Atlas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiteru</forename><surname>Homma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<editor>Anderson, D.Z.</editor>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Random graph isomorphism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?szl?</forename><surname>Babai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Erd?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Selkow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stanley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Computing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="628" to="635" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Emergence of scaling in random networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert-Laszlo</forename><surname>Barab?si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?ka</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="issue">5439</biblScope>
			<biblScope unit="page" from="509" to="512" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fr?d?ric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warde</forename><forename type="middle">-</forename><surname>Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python for Scientific Computing Conference (SciPy)</title>
		<meeting>the Python for Scientific Computing Conference (SciPy)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Martin. Tight lower and upper bounds for the complexity of canonical colour refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Berkholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">S</forename><surname>Bonsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grohe</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Symposium on Algorithms</title>
		<meeting>the European Symposium on Algorithms</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="145" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Shortestpath kernels on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth IEEE International Conference on Data Mining (ICDM)</title>
		<meeting>the Fifth IEEE International Conference on Data Mining (ICDM)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wojciech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Le-Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Libsvm: A library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Chung And</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
		<idno>27:1-27:27</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Corwin. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosa</forename><forename type="middle">L</forename><surname>De Compadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gargi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">J</forename><surname>Shusterman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hansch</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Chem</title>
		<imprint>
			<biblScope unit="issue">34</biblScope>
			<biblScope unit="page" from="786" to="797" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distinguishing enzyme structures from non-enzymes without alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">D</forename><surname>Dobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Doig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Molecular Biology</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The weisfeiler-lehman method and graph isomorphism testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><forename type="middle">L</forename><surname>Douglas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1101.5211</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rafael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2215" to="2223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised learning of distributions of binary vectors using two layer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunihiko</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On graph kernels: Hardness results and efficient alternatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Gaertner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual Conference on Computational Learning Theory</title>
		<meeting>the 16th Annual Conference on Computational Learning Theory</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="129" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Convolution kernels on discrete structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Haussler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of California at Santa Cruz</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Receptive fields and functional architecture of monkey striate cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">H</forename><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wiesel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Torsten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Physiology</title>
		<imprint>
			<biblScope unit="volume">195</biblScope>
			<biblScope unit="page" from="215" to="243" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Counting belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sriraam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI)</title>
		<meeting>the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="277" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Power iterated color refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mladenov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the Twenty-Eighth AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1904" to="1910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The skew spectrum of graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning (ICML)</title>
		<meeting>the 25th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="496" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Diffusion kernels on graphs and other discrete input spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Machine Learning (ICML)</title>
		<meeting>the 19th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="315" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The graphlet spectrum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Machine Learning (ICML)</title>
		<meeting>the 26th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>L?on</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Community structure in large networks: Natural cluster sizes and the absence of large well-defined clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internet Mathematics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="123" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<title level="m">Gated graph sequence neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Isomorphism of graphs of bounded valence can be tested in polynomial time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><forename type="middle">M</forename><surname>Luks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="issue">25</biblScope>
			<biblScope unit="page" from="42" to="65" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Practical graph isomorphism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><forename type="middle">D</forename><surname>Mckay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adolfo</forename><surname>Piperno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">{II}. Journal of Symbolic Computation</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="94" to="112" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Network motifs: simple building blocks of complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Milo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shen-Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Itzkovitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shalev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kashtan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitri</forename><surname>Chklovskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">298</biblScope>
			<biblScope unit="issue">5594</biblScope>
			<biblScope unit="page" from="824" to="827" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The complexity of mckays canonical labeling algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takunari</forename><surname>Miyazaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Groups and Computation II</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="239" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The structure of scientific collaboration networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ej</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="404" to="409" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graph invariant kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Orsini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Raedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="678" to="689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Petri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tobias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>the 12th International Conference on Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Statistical evaluation of the predictive toxicology challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toivonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hannu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ashwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">D</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Helma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1183" to="1193" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicol</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1201" to="1242" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Comparison of descriptor spaces for chemical compound retrieval and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikil</forename><surname>Wale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Data Mining (ICDM)</title>
		<meeting>the International Conference on Data Mining (ICDM)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="678" to="689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Atomnet: A deep convolutional neural network for bioactivity prediction in structure-based drug discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Izhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Dzamba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Heifets</surname></persName>
		</author>
		<idno>abs/1510.02855</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">A reduction of a graph to a canonical form and an algebra arising during this reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Lehman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="12" to="16" />
		</imprint>
	</monogr>
	<note type="report_type">Nauchno-Technicheskaya Informatsia</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
