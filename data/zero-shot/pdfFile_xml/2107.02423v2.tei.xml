<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Text-to-Image Synthesis Using Contrastive Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Georgia State University</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiulong</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Georgia State University</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Tak??</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)</orgName>
								<address>
									<addrLine>Masdar City</addrLine>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajshekhar</forename><surname>Sunderraman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Georgia State University</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihao</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Georgia State University</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Text-to-Image Synthesis Using Contrastive Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The goal of text-to-image synthesis is to generate a visually realistic image that matches a given text description. In practice, the captions annotated by humans for the same image have large variance in terms of contents and the choice of words. The linguistic discrepancy between the captions of the identical image leads to the synthetic images deviating from the ground truth. To address this issue, we propose a contrastive learning approach to improve the quality and enhance the semantic consistency of synthetic images. In the pretraining stage, we utilize the contrastive learning approach to learn the consistent textual representations for the captions corresponding to the same image. Furthermore, in the following stage of GAN training, we employ the contrastive learning method to enhance the consistency between the generated images from the captions related to the same image. We evaluate our approach over two popular text-to-image synthesis models, AttnGAN and DM-GAN, on datasets CUB and COCO, respectively. Experimental results have shown that our approach can effectively improve the quality of synthetic images in terms of three metrics: IS, FID and R-precision. Especially, on the challenging COCO dataset, our approach boosts the FID significantly by 29.60% over AttnGAN and by 21.96% over DM-GAN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The objective of the text-to-image synthesis problem is to generate high-quality images from the specific text descriptions. It is a fundamental problem with a wide range of practical applications, including art generation, image editing, and computer-aided design. Most recently proposed text-to-image synthesis methods <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b43">44]</ref> are based on Generative Adversarial Networks (GANs) <ref type="bibr" target="#b6">[7]</ref>. Conditioned on the text descriptions, the GAN-based models can generate realistic images with consistent semantic meaning. In practice, one image is associated to multiple captions in the datasets. These text descriptions annotated by humans for the same image are highly subjective and diverse in terms of contents and choice of words. Additionally, some text descriptions do not even provide sufficient semantic information to guide the image generation. The linguistic variance and inadequacy between the captions of the identical image leads to the synthetic images conditioned on them deviating from the ground truth.</p><p>To address this issue, we propose a novel contrastive learning approach to improve the quality and enhance the semantic consistency of synthetic images. In the image-text matching task, we pretrain an image encoder and a text encoder to learn the semantically consistent visual and textual representations of the image-text pair. Meanwhile, we learn the consistent textual representations by pushing together the captions of the same image and pushing way the captions of different images via the contrastive loss. The pretrained image encoder and text encoder are leveraged to extract consistent visual and textual features in the following stage of GAN training. Then we also utilize the contrastive loss to minimize the distance of the fake images generated from text descriptions related to the same ground truth image while maximizing those related to different ground truth images. We generalize the existing text-to-image models to a unified framework so that our approach can be integrated into them to improve their performance. We evaluate our approach over two popular base models, AttnGAN <ref type="bibr" target="#b41">[42]</ref> and DM-GAN <ref type="bibr" target="#b47">[48]</ref> on datasets CUB <ref type="bibr" target="#b36">[37]</ref> and COCO <ref type="bibr" target="#b19">[20]</ref>. The experimental results have shown that our approach can effectively improve the quality of the synthetic images in terms of Inception Score (IS) <ref type="bibr" target="#b26">[27]</ref>, Fr?chet Inception Distance (FID) <ref type="bibr" target="#b10">[11]</ref>, and R-precisions <ref type="bibr" target="#b41">[42]</ref>.</p><p>The contributions of our work can be summarized as follows: 1) We propose a novel contrastive learning approach to learn the semantically consistent visual and textual representations in the image-text matching task. 2) We propose a novel contrastive learning approach to enhance the semantic consistency of the synthetic images in the stage of GAN training. 3) Our approach can be incorporated into the exist-ing text-to-image models to improve their performance. Extensive experimental results demonstrate the effectiveness of our approach. Our source code is publicly available at https://github.com/huiyegit/T2I_CL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Text-to-Image Generation</head><p>Recently, a great number of studies <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b43">44]</ref> present promising results on the text-to-image synthesis task, most of which make use of GANs as the backbone model. We briefly summarizes some of them that are most related to our approach. Zhang et al. <ref type="bibr" target="#b44">[45]</ref> propose the stacked GAN architecture which produces images from low-resolution to high-resolution. AttnGAN <ref type="bibr" target="#b41">[42]</ref> presents an attention mechanism, where the Deep Attentional Multimodal Similarity Model (DAMSM) is able to compute the similarity between the generated image and the caption using both the global sentence level information and the finegrained word level information. DM-GAN <ref type="bibr" target="#b47">[48]</ref> introduces a dynamic memory generative adversarial network to generate high-quality images. It utilizes a dynamic memory module to refine the initial generated image, a memory writing gate to highlight the relevant text information and a response gate to update image representations. SD-GAN <ref type="bibr" target="#b42">[43]</ref> employs a Siamese structure with a pair of texts as input and trains the model with the contrastive loss. The conditional batch normalization is adopted for fine-grained image generation. Compared with the Siamese structure in SD-GAN, our approach is derived from the recent development of the contrastive learning paradigm, therefore, it has the advantage of better performance and less computational cost. Furthermore, we generalize our approach so that it can be applied to the existing GAN-based models for text-to-image synthesis. XMC-GAN <ref type="bibr" target="#b43">[44]</ref> has also applied the contrastive learning approach in the text-to-image generation. However, the objectives of contrastive loss in our approach are different from those of XMC-GAN. We compute the contrastive losses of the caption-caption pair and the fake-fake pair, which are complementary to the contrastive losses in XMC-GAN. In this work, we choose AttnGAN and DM-GAN as the base models to evaluate our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Image-Text Matching</head><p>The text-to-image synthesis involves the subtask imagetext matching, which refers to learning the joint imagetext representation to maximize the semantic similarity for an image-sentence pair. Liwei et al. <ref type="bibr" target="#b37">[38]</ref> and Michael et al. <ref type="bibr" target="#b39">[40]</ref> have leveraged the triplet loss to learn the joint image-text embedding for the image-text retrieval and the video-text representation for the video-text action retrieval task, respectively. Tao et al. <ref type="bibr" target="#b41">[42]</ref> propose the Deep At-tentional Multimodal Similarity Model (DAMSM) to learn the fine-grained image-text representation for text-to-image synthesis. DAMSM ( <ref type="figure" target="#fig_0">Figure 1a</ref>) trains an image encoder and a text encoder jointly to encode sub-regions of the image and words of the sentence to a common semantic space, and computes a fine-grained image-text matching loss for image generation. However, the variations exist in the text representations corresponding to the same image, which leads to the generated images deviating from the ground truth image. To address this issue, we utilize the contrastive learning approach to push together the text representations related to the identical image and push away the text representations related to different images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Contrastive Learning</head><p>Contrastive learning has recently attracted great interest due to its empirical success in self-supervised representation learning in computer vision. In the last two years, various contrastive methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39]</ref> for visual representations have been proposed. SimCLR <ref type="bibr" target="#b1">[2]</ref> presents three major findings to learn better representations, including composition of data augmentations, a learnable nonlinear transformation between the representation and the contrastive loss, and large batch size and training step. Similar to SimCLR, we adopt the simple contrastive learning framework. In order to integrate the contrative learning approach into the GAN-based models with simple implementation and small computational cost, our approach does not have the learnable nonlinear transformation or large batch size. We set the same training batch sizes as our baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Contrastive Learning for Pre-training</head><p>In the text-to-image synthesis task, the purpose of imagetext matching is to learn the text representations which are semantically consistent with the corresponding images. Since the text representations will be leveraged as the conditions to guide the image generation, it is beneficial to develop a more effective pre-training approach to improve the quality of synthetic images.</p><p>Inspired by recent contrastive learning algorithms, we propose a novel approach for the pre-training of image-text matching. We learn the textual representations to match the visual representations via the DAMSM loss. Moreover, we train the textual representations by pushing together the captions corresponding to the same image and pushing away the captions corresponding to different images via the contrastive loss. As illustrated in <ref type="figure" target="#fig_0">Figure 1b</ref>, our framework consists of the following three major components.</p><p>Data sampling. At each training step, we sample a minibatch of images x, captions t and captions t , where both captions t and t are corresponding to images x. For image-text matching, we consider two positive imagecaption pairs (x i , t i ) and (x i , t i ) for each image x i to calculate the DAMSM loss. Furthermore, we consider the caption-caption pair (t i , t i ) as the positive pair to calculate the contrastive loss.</p><p>Image encoder f and text encoder g. We adopt an image encoder f to extract the visual vector representations and sub-region features from the image samples. Furthermore, we utilize a text encoder g to extract the textual vector representations and word features from the text samples. The text encoder g is shared in the framework. Our architecture has the flexibility of allowing various choices of deep neural network models. To have a fair comparison with the baselines, we adopt the same Inception-v3 <ref type="bibr" target="#b30">[31]</ref> and Bidirectional Long Short-Term Memory (Bi-LSTM) <ref type="bibr" target="#b28">[29]</ref> to instantiate the image encoder f and text encoder g.</p><p>Loss function. Similar to the baselines, we adopt the DAMSM loss as image-text matching loss. Moreover, we define the contrastive loss on pairs of two branches of input captions. We compute the contrastive loss to minimize the distance of textual representations related to the same image while maximizing those related to different images. We utilize the Normalized Temperature-scaled Cross Entropy Loss (NT-Xent) <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b1">2]</ref> as the contrastive loss. Given a pair, let sim(a, b) = a T b/ a b denote the dot product between l 2 normalized a and b. Then the loss function for the ith sample is defined as</p><formula xml:id="formula_0">L(i) = ? log exp(sim(u i , u j )/? ) 2N k=1 1 k =i exp(sim(u i , u k )/? ) ,<label>(1)</label></formula><p>where the ith and jth sample make the positive pair, 1 k =i is an indicator function whose value is 1 iff k = i , ? denotes a temperature parameter and N is the batch size (e.g., N images and 2N captions). The overall contrastive loss is computed across all positive pairs in a minibatch, which can </p><formula xml:id="formula_1">v = f (x) // image representation 6: e = g(t)</formula><p>// text representation <ref type="bibr">7:</ref> e = g(t )</p><p>// text representation <ref type="bibr" target="#b7">8</ref>:</p><formula xml:id="formula_2">L 1 = DAMSM(v, e)</formula><p>// image-text matching loss 9:</p><formula xml:id="formula_3">L 2 = DAMSM(v, e )</formula><p>// image-text matching loss <ref type="bibr">10:</ref> L c = NT-Xent(e, e ) // Equation <ref type="formula" target="#formula_0">2   11</ref>:</p><formula xml:id="formula_4">L = L 1 + L 2 + L c 12:</formula><p>Update networks f and g to minimize L 13: end for be defined as</p><formula xml:id="formula_5">L c = 1 2N 2N i=1 L(i)<label>(2)</label></formula><p>Algorithm 1 summarizes the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Contrastive learning for GAN training</head><p>In practice, the captions annotated by humans for the same image have large variance in terms of contents and the choice of words, especially when the scenes are complex. The linguistic discrepancy between the captions of the identical image leads to the synthetic images conditioned on them deviating from the ground truth. Inspired by recent contrastive learning approaches, we apply the contrastive learning method to enhance the consistency between the generated images from the captions related to the same image and motivate them to be closer to the ground truth. Our approach consists of the following three major components.</p><p>Data sampling. The data sampling approach is similar to the one in the pre-training stage. At each training step, we sample a minibatch of images x, captions t and captions t in the same way as in Section 3.  generative model outputs the fake images x and x conditioned on captions t and t , respectively. Then we consider the image-image pair (x i , x i ) as the positive pair in the contrastive learning.</p><p>Model architecture. In this section, we derive our framework from the vanilla GANs step by step. GANs are a family of powerful generative models that estimate the data distribution through an adversarial learning process, in which a generator network G produces synthetic data given the input noise z and a discriminator network D distinguishes the true data from the generated data. The generator G is optimized to output realistic samples to fool the discriminator D. Formally, the generator G and discriminator D are following the minimax objective:</p><formula xml:id="formula_6">min G max D E x?Pr [log(D(x))]+ E z?Pz [log(1?D(G(z)))],<label>(3)</label></formula><p>where x is a real sample from the data distribution P r , and the input z is sampled from some prior distribution P z , such as a uniform or Gaussian distribution.</p><p>Most of the recent works <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b3">4]</ref> on the text-to-image synthesis problem are based on GANs. We generalize these approaches to a unified framework, as shown in <ref type="figure" target="#fig_1">Figure 2a</ref>. The framework is extended from the GANs with the auxiliary information e, which is encoded from the input caption t by a pre-trained text encoder g. Then both the generator G and discriminator D are conditioned on this textual condition e. The training process is similar to the standard GANs with the following objective: </p><formula xml:id="formula_7">LG1 = 1 N N i=1 log(1 ? D(G(zi, ei), ei)) 15: LG2 = 1 N N i=1 log(1 ? D(G(zi, e i ), e i )) 16: v = f (G(z, e))</formula><p>// image representation <ref type="bibr" target="#b16">17</ref>:</p><formula xml:id="formula_8">v = f (G(z, e ))</formula><p>// image representation <ref type="bibr" target="#b17">18</ref>: <ref type="formula" target="#formula_0">2   19</ref>:</p><formula xml:id="formula_9">L c = NT-Xent (v, v ) // Equation</formula><formula xml:id="formula_10">L G = L G1 + L G2 + ? c L c 20:</formula><p>Update G to minimize L G 21: end for</p><p>We further extend the generalized text-to-image framework to a Siamese structure and integrate the contrastive learning approach into it. As shown in <ref type="figure" target="#fig_1">Figure 2b</ref>, the image encoder f takes the fake images x and x as input, and extracts the visual representations v and v to compute the contrastive loss. The two branches of the architecture share the identical generator G, discriminator D, image encoder f and text encoder g. The image encoder f and text encoder g are pre-trained in the image-text matching task and work in the evaluation mode in the phase of GAN training.</p><p>Loss function. In addition to the adversarial losses from Equation 4, we define the contrastive loss on pairs of fake images generated from two branches of input captions. We utilize the contrastive loss to minimize the distance of the fake images generated from two text descriptions related to the same image while maximizing those related to different images. We apply the same NT-Xent loss in Section 3.1. Algorithm 2 summarizes the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets. Following the previous works, we evaluate our approach on datasets CUB <ref type="bibr" target="#b36">[37]</ref> and COCO <ref type="bibr" target="#b19">[20]</ref>. The CUB dataset contains 200 bird species with 11,788 images, where 150 species with 8,855 images are used as the training data, and the remaining 50 species with 2,933 images as the test data. Each image has 10 related captions in the CUB dataset. The COCO dataset has 80k images for training and 40k images for evaluation. Each image has 5 related captions in the COCO dataset. Evaluation Metric. We choose the Inception Score (IS) <ref type="bibr" target="#b26">[27]</ref>, Fr?chet Inception Distance (FID) <ref type="bibr" target="#b10">[11]</ref>, and Rprecisions <ref type="bibr" target="#b41">[42]</ref> as the quantitive metrics to evaluate the performance. The IS calculates the KL-divergence between the conditional and marginal probability distributions. In general, a larger IS indicates the generative model can synthesize fake images with better diversity and quality. The FID computes the Fr?chet distance between synthetic and real images in the feature space extracted from the pretrained Inception v3 model. A smaller FID indicates the synthetic data is more realistic and similar to the true data. R-precision calculates the precision of the image-text retrieval task to evaluate to what extent the synthetic images match the input captions. The higher R-precision indicates the generated images have greater consistency with the text descriptions. After training, the model generates 30,000 images conditioned on the captions in the test set for evaluation. The source code to calculate the three metrics is from the public website 1 .</p><p>Note that several previous works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b32">33]</ref> have pointed out that IS can not provide useful guidance to evaluate the quality of the synthetic images on dataset COCO. Nevertheless, we still report the IS on COCO as the auxiliary metric. We consider the FID as the primary metric among the three in terms of robustness and effectiveness. Implementation details. We choose two popular text-toimage synthesis models, AttnGAN <ref type="bibr" target="#b41">[42]</ref> and DM-GAN <ref type="bibr" target="#b47">[48]</ref>, to evaluate our approach. Note that both AttnGAN and  <ref type="table">Table 2</ref>. Comparison of our approach and DM-GAN over the datasets CUB and COCO. The notations ?, ?, * , bold font and CL have the same meanings as the ones in <ref type="table">Table 1</ref> DM-GAN are the stacked architecture with 3 generatordiscriminator pairs. To reduce the computational cost, we only calculate the contrastive loss of the fake images of size 256x256 from the last generator. We retain the setting of parameters in the original baselines except the ? value used in AttnGAN. We find that ? = 5 is reported in the paper and used in the source code. However, when we ran the source code of AttnGAN with this value, the R-precision is about 58.80, which has a large difference from 67.21 reported in the paper. When we changed ? to 10, we got 67.00 for R-precision, which is consistent with the one reported in the paper as well as the IS score and FID. We believe there is a typo of ? value in the AttnGAN paper. Therefore, we set ? = 10 and adopt it in all of our experiments.</p><p>Following the same setting of the configuration files of the baselines, we train our novel model based on AttnGAN with 600 epochs on CUB and 120 epochs on COCO, and the other model based on DM-GAN with 800 epochs on CUB and 200 epoch on COCO. We evaluate the IS, FID and Rprecision of the checkpoint every 50 epochs on CUB and 10 epochs on COCO. We choose the checkpoint with the best FID and report the corresponding IS and R-precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Text-to-Image Quality</head><p>We apply our contrastive learning approach to two baselines AttnGAN <ref type="bibr" target="#b41">[42]</ref> and DM-GAN <ref type="bibr" target="#b47">[48]</ref>, and compare the performances with them over datasets CUB and COCO. The experimental results are reported in <ref type="table">Table 1</ref> and 2.</p><p>As shown in <ref type="table">Table 1</ref>, our approach improves the three metrics IS, FID and R-precision over the datasets CUB and COCO. The IS is improved from 4.33 to 4.42 on CUB and the bird has a yellow crown and a black eyering that is round AttnGAN AttnGAN + CL</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DM-GAN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DM-GAN + CL</head><p>this bird has a green crown black primaries and a white belly this bird has wings that are black and has a white belly this bird has wings that are red and has a yellow belly this bird has wings that are blue and has a red belly this small bird has a deep blue crown, back and rump and a bright white belly a blue bird with a white throat, breast, belly and abdomen and a small black pointed beak yellow abdomen with a black eye without an eye ring and beak quite short <ref type="figure">Figure 3</ref>. Comparison of example images between our approach and baselines on the CUB dataset.    <ref type="figure">Figure 5</ref>. Comparison of example images between our approach and baselines. from 23.71 to 25.70 on COCO. For the relatively more suitable metric FID, our approach boosts the baseline AttnGAN significantly by 21% on CUB and 29.60% on COCO, respectively. Meanwhile, our approach achieves higher Rprecision with the gain of 2.55 on CUB and 2.58 on COCO. As shown in <ref type="table">Table 2</ref>, in comparison to DM-GAN, our approach also improves the three metrics IS, FID and Rprecision over CUB and COCO. The IS is improved from 4.66 to 4.77 on CUB and from 32.37 to 33.34 on COCO.</p><p>Regarding the metric FID , our approach has the value of 14.38 with a small gain of 0.72 on CUB, while boosts the baseline DM-GAN significantly by 21.96% on COCO. Furthermore, our approach achieves better R-Precision with the improvement of 3.13 on CUB and 1.31 on COCO. The dataset COCO is more challenging than CUB, as it has more complex scenes and the captions have greater variance to describe the identical image. However, it is noteworthy that our approach significantly improves the FID by 29.60% over the baseline AttnGAN and 21.96% over DM-GAN. In summary, the quantitative experimental results demonstrate that our contrastive learning approach can effectively improve the quality and enhance the consistency of the synthetic images generated from diverse captions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Visual Quality</head><p>To further compare our proposed approach with the baselines, we visualize the synthetic images generated from the typical example captions. As shown in <ref type="figure">Figure 3</ref>, compared with the baseline AttnGAN, the images generated from our approach are more realistic and better match with the text descriptions in most cases. In the 8th column, the bird in the image from AttnGAN fails seriously with two heads, while the one from our approach has the reasonable appearance. In the 2nd column, we can see the vivid green crown in the bird from our approach, which matches the description "green crown" well, while the image from AttnGAN does not show this feature of the bird. As shown in <ref type="figure">Figure 3</ref>, the comparison between our approach and the baseline DM-GAN is similar to previous comparison. In the 3rd column, the image from our approach has the correct white belly to match the text description "while belly", while the image from DM-GAN has the additional incorrect red color in the belly. <ref type="figure" target="#fig_4">Figure 4</ref> shows the example images on COCO from our approach and the baselines AttnGAN and DM-GAN. It is challenging to generate photo-realistic images for the models showed in the figure. However, compared with the baselines, the images generated from our approach are more realistic and better match with the text descriptions in some cases. In the 4th column, the boat in the image from our approach has the red and while color, which aligns with the caption, while the image from AttnGAN does not show the red boat. In the 5th column, the image from our approach has better shape of cows than the one from AttnGAN. As shown in the 3rd column, the image from our approach has the basic shape of a girl, while it can not be observed in the image from DM-GAN at all.  <ref type="table">Table 4</ref>. Ablation Study of our approach on DM-GAN over the CUB and COCO datasets. ? , CL1 and CL2 have the same meanings as the ones in <ref type="table" target="#tab_4">Table 3</ref>.</p><p>We also visualize the example images generated from multiple captions corresponding to the same ground truth image. Compared with the baselines AttnGAN and DM-GAN, the images generated from our approach are more realistic and closer to the ground truth images. As shown in the 1st column of <ref type="figure">Figure 5(a)</ref>, the image from our approach has the black color in the wings, which is consistent with the ground truth image. Although the 1st caption does not have the text description of "black color" explicitly, our model is still able to train its semantic text embedding to contain this information from other captions related to the same image via our contrastive learning approach. Another similar example is shown in <ref type="figure">Figure 5</ref>(c). The 3rd caption does not provide the text description of "the ocean", the image from our approach can still have the ocean scene to be consistent with the ground truth, while the baseline DM-GAN is not able to achieve this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>In this work, the contrastive learning approach is applied in two stages: image-text matching and the training of GANs. We combine our novel approach for the imagetext matching with the baselines AttnGAN and DM-GAN, and conduct experiments to evaluate the effectiveness of it. The experimental results are shown in <ref type="table" target="#tab_4">Table 3</ref> and 4. Compared with the two baselines AttnGAN and DM-GAN, our contrastive learning approach for the image-text matching task can help improve the performance in terms of the IS, FID and R-precision on datasets CUB and COCO, with one exception that the IS of our approach is about 0.55 smaller than DM-GAN on COCO. When we add the contrastive  <ref type="table">Table 6</ref>. Ablation study on different choices of temperature ? for contrastive loss.</p><p>learning approach for GAN training into our previous approach, our complete approach shows further improvements in terms of the IS, FID and R-precision on datasets CUB and COCO, with the exception that the IS of our complete approach based on AttnGAN is about 0.11 smaller than our previous one on COCO. The experimental results demonstrate that our contrastive learning approach in the imagetext matching task and GAN training can help improve the performance of text-to-image synthesis, respectively. We adjust the hyperparameters to investigate the impact to the performance of our approach. We tune the weight ? c in {0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0} and the temperature ? in {0.1, 0.2, 0.5, 1.0} for the contrastive loss. The experiments are conducted on the CUB dataset. In each case, we evaluate the checkpoint every 50 epochs and choose the one with the best FID. <ref type="table">Table 5</ref> shows the results of weight ? c . We find that FID is not very sensitive to this hyperparameter as well as IS and R-precision. When ? c is 0.2, the model has the best FID score 16.34, which is improved by 1.42, compared with the worst value 17.76. <ref type="table">Table 6</ref> shows the results of temperature ? . Similar to the weight ? c , it can be observed that ? has a small impact to the performance of the model. The difference between the largest FID and smallest one is 1.34.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have shown how to incorporate the contrastive learning method into prior text-to-image models to improve their performance. Firstly, we train the imagetext matching task to push together the textual representations corresponding to the same image through the contrastive loss. Furthermore, we employ the contrastive learning method to enhance the consistency between generated images from the captions related to the same image. We propose a generalized framework for the existing text-toimage models, and evaluate our approach on two baselines AttnGAN and DM-GAN. Extensive experiments demonstrate that our approach outperforms the two strong baselines in terms of three metrics. Especially, on the challenging COCO dataset, our approach boosts the FID significantly by 29.60% over AttnGAN and by 21.96% over DM-GAN. Since the image-text representation learning is a fundamental task, we believe our approach has potential applicability in a wide range of cross domain tasks, such as visual question answering, image-text retrieval as well as text-to-image synthesis. We leave the extension to these tasks as a future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Architectures of original DAMSM and our approach for image-text matching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Architectures of original approach and our approach for text to image synthesis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(D(x, e))]+ E z?Pz [log(1?D(G(z, e), e))] (4) Algorithm 2 Contrastive learning for GAN training Input: Batch size N , temperature ? , coefficient ?c, generator G, discriminator D, pre-trained image encoder f , pre-trained text encoder g. Output: Optimized G and D. 1: for {1, ? ? ? , # of training iterations} do 2: Sample a minibatch of images x ? Pr 3: Sample a minibatch of latent variable z ? Pz 4: Sample a minibatch of captions t associated with x 5: Sample another minibatch of captions t associated with x 6: e = g(t) 7: e = g(t ) 8: LD1 = 1 N N i=1 [log D(xi, ei)+log(1?D(G(zi, ei), ei))] 9: LD2 = 1 N N i=1[log D(xi, e i )+log(1?D(G(zi, e i ), e i ))] 10: LD = LD1 + LD2 11: Update D to minimize LD 12: Sample noise z, captions t and t as step 3, 4 and 513:  Compute e, e as step 6 and 714:  </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Comparison of example images between our approach and baselines on the COCO dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>COCO 23.71 ? .38 33.99 83.97 ? .78 AttnGAN + CL COCO 25.70 ? .62 23.93 86.55 ? .51 Table 1. Comparison of our approach and AttnGAN over the datasets CUB and COCO. ? denotes the higher value the better quality. ? denotes the lower value the better quality. * indicates the results are obtained from the pre-trained model released publicly by the authors. The bold font represents better performance. CL denotes the proposed constrastive learning approach in this work.</figDesc><table><row><cell>Method</cell><cell>Dataset</cell><cell>IS ?</cell><cell cols="2">FID ? R-Precision ?</cell></row><row><cell>AttnGAN  *</cell><cell>CUB</cell><cell>4.33 ? .07</cell><cell>20.85</cell><cell>67.09 ? .83</cell></row><row><cell>AttnGAN + CL</cell><cell>CUB</cell><cell>4.42 ? .05</cell><cell>16.34</cell><cell>69.64 ? .63</cell></row><row><cell>AttnGAN  Method</cell><cell>Dataset</cell><cell>IS ?</cell><cell cols="2">FID ? R-Precision ?</cell></row><row><cell>DM-GAN  *</cell><cell>CUB</cell><cell>4.66 ? .06</cell><cell>15.10</cell><cell>75.86 ? .83</cell></row><row><cell>DM-GAN + CL</cell><cell>CUB</cell><cell>4.77 ? .05</cell><cell>14.38</cell><cell>78.99 ? .66</cell></row><row><cell>DM-GAN  *</cell><cell>COCO</cell><cell>32.37? .29</cell><cell>26.64</cell><cell>92.09 ? .50</cell></row><row><cell cols="4">DM-GAN + CL COCO 33.34 ? .51 20.79</cell><cell>93.40 ? .39</cell></row></table><note>1 https://github.com/MinfengZhu/DM-GAN*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Ablation study of our approach on AttnGAN over the datasets CUB and COCO. ? indicates we retrain the model with the same setting of hyperparameters. CL1 and CL2 denote the constrastive learning approach in the pre-training and GAN training, respectively.</figDesc><table><row><cell>Method</cell><cell>Dataset</cell><cell>IS ?</cell><cell cols="2">FID ? R-Precision ?</cell></row><row><cell>AttnGAN  ?</cell><cell>CUB</cell><cell>4.29 ? .05</cell><cell>19.16</cell><cell>68.02 ? .98</cell></row><row><cell>+ CL1</cell><cell>CUB</cell><cell>4.31 ? .02</cell><cell>17.97</cell><cell>69.11 ? .63</cell></row><row><cell>+ CL1 + CL2</cell><cell>CUB</cell><cell>4.42 ? .05</cell><cell>16.34</cell><cell>69.64 ? .63</cell></row><row><cell>AttnGAN  ?</cell><cell cols="3">COCO 25.05 ? .64 30.67</cell><cell>84.24 ? .58</cell></row><row><cell>+ CL1</cell><cell cols="3">COCO 25.87 ? .41 26.89</cell><cell>85.93 ? .63</cell></row><row><cell cols="4">+ CL1 + CL2 COCO 25.70 ? .62 23.93</cell><cell>86.55 ? .51</cell></row><row><cell>Method</cell><cell>Dataset</cell><cell>IS ?</cell><cell cols="2">FID ? R-Precision ?</cell></row><row><cell>DM-GAN  ?</cell><cell>CUB</cell><cell>4.67 ? .06</cell><cell>15.55</cell><cell>75.88 ? .89</cell></row><row><cell>+ CL1</cell><cell>CUB</cell><cell>4.71 ? .05</cell><cell>14.56</cell><cell>76.74 ? .88</cell></row><row><cell>+ CL1 + CL2</cell><cell>CUB</cell><cell>4.77 ? .05</cell><cell>14.38</cell><cell>78.99 ? .66</cell></row><row><cell>DM-GAN  ?</cell><cell cols="3">COCO 31.53 ? .39 27.04</cell><cell>91.82 ? .49</cell></row><row><cell>+ CL1</cell><cell cols="3">COCO 30.98 ? .69 25.29</cell><cell>92.10 ? .56</cell></row><row><cell cols="4">+ CL1 + CL2 COCO 33.34 ? .51 20.79</cell><cell>93.40 ? .39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>.28 ? .05 17.60 70.06 ? .84 0.2 4.42 ? .05 16.34 69.64 ? .63 0.5 4.35 ? .05 17.76 69.08 ? .63 1.0 4.31 ? .07 16.72 69.28 ? .77 2.0 4.41 ? .05 16.90 68.79 ? .38 5.0 4.49 ? .08 17.19 67.64 ? .85 10.0 4.43 ? .07 17.53 70.25 ? .55 Table 5. Ablation study on different choices of weight ?c for contrastive loss. .44 ? .06 17.12 69.62 ? .78 0.2 4.44 ? .05 17.55 68.50 ? .85 0.5 4.42 ? .05 16.34 69.64 ? .63 1.0 4.43 ? .05 17.68 69.86 ? .92</figDesc><table><row><cell>Method</cell><cell>? c</cell><cell>IS ?</cell><cell>FID ? R-Precision ?</cell></row><row><cell cols="2">AttnGAN + CL 0.1 4Method ?</cell><cell>IS ?</cell><cell>FID ? R-Precision ?</cell></row><row><cell></cell><cell>0.1 4</cell><cell></cell><cell></cell></row><row><cell>AttnGAN + CL</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank the anonymous reviewers for their comments and suggestions, which helped improve the quality of this paper. We would also gratefully acknowledge the support of VMware Inc. for its university research fund to this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adversarial learning of semantic relevance in text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Youngjune L Gwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3272" to="3279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rifegan: Rich feature generation for text-toimage synthesis from prior knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanling</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10911" to="10920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Yen-Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00224</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Debiased contrastive learning</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tell, draw, and repeat: Generating and modifying images based on continual linguistic instruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Layla</forename><forename type="middle">El</forename><surname>Asri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10304" to="10312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Contrastive multi-view representation learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaveh</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amir Hosein Khasahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4116" to="4126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Henaff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4182" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6629" to="6640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Generating multiple objects at spatially distinct locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Hinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wermter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00686</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inferring semantic layout for hierarchical textto-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingdong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7986" to="7994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image generation from scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1219" to="1228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bulent</forename><surname>Mert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noe</forename><surname>Sariyildiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pion</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01028</idno>
		<title level="m">Philippe Weinzaepfel, and Diane Larlus. Hard negative mixing for contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Controllable text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Object-driven text-to-image synthesis via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12174" to="12182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cpgan: Contentparsing generative adversarial networks for text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiadong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="491" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6707" to="6717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learn, imagine and create: Text-to-image generation from prior knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duanqing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="887" to="897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mirrorgan: Learning text-to-image generation by redescription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duanqing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1505" to="1514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12092</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1060" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrit</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04592</idno>
		<title level="m">Contrastive learning with hard negative samples</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A theoretical analysis of contrastive unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikunj</forename><surname>Saunshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orestis</forename><surname>Plevrakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrishikesh</forename><surname>Khandeparkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5628" to="5637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multiclass n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1857" to="1865" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Text2scene: Generating compositional scenes from textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuwen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6710" to="6719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songsong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yuan</forename><surname>Jing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.05865</idno>
		<title level="m">Deep fusion generative adversarial networks for text-to-image synthesis</title>
		<meeting><address><addrLine>Df-gan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">What makes for good views for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10243</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Retrievegan: Image synthesis via differentiable patch retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yu</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="242" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Pietro Perona, and Serge Belongie. The caltech-ucsd birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning deep structure-preserving image-text embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5005" to="5013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9929" to="9939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fine-grained action retrieval through multiple partsof-speech embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="450" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attngan: Finegrained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1316" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Semantics disentangling for text-toimage generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2327" to="2336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Cross-modal contrastive learning for text-toimage generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yu Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="833" to="842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Stack-gan++: Realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1947" to="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Photographic text-to-image synthesis with a hierarchically-nested adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanpu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6199" to="6208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dm-gan: Dynamic memory generative adversarial networks for textto-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minfeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingbo</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5802" to="5810" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
