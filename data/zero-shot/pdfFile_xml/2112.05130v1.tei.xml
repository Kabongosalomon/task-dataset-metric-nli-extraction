<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal Conditional Image Synthesis with Product-of-Experts GANs NVIDIA sketch (b) text (a) (d) seg (c) (e) (f) (g)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
						</author>
						<title level="a" type="main">Multimodal Conditional Image Synthesis with Product-of-Experts GANs NVIDIA sketch (b) text (a) (d) seg (c) (e) (f) (g)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing conditional image synthesis frameworks generate images based on user inputs in a single modality, such as text, segmentation, sketch, or style reference. They are often unable to leverage multimodal user inputs when available, which reduces their practicality. To address this limitation, we propose the Product-of-Experts Generative Adversarial Networks (PoE-GAN) framework, which can synthesize images conditioned on multiple input modalities or any subset of them, even the empty set. PoE-GAN consists of a productof-experts generator and a multimodal multiscale projection discriminator. Through our carefully designed training scheme, PoE-GAN learns to synthesize images with high quality and diversity. Besides advancing the state of the art in multimodal conditional image synthesis, PoE-GAN also outperforms the best existing unimodal conditional image synthesis approaches when tested in the unimodal setting. The project website is available at this link.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>sketch seg text (g) Result using all inputs (text + sketch + seg) Results using a subset of inputs Inputs input combinations <ref type="figure">Figure 1</ref>. Given conditional inputs in multiple modalities (the left column), our approach can synthesize images that satisfy all input conditions (the right column, (g)) or any arbitrary subset of the input conditions (the middle column, (a)-(f)) with a single model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Conditional image synthesis allows users to use their creative inputs to control the output of image synthesis methods. It has found applications in many content creation tools. Over the years, a variety of input modalities have been studied, mostly based on conditional GANs <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b40">41]</ref>. To this end, we have various single modality-to-image models. When the input modality is text, we have the text-to-image model <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b81">82]</ref>. When the input modality is a segmentation mask, we have the segmentation-to-image model <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b64">65]</ref>. When the input modality is a sketch, we have the sketch-to-image model <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b51">52]</ref>.</p><p>However, different input modalities are best suited for conveying different types of conditioning information. For example, as seen in the first column of <ref type="figure">Fig. 1</ref>, segmentation makes it easy to define the coarse layout of semantic classes in an image-the relative locations and sizes of sky, cloud, mountain, and water regions. Sketch allows us to specify the structure and details within the same semantic region, such as individual mountain ridges. On the other hand, text is well-suited for modifying and describing objects or regions in the image, which cannot be achieved by using segmentation or sketch, e.g. 'frozen lake' and 'pink clouds' in <ref type="figure">Fig. 1</ref>. Nevertheless, despite this synergy among modalities, prior work has considered image generation conditioned on each modality as a distinct task and studied it in isolation. Existing models thus fail to utilize complementary information available in different modalities. Clearly, a conditional generative model that can combine input information from all available modalities would be of immense value.</p><p>Even though the benefits are enormous, the task of conditional image synthesis with multiple input modalities poses several challenges. First, it is unclear how to combine multiple modalities with different dimensions and structures in a single framework. Second, from a practical standpoint, the generator needs to handle missing modalities since it is cumbersome to ask users to provide every single modality all the time. This means that the generator should work well even when only a subset of modalities are provided. Lastly, conditional GANs are known to be susceptible to mode collapse <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b40">41]</ref>, wherein the generator produces identical images when conditioned on the same inputs. This makes it difficult for the generator to produce diverse output images that capture the full conditional distribution when conditioned on an arbitrary set of modalities.</p><p>We present Product-of-Experts Generative Adversarial Networks (PoE-GAN), a framework that can generate images conditioned on any subset of the input modalities presented during training, as illustrated in <ref type="figure">Fig. 1 (a)</ref>-(g). This framework provides users unprecedented control, allowing them to specify exactly what they want using multiple complementary input modalities. When users provide no inputs, it falls back to an unconditional GAN model <ref type="bibr">[2, 12, 21, 23-25, 39, 47]</ref>. One key ingredient of our framework is a novel product-of-experts generator that can effectively fuse multimodal user inputs and handle missing modalities (Sec. 3.1). A novel hierarchical and multiscale latent representation leads to better usage of the structure in spatial modalities, such as segmentation and sketch (Sec. 3.2). Our model is trained with a multimodal projection discriminator (Sec. 3.4) together with contrastive losses for better input-output alignment. In addition, we adopt modality dropout for additional robustness to missing inputs (Sec. <ref type="bibr">3.5)</ref>. Extensive experiment results show that PoE-GAN outperforms prior work in both multimodal and unimodal settings (Sec. 4), including state-of-the-art approaches specifically designed for a single modality. We also show that PoE-GAN can generate diverse images when conditioned on the same inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Image Synthesis. Our network architecture design is inspired by previous work in unconditional image synthesis. Our decoder employs some techniques proposed in Style-GAN <ref type="bibr" target="#b23">[24]</ref> such as global modulation. Our latent space is constructed in a way similar to hierarchical variational autoencoders (VAEs) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b61">62]</ref>. While hierarchical VAEs encode the image itself to the latent space, our network encodes conditional information from different modalities into a unified latent space. Our discriminator design is inspired by the projection discriminator <ref type="bibr" target="#b39">[40]</ref> and multiscale discriminators <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b64">65]</ref>, which we extend to our multimodal setting.</p><p>Multimodal Image Synthesis. Prior work <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b67">68]</ref> has used VAEs <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b49">50]</ref> to learn the joint distribution of multiple modalities. Some of them <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b67">68]</ref> use a product-of-experts inference network to approximate the posterior distribution. This is conceptually similar to how our generator combines information from multiple modalities. While their goal is to estimate the complete joint distribution, we focus on learning the image distribution conditioned on other modalities. Besides, our framework is based on GANs rather than VAEs and we perform experiments on high-resolution and large-scale datasets, unlike the above work. Recently, Xia et al. <ref type="bibr" target="#b68">[69]</ref> propose a GANbased multimodal image synthesis method named TediGAN. Their method relies on a pretrained unconditional generator. However, such a generator is difficult to train on a complex dataset such as MS-COCO <ref type="bibr" target="#b30">[31]</ref>. Concurrently, Zhang et al. <ref type="bibr" target="#b79">[80]</ref> propose a multimodal image synthesis method based on VQGAN. The way they combine different modalities is similar to our baseline using concatenation and modality dropout (Sec. 4.2). We will show that our product-of-experts generator design significantly improves upon this baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Product-of-experts GANs</head><p>Given a dataset of images x paired with M different input modalities (y 1 , y 2 , ..., y M ), our goal is to train a single generative model that learns to capture the image distribution conditioned on an arbitrary subset of possible modalities p(x|Y), ?Y ? {y 1 , y 2 , ..., y M }. In this paper, we consider four different modalities including text, semantic segmentation, sketch, and style reference. Note that our framework is general and can easily incorporate additional modalities. S1 S2 S1 ? S2 (a) Intersection of sets Learning image distributions conditioned on any subset of M modalities is challenging because it requires a single generator to simultaneously model 2 M distributions. Of particular note, the generator needs to capture the unconditional image distribution p(x) when Y is an empty set, and the unimodal conditional distributions p(x|y i ), ?i ? {1, 2, ..., M }, such as the image distribution conditioned on text alone. These settings have been popular and widely studied in isolation, and we aim to bring them all under a unified framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Product-of-experts modeling</head><p>Intuitively, each input modality adds a constraint the synthesized image must satisfy. The set of images that satisfies all constraints is the intersection of the sets each of which satisfies one individual constraint. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, we model this by making a strong assumption that the jointly conditioned probability distribution p(x|y i , y j ) is proportional to the product of singly conditioned probability distributions p(x|y i ) and p(x|y j ). Under this setting, for the product distribution to have a high density in a region, each of the individual distributions needs to have a high density in that region, thereby satisfying each constraint. This idea of combining several distributions ("experts") by multiplying them has been previously referred to as product-of-experts <ref type="bibr" target="#b16">[17]</ref>.</p><p>Our generator is trained to map a latent code z to an image x. Since the output image is uniquely determined by the latent code, the problem of estimating p(x|Y) is equivalent to that of estimating p(z|Y). We use product-of-experts to model the conditional latent distribution</p><formula xml:id="formula_0">p(z|Y) ? p (z) yi?Y q(z|y i ) ,<label>(1)</label></formula><p>where p (z) is the prior distribution and each expert q(z|y i ) is a distribution predicted by the encoder of that single modality. When no modalities are given, the latent distribution is simply the prior. As more modalities are provided, the number of constraints increases, the space of possible output images shrinks, and the latent distribution becomes narrower.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multiscale and hierarchical latent space</head><p>Some of the modalities we consider (e.g., sketch, segmentation) are two-dimensional and naturally contain information at multiple scales. Therefore, we devise a hierarchical  latent space with latent variables at different resolutions. This allows us to directly pass information from each resolution of the spatial encoder to the corresponding resolution of the latent space, so that the high-resolution control signals can be better preserved. Mathematically, our latent code is partitioned into groups z = (z 0 , z 1 , ..., z N ) where z 0 ? R c0 is a feature vector and z k ? R c k ?r k ?r k , 1 ? k ? N are feature maps of increasing resolutions (r k+1 = 2r k , r 1 = 4, r N is the image resolution). We can therefore decompose the prior p (z) into N k=0 p (z k |z &lt;k ) and the experts q(z|y i ) into N k=0 q(z k |z &lt;k , y i ). This design is similar to the prior and posterior networks in hierarchical VAEs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b61">62]</ref>. The difference is that our encoder encodes the conditional information in the input modalities rather than the image itself. Following Eq. (1), we assume the conditional latent distribution at each resolution is a product-of-experts</p><formula xml:id="formula_1">p(z k |z &lt;k , Y) ? p (z k |z &lt;k ) yi?Y q(z k |z &lt;k , y i ) ,<label>(2)</label></formula><p>where p (z k |z &lt;k ) = N ? k 0 , ? k 0 and q(z k |z &lt;k , y i ) = N ? k i , ? k i are independent Gaussian distributions with mean and standard deviation parameterized by a neural network. <ref type="bibr" target="#b0">1</ref> It can be shown <ref type="bibr" target="#b66">[67]</ref> that the product of Gaussian experts is also a Gaussian p(z k |z &lt;k , Y) = N (? k , ? k ), with <ref type="figure" target="#fig_2">Figure 3</ref> shows an overview of our generator architecture. We encode each modality into a feature vector which is then aggregated in Global PoE-Net. We use convolutional networks with input skip connections to encode segmentation 1 Except for p (z 0 ), which is simply a standard Gaussian distribution. and sketch maps, a residual network to encode style images, and CLIP <ref type="bibr" target="#b45">[46]</ref> to encode text. Details of all modality encoders are given in Appendix B. The decoder generates the image using the output of Global PoE-Net and skip connections from the segmentation and sketch encoders.</p><formula xml:id="formula_2">? k = ? k 0 (? k 0 ) 2 + i ? k i (? k i ) 2 1 (? k 0 ) 2 + i 1 (? k i ) 2 ?1 , ? k = 1 (? k 0 ) 2 + i 1 (? k i ) 2 ?1 .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Generator architecture</head><p>In Global PoE-Net ( <ref type="figure">Fig. 4</ref>), we predict a Gaussian q(z 0 |y i ) = N ? 0 i , ? 0 i from the feature vector of each modality using an MLP. We then compute the product of Gaussians including the prior p (z 0 ) = N (? 0 0 , ? 0 0 ) = N (0, I) and sample z 0 from the product distribution. We use another MLP to convert z 0 to another feature vector w.</p><p>The decoder mainly consists of a stack of residual blocks 2 <ref type="bibr" target="#b14">[15]</ref>, each of which is shown in <ref type="figure">Fig. 5</ref>. Local PoE-Net samples the latent feature map z k at the current resolution from the product of p (z k |z &lt;k ) = N ? k 0 , ? k 0 and</p><formula xml:id="formula_3">q(z k |z &lt;k , y i ) = N ? k i , ? k i , ?y i ? Y, where ? k 0 , ? k 0</formula><p>is computed from the output of the last layer and ? k i , ? k i is computed by concatenating the output of the last layer and the skip connection from the corresponding modality. Note that only modalities that have skip connections (segmentation and sketch, i.e. i = 1, 4) contribute to the computa- <ref type="bibr" target="#b1">2</ref> Except for the first layer that convolves a constant feature map and the last layer that convolves the previous output to synthesize the output.  LG</p><formula xml:id="formula_4">-AdaIN(h k , z k , w) = ? w ? z k h k ??(h k ) ?(h k ) + ? z k + ? w ,<label>(4)</label></formula><p>where h k is a feature map in the residual branch after convolution, ?(h k ) and ?(h k ) are channel-wise mean and standard deviation. ? w , ? w are feature vectors computed from w, while ? z k , ? z k are feature maps computed from z k . The LG-AdaIN layer can be viewed as a combination of AdaIN <ref type="bibr" target="#b17">[18]</ref> and SPADE <ref type="bibr" target="#b42">[43]</ref> that takes both a global feature vector and a spatially-varying feature map to modulate the activations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Multiscale multimodal projection discriminator</head><p>Our discriminator receives the image x and a set of conditions Y as inputs and produces a score D(x, Y) = sigmoid(f (x, Y)) indicating the realness of x given Y . Under the GAN objective <ref type="bibr" target="#b11">[12]</ref>, the optimal solution of f is</p><formula xml:id="formula_5">f * (x, Y) = log q(x) p(x) unconditional term + yi?Y log q(y i |x) p(y i |x) conditional term ,<label>(5)</label></formula><p>if we assume conditional independence of different modalities given x. The projection discriminator (PD) <ref type="bibr" target="#b39">[40]</ref> proposes to use the inner product to estimate the conditional term. This implementation restricts the conditional term to be relatively simple, which imposes a good inductive bias that leads to strong empirical results. We propose a multimodal projection discriminator (MPD) that generalizes PD to our multimodal setting. As shown in <ref type="figure">Fig. 6</ref>, the original PD first encodes both the image and the conditional input into a shared latent space. It then uses a linear layer to estimate the unconditional term from the image embedding and uses the inner product between the image embedding and the conditional embedding to estimate the conditional term. The unconditional term and the conditional term are summed to obtain the final discriminator logits. In our multimodal scenario, we simply encode each observed modality and add its inner product with the image embedding to the final loss</p><formula xml:id="formula_6">f (x, Y) = Linear(D x (x)) + yi?Y D T yi (y i )D x (x) . (6)</formula><p>For spatial modalities such as segmentation and sketch, it is more effective to enforce their alignment with the image in multiple scales <ref type="bibr" target="#b33">[34]</ref>. As shown in <ref type="figure">Fig. 7</ref>, we encode the image and spatial modalities into feature maps of different resolutions and compute the MPD loss at each resolution. We compute a loss value at each location and resolution, and obtain the final loss by averaging first across locations then across resolutions. We name the resulting discriminator as the multiscale multimodal projection discriminator (MMPD) and describe its details in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Losses and training procedure</head><p>Latent regularization. Under the product-of-experts assumption (Eq. (1)), the marginalized conditional latent distribution should match the unconditional prior distribution:</p><formula xml:id="formula_7">p(z|y i )p(y i )dy i = p(z|?) = p (z) .<label>(7)</label></formula><p>To this end, we minimize the Kullback-Leibler (KL) divergence from the prior distribution p (z) to the conditional latent distribution p(z|y i ) at every resolution</p><formula xml:id="formula_8">L KL = yi?Y ? i k ? k E p(z &lt;k |yi) D KL (p(z k |z &lt;k , y i )||p (z k |z &lt;k )) ,<label>(8)</label></formula><p>where ? k is a resolution-dependent rebalancing weight and ? i is a modality-specific loss weight. We describe both weights in detail in Appendix B.</p><p>The KL loss also reduces conditional mode collapse since it encourages the conditional latent distribution to be close to the prior and therefore have high entropy. From the perspective of information bottleneck <ref type="bibr" target="#b0">[1]</ref>, the KL loss encourages each modality to only provide the minimum information necessary to specify the conditional image distribution.</p><p>Contrastive losses. The contrastive loss has been widely adopted in representation learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref> and more recently in image synthesis <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b74">75]</ref>. Given a batch of paired vectors (u, v) = {(u i , v i ), i = 1, 2, ..., N }, the symmetric cross-entropy loss <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b78">79]</ref> maximizes the similarity of the vectors in a pair while keeping non-paired vectors apart</p><formula xml:id="formula_9">L ce (u, v) = ? 1 2N N i=1 log exp(cos(u i , v i )/? ) N j=1 exp(cos(u i , v j )/? ) ? 1 2N N i=1 log exp(cos(u i , v i )/? ) N j=1 exp(cos(u j , v i )/? ) ,<label>(9)</label></formula><p>where ? is a temperature hyper-parameter. We use two kinds of pairs to construct two contrastive loss terms: the image contrastive loss and the conditional contrastive loss.</p><p>The image contrastive loss maximizes the similarity between a real image x and a random fake imagex synthesized based on the corresponding conditional inputs:</p><formula xml:id="formula_10">L cx = L ce (E vgg (x), E vgg (x)) ,<label>(10)</label></formula><p>where E vgg is a pretrained VGG <ref type="bibr" target="#b55">[56]</ref> encoder. This loss is similar to the perceptual loss widely used in conditional synthesis but has been found to perform better <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b74">75]</ref>.</p><p>The conditional contrastive loss aims to better align images with the corresponding conditions. Specifically, the discriminator is trained to maximize the similarity between its embedding of a real image x and the conditional input y i .</p><formula xml:id="formula_11">L D cy = L ce (D x (x), D yi (y i )) ,<label>(11)</label></formula><p>where D x and D yi are two modules in the discriminator that extract features from x and y i , respectively, as shown in Eq. (6) and <ref type="figure">Fig. 6b</ref>. The generator is trained with the same loss, but using the generated imagex instead of the real image to compute the discriminator embedding,</p><formula xml:id="formula_12">L G cy = L ce (D x (x), D yi (y i )) .<label>(12)</label></formula><p>In practice, we only use the conditional contrastive loss for the text modality since it consumes too much GPU memory to use the conditional contrastive loss for the other modalities, especially when the image resolution and batch size are large. A similar image-text contrastive loss is used in XMC-GAN <ref type="bibr" target="#b74">[75]</ref>, where they use a non-symmetric cross-entropy loss that only includes the first term in Eq. <ref type="formula" target="#formula_9">(9)</ref>.</p><p>Full training objective. In summary, the generator loss L G and the discriminator loss L D can be written as</p><formula xml:id="formula_13">L G = L G GAN + L KL + ? 1 L cx + ? 2 L G cy ,<label>(13)</label></formula><formula xml:id="formula_14">L D = L D GAN + ? 2 L D cy + ? 3 L GP ,<label>(14)</label></formula><p>where L G GAN and L D GAN are non-saturated GAN losses <ref type="bibr" target="#b11">[12]</ref>, L GP is the R 1 gradient penalty loss <ref type="bibr" target="#b37">[38]</ref>, and ? 1 , ? 2 , ? 3 are weights associated with the loss terms. A man riding a snowboard down a snow covered slope.</p><p>A red stop sign sitting on the side of a road. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Uncond Text</head><p>Seg Sketch All StyleGAN2 <ref type="bibr" target="#b24">[25]</ref> 11.7 - Modality dropout. By design, our generator, discriminator, and loss terms are able to handle missing modalities. We also find that randomly dropping out some input modalities before each training iteration further improves the robustness of the generator towards missing modalities at test time.</p><formula xml:id="formula_15">- - - SPADE-Seg [43] - - 48.6 - - pSp-Seg [51] - - 44.1 - - SPADE-Sketch [43] - - - 33.0 - pSp-Sketch [51] - - -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate the proposed approach on several datasets, including MM-CelebA-HQ <ref type="bibr" target="#b68">[69]</ref>, MS-COCO 2017 <ref type="bibr" target="#b30">[31]</ref> with COCO-Stuff annotations <ref type="bibr" target="#b2">[3]</ref>, and a proprietary dataset of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Main results</head><p>We compare PoE-GAN with a recent multimodal image synthesis method named TediGAN <ref type="bibr" target="#b68">[69]</ref> and also with stateof-the-art approaches specifically designed for each modality.</p><p>A lake in the desert with mountains at a distance.</p><p>A waterfall in sunset.</p><p>A mountain with pine trees in a starry winter night. For text-to-image, we compare with DF-GAN <ref type="bibr" target="#b59">[60]</ref> and DM-GAN + CL <ref type="bibr" target="#b72">[73]</ref> on MS-COCO. Since the original models are trained on the 2014 split, we retrain their models on the 2017 split using the official code. For segmentation-to-image synthesis, we compare with SPADE <ref type="bibr" target="#b42">[43]</ref>, VQGAN <ref type="bibr" target="#b9">[10]</ref>, OASIS <ref type="bibr" target="#b52">[53]</ref>, and pSp <ref type="bibr" target="#b50">[51]</ref>. For sketch-to-image synthesis, we compare with SPADE <ref type="bibr" target="#b42">[43]</ref> and pSp <ref type="bibr" target="#b50">[51]</ref>. We additionally compare with StyleGAN2 <ref type="bibr" target="#b24">[25]</ref> in the unconditional setting. We use Clean-FID <ref type="bibr" target="#b43">[44]</ref> for benchmarking due to its reported benefits over previous implementations of FID <ref type="bibr" target="#b15">[16]</ref>. <ref type="bibr" target="#b2">3</ref> Results on MM-CelebA-HQ and MS-COCO are summarized in Tab. 1 and Tab. 2, respectively. As shown in Tab. 1, PoE-GAN obtains a much lower FID than TediGAN in all settings. In Appendix C.3, we compare PoE-GAN with Te-diGAN in more detail and show that PoE-GAN is faster and more general than TediGAN. When conditioned on a single modality, PoE-GAN surprisingly outperforms the state-ofthe-art method designed specifically for that modality on both datasets, although PoE-GAN is trained for a more general purpose. We believe our proposed architecture and train- <ref type="bibr" target="#b2">3</ref> The baseline scores differ slightly from those in the original papers.</p><p>ing scheme are responsible for the improved performance. In <ref type="figure" target="#fig_6">Fig. 8</ref> and <ref type="figure" target="#fig_7">Fig. 9</ref>, we qualitatively compare PoE-GAN with previous segmentation-to-image and text-to-image methods on MS-COCO. We find that PoE-GAN produces images of much better quality and can synthesize realistic objects with complex structures, such as human faces and stop signs. More qualitative comparisons are included in Appendix C.6.</p><p>In <ref type="figure" target="#fig_8">Fig. 10</ref>, we show example images generated by our PoE-GAN using multiple input modalities on the landscape dataset. Our model is able to synthesize a wide range of landscapes in high resolution with photo-realistic details. More results are included in Appendix C.6, where we additionally show that PoE-GAN can generate diverse images when given the same conditional inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation studies</head><p>In Tabs. 3 and 4, we analyze the importance of different components of PoE-GAN. We use LPIPS <ref type="bibr" target="#b77">[78]</ref> as an additional metric to evaluate the diversity of images conditioned on the same input. Specifically, we randomly sample two output images conditioned on the same input and report the average LPIPS distance between the two outputs. A higher Uncond Text Segmentation Sketch All  <ref type="table">Table 3</ref>. Ablation study on MM-CelebA-HQ (256?256). The best scores are highlighted in bold and the second best ones are underlined.</p><formula xml:id="formula_16">Methods FID ? FID ? LPIPS ? FID ? LPIPS ? FID ? LPIPS ? FID ? LPIPS</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Uncond</head><p>Text Segmentation Sketch All LPIPS score would indicate more diverse outputs. First, we compare our product-of-experts generator, row (g) in Tabs. 3 and 4, with a baseline that simply concatenates the embedding of all modalities, while performing modality dropout (missing modality embeddings are set to zero). As seen in row (a), this baseline only works well when all modalities are available. Its FID significantly drops when some modalities are missing. Further, the generated images have low diversity as indicated by the low LPIPS score. This is, however, not surprising as previous work has shown that conditional GANs are prone to mode collapse <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b80">81]</ref>.</p><formula xml:id="formula_17">Methods FID ? FID ? LPIPS ? FID ? LPIPS ? FID ? LPIPS ? FID ? LPIPS ? (a)</formula><p>Row (b) of Tabs. 3 and 4 shows that the KL loss is important for training our model. Without it, our model suffers from low sample diversity and lack of robustness towards missing modalities, similar to the concatenation baseline described above. The variances of individual experts become near zero without the KL loss. The latent code z k then becomes a deterministic weighted average of the mean of each expert, which is equivalent to concatenating all modality embeddings and projecting it with a linear layer. This explains why our model without the KL loss behaves similarly to the concatenation baseline. Row (c) shows that our modality dropout scheme is important for handling missing modalities. Without it, the model tends to overly rely on the most informative modality, such as segmentation in MS-COCO.</p><p>To evaluate the proposed multiscale multimodal discriminator architecture, we replace MMPD with a discriminator that receives concatenated images and all conditional inputs. Row (d) shows that MMPD is much more effective than such a concatenation-based discriminator in all settings.</p><p>Finally in rows (e) and (f), we show that contrastive losses are useful but not essential. The image contrastive loss slightly improves FID in most settings, while the text contrastive loss improves FID for text-to-image synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We introduce a multimodal conditional image synthesis model based on product-of-experts and show its effectiveness for converting an arbitrary subset of input modalities to an image satisfying all conditions. While empirically superior than the prior multimodal synthesis work, it also outperforms state-of-the-art unimodal conditional image synthesis approaches when conditioned on a single modality.</p><p>Limitations. If different modalities provide contradictory information, PoE-GAN produces unsatisfactory outputs as shown in Appendix D. PoE-GAN trained in the unimodal setting performs better than the model trained in the multimodal setting, when tested in the unimodal setting. This indicates room for improvement in the fusing of multiple modalities, and we leave this for future work.</p><p>Potential negative societal impacts. Image synthesis networks can help people express themselves and artists create digital content, but they can undeniably also be misused for visual misinformation <ref type="bibr" target="#b65">[66]</ref>. Enabling users to synthesize images using multiple modalities makes it even easier to create a desired fake image. We encourage research that helps detect or prevent these potential negative misuses. We will provide implementations and training data to help forensics researchers detect fake images <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b63">64]</ref> and develop effective schemes for watermarking networks and training data <ref type="bibr" target="#b73">[74]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We evaluate the proposed PoE-GAN approach for multimodal conditional image synthesis on three datasets, including MM-CelebA-HQ <ref type="bibr" target="#b68">[69]</ref>, MS-COCO <ref type="bibr" target="#b30">[31]</ref>, and a proprietary dataset of landscape images using four modalities including text, segmentation, sketch, and style reference. The style is extracted from ground truth images and the other modalities are obtained from either human annotation or pseudo-labeling methods. We describe details about each dataset in the following.</p><p>MM-CelebA-HQ contains 30,000 images of celebrity faces with a resolution of 1024?1024, which are created from the original CelebA dataset <ref type="bibr" target="#b34">[35]</ref> using a procedure that improves image quality and resolution by Karras et al. <ref type="bibr" target="#b20">[21]</ref>. Each image is annotated with text, segmentation, and sketch. While the segmentation maps are annotated by humans <ref type="bibr" target="#b28">[29]</ref>, the text descriptions are automatically generated from the ground truth attribute labels by Xia et al. <ref type="bibr" target="#b68">[69]</ref>. On the other hand, sketch maps are generated by Chen et al. <ref type="bibr" target="#b5">[6]</ref> using the Photoshop edge extractor and sketch simplification <ref type="bibr" target="#b54">[55]</ref>. We use the pretrained CLIP <ref type="bibr" target="#b45">[46]</ref> text encoder to encode the text before feeding them to the generator. We use the standard train-test split <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b68">69]</ref> which yields 24,000 images in the training set and 6,000 images in the test set. We use images in the original resolution (1024?1024) in our main experiment and use images downsampled to 256?256 in ablation studies.</p><p>MS-COCO contains 123,287 images of complex indoor/outdoor scenes, containing various common objects. We use the segmentation maps provided in COCO-Stuff <ref type="bibr" target="#b2">[3]</ref> as the ground truth segmentation maps for the images. In MS-COCO, each image has up to 5 text descriptions. We use the pretrained CLIP text encoder to extract a feature vector per description. We additionally annotate each image with a sketch map produced by running HED <ref type="bibr" target="#b69">[70]</ref> edge detector followed by a sketch simplification process <ref type="bibr" target="#b54">[55]</ref>. We use the 2017 split, which leads to 118,287 training images and 5,000 test images. We use images resized to 256?256 in our main experiment and to 64?64 in ablation studies.</p><p>Landscape is a proprietary dataset containing around 10 million landscape images of resolution higher than 1024?1024. It does not come with any manual annotation and we use DeepLab-v2 <ref type="bibr" target="#b4">[5]</ref> to produce pseudo segmentation annotation and HED <ref type="bibr" target="#b69">[70]</ref> with sketch simplification <ref type="bibr" target="#b54">[55]</ref> to produce pseudo sketch annotation. For the text annotation, we use the CLIP image embedding as the pseudo text embedding to train our model. We randomly choose 50,000 images as the test set and use the rest of the images as the training set. Images are randomly cropped to 1024?1024 during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation details</head><p>Our implementation is based on Pytorch <ref type="bibr" target="#b44">[45]</ref>. We use the Adam optimizer <ref type="bibr" target="#b25">[26]</ref> with ? 1 = 0 and ? 2 = 0.99 and the same constant learning rate for both the generator and the discriminator. The final model weight is given by an exponential moving average of the generator's weights during the course of training. To stabilize GAN training, we employ leaky ReLU <ref type="bibr" target="#b36">[37]</ref> with slope 0.2, R 1 gradient penalty <ref type="bibr" target="#b37">[38]</ref> with lazy regularization <ref type="bibr" target="#b24">[25]</ref> applied every 16 iterations, equalized learning rate <ref type="bibr" target="#b24">[25]</ref>, anti-aliased resampling <ref type="bibr" target="#b76">[77]</ref>, and clip the gradient norms at 10. We also limit the range of the Gaussian log variance in product-of-experts layers to (??, ?) by applying the activation function ? tanh( . ? ). We use ? = 1 for the prior expert and ? = 10 for experts predicted from input modalities. We use mixed-precision training in all of our experiments and clamp the output of every convolutional/fully-connected layer to ?256 <ref type="bibr" target="#b21">[22]</ref>. We use a dropout rate of 0.5 when performing modality dropout. The contrastive loss temperature is initialized at 0.3 and learnable during training. We use the relu5 1 feature in VGG-19 to compute the image contrastive loss.</p><p>Inspired by NVAE <ref type="bibr" target="#b61">[62]</ref>, we rebalance the KL terms of different resolutions. The rebalancing weight ? k in Eq. (8) of the main paper is proportional to the unbalanced KL term and inversely proportional to the resolution:</p><formula xml:id="formula_18">? k ? 1 w k h k E p(yi) [E p(z &lt;k |yi) [ D KL (p(z k |z &lt;k , y i )||p (z k |z &lt;k ))]] ,<label>(15)</label></formula><p>with the constraint that 1 N N k=1 ? k = 1. The rebalancing weights encourage having the amount of information encoded in each latent variable. We use a running average of the rebalancing weights with a decay factor of 0.99.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Decoder</head><p>We introduced our decoder design in Sec. 3.1 of the main paper. Here, we provide additional details. In Global PoE-Net ( <ref type="figure">Fig. 4 of the main paper)</ref>, each MLP consists of 4 fully-connected layers with a hidden dimension 4 times smaller than the input dimension. Both z 0 and w are 512dimensional. The output MLP has two layers with a hidden dimension of 512.</p><p>In Local PoE-Net ( <ref type="figure">Fig. 5</ref> of the main paper), each CNN similarly contains 4 convolutional layers with the number of filters 4 times smaller than the input channel size. The first and last convolutions have 1?1 kernels, and the convolutions in the middle have a kernel size of 3. The dimension of w is 512. The dimensions of z k 's are described in Appendix B.4. <ref type="figure">Fig. 11</ref> shows the architecture of encoders used in our generator to encode each modality. The text encoder <ref type="figure">(Fig. 11a</ref>  Inference time (per image) 0.07s 0.04s 0.06s 0.02s 0.12s <ref type="table">Table 6</ref>. Hardware and training/inference speed on different datasets. We train all models using NVIDIA Tesla V100 GPUs, except for the Landscape model which is trained using NVIDIA Ampere A100 GPUs with 80 GB of memory. The inference time is evaluated on a workstation with a single NVIDIA TITAN RTX GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Encoders</head><p>is a 4-layer MLP with dimension 512 that processes the CLIP embedding of a caption. The segmentation and sketch encoders are CNNs with skip connections from the input, which are illustrated in <ref type="figure">Fig. 11b</ref> and <ref type="figure">Fig. 11c</ref>. The segmentation or sketch map is downsampled multiple times. The embeddings of the downsampled map are added to the intermediate outputs of the corresponding convolutional layers. The intermediate outputs of convolutional layers are provided to the decoder via skip connections. The style encoder for encoding the reference style image is a residual network with instance normalization <ref type="bibr" target="#b60">[61]</ref>. As shown in <ref type="figure">Fig. 11d</ref>, we obtain the style code by concatenating the mean and standard deviation of the output of every residual block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Discriminator</head><p>As shown in <ref type="figure">Fig. 7</ref> of the main paper, our discriminator encodes the image and the other modalities into multiscale feature maps and computes the MPD loss at each scale. We use a residual network to encode the image. For encoding other modalities, we use the same architecture as described in the previous section (Appendix B.2 and <ref type="figure">Fig. 11</ref>). However, the parameters are not shared with those encoders used in the generator. For encoders (the style encoder or the text encoder) that only outputs a single feature vector, we spatially replicate the feature vector to different resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Hyper-parameters</head><p>Tab. 5 shows the hyper-parameters used on all the benchmark datasets, including learning rate, batch size, loss weights, and channel size. The decoder and discriminator have the same channel size, which is always 4 times larger than the channel size of the modality encoders. We hence omit the channel size of modality encoders from the table. The layer operating at the highest resolution always has the smallest number of channels (denoted as "base # channels for DecDis"). The number of channels doubles while the  <ref type="figure">Figure 11</ref>. Architecture of encoders. We use a pretrained CLIP <ref type="bibr" target="#b45">[46]</ref> and an MLP to encoder text, a convolutional network with input skip connections to encode segmentation/sketch maps, and a residual network to encoder style images.</p><p>StyleGAN2 <ref type="bibr" target="#b24">[25]</ref> SPADE-Seg <ref type="bibr" target="#b42">[43]</ref> pSp-Seg <ref type="bibr">[</ref>   resolution halves until it reaches a maximum number (denoted as "maximum # channels for DecDis"). The "Base # of channels for Latent" refers to the channel size of the feature map z N i to the decoder extracted from the spatial modalities (segmentation and sketch) in the highest resolution (the largest value of k is N ). We also note that the channel number of ? k i or ? k i equals that of z k i . As explained in Appendix B.2, we double the channel size as we halve the spatial resolution. The channel number for z k i doubles until it reaches the "Maximum # of channels for Latent," the channel number for z 0 i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. Hardware and speed</head><p>We report the computation infrastructure used in our experiments Tab. 6. We also report the training and inference speed of our model for different datasets in the table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional results</head><p>We provide additional experimental results in this section, including more comparison with baselines and more analysis on the proposed approach. Please check the accompanying video for additional results on the landscape dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Model size comparison</head><p>In Tab. 7 and Tab. 8, we compare the number of parameters used in PoE-GAN and in our baselines. We show that PoE-GAN does not use significantly more parameters -actually it uses fewer parameters than some of the single-modal baselines, although PoE-GAN is trained for a much more challenging task. This shows that our improvement does not come from using a larger model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Comparison on MM-CelebA-HQ (256?256)</head><p>In Tab. 9, We compare PoE-GAN trained on the 256?256 MM-CelebA-HQ dataset with the text-to-image baselines reported in TediGAN <ref type="bibr" target="#b68">[69]</ref> and M6-UFC <ref type="bibr" target="#b79">[80]</ref>. Our model achieves a significantly lower FID than previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Additional comparison with TediGAN</head><p>We provide a more detailed comparison between PoE-GAN and TediGAN, the only existing multimodal image synthesis method that can produce high-resolution images. TediGAN mainly contains four steps: 1) encodes different modalities into the latent space of StyleGAN, 2) mixes the latent code according to a hand-designed rule, 3) generates the image from the latent code using StyleGAN, and 4) iteratively refines the image. It has several disadvantages Method AttnGAN <ref type="bibr" target="#b70">[71]</ref> ControlGAN <ref type="bibr" target="#b29">[30]</ref> DF-GAN <ref type="bibr">[</ref>    we compare a PoE-GAN model trained using multiple modalities with the same PoE-GAN model but trained using one single modality. We compare their performance when applied to convert the user input in a single modality to the output image. This experiment helps understand the penalty the model pays for the multimodal synthesis capability. As shown in Tab. 10 and Tab. 11, the model trained for a specific modality always slightly outperforms the joint model when conditioned on that modality. This indicates that the increased task complexity of an additional modality outweighs the benefits of additional annotations from that modality. This result also shows that the improvement of PoE-GAN over state-of-the-art unimodal image synthesis methods comes from our architecture and training scheme rather than additional annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5. User study</head><p>We conduct a user study to compare PoE-GAN with stateof-the-art text-to-image and segmentation-to-image synthesis methods on MS-COCO. We show users two images generated by different algorithms from the same conditional input and ask them which one is more realistic. As shown in Tab. <ref type="bibr" target="#b11">12</ref>   <ref type="table" target="#tab_1">Table 13</ref>. User study on segmentation-to-image synthesis. Each column shows the percentage of users that prefer the image generated by our model over that generated by the baseline method.</p><p>Huge ocean waves clash into rocks.</p><p>A beach with black sand and palm trees. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Limitation</head><p>We find that the PoE-GAN model does not work well when conditioned on contradictory multimodality inputs. For example, when the segmentation and text are contradictory to each other, the text input is usually ignored, as shown in <ref type="figure" target="#fig_0">Fig. 12</ref>. In the product-of-experts formulation, an expert with a larger variance will have a smaller influence on the product distribution, and we indeed find the variance of the text expert is usually larger than that of the segmentation expert, which explains the behavior of our model.</p><p>In addition, as discussed in Appendix C.4, the PoE-GAN model still pays the penalty in terms of a higher FID score as achieving the multimodal conditional image synthesis capability. This indicates room for improvement in the fusing of multiple modalities, and we leave this for future work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Condition #1 Condition #2 Samples</head><p>This person is wearing hat, lipstick. She has big lips, and wavy hair.</p><p>This smiling man has bushy eyebrows, and bags under eyes.</p><p>The person has goatee, mustache, and sideburns. <ref type="figure" target="#fig_2">Figure 13</ref>. Examples of multimodal conditional image synthesis on MM-CelebA-HQ. We show three random samples from PoE-GAN conditioned on two modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Condition #1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Condition #2 Samples</head><p>Large mustard yellow commercial airplane parked in the airport.</p><p>A rain covered terrain after a night of rain. <ref type="figure">Figure 14</ref>. Examples of multimodal conditional image synthesis on MS-COCO. We show three random samples from PoE-GAN conditioned on two modalities (from top to bottom: text + segmentation, text + sketch, and segmentation + sketch).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Condition #1 Condition #2 (Style) Samples</head><p>A person on a surf board riding a wave.</p><p>A person on a surf board riding a wave. <ref type="figure">Figure 15</ref>. Examples of multimodal conditional image synthesis on MS-COCO. We show three random samples from PoE-GAN conditioned on two modalities, one being text/segmentation/sketch and another being style reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Condition #1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Condition #2 Samples</head><p>Trees near a lake in an autumn rainy day.</p><p>Huge ocean waves crash into rocks in a day with colorful clouds.</p><p>Waterfall and river between mountains.</p><p>A white sand beach near cyan ocean. <ref type="figure">Figure 16</ref>. Examples of multimodal conditional image synthesis on Landscape. We show three random samples from PoE-GAN conditioned on two modalities (from top to bottom: text + segmentation, text + sketch, and segmentation + sketch).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Condition #1</head><p>Condition #2 (Style) Samples A lake surrounded by mountains.</p><p>A lake surrounded by mountains. <ref type="figure">Figure 17</ref>. Examples of multimodal conditional image synthesis on Landscape. We show three random samples from PoE-GAN conditioned on two modalities, one being text/segmentation/sketch and another being style reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground truth DF-GAN DM-GAN + CL PoE-GAN (Ours)</head><p>A kitchen with a counter and a table with chairs.</p><p>A view of mountains from the window of a jet airplane.</p><p>A blueberry cake is on a plate and is topped with butter.</p><p>Three men each holding something and posing for a picture.</p><p>A plate holds a good size portion of a cooked, mixed dish that includes broccoli and pasta.</p><p>A red blue and yellow train and some people on a platform.</p><p>A man blowing out candles on a birthday cake.</p><p>A desk set up as a workstation with a laptop.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>The product of distributions is analogous to the intersection of sets. The product distribution has a high density where both distributions have relatively high densities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>An overview of our generator. The detailed architectures of Global PoE-Net and the decoder are shown in Fig. 4 and Fig. 5 respectively. The architectures of all modality encoders are provided in Appendix B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Global PoE-Net. We sample a latent feature vector z 0 using product-of-experts (Eq. (3) in Sec. 3.2), which is then processed by an MLP to output a feature vector w. Product-of-Experts (Eq. (3A residual block in our decoder. Local PoE-Net samples a latent feature map z k using product-of-experts (Eq. (3) in Sec. 3.2). Here ? denotes concatenation. LG-AdaIN uses w and z k to modulate the feature activations in the residual branch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Comparison between the standard projection discriminator and our multimodal projection discriminator (MPD). Our multiscale multimodal projection discriminator. The image and other modalities are encoded into feature maps of multiple resolutions and we compute the MPD loss at each resolution. tion. Other modalities (text and style reference) only provide global information but not local details. The latent feature map z k produced by Local PoE-Net and the feature vector w produced by Global PoE-Net are fed to our local-global adaptive instance normalization (LG-AdaIN) layer,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Qualitative comparison of segmentation-to-image synthesis results on MS-COCO 2017.TextGround truth DF-GAN DM-GAN + CL PoE-GAN (Ours)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Qualitative comparison of text-to-image synthesis results on MS-COCO 2017.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .</head><label>10</label><figDesc>Examples of multimodal conditional image synthesis results produced by PoE-GAN trained on the 1024?1024 landscape dataset. We show the segmentation/sketch/style inputs on the bottom right of the generated images for the results in the first row. The results in the second row additionally leverage text inputs, which are shown below the corresponding generated images. Please zoom in for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 .</head><label>12</label><figDesc>Examples generated by PoE-GAN when conditioned on contradictory segmentation and text inputs. The text input is simply ignored.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 18 .Figure 19 .</head><label>1819</label><figDesc>Additional qualitative comparison of text-to-image synthesis on MS-COCO 2017. Additional qualitative comparison of segmentation-to-image synthesis on MS-COCO 2017.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 20 .</head><label>20</label><figDesc>Uncurated unconditional results on the 1024?1024 MM-CelebA-HQ dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 21 .</head><label>21</label><figDesc>Uncurated unconditional results on the 256?256 MS-COCO dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 22 .</head><label>22</label><figDesc>Uncurated unconditional results on the 1024?1024 landscape dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>45.8</cell><cell>-</cell></row><row><cell>TediGAN [69]</cell><cell>-</cell><cell>38.4</cell><cell>45.1</cell><cell>45.1</cell><cell>45.1</cell></row><row><cell>PoE-GAN (Ours)</cell><cell>10.5</cell><cell>10.1</cell><cell>9.9</cell><cell>9.9</cell><cell>8.3</cell></row></table><note>Comparison on MM-CelebA-HQ (1024?1024) using FID. We evaluate models conditioned on different modalities (from left to right: no conditions, text, segmentation, sketch, and all three modalities). The best scores are highlighted in bold.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison on MS-COCO 2017 (256?256) using FID. We evaluate models conditioned on different modalities (from left to right: no conditions, text, segmentation, sketch, and all three modalities). The best scores are highlighted in bold. landscape images. Images are labeled with all input modalities obtained from either manual annotation or pseudolabeling methods. More details about the datasets and the pseudo-labeling procedure are in Appendix A.</figDesc><table><row><cell></cell><cell cols="2">Uncond Text</cell><cell cols="2">Seg Sketch</cell><cell>All</cell></row><row><cell>StyleGAN2 [25]</cell><cell>43.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DF-GAN [60]</cell><cell>-</cell><cell>45.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">DM-GAN + CL [73] -</cell><cell>29.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SPADE-Seg [43]</cell><cell>-</cell><cell>-</cell><cell>22.1</cell><cell>-</cell><cell>-</cell></row><row><cell>VQGAN [10]</cell><cell>-</cell><cell>-</cell><cell>21.6</cell><cell>-</cell><cell>-</cell></row><row><cell>OASIS [53]</cell><cell>-</cell><cell>-</cell><cell>19.2</cell><cell>-</cell><cell>-</cell></row><row><cell>SPADE-Sketch [43]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>63.7</cell><cell>-</cell></row><row><cell>PoE-GAN (Ours)</cell><cell>43.4</cell><cell>20.5</cell><cell>15.8</cell><cell>25.5</cell><cell>13.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="2">Concatenation + modality dropout 59.1</cell><cell>30.7</cell><cell>0.40</cell><cell>20.4</cell><cell>0.16</cell><cell>36.1</cell><cell>0.27</cell><cell>16.6</cell><cell>0.12</cell></row><row><cell>(b) Ours w/o KL loss</cell><cell>59.3</cell><cell>30.5</cell><cell>0.39</cell><cell>21.5</cell><cell>0.16</cell><cell>33.0</cell><cell>0.27</cell><cell>16.9</cell><cell>0.12</cell></row><row><cell>(c) Ours w/o modality dropout</cell><cell>86.2</cell><cell>87.8</cell><cell>0.58</cell><cell>19.9</cell><cell>0.44</cell><cell>85.1</cell><cell>0.55</cell><cell>18.7</cell><cell>0.47</cell></row><row><cell>(d) Ours w/o MMPD</cell><cell>43.1</cell><cell>40.2</cell><cell>0.64</cell><cell>21.7</cell><cell>0.46</cell><cell>45.5</cell><cell>0.57</cell><cell>21.1</cell><cell>0.42</cell></row><row><cell>(e) Ours w/o image contrastive loss</cell><cell>25.7</cell><cell>21.3</cell><cell>0.64</cell><cell>18.0</cell><cell>0.50</cell><cell>37.9</cell><cell>0.61</cell><cell>18.5</cell><cell>0.54</cell></row><row><cell>(f) Ours w/o text contrastive loss</cell><cell>27.6</cell><cell>26.0</cell><cell>0.66</cell><cell>17.4</cell><cell>0.46</cell><cell>33.5</cell><cell>0.55</cell><cell>17.9</cell><cell>0.43</cell></row><row><cell>(g) Ours</cell><cell>26.6</cell><cell>22.2</cell><cell>0.65</cell><cell>17.1</cell><cell>0.47</cell><cell>30.2</cell><cell>0.58</cell><cell>17.1</cell><cell>0.44</cell></row></table><note>. Ablation study on MS-COCO 2017 (64?64). The best scores are highlighted in bold and the second best ones are underlined.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Hyper-parameters on different datasets.</figDesc><table><row><cell></cell><cell>MM-CelebAHQ</cell><cell>MM-CelebAHQ</cell><cell>MS-COCO</cell><cell>MS-COCO</cell><cell>Landscape</cell></row><row><cell></cell><cell>(1024 ? 1024)</cell><cell>(256 ? 256)</cell><cell>(256 ? 256)</cell><cell>(64 ? 64)</cell><cell>(1024 ? 1024)</cell></row><row><cell>Number of GPUs</cell><cell>16</cell><cell>8</cell><cell>32</cell><cell>8</cell><cell>256</cell></row><row><cell>Training time</cell><cell>71h</cell><cell>35h</cell><cell>85h</cell><cell>76h</cell><cell>101h</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Number of parameter of compared models on MM-CelebA-HQ (1024?1024).</figDesc><table><row><cell>StyleGAN2 [25]</cell><cell>DF-GAN [60]</cell><cell>DM-GAN + CL [73]</cell><cell>SPADE-Seg [51]</cell><cell>VQGAN [10]</cell><cell>OASIS [53]</cell><cell>SPADE-Sketch [51]</cell><cell>PoE-GAN</cell></row><row><cell>25M</cell><cell>12M</cell><cell>32M</cell><cell>280M</cell><cell>798M</cell><cell>94M</cell><cell>95M</cell><cell>142M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc>Number of parameters of compared models on MS-COCO 2017 (256?256).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 .</head><label>9</label><figDesc>Comparison of text-to-image synthesis on MM-CelebA-HQ (256?256). ?: the higher the better, ?: the lower the better.</figDesc><table><row><cell></cell><cell>Uncond</cell><cell>Text</cell><cell>Seg</cell><cell>Sketch</cell><cell>All</cell></row><row><cell>Ours (Uncond)</cell><cell>13.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours (Text)</cell><cell>13.8</cell><cell>13.4</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours (Seg)</cell><cell>14.2</cell><cell>-</cell><cell>10.9</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours (Sketch)</cell><cell>14.2</cell><cell>-</cell><cell>-</cell><cell>9.5</cell><cell>-</cell></row><row><cell>Ours (All)</cell><cell>14.9</cell><cell>13.7</cell><cell>12.9</cell><cell>9.9</cell><cell>8.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 .</head><label>10</label><figDesc>Comparison on MM-CelebA-HQ (256?256) using FID. We compare our PoE-GAN trained using all modalities with PoE-GAN trained using a single modality. Note that PoE-GAN trained using a single modality can also be used for unconditional synthesis and we also report the achieved FID in thetable.compared with our PoE-GAN:1. TediGAN relies on a pretrained unconditional generator, and its image quality is upper bounded by that unconditional model. This is not ideal because unconditional models usually produce images of lower quality than conditional ones. On the other hand, PoE-GAN learns conditional and unconditional generation simultaneously, and its FID improves when more conditions are provided, as shown in Tab. 1 of the main paper.2. TediGAN uses a handcrafted rule to combine the latent code from different modalities. Specifically, the StyleGAN latent space contains 14 layers of latent vectors. TediGAN uses the top-layer latent vectors from one modality and the bottom-layer latent vectors from another modality when combining two modalities. This combination rule cannot be generalized to other generator architectures and other modalities. In contrast, we use product-of-experts with learned parameters to combine different modalities which is more general. 3. Sampling from TediGAN is very slow due to its instance-level optimization. It takes 51.2 seconds to generate a 1024?1024 image with TediGAN, while PoE-GAN only needs 0.07 seconds.C.4. Training PoE-GAN with a single modalityAlthough PoE-GAN is designed for the multimodal conditional image synthesis task, it can be trained in a single modality setting. That is, we can train a pure segmentationto-image model, a pure sketch-to-image model, or a pure text-to-image model using the same PoE-GAN model. Here,</figDesc><table><row><cell></cell><cell>Uncond</cell><cell>Text</cell><cell>Seg</cell><cell>Sketch</cell><cell>All</cell></row><row><cell>Ours (Uncond)</cell><cell>23.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours (Text)</cell><cell>24.0</cell><cell>21.1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours (Seg)</cell><cell>25.5</cell><cell>-</cell><cell>16.3</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours (Sketch)</cell><cell>24.7</cell><cell>-</cell><cell>-</cell><cell>24.7</cell><cell>-</cell></row><row><cell>Ours (All)</cell><cell>26.6</cell><cell>22.2</cell><cell>17.1</cell><cell>30.2</cell><cell>17.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 .</head><label>11</label><figDesc>Comparison on MS-COCO 2017 (64?64) using FID. We compare our PoE-GAN trained using all modalities with PoE-GAN trained using a single modality. Note that PoE-GAN trained using a single modality can also be used for unconditional synthesis and we also report the achieved FID in the table.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 .</head><label>12</label><figDesc>and Tab. 13, the majority of users prefer PoE-GAN over the baseline methods.C.6. Additional qualitative examplesIn Figs. 13 to 17, we show that PoE-GAN can generate diverse images when conditioned on two different input modalities. We show additional qualitative comparison of text-to-image synthesis and segmentation-to-image synthesis on MS-COCO inFigs. 18 and 19respectively. Figs. 20 to 22 show uncurated samples generated unconditionally on MM-CelebA-HQ, MS-COCO, and Landscape. User study on text-to-image synthesis. Each column shows the percentage of users that prefer the image generated by our model over that generated by the baseline method.</figDesc><table><row><cell></cell><cell>DF-GAN [60]</cell><cell cols="2">DM-GAN + CL [73]</cell></row><row><cell>Ours vs.</cell><cell>82.1%</cell><cell></cell><cell>72.9%</cell></row><row><cell></cell><cell cols="3">SPADE [43] VQGAN [10] OASIS [53]</cell></row><row><cell>Ours vs.</cell><cell>69%</cell><cell>66.7%</cell><cell>64.9%</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We thank Jan Kautz, David Luebke, Tero Karras, Timo Aila, and Zinan Lin for their feedback on the manuscript. We thank Daniel Gifford and Andrea Gagliano on their help on data collection.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep variational information bottleneck</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Alexander A Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">COCO-Stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">What makes fake images detectable? understanding properties that generalize</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">DeepFaceDrawing: Deep generation of face images from sketches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanchao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SketchyGAN: Towards diverse and realistic sketch to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengling</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Very deep VAEs generalize autoregressive models and can outperform them on images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Interactive sketch &amp; fill: Multiclass sketch-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnab</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Puneet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Dokania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Generative adversarial nets. NeurIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lars Petersson, and Mohammad Ali Armin. Dual contrastive learning for unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junlin</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><surname>Shoeiby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Bernhard Nessler, and Sepp Hochreiter</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Progressive growing of GANs for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Alias-free generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>H?rk?nen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of StyleGAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Multimodal variational autoencoders for semi-supervised learning: In defense of product-of-experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Kutuzova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oswin</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mads</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Igel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.07240</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Maskgan: Towards diverse and interactive facial image manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Cheng-Han Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Controllable text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generative adversarial networks for image and video synthesis: Algorithms and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">DivCo: Diverse conditional image synthesis via contrastive generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching</forename><forename type="middle">Lam</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to predict layout-to-image conditional convolutions for semantic image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">BIVA: A very deep hierarchy of latent variables for generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Maal?e</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Fraccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Li?vin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Which training methods for GANs do actually converge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">cGANs with projection discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Contrastive learning for unpaired image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11222</idno>
		<title level="m">On buggy resizing libraries and surprising subtleties in FID calculation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, 2021</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Encoding in style: a StyleGAN encoder for image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Alaluf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yotam</forename><surname>Nitzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stav</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021. 6</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Scribbler: Controlling deep image synthesis with sketch and color</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patsorn</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">You only need adversarial supervision for semantic image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Sch?nfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Sushko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Variational mixture-of-experts autoencoders for multi-modal deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuge</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooks</forename><surname>Siddharth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning to simplify: fully convolutional networks for rough sketch cleanup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Sasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Ishikawa</surname></persName>
		</author>
		<idno>2016. 12</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Ladder variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapani</forename><surname>Casper Kaae S?nderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maal?e</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>S?ren Kaae S?nderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Generalized multimodal ELBO</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imant</forename><surname>Sutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><forename type="middle">E</forename><surname>Daunhawer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vogt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Joint multimodal learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masahiro</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kotaro</forename><surname>Nakayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songsong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yuan</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingkun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Df-Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.05865</idno>
		<title level="m">Deep fusion generative adversarial networks for text-to-image synthesis</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Improved texture networks: Maximizing quality and diversity in feed-forward stylization and texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">NVAE: A deep hierarchical variational autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Generative models of visually grounded imagination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">CNN-generated images are surprisingly easy to spot... for now</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng-Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">The emergence of deepfake technology: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mika</forename><surname>Westerlund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technology Innovation Management Review</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Products of gaussians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">N</forename><surname>Agakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Felderhof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Multimodal generative models for scalable weakly-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">TediGAN: Text-guided diverse face image generation and manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujiu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing-Hao</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">AttnGAN: Finegrained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Diversity-sensitive conditional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingdong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunseok</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianchen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Improving text-to-image synthesis using contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiulong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Takac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajshekhar</forename><surname>Sunderraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihao</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.02423</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Artificial fingerprinting for generative models: Rooting deepfake attribution in training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladislav</forename><surname>Skripniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahar</forename><surname>Abdelnabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Cross-modal contrastive learning for text-toimage generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yu Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Stack-GAN: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Making convolutional networks shiftinvariant again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhide</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curtis</forename><forename type="middle">P</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langlotz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00747</idno>
		<title level="m">Contrastive learning of medical visual representations from paired images and text</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">M6-UFC: Unifying multi-modal controls for conditional image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2021</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Toward multimodal image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">DM-GAN: Dynamic memory generative adversarial networks for text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minfeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingbo</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
