<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporally Efficient Vision Transformer for Video Instance Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shusheng</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Applied Research Center (ARC)</orgName>
								<address>
									<region>Tencent PCG</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">International Digital Economy Academy (IDEA)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiemin</forename><surname>Fang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Applied Research Center (ARC)</orgName>
								<address>
									<region>Tencent PCG</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Applied Research Center (ARC)</orgName>
								<address>
									<region>Tencent PCG</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Temporally Efficient Vision Transformer for Video Instance Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently vision transformer has achieved tremendous success on image-level visual recognition tasks. To effectively and efficiently model the crucial temporal information within a video clip, we propose a Temporally Efficient Vision Transformer (TeViT) for video instance segmentation (VIS). Different from previous transformer-based VIS methods, TeViT is nearly convolution-free, which contains a transformer backbone and a query-based video instance segmentation head. In the backbone stage, we propose a nearly parameter-free messenger shift mechanism for early temporal context fusion. In the head stages, we propose a parameter-shared spatiotemporal query interaction mechanism to build the one-to-one correspondence between video instances and queries. Thus, TeViT fully utilizes both framelevel and instance-level temporal context information and obtains strong temporal modeling capacity with negligible extra computational cost. On three widely adopted VIS benchmarks, i.e., YouTube-VIS-2019, YouTube-VIS-2021, and OVIS, TeViT obtains state-of-the-art results and maintains high inference speed, e.g., 46.6 AP with 68.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video Instance Segmentation (VIS) <ref type="bibr" target="#b61">[62]</ref> is a representative and challenging video understanding task that requires detecting, segmenting and tracking video instances across frames simultaneously. Similar to other instance-level video recognition tasks, making full use of temporal context information is critical for building high-performance VIS systems. Vision transformer (ViT) <ref type="bibr" target="#b13">[14]</ref>, which is based on self-attention <ref type="bibr" target="#b50">[51]</ref>, has shown strong long-range context modeling ability and obtained great successes on im-age classification <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b53">54]</ref>, object detection <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b67">68]</ref>, semantic segmentation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b58">59]</ref>, instance segmentation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b65">66]</ref>, and video recognition <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b66">67]</ref>.</p><p>Recently, how to design ViTs for instance-level video understanding, especially VIS, becomes an emerging problem. Different from the detection transformers <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b67">68]</ref>, semantic segmentation transformers <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b58">59]</ref>, and instance segmentation transformers <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b65">66]</ref>, which focus on 2D contextual information modeling, VIS transformers additionally require to perform temporal context modeling. To this end, VisTR <ref type="bibr" target="#b56">[57]</ref> firstly proposes a transformer encoder to fuse patch features from a sequence of frames using a CNN backbone and leverages a query-based decoder to predict video instances, IFC <ref type="bibr" target="#b22">[23]</ref> introduces memory tokens to store frame-level features and performs cross-frame feature interaction by computing self-attention among memory tokens, and then decodes instance-level results using a conditional mask head.</p><p>In this paper, we focus on the efficiency of modeling temporal information for ViT-based VIS. This is a very important problem since (1) computing self-attention among all video patches has extremely high time and space complexity <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b1">(2)</ref> additional multi-head self attention (MHSA) layers for temporal modeling have extra parameters and are sensitive to pre-training <ref type="bibr" target="#b22">[23]</ref>, (3) the CNN or transformer backbones in these methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b63">64]</ref> only support single frame feature extraction and fail to capture temporal information in the backbone stage. To remedy the above issues, we present Temporally Efficient ViT (TeViT) to fully utilize temporal contextual information for efficient and effective video instance segmentation.</p><p>TeViT contains a transformer backbone and a series of query-based VIS heads. In the backbone stage, we use messenger tokens <ref type="bibr" target="#b15">[16]</ref> to extract intra-frame information via self-attention and propose a messenger shift mechanism for frame-level context modeling, in which messenger tokens are divided into several groups to perform temporal shift with various of time steps. Different from previous VIS methods, the messenger shift transformer enables early temporal feature fusion. In the head stages, we convert the QueryInst <ref type="bibr" target="#b17">[18]</ref> instance segmentation head into our VIS head by reusing the multi-head self-attention (MHSA) <ref type="bibr" target="#b50">[51]</ref> parameters for instance-level temporal information interaction. The instance-level MHSA fuses the features for a single video instance among input frames, thus it realizes the concept of a video instance as a query.</p><p>Experiments are conducted on three large-scale VIS datasets, i.e., YouTube-VIS-2019 <ref type="bibr" target="#b61">[62]</ref>, YouTube-VIS-2021 <ref type="bibr" target="#b60">[61]</ref>, and OVIS <ref type="bibr" target="#b39">[40]</ref>. New state-of-the-art (SoTA) performance has been obtained, e.g., TeViT obtains 46.6 AP with 68.9 FPS on YouTube-VIS-2019. Our main contributions are summarized as follows.</p><p>? TeViT is the first video instance segmentation transformer that can efficiently capture temporal contextual information at both frame level and instance level.</p><p>? Benefiting from the flexibility of self-attention, the proposed temporal modeling modules, i.e., messenger token shift and spatiotemporal query interaction, both are friendly to the image-level pre-trained models, cost marginal extra computation overhead and parameters.</p><p>? TeViT is a nearly convolution-free framework and obtains SoTA VIS results. In TeViT, the concepts of "early temporal feature fusion" and "a video instance as a query" shield lights on how to build effective video transformers for instance-level recognition tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Video instance segmentation. How to achieve efficiently temporal modeling is always the focus of video tasks, such as video object segmentation (VOS) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b38">39]</ref>, multi-object tracking and segmentation (MOTS) <ref type="bibr" target="#b51">[52]</ref> and VIS <ref type="bibr" target="#b61">[62]</ref>. Though VOS and MOTS are very related with VIS, MOTS mainly focuses on the urban scene understanding and VOS aims at tracking specific object by a given mask. Representative VIS works are reviewed as follows. Mask-Track R-CNN <ref type="bibr" target="#b61">[62]</ref> extends Faster R-CNN <ref type="bibr" target="#b40">[41]</ref> and Mask R-CNN <ref type="bibr" target="#b19">[20]</ref> to VIS with a tracking branch and external memory that saves instance features across multiple frames. MaskProp <ref type="bibr" target="#b2">[3]</ref> builds on the Hybrid Task Cascade Network <ref type="bibr" target="#b9">[10]</ref> and propagates instance region features to adjacent frames to perform temporal modeling. STEm-Seg <ref type="bibr" target="#b1">[2]</ref> treats video clip as 3D spatiotemporal volume and captures temporal information by 3D convolutional backbone network. CompFeat <ref type="bibr" target="#b18">[19]</ref> refines temporal features at both frame-level and instance-level. CrossVIS <ref type="bibr" target="#b62">[63]</ref> introduces a crossover learning scheme upon <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref> to make use of contextual information across video frames. Se-qMask R-CNN <ref type="bibr" target="#b26">[27]</ref> establishes temporal relation across frames by adding an extra sequence propagation head upon Mask R-CNN. Both VisRGNN <ref type="bibr" target="#b23">[24]</ref> and VisSTG <ref type="bibr" target="#b52">[53]</ref> model temporal information in VIS by a graph neural network. VisTR <ref type="bibr" target="#b56">[57]</ref> proposes the first fully end-to-end VIS method upon DETR <ref type="bibr" target="#b6">[7]</ref>, temporal contexts are fused by the multihead attention mechanism in transformer encoder layers. IFC <ref type="bibr" target="#b22">[23]</ref> presents inter-frame communication to exchange frame-level information. In this paper, we present a temporally efficient framework to model temporal contexts at both frame-level and instance-level. Vision Transformer. Transformer <ref type="bibr" target="#b50">[51]</ref> is firstly proposed to model long-range sequence data in natural language process (NLP). ViT <ref type="bibr" target="#b13">[14]</ref> firstly adopts transformer to image domain. After that various high-performance vision transformers <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55]</ref> have been proposed as backbones for image understanding. Beyond serving as backbone networks, transformer has motivated lots of novel object detection <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b67">68]</ref>, instance segmentation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b65">66]</ref>, and semantic segmentation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b58">59]</ref> frameworks. Recently, VisTR <ref type="bibr" target="#b56">[57]</ref>, IFC <ref type="bibr" target="#b22">[23]</ref>, QueryTrack <ref type="bibr" target="#b63">[64]</ref>, and TCIS <ref type="bibr" target="#b37">[38]</ref> bring transformer to video instance segmentation and achieve excellent performance. In this paper, we investigate how to efficiently model temporal context across video frames and propose TeViT. TeViT is a nearly convolution-free transformer while VisTR and IFC both use ResNet <ref type="bibr" target="#b20">[21]</ref> backbone. Temporal context modeling. Temporal context modeling is the key issue in video understanding. A lot of works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b59">60]</ref> model temporal context by 3D convolutional block. TSM <ref type="bibr" target="#b27">[28]</ref> proposes an efficient temporal shift module by moving the convolutional feature map along the temporal dimension. Non-local network <ref type="bibr" target="#b55">[56]</ref> applies self-attention to capture long-range spatiotemporal dependencies directly. Recently, TimeSformer <ref type="bibr" target="#b3">[4]</ref>, ViViT <ref type="bibr" target="#b0">[1]</ref>, VidTR <ref type="bibr" target="#b66">[67]</ref>, and MViT <ref type="bibr" target="#b14">[15]</ref> extend ViT to capture spatiotemporal context for video classification. Video Swin Transformer <ref type="bibr" target="#b32">[33]</ref> extends Swin Transformer <ref type="bibr" target="#b31">[32]</ref> to video by conducting shift window MHSA in both space and time. TokShift <ref type="bibr" target="#b64">[65]</ref> proposes a temporal shift mechanism on CLASS tokens of ViT. Different from these video transformers focus on video classification, we target at building temporally efficient transformer for instance-level video understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overall Architecture</head><p>The overall architecture of our VIS method TeViT is shown in <ref type="figure">Fig. 1</ref>, which contains a transformer-based backbone network and a query-driven head network. Given a sequence of video frames, the transformer backbone performs feature extraction and generates multi-scale pyramid features. The query-driven head network takes randomly initialized instance queries with backbone feature maps to      <ref type="figure">Figure 1</ref>. The overall illustration of our TeViT framework. TeViT contains a messenger shift transformer backbone and a series of spatiotemporal query-driven instance heads. The messenger shift mechanism performs efficient frame-level temporal modeling by simply shifting messenger tokens along the temporal axis. Spatiotemporal query interaction conducts two successive and parameter-shared multi-head self attention (MHSA) with feed forward network (FFN) upon video instance queries. The "Dynamic Conv" design follows QueryInst <ref type="bibr" target="#b17">[18]</ref>. Best viewed in color.</p><formula xml:id="formula_0">E i v Y B 7 V A y a a Y N T T J j k i m U o d / h x o U i b v 0 Y d / 6 N m b Y L b T 0 Q O J x z L / f k B D F n 2 r j u t 5 N b W V 1 b 3 8 h v F r a 2 d 3 b 3 i v s H D R 0 l i t A 6 i X i k W g H W l D N J 6 4 Y Z T l u x o l g E n D a D 4 W 3 m N 0 d U a R b J R z O O q S 9 w X 7 K Q E W y s 5 H c E N g M l 0 r v q w / W k W y y 5 Z X c K t E y 8 O S n B H L V u 8 a v T i 0 g i q D S</formula><formula xml:id="formula_1">R D p V S B Y k T / N R p Y W M h W b C o I = " &gt; A A A B 8 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G P R i y e p a D + g C W W z 3 b Z L N 5 u w O x F K 6 N / w 4 k E R r / 4 Z b / 4 b t 2 0 O 2 v p g 4 P H e D D P z w k Q K g 6 7 7 7 R R W V t f W N 4 q b p a 3 t n d 2 9 8 v 5 B 0 8 S p Z r z B Y h n r d k g N l 0 L x B g q U v J 1 o T q N Q 8 l Y 4 u p n 6 r S e u j Y j V I 4 4 T H k R 0 o E R f M I p W 8 u + 6 2 c O E + C g i b r r l i l t 1 Z y D L x M t J B X L U u + U v v x e z N O I K m a T G d D w 3 w S C j G g W T f F L y U 8 M T y k Z 0 w D u W K m q X B N n s 5 g k 5 s U q P 9 G N t S y G Z q b 8 n M h o Z M 4 5 C 2 x l R H J p F b y r + 5 3 V S 7 F 8 F m V B J i l y x + a J + K g n G Z B o A 6 Q n N G c q x J Z R p Y W 8 l b E g 1 Z W h j K t k Q v M W X l 0 n z r O p d V M / v z y u 1 6 z y O I h z B M Z y C B 5 d Q g 1 u o Q w M Y J P A M r / D m p M 6 L 8 + 5 8 z F s L T j 5 z C H / g f P 4 A 1 Y 2 R k Q = = &lt; / l a t e x i t &gt; N S ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m x g v 8 4 y Z z 8 0 0 J n f y b 3 f W Q 8 Z o j C k = " &gt; A A A B 8 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G P R S 0 9 S w X 5 A E 8 p m u 2 2 X b j Z h d y K U 0 L / h x Y M i X v 0 z 3 v w 3 b t s c t P X B w O O 9 G W b m h Y k U B l 3 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o l T z X i T x T L W n Z A a L o X i T R Q o e S f R n E a h 5 O 1 w f D f z 2 0 9 c G x G r R 5 w k P I j o U I m B Y B S t 5 P s o I m 7 I f S + r T 3 v l i l t 1 5 y C r x M t J B X I 0 e u U v v x + z N O I K m a T G d D 0 3 w S C j G g W T f F r y U 8 M T y s Z 0 y L u W K m p X B d n 8 5 i k 5 s 0 q f D G J t S y G Z q 7 8 n M h o Z M 4 l C 2 x l R H J l l b y b + 5 3 V T H N w E m V B J i l y x x a J B K g n G Z B Y A 6 Q v N G c q J J Z R p Y W 8 l b E Q 1 Z W h j K t k Q v O W X V 0 n r o u p d V S 8 f L i u 1 2 z y O I p z A K Z y D B 9 d Q g z o 0 o A k M E n i G V 3 h z U u f F e X c + F q 0 F J 5 8 5 h j 9 w P n 8 A x e K R h g = = &lt; / l a t e x i t &gt; ?N H ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? &lt; l a</formula><formula xml:id="formula_2">g v P R T r u I E m K L z Q 2 E i M E Q 4 q w A P u G Y U x M Q S Q j W 3 W T E d E U 0 o 2 K J K t g R v 8 c v L p H V W 9 c 6 r t d t a p X 6 V 1 1 F E R + g Y n S I P X a A 6 u k E N 1 E Q U a f S M X t G b 8 + S 8 O O / O</formula><formula xml:id="formula_3">V i x 6 0 b z S q a N C P u G l / X F s = " &gt; A A A C C 3 i c b V A 9 S w N B E N 2 L X z F + n V r a L A b B K t x J U M u g j X Y J m A 9 I Q t j b T J I l u 3 v H 7 p 5 w H O l t / C s 2 F o r Y + g f s / D d u k h M 0 8 c H A 4 7 0 Z Z u Y F E W f a e N 6 X k 1 t Z X V v f y G 8 W t r Z 3 d v f c / Y O G D m N F o U 5 D H q p W Q D R w J q F u m O H Q i h Q Q E X B o B u P r q d + 8 B 6 V Z K O 9 M E k F X k K F k A</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " predict video instances. Our whole network is end-to-end for both training and inference.</p><formula xml:id="formula_4">x U i x B D M r u 9 a R 0 b T o x E W 4 Y 5 k 0 S b 4 = " &gt; A A A C A 3 i c b V D L S s N A F J 3 4 r P U V d a e b w S I I Q k m k q M u q U L p R K t o H t K F M p p N 2 6 G Q S Z i Z C C Q E 3 / o o b F 4 q 4 9 S f c + T d O 2 g j a e m D g z D n 3 c u 8 9 b s i o V J b 1 Z c z N L y w u L e d W 8 q t r 6 x u b 5 t Z 2 Q w a R w K S O A x a I l o s k Y Z S T u q K K k V Y o C P J d R p r u 8 D L 1 m / d E S B r w O z U K i e O j P q c e x U h p q W v u d n y k B s K P r 6 q 3 5 8 n R z 6 9 S u U 6 6 Z s E q W m P A W W J n p A A y 1 L r m Z 6 c X 4 M g n X G G G p G z b V q i c G A l F M S N J v h N J E i I 8 R H 3 S 1 p Q j n 0 g n H t + Q w A O t 9 K A X C P 2 4 g m P 1 d 0 e M f C l H v q s r 0 x 3 l t J e K / 3 n t S H l n T k x 5 G C n C 8 W S Q F z G o A p g G A n t U E K z Y S B O E B d W 7 Q j x A A m G l Y 8 v r E O</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Messenger Shift Transformer Backbone</head><p>In previous VIS methods, the backbone networks only perform feature extraction in per-frame fashion <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b56">57]</ref> and neglect the rich contextual information inherent in video frames. In contrast, inspired by MSG-Transformer <ref type="bibr" target="#b15">[16]</ref>, we propose messenger shift transformer (MsgShifT) which performs highly efficient temporal context modeling in a bottom-up manner, as shown in <ref type="figure">Fig. 1 (left)</ref>. Without loss of generality, we build MsgShifT based on the pyramid vision transformer (PVT) <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55]</ref>.</p><p>To be specific, given an input video with T frames of resolution H ? W , denoted as</p><formula xml:id="formula_5">{x i } T i=1 ? R T ?3?H?W , we</formula><p>first divide these frames into T ? HW P 2 patch tokens frameby-frame, where P denotes the size of each patch. Then we feed the flattened patch tokens to a linear projection and get embedded patches f 0 i T i=1 with size of T ? HW P 2 ? C, C denotes the channel dimension. Meanwhile, a group of randomly initialized learnable embeddings with size of M ? C are introduced as messenger tokens, denoted as m 0 , where M indicates the number of messenger tokens. Then we simply copy and concatenate messenger tokens with patch tokens:</p><formula xml:id="formula_6">[f 0 i , m 0 i ] T i=1 ? R T ?( HW P 2 +M )?C ,<label>(1)</label></formula><p>where m 0 i indicates the copycat of messenger tokens m 0 . The concatenated joint tokens</p><formula xml:id="formula_7">[f 0 i , m 0 i ] T i=1</formula><p>are taken as inputs for our MsgShifT.</p><p>Our transformer architecture consists of N S = 4 stages and each stage has the same architecture as in <ref type="figure">Fig. 1 (left)</ref>. The multi-head self attention (MHSA) and feed forward network (FFN) act on the concatenated joint tokens in a per-frame manner:</p><formula xml:id="formula_8">[f l i , m l i ] T i=1 = FFN l MHSA l [f l?1 i , m l?1 i ] T i=1</formula><p>.</p><p>(2) Next, a messenger shift manipulation performs temporal information exchange across video frames.</p><p>In short, the messenger shift mechanism takes temporal messenger tokens m l i T i=1 as inputs and builds temporal context modeling by shifting messenger tokens along the temporal axis. <ref type="figure">Fig. 2</ref> gives a detailed illustration. First, messenger tokens are divided into G = 4 groups and shifted along the temporal axis with different time steps (S = 1 or 2) and direction (forward or backward). With various time steps and directions, messenger tokens are able to achieve temporal context exchange with both past and future frames. Moreover, for every two messenger shift operations, we apply an inverse operation to the second one, which implies the messenger tokens will be shifted back to their original corresponding frames after two contiguous messenger shift manipulations. This design aims to maintain a stable temporal receptive field as the network goes deeper.</p><p>After the above process, the messenger tokens and patch tokens go through one of four stages, and the output tokens are reshaped to feature maps F 1 i T i=1 which is 1 4 smaller than the original image. In the same way, using the output messenger tokens and patch tokens of prior stage as inputs, we obtain the following pyramid feature maps</p><formula xml:id="formula_9">F 2 i T i=1 , F 3 i T i=1 and F 4 i T i=1</formula><p>, whose strides are 8, 16 and 32 pixels with respect to the input image. The pyramid feature maps</p><formula xml:id="formula_10">F 1 i , F 2 i , F 3 i , F 4 i T i=1</formula><p>will be used to predict video instances in the head network.</p><p>MsgShifT performs early temporal fusion in the backbone network, while the previous transformer-based VIS approaches <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b63">64]</ref> only perform temporal feature fusion using transformer encoders after image-level feature extraction. It is almost parameter-free, friendly to imagelevel pre-training models, and brings negligible computation costs. The messenger tokens are randomly initialized and the shift manipulation has no parameter, so this module is insensitive to the pre-training process, which will be further discussed in the experiments in Tab. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Spatiotemporal Query Interaction Head</head><p>MsgShifT achieves frame-level spatiotemporal context modeling. Meanwhile, in the VIS head network, our method still emphasizes temporally efficient spatiotemporal context modeling, but at the instance level. To this end, we propose a spatiotemporal query interaction (STQI) head network based on the recent SoTA query-based image-level instance segmentation method, i.e., QueryInst <ref type="bibr" target="#b17">[18]</ref>.</p><p>As shown in <ref type="figure">Fig. 1 (right)</ref>, our head network contains N H = 6 STQI heads and takes a fixed-length instance queries Q ? R Nq?C along with pyramid features extracted by MsgShifT</p><formula xml:id="formula_11">F 1 i , F 2 i , F 3 i , F 4 i T i=1</formula><p>as inputs, and generates N q ? T instance predictions. N q and C denotes the numbers and the channel dimensions of instance query respectively. Instance queries Q are randomly initialized and optimized during training. Additionally, our VIS network also contains a set of proposal boxes B ? R Nq?4 as prior proposals, for more details about this, we refer readers to QueryInst <ref type="bibr" target="#b17">[18]</ref>.</p><p>Instance queries are firstly copied by T times to each frame. Two successive and parameter-shared MHSA mod-ules act on instance queries along spatial and temporal dimensions:Q</p><formula xml:id="formula_12">1:T 1:Nq = MHSA Q i 1:Nq T i=1 ,<label>(3)</label></formula><formula xml:id="formula_13">Q 1:T 1:Nq = MHSA Q 1:T j Nq j=1 .<label>(4)</label></formula><p>"1:K" denotes ranging from 1 to K. Enhanced instance queries Q are fed into a dynamic convolution module and perform interactions with instance region features. Its output serves as the input queries of the next head. Finally, task specific heads (i.e., classification head, box head and mask head) predict a sequence of video instances:</p><formula xml:id="formula_14">? t i 1:T 1:Nq = p t i (c),b t i ,m t i 1:T 1:Nq ,<label>(5)</label></formula><p>wherep(c),b andm denotes predicted confidence scores, bounding boxes and instance foreground masks, respectively.</p><p>The advantages of STQI mainly stem from the minimum modifications on the still-image instance prediction head in <ref type="bibr" target="#b17">[18]</ref>. STQI achieves highly efficient temporal context modeling at instance-level by a parameter-shared MHSA (Eq. 3 and Eq. 4) while does not involve extra parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Matching and Loss Function</head><p>The loss function is motivated by <ref type="bibr" target="#b6">[7]</ref>. We first compute the one-to-one assignment between predicted instances and ground-truth annotations. The ground-truth annotations are denoted as follows:</p><formula xml:id="formula_15">y t j 1:T 1:Ngt = c t j , b t j , m t j 1:T 1:Ngt ,<label>(6)</label></formula><p>in which N gt indicates the number of ground-truth video instances, c, b and m indicates the category, bounding box and mask respectively. We then perform sequence-level bipartite matching between predictions and annotations by Hungarian algorithm <ref type="bibr" target="#b25">[26]</ref>. The cost matrix with size of N q ?N gt between each predicted video instance and each annotation is defined as follows.</p><formula xml:id="formula_16">L Hung (? 1:T i , y 1:T j ) = ? cls ? L cls (p 1:T i (c), p 1:T j ) + ? L1 ? L L1 (b 1:T i , b 1:T j ) + ? giou ? L giou (b 1:T i , b 1:T j ),<label>(7)</label></formula><p>where L cls indicates the focal loss <ref type="bibr" target="#b28">[29]</ref> for classification, L L1 and L giou indicates the L1 loss and GIoU loss <ref type="bibr" target="#b41">[42]</ref> respectively. ? cls , ? L1 , ? giou ? R are hyper-parameters which we simply follow <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b67">68]</ref>. Besides we use dice coefficient <ref type="bibr" target="#b34">[35]</ref> to optimize mask predictions. For more details, please refer to <ref type="bibr" target="#b17">[18]</ref>.  <ref type="bibr" target="#b61">[62]</ref>. "?" under "MST" indicates using multi-scale training strategy, The FPS is measured with a single TESLA V100 GPU. All methods in the figure are organized into four groups. According to their basic architectures, the first two groups of methods are built upon CNN architecture, while the last two are transformer-based. According to their training inference paradigms, the first group follows the online and track-by-detect fashion, while the rest all follow offline and sequence-insequence-out paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Online and Offline Inference</head><p>Our method is flexible for both offline and online inference. Under the offline scenario, our TeViT takes the whole video clips as inputs and then outputs all possible video instances with a single run. No post-tracking process is needed. When it comes to the near online <ref type="bibr" target="#b1">[2]</ref> scenario, an entire video is split into several overlapping segments. TeViT takes clips in time order and generates predictions. A rule-based post-tracking procedure is applied to linking instances across different video clips. For instances from two overlapping video clips, we first compute the similarity score between each instance, and then a Hungarian matcher gives the assignment according to the similarity matrix. The similarity score is defined as a combination of box IoU and mask IoU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Metrics</head><p>We evaluate TeViT on three challenging video instance segmentation benchmarks, i.e., YouTube-VIS-2019 <ref type="bibr" target="#b61">[62]</ref>, YouTube-VIS-2021 <ref type="bibr" target="#b60">[61]</ref>, and OVIS <ref type="bibr" target="#b39">[40]</ref>. YouTube-VIS-2019 is the first dataset that focuses on the VIS problem. It contains 40 common object categories, 4, 883 unique video instances and about 131k high-quality instance-level annotations. YouTube-VIS-2021 dataset is the new version of YouTube-VIS-2019 with 1.5? more video frames and 2? more annotations. OVIS dataset aims to explore the VIS problem under high-occlusion scenarios. It consists of 296k high-quality instance masks and 5.80 instances per video from 25 semantic categories. Following previous works, we report the performance on the validation set for all three datasets. We follow the standard VIS evaluation metrics defined in <ref type="bibr" target="#b61">[62]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>TeViT is built upon the mmdetection toolbox <ref type="bibr" target="#b10">[11]</ref>. Unless otherwise noted, hyper-parameters follow the settings of QueryInst <ref type="bibr" target="#b17">[18]</ref>. We use N q = 100 video instance queries as <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23]</ref>. Due to the temporal efficient designs in TeViT, we do not need to create pseudo video data, e.g. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23]</ref>, to train the temporal modeling parameters, instead, we first train a transformer-based QueryInst for imagelevel instance segmentation on the COCO dataset <ref type="bibr" target="#b29">[30]</ref> and then initialize TeViT with the COCO pre-trained QueryInst weights. Besides, we provide a MindSpore <ref type="bibr" target="#b35">[36]</ref> implementation of TeViT.  <ref type="table">Table 3</ref>. Comparisons on OVIS dataset. Methods with superscript " ?" and " ?" are reported in <ref type="bibr" target="#b62">[63]</ref> and <ref type="bibr" target="#b39">[40]</ref> respectively.</p><p>When training on the VIS datasets, we use the AdamW <ref type="bibr" target="#b24">[25]</ref> optimizer with an initial learning rate of 0.00025, and a weight decay of 0.0001. Especially, the backbone learning rate is slightly lower with a multiplier set to 0.1. We also apply gradient clipping with a maximal gradient norm of 0.1. TeViT is trained with a batch size of 16 and a clip length of T = 5. The total training process contains 12 epochs, and the learning rate is decreased by 10 at the 8-th and 11-th epoch respectively. For example, our TeViT can be trained in about 4 hours with 8 Tesla V100 GPUs on YouTube-VIS-2019, which is much faster than previous transformer-based method (i.e., VisTR <ref type="bibr" target="#b56">[57]</ref>). The number of instance queries N q is set to 100 for all experiments. Following <ref type="bibr" target="#b61">[62]</ref>, all input frames are resized to 360 ? 640 in single-scale experiments. Settings of multiscale training simply follow <ref type="bibr" target="#b5">[6]</ref>. For inference, all frames are resized to 360 ? 640 regardless of the training setups. During inference, we use T = 36 for most results and report the near online results in ablation study. For main results, we evaluate our framework on YouTube-VIS-2019, YouTube-VIS-2021, and OVIS datasets, with PVT-B1 <ref type="bibr" target="#b54">[55]</ref> based MsgShifT as backbone.</p><p>It's noted that all reported results in main results and ablation studies are average performance from multiple runs (i.e., we choose five different random seeds and run each random seed for three times). The standard deviations (i.e., ? AP in following tables) are calculated in the same way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Main Results</head><p>Main results on YouTube-VIS-2019 dataset. We compare our TeViT to state-of-the-art methods on YouTube-VIS-2019 dataset in Tab. 1. The longest video in YouTube-VIS-2021 dataset only contains 36 frames, so that our TeViT executes fully offline inference on this dataset. Without bells and whistles, our TeViT achieves 45.9 AP when using a single-scale training strategy and outperforms the previous state-of-the-art methods by a large margin. Multi-scale training strategy further boosts the performance to 46.6 AP. Meanwhile, our method also achieves competitive inference speed. With about 10 AP higher, our method is still faster than VisTR. Main results on YouTube-VIS-2021 dataset. Tab. 2 shows the final results of several VIS methods and ours on YouTube-VIS-2021 dataset. Due to the video length in YouTube-VIS-2021 is longer than our inference clip length (T = 36), TeViT performs near online tracking described in Sec. 3.5 on this dataset. TeViT obtains 37.9 AP, outperforming the previous state-of-the-art method by 2.7 AP. Main results on OVIS dataset. The results on the OVIS dataset are shown in Tab. 3. Our method also performs near online inference on OVIS dataset. TeViT achieves a relatively higher performance of 17.4 AP on the val split, surpassing previous state-of-the-art methods. Compared to CMaskTrack R-CNN <ref type="bibr" target="#b39">[40]</ref> which presents an elaboratedesigned feature calibration plug-in to alleviate occlusion, our TeViT still gains 2.0 AP improvement, which shows that our temporal context modeling designs are helpful to segment occluded instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>Effect of frame-level &amp; instance-level temporal context modeling. We investigate the effects of messenger shift mechanism and spatiotemporal query interaction individually and simultaneously in Tab. 4. Using messenger shift mechanism and spatiotemporal query interaction individually brings 0.6 and 2.7 AP improvements respectively. The results show that both frame-level and instance-level temporal context modeling can obviously improve VIS performance. In addition, the instance-level one brings more significant performance gain. The two designs together brings 3.4 (&gt; 0.6 + 2.7) AP improvements over a highperformance baseline. Besides the remarkable performance improvements, our designs only bring 0.27% computation overhead on our baseline (82.19 GFLOPs vs. 81.97 GFLOPs), which demonstrates our design is very efficient. Variants of spatiotemporal query interaction. In Tab. 5, we investigate the effectiveness of our spatiotemporal query interaction comparing to its variants. A naive query interaction method in <ref type="bibr" target="#b17">[18]</ref> without using temporal interaction, denoted as "Spatial Only", serves as a baseline. "Fused Space-Time" in the Row 2 denotes fusing video instance queries together and performing spatial and temporal interaction within a single MHSA, which is the same as in <ref type="bibr" target="#b56">[57]</ref>. As the results show: (1) Our spatiotemporal query interaction  <ref type="table">Table 5</ref>. Variants of spatiotemporal query interaction. "Spatial Only" denotes the image-level instance segmentation heads in <ref type="bibr" target="#b17">[18]</ref>, "Fused Space-Time" denotes applying MHSA to all video instance queries at a single run, which is the same as in <ref type="bibr" target="#b56">[57]</ref>.</p><p>Manip. AP ? ?AP AP50 AP75 achieves the best performance among three variants. <ref type="formula">(2)</ref> Compared to the spatial-only query interaction, joint spatiotemporal query interaction brings only 0.8 AP improvements. However, our method achieves 2.7 AP gains. We argue this is because the one-to-one corresponding between instance queries are misaligned in the joint spatiotemporal attention, while ours is not. Different messenger token manipulation methods. We compare our messenger shift mechanism with two other optional manipulations in Tab. 6. None indicates there are no extra manipulations conducted on messenger tokens, thus no temporal information is employed. MHSA+FFN stands for the same operation in <ref type="bibr" target="#b22">[23]</ref> which performs extra MHSA and FFN on messenger tokens, and Shift denotes our messenger shift mechanism. Different from <ref type="bibr" target="#b22">[23]</ref>, we do not conduct any extra pre-training process so that both the messenger tokens and MHSA layers with FFN are randomly initialized and trained from scratch. As results have shown: We also compare our messenger shift mechanism with other optional frame-level feature aggregation manipulations in Tab. 7. None indicates no manipulation is conducted to aggregate frame-level temporal features. Conv. and MHSA + FFN denotes using newly introduced con-volution layers or transformer layers to achieve temporal feature aggregation. As results have shown, our messenger shift mechanism achieves the best performance (i.e., 45.9 AP) compared to its all counterparts. Meanwhile, we observe apparent performance decrease by using newly introduced Conv. or MHSA + FFN as aggregation layers. We infer such a performance decrease comes from the enormous newly introduced parameters, while our messenger shift mechanism eliminates this performance decrease by the nearly parameter-free design. Number of messenger tokens. In Tab. 8, we test our method with number of messenger tokens increases from 8 to 32. Compared to less messenger tokens (M = 8, 16), more messenger tokens (M = 32) achieves better results. Unless specified, our experiments are conducted with 32 messenger tokens.</p><p>Training and inference clip length. We also investigate the effects of clip length in both the training and testing phase. From Tab. 9, we find that: (1) Our method shows great tolerance to short length of training clip. Only trained with 2 or 3 frames, our method can effectively learn temporal context and obtains comparable results to previous methods.</p><p>(2) The performance improvements by increasing the length of the training clip gradually gets saturated. Increasing training clip length from 2 to 3 and 3 to 5 brings 3.2 AP and 1.6 AP gains respectively while increasing training clip length from 5 to 7 only obtains slight 0.4 AP profit. Besides, a longer training clip requires more training computations and memory budgets. To this end, we set the training clip length of our method to T = 5 as a compromise between performance and training costs. Tab. 10 gives the results of TeViT under different in- ference settings. "T" indicates the input clip length during the inference phase, and "S" indicates strides. It shows that our TeViT obtains promising performance under various inference setups. Even with T = 5 and S = 3, TeViT still achieves 41.7 AP, which indicates TeViT can serve as a strong baseline for both offline and online video understanding scenarios.</p><p>Performance under ResNet backbone. We compare our method with previous transformer-based methods using ResNet-50 <ref type="bibr" target="#b20">[21]</ref> as backbone network in Tab. 11. As results show, our method achieves 41.7 AP on YouTube-VIS-2019 dataset with single-scale training (Row 4), even outperforming VisTR and IFC with multi-scale training strategy. The performance goes a step further to 42.3 AP when the multi-scale strategy is applied (Row 5). It's worth noting that with ResNet-50 as backbone network, the messenger tokens, and messenger shift mechanism are unable to proceed, so our method achieves such high performance with only the STQI mechanism. We also investigate the improvements by our STQI head. Simply taking off the spatial MHSA in Eq. 4, the final performance drops from 41.7 AP to 36.8 AP (Row 3), demonstrating the effectiveness of our STQI head directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Revisiting messenger tokens.</head><p>Inspired by MSG-Transformer <ref type="bibr" target="#b15">[16]</ref>, we re-initialize messenger tokens in inference phase and obvious the influence on performance in Tab. 12. As the results show, when we re-initialize messenger tokens to zero, the performance merely drops 0.4 AP (compare Row 2 to Row 1). Randomly initialize the messenger token in inference phase leads to a similar performance decrease (Row 3). We think this phenomenon implies that the messenger tokens contain only a few or not specific information in themselves. On the contrary, they play the role of summarizing frame-level contexts, and exchanging them across adjacent frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we provide lightweight and effective solutions to fully exploit temporal context for VIS. Based on existing ViTs and query-based image-level instance segmentation methods, we proposes the TeViT VIS method that contains the messenger shift and spatiotemporal query interaction mechanisms. TeViT performs both frame-level and instance-level temporal feature interactions while only bringing a few parameters and marginal extra computational costs. Experiments on YouTube-VIS-2019, YouTube-VIS-2021, and OVIS show that TeViT can obtain remarkably better results than previous SoTA methods, e.g., IFC, VisTR, MaskProp, and STEm-Seg. We believe the proposed temporal context modeling mechanisms have great potential to be extended to other video understanding tasks. Limitations. Although the extensive experiments have demonstrated the capacity and efficiency of our TeViT on temporal context modeling, it still suffers effects from occlusion, motion deformation and long time-span videos (i.e., results of TeViT in Tab. 2 and Tab. 3 are far from satisfying). We leave these promising directions as future work. Broader impact. Although our research does not make direct negative impacts in society, it may be misused by illegal video applications, which could be a potential invasion to human privacy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>&lt;</head><label></label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " x U i x B D M r u 9 a R 0 b T o x E W 4 Y 5 k 0 S b 4 = " &gt; A A A C A 3 i c b V D L S s N A F J 3 4 r P U V d a e b w S I I Q k m k q M u q U L p R K t o H t K F M p p N 2 6 G Q S Z i Z C C Q E 3 / o o b F 4 q 4 9 S f c + T d O 2 g j a e m D g z D n 3 c u 8 9 b s i o V J b 1 Z c z N L y w u L e d W 8 q t r 6 x u b 5 t Z 2 Q w a R w K S O A x a I l o s k Y Z S T u q K K k V Y o C P J d R p r u 8 D L 1 m / d E S B r w O z U K i e O j P q c e x U h p q W v u d n y k B s K P r 6 q 3 5 8 n R z 6 9 S u U 6 6 Z s E q W m P A W W J n p A A y 1 L r m Z 6 c X 4 M g n X G G G p G z b V q i c G A l F M S N J v h N J E i I 8 R H 3 S 1 p Q j n 0 g n H t + Q w A O t 9 K A X C P 2 4 g m P 1 d 0 e M f C l H v q s r 0 x 3 l t J e K / 3 n t S H l n T k x 5 G C n C 8 W S Q F z G o A p g G A n t U E K z Y S B O E B d W 7 Q j x A A m G l Y 8 v r E O z p k 2 d J 4 7 h o n x R L N 6 V C + S K L I w f 2 w D 4 4 B D Y 4 B W V Q B T V Q B x g 8 g C f w A l 6 N R + P Z e D P e J 6 V z R t a z A / 7 A + P g G y p u X o A = = &lt; / l a t e x i t &gt; MHSA + FFN &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x U i x B D M r u 9 a R 0 b T o x E W 4 Y 5 k 0 S b 4 = " &gt; A A A C A 3 i c b V D L S s N A F J 3 4 r P U V d a e b w S I I Q k m k q M u q U L p R K t o H t K F M p p N 2 6 G Q S Z i Z C C Q E 3 / o o b F 4 q 4 9 S f c + T d O 2 g j a e m D g z D n 3 c u 8 9 b s i o V J b 1 Z c z N L y w u L e d W 8 q t r 6 x u b 5 t Z 2 Q w a R w K S O A x a I l o s k Y Z S T u q K K k V Y o C P J d R p r u 8 D L 1 m / d E S B r w O z U K i e O j P q c e x U h p q W v u d n y k B s K P r 6 q 3 5 8 n R z 6 9 S u U 6 6 Z s E q W m P A W W J n p A A y 1 L r m Z 6 c X 4 M g n X G G G p G z b V q i c G A l F M S N J v h N J E i I 8 R H 3 S 1 p Q j n 0 g n H t + Q w A O t 9 K A X C P 2 4 g m P 1 d 0 e M f C l H v q s r 0 x 3 l t J e K / 3 n t S H l n T k x 5 G C n C 8 W S Q F z G o A p g G A n t U E K z Y S B O E B d W 7 Q j x A A m G l Y 8 v r E O z p k 2 d J 4 7 h o n x R L N 6 V C + S K L I w f 2 w D 4 4 B D Y 4 B W V Q B T V Q B x g 8 g C f w A l 6 N R + P Z e D P e J 6 V z R t a z A / 7 A + P g G y p u X o A = = &lt; / l a t e x i t &gt; MHSA + FFN &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x U i x B D M r u 9 a R 0 b T o x E W 4 Y 5 k 0 S b 4 = " &gt; A A A C A 3 i c b V D L S s N A F J 3 4 r P U V d a e b w S I I Q k m k q M u q U L p R K t o H t K F M p p N 2 6 G Q S Z i Z C C Q E 3 / o o b F 4 q 4 9 S f c + T d O 2 g j a e m D g z D n 3 c u 8 9 b s i o V J b 1 Z c z N L y w u L e d W 8 q t r 6 x u b 5 t Z 2 Q w a R w K S O A x a I l o s k Y Z S T u q K K k V Y o C P J d R p r u 8 D L 1 m / d E S B r w O z U K i e O j P q c e x U h p q W v u d n y k B s K P r 6 q 3 5 8 n R z 6 9 S u U 6 6Z s E q W m P A W W J n p A A y 1 L r m Z 6 c X 4 M g n X G G G p G z b V q i c G A l F M S N J v h N J E i I 8 R H 3 S 1 p Q j n 0 g n H t + Q w A O t 9 K A X C P 2 4 g m P 1 d 0 e M f C l H v q s r 0 x 3 l t J e K / 3 n t S H l n T k x 5 G C n C 8 W S Q F z G o A p g G A n t U E K z Y S B O E B d W 7 Q j x A A m G l Y 8 v r E O z p k 2 d J 4 7 h o n x R L N 6 V C + S K L I w f 2 w D 4 4 B D Y 4 B W V Q B T V Q B x g 8 g C f wA l 6 N R + P Z e D P e J 6 V z R t a z A / 7 A + P g G y p u X o A = = &lt; / l a t e x i t &gt; MHSA + FFN &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x U i x B D M r u 9 a R 0 b T o x E W 4 Y 5 k 0 S b 4 = " &gt; A A A C A 3 i c b V D L S s N A F J 3 4 r P U V d a e b w S I I Q k m k q M u q U L p R K t o H t K F M p p N 2 6 G Q S Z i Z C C Q E 3 / o o b F 4 q 4 9 S f c + T d O 2 g j a e m D g z D n 3 c u 8 9 b s i o V J b 1 Z c z N L y w u L e d W 8 q t r 6 x u b 5 t Z 2 Q w a R w K S O A x a I l o s k Y Z S T u q K K k V Y o C P J d R p r u 8 D L 1 m / d E S B r w O z U K i e O j P q c e x U h p q W v u d n y k B s K P r 6 q 3 5 8 n R z 6 9 S u U 6 6 Z s E q W m P A W W J n p A A y 1 L r m Z 6 c X 4 M g n X G G G p G z b V q i c G A l F M S N J v h N J E i I 8 R H 3 S 1 p Q j n 0 g n H t + Q w A O t 9 K A X C P 2 4 g m P 1 d 0 e M f C l H v q s r 0 x 3 l t J e K / 3 n t S H l n T k x 5 G C n C 8 W S Q F z G o A p g G A n t U E K z Y S B O E B d W 7 Q j x A A m G l Y 8 v r E O z p k 2 d J 4 7 h o n x R L N 6 V C + S K L I w f 2 w D 4 4 B D Y 4 B W V Q B T V Q B x g 8 g C f w A l 6 N R + P Z e D P e J 6 V z R t a z A / 7 A + P g G y p u X o A = = &lt; / l a t e x i t &gt; MHSA + FFN &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x U i x B D M r u 9 a R 0 b T o x E W 4 Y 5 k 0 S b 4 = " &gt; A A A C A 3 i c b V D L S s N A F J 3 4 r P U V d a e b w S I I Q k m k q M u q U L p R K t o H t K F M p p N 2 6 G Q S Z i Z C C Q E 3 / o o b F 4 q 4 9 S f c + T d O 2 g j a e m D g z D n 3 c u 8 9 b s i o V J b 1 Z c z N L y w u L e d W 8 q t r 6 x u b 5 t Z 2 Q w a R w K S O A x a I l o s k Y Z S T u q K K k V Y o C P J d R p r u 8 D L 1 m / d E S B r w O z U K i e O j P q c e x U h p q W v u d n y k B s K P r 6 q 3 5 8 n R z 6 9 S u U 6 6 Z s E q W m P A W W J n p A A y 1 L r m Z 6 c X 4 M g n X G G G p G z b V q i c G A l F M S N J v h N J E i I 8 R H 3 S 1 p Q j n 0 g n H t + Q w A O t 9 K A X C P 2 4 g m P 1 d 0 e M f C l H v q s r 0 x 3 l t J e K / 3 n t S H l n T k x 5 G C n C 8 W S Q F z G o A p g G A n t U E K z Y S B O E B d W 7 Q j x A A m G l Y 8 v r E O z p k 2 d J 4 7 h o n x R L N 6 V C + S K L I w f 2 w D 4 4 B D Y 4 B W V Q B T V Q B x g 8 g C f w A l 6 N R + P Z e D P e J 6 V z R t a z A / 7 A + P g G y p u X o A = = &lt; / l a t e x i t &gt; MHSA + FFN &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x U i x B D M r u 9 a R 0 b T o x E W 4 Y 5 k 0 S b 4= " &gt; A A A C A 3 i c b V D L S s N A F J 3 4 r P U V d a e b w S I I Q k m k q M u q U L p R K t o H t K F M p p N 2 6 G Q S Z i Z C C Q E 3 / o o b F 4 q 4 9 S f c + T d O 2 g j a e m D g z D n 3 c u 8 9 b s i o V J b 1 Z c z N L y w u L e d W 8 q t r 6 x u b 5 t Z 2 Q w a R w K S O A x a I l o s k Y Z S T u q K K k V Y o C P J d R p r u 8 D L 1 m / d E S B r w O z U K i e O j P q c e x U h p q W v u d n y k B s K P r 6 q 3 5 8 n R z 6 9 S u U 6 6 Z s E q W m P A W W J n p A A y 1 L r m Z 6 c X 4 M g n X G G G p G z b V q i c G A l F M S N J v h N J E i I 8 R H 3 S 1 p Q j n 0 g n H t + Q w A O t 9 K A X C P 2 4 g m P 1 d 0 e M f C l H v q s r 0 x 3 l t J e K / 3 n t S H l n T k x 5 G C n C 8 W S Q F z G o A p g G A n t U E K z Y S B O E B d W 7 Q j x A A m G l Y 8 v r E O z p k 2 d J 4 7 h o n x R L N 6 V C + S K L I w f 2 w D 4 4 B D Y 4 B W V Q B T V Q B x g 8 g C f w A l 6 N R + P Z e D P e J 6 V z R t a z A / 7 A + P g G y p u X o A = = &lt; / l a t e x i t &gt;MHSA + FFN&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N k n e 6 d m F a E M t I a U c j k V 6 s c 9 K 0 S U = " &gt; A A A B 9 H i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s y I q M u q m 2 6 E i v Y B 7 V A y a a Y N T T J j k i m U o d / h x o U i b v 0 Y d / 6 N m b Y L b T 0 Q O J x z L / f k B D F n 2 r j u t 5 N b W V 1 b 3 8 h v F r a 2 d 3 b 3 i v s H D R 0 l i t A 6 i X i k W g H W l D N J 6 4 Y Z T l u x o l g E n D a D 4 W 3 m N 0 d U a R b J R z O O q S 9 w X 7 K Q E W y s 5 H c E N g M l 0 r v q w / W k W y y 5 Z X c K t E y 8 O S n B H L V u 8 a v T i 0 g i q D S E Y 6 3 b n h s b P 8 X K M M L p p N B J N I 0 x G e I + b V s q s a D a T 6 e h J + j E K j 0 U R s o + a d B U / b 2 R Y q H 1 W A R 2 M g u p F 7 1 M / M 9 r J y a 8 8 l M m 4 8 R Q S W a H w o Q j E 6 G s A d R j i h L D x 5 Z g o p j N i s g A K 0 y M 7 a l g S / A W v 7 x M G m d l 7 6 J 8 f n 9 e q t z M 6 8 j D E R z D K X h w C R W o Q g 3 q Q O A J n u E V 3 p y R 8 + K 8 O x + z 0 Z w z 3 z m E P 3 A + f w C r P Z I L &lt; / l a t e x i t &gt; MHSA &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N k n e 6 d m F a E M t I a U c j k V 6 s c 9 K 0 S U = " &gt; A A A B 9 H i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s y I q M u q m 2 6 E i v Y B 7 V A y a a Y N T T J j k i m U o d / h x o U i b v 0 Y d / 6 N m b Y L b T 0 Q O J x z L / f k B D F n 2 r j u t 5 N b W V 1 b 3 8 h v F r a 2 d 3 b 3 i v s H D R 0 l i t A 6 i X i k W g H W l D N J 6 4 Y Z T l u x o l g E n D a D 4 W 3 m N 0 d U a R b J R z O O q S 9 w X 7 K Q E W y s 5 H c E N g M l 0 r v q w / W k W y y 5 Z X c K t E y 8 O S n B H L V u 8 a v T i 0 g i q D S E Y 6 3 b n h s b P 8 X K M M L p p N B J N I 0 x G e I + b V s q s a D a T 6 e h J + j E K j 0 U R s o + a d B U / b 2 R Y q H 1 W A R 2 M g u p F 7 1 M / M 9 r J y a 8 8 l M m 4 8 R Q S W a H w o Q j E 6 G s A d R j i h L D x 5 Z g o p j N i s g A K 0 y M 7 a l g S / A W v 7 x M G m d l 7 6 J 8 f n 9 e q t z M 6 8 j D E R z D K X h w C R W o Q g 3 q Q O A J n u E V 3 p y R 8 + K 8 O x + z 0 Z w z 3 z m E P 3 A + f w C r P Z I L &lt; / l a t e x i t &gt; MHSA &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N k n e 6 d m F a E M t I a U c j k V 6 s c 9 K 0 S U = " &gt; A A A B 9 H i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s y I q M u q m 2 6 E i v Y B 7 V A y a a Y N T T J j k i m U o d / h x o U i b v 0 Y d / 6 N m b Y L b T 0 Q O J x z L / f k B D F n 2 r j u t 5 N b W V 1 b 3 8 h v F r a 2 d 3 b 3 i v s H D R 0 l i t A 6 i X i k W g H W l D N J 6 4 Y Z T l u x o l g E n D a D 4 W 3 m N 0 d U a R b J R z O O q S 9 w X 7 K Q E W y s 5 H c E N g M l 0 r v q w / W k W y y 5 Z X c K t E y 8 O S n B H L V u 8 a v T i 0 g i q D S E Y 6 3 b n h s b P 8 X K M M L p p N B J N I 0 x G e I + b V s q s a D a T 6 e h J + j E K j 0 U R s o + a d B U / b 2 R Y q H 1 W A R 2 M g u p F 7 1 M / M 9 r J y a 8 8 l M m 4 8 R Q S W a H w o Q j E 6 G s A d R j i h L D x 5 Z g o p j N i s g A K 0 y M 7 a l g S / A W v 7 x M G m d l 7 6 J 8 f n 9 e q t z M 6 8 j D E R z D K X h w C R W o Q g 3 q Q O A J n u E V 3 p y R 8 + K 8 O x + z 0 Z w z 3 z m E P 3 A + f w C r P Z I L &lt; / l a t e x i t &gt; MHSA &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N k n e 6 d m F a E M t I a U c j k V 6 s c 9 K 0 S U = " &gt; A A A B 9 H i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s y I q M u q m 2 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>E Y 6 3 b n h s b P 8 X K M M L p p N B J N I 0 x G e I + b V s q s a D a T 6 e h J + j E K j 0 U R s o + a d B U / b 2 R Y q H 1 W A R 2 M g u p F 7 1 M / M 9 r J y a 8 8 l M m 4 8 R Q S W a H w o Q j E 6 G s A d R j i h L D x 5 Z g o p j N i s g A K 0 y M 7 a l g S / A W v 7 x M G m d l 7 6 J 8 f n 9 e q t z M 6 8 j D E R z D K X h w C R W o Q g 3 q Q O A J n u E V 3 p y R 8 + K 8 O x + z 0 Z w z 3 z m E P 3 A + f w C r P Z I L &lt; / l a t e x i t &gt; MHSA &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 H M 7 z</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " 8 6 k A B k r o B + 8 q D q s K d D U f X q H C X N c = " &gt; A A A B 9 X i c b V D L S g M x F M 3 U V 6 2 v q k s 3 w S K 4 K j N S 1 G X R j c u K 9 g H t W D J p p g 1 N M k N y R y l D / 8 O N C 0 X c + i / u / B s z 7 S y 0 9 U D g c M 6 9 3 J M T x I I b c N 1 v p 7 C y u r a + U d w s b W 3 v 7 O 6 V 9 w 9 a J k o 0 Z U 0 a i U h 3 A m K Y 4 I o 1 g Y N g n V g z I g P B 2 s H 4 O v P b j 0 w b H q l 7 m M T M l 2 S o e M g p A S s 9 9 C S B k Z b p 3 Y i H M O 2 X K 2 7 V n Q E v E y 8 n F Z S j 0 S 9 / 9 Q Y R T S R T Q A U x p u u 5 M f g p 0 c C p Y N N S L z E s J n R M h q x r q S K S G T + d p Z 7 i E 6 s M c B h p + x T g m f p 7 I y X S m I k M 7 G S W 0 i x 6 m f i f 1 0 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>x 3 y 0 4 O</head><label>4</label><figDesc>Q 7 h + g P n M 8 f G V K S 6 g = = &lt; / l a t e x i t &gt; Shift &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " k 3 q D h D q l 7 z J h 4 k o P 6 H L H S F Y d r b Y = " &gt; A A A C C n i c b V D L S g M x F M 3 4 r P U 1 6 t J N t A i u y o w U d V l 0 o 7 u K 9 g G d o W T S T B u a Z I Y k I 5 R h 1 m 7 8 F T c u F H H r F 7 j z b 8 y 0 I 2 j r g c D h n H P J v S e I G V X a c b 6 s h c W l 5 Z X V 0 l p 5 f W N z a 9 v e 2 W 2 p K J G Y N H H E I t k J k C K M C t L U V D P S i S V B P G C k H Y w u c 7 9 9 T 6 S i k b j T 4 5 j 4 H A 0 E D S l G 2 k g 9 + 8 D j S A 8 l T 6 9 F H i M Z 9 O C P d D u k o c 5 6 d s W p O h P A e e I W p A I K N H r 2 p 9 e P c M K J 0 J g h p b q u E 2 s / R V J T z E h W 9 h J F Y o R H a E C 6 h g r E i f L T y S k Z P D J K H 4 a R N E 9 o O F F / T 6 S I K z X m g U n m W 6 p Z L x f / 8 7 q J D s / 9 l I o 4 0 U T g 6 U d h w q C O Y N 4 L 7 F N J s G Z j Q x C W 1 O w K 8 R B J h L W p p W x K c G d P n i e t k 6 p 7 W q 3 d 1 C r 1 i 6 K O E t g H h + A Y u O A M 1 M E V a I A m w O A B P I E X 8 G o 9 W s / W m / U + j S 5 Y x c w e + A P r 4 x s d 7 Z s u &lt; / l a t e x i t &gt; Inverse Shift &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f G n u 4 J Y H d S R d b F 3 D u 3 N i O c 3 2 K Z w = " &gt; A A A C C X i c b V D L S s N A F J 3 U V 6 2 v q E s 3 g 0 V w V R I R d V m s C 5 c V 7 A O a U C b T S T t 0 M h N m J o U Q s n X j r 7 h x o Y h b / 8 C d f + O 0 D a K t B y 4 c z r m X e + 8 J Y k a V d p w v q 7 S y u r a + U d 6 s b G 3 v 7 O 7 Z + w d t J R K J S Q s L J m Q 3 Q I o w y k l L U 8 1 I N 5 Y E R Q E j n W D c m P q d C Z G K C n 6 v 0 5 j 4 E R p y G l K M t J H 6 N v Q i p E c y y m 5 S j i K K c + j 9 S A 3 B J 3 n f r j o 1 Z w a 4 T N y C V E G B Z t / + 9 A Y C J x H h G j O k V M 9 1 Y u 1 n S G q K G c k r X q J I j P A Y D U n P U L O V K D + b f Z L D E 6 M M Y C i k K a 7 h T P 0 9 k a F I q T Q K T O f 0 S L X o T c X / v F 6 i w y s / o z x O N O F 4 v i h M G N Q C T m O B A y o J 1 i w 1 B G F J z a 0 Q j 5 B E W J v w K i Y E d / H l Z d I + q 7 k X t f O 7 8 2 r 9 u o i j D I 7 A M T g F L r g E d X A L m q A F M H g A T + A F v F q P 1 r P 1 Z r 3 P W 0 t W M X M I / s D 6 + A Y p r 5 q l &lt; / l a t e x i t &gt; Dynamic Conv &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U 5 / c + B /</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>0 a J s V L P P e 4 I Y k Z K p L d S G y I p T H A H / 2 i 1 G F Q y 6 b l F r + T N g J e J n 5 E i y l D t u Z + d f k h j A d J Q T r R u + 1 5 k u i l R h l E O k 0 I n 1 h A R O i Z D a F s q i Q D d T W e / T P C J V f p 4 E C p b 0 u C Z + n s i J U L r R A S 2 c 3 q l X v S m 4 n 9 e O z a D y 2 7 K Z B Q b k H S + a B B z b E I 8 D Q b 3 m Q J q e G I J o Y r Z W z E d E U W o s f E V b A j + 4 s v L p H F W 8 s 9 L 5 V q 5 W L n K 4 s i j I 3 S M T p G P L l A F 3 a A q q i O K H t A T e k G v z q P z 7 L w 5 7 / P W n J P N H K I / c D 6 + A f s b m 6 k = &lt; / l a t e x i t &gt; Instance Query &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a + i 2 x e S h 0 j M T n n k Y O p z s u g o R i B U = " &gt; A A A C C H i c b V D L S s N A F J 3 U V 6 2 v q E s X D h b B V U m k q M u i G 5 c V + o I m l M l 0 0 g 6 d z I S Z i V B C l m 7 8 F T c u F H H r J 7 j z b 5 y 0 E b T 1 w I X D O f d y 7 z 1 B z K j S j v N l l V Z W 1 9 Y 3 y p u V r e 2 d 3 T 1 7 / 6 C j R C I x a W P B h O w F S B F G O W l r q h n p x Z K g K G C k G 0 x u c r 9 7 T 6 S i g r f 0 N C Z + h E a c h h Q j b a S B f e x F S I 9 l l D a R x u M M e v B H a I k J 4 d n A r j o 1 Z w a 4 T N y C V E G B 5 s D + 9 I Y C J x H h G j O k V N 9 1 Y u 2 n S G q K G c k q X q J I j P A E j U j f U I 4 i o v x 0 9 k g G T 4 0 y h K G Q p r i G M / X 3 R I o i p a Z R Y D r z K 9 W i l 4 v / e f 1 E h 1 d + S n m c a M L x f F G Y M K g F z F O B Q y o J 1 m x q C M K S m l s h H i O J s D b Z V U w I 7 u L L y 6 R z X n M v a v W 7 e r V x X c R R B k f g B J w B F 1 y C B r g F T d A G G D y A J / A C X q 1 H 6 9 l 6 s 9 7 n r S W r m D k E f 2 B 9 f A N W Z p o x &lt; / l a t e x i t &gt; Patch Token &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N a m p M / u f c F l p g J c W 5 U a O I q J T l v 4 = " &gt; A A A C 3 3 i c j V H L S s N A F D 3 G V 3 1 H 3 e k m W A R X J Z W i L k U 3 b o Q K r R Z s K Z N 0 W k P z Y m Y i S C m 4 c y d u / Q G 3 + j f i H + h f e G e M o B b R C U n O n H v P m b n 3 e m k Y S O W 6 L 2 P W + M T k 1 H R h Z n Z u f m F x y V 5 e O Z V J J n x e 9 5 M w E Q 2 P S R 4 G M a + r Q I W 8 k Q r O I i / k Z 1 7 / U M f P L r m Q Q R L X 1 F X K W x H r x U E 3 8 J k i q m 2 v N S O m L k Q 0 O O Z S 8 r j H h d N 0 a k m f x 8 O 2 X X R L r l n O K C j n o I h 8 V R P 7 G U 1 0 k M B H h g g c M R T h E A y S n n O U 4 S I l r o U B c Y J Q Y O I c Q 8 y S N q M s T h m M 2 D 5 9 e 7 Q 7 z 9 m Y 9 t p T G r V P p 4 T 0 C l I 6 2 C R N Q n m C s D 7 N M f H M O G v 2 N + + B 8 d R 3 u 6 K / l 3 t F x C p c E P u X 7 j P z v z p d i 0 I X e 6 a G g G p K D a O r 8 3 O X z H R F 3 9 z 5 U p U i h 5 Q 4 j T s U F 4 R 9 o / z s s 2 M 0 0 t S u e 8 t M / N V k a l b v / T w 3 w 5 u + J Q 2 4 / H O c o + B 0 u 1 T e K V V O K s X 9 g 3 z U B a x j A 1 s 0 z 1 3 s 4 w h V 1 M n 7 G g 9 4 x J P F r B v r 1 r r 7 S L X G c s 0 q v i 3 r / h 0 s 3 5 p c &lt; / l a t e x i t &gt; Messenger Token</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>S = 1 1 2 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 6 H L b 4 FS = 1 1 2 2 &lt;MessengerFigure 2 .</head><label>2146422</label><figDesc>z p k 2 d J 4 7 h o n x R L N 6 V C + S K L I w f 2 w D 4 4 B D Y 4 B W V Q B T V Q B x g 8 g C f w A l 6 N R + P Z e D P e J 6 V z R t a z A / 7 A + P g G y p u X o A = = &lt; / l a t e x i t &gt; MHSA + FFN &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x U i x B D M r u 9 a R 0 b T o x E W 4 Y 5 k 0 S b 4 = " &gt; A A A C A 3 i c b V D L S s N A F J 3 4 r P U V d a e b w S I I Q k m k q M u q U L p R K t o H t K F M p p N 2 6 G Q S Z i Z C C Q E 3 / o o b F 4 q 4 9 S f c + T d O 2 g j a e m D g z D n 3 c u 8 9 b s i o V J b 1 Z c z N L y w u L e d W 8 q t r 6 x u b 5 t Z 2 Q w a R w K S O A x a I l o s k Y Z S T u q K K k V Y o C P J d R p r u 8 D L 1 m / d E S B r w O z U K i e O j P q c e x U h p q W v u d n y k B s K P r 6 q 3 5 8 n R z 6 9 S u U 6 6 Z s E q W m P A W W J n p A A y 1 L r m Z 6 c X 4 M g n X G G G p G z b V q i c G A l F M S N J v h N J E i I 8 R H 3 S 1 p Q j n 0 g n H t + Q w A O t 9 K A X C P 2 4 g m P 1 d 0 e M f C l H v q s r 0 x 3 l t J e K / 3 n t S H l n T k x 5 G C n C 8 W S Q F z G o A p g G A n t U E K z Y S B O E B d W 7 Q j x A A m G l Y 8 v r E O z p k 2 d J 4 7 h o n x R L N 6 V C + S K L I w f 2 w D 4 4 B D Y 4 B W V Q B T V Q B x g 8 g C f w A l 6 N R + P Z e D P e J 6 V z R t a z A / 7 A + P g G y p u X o A = = &lt; / l a t e x i t &gt; MHSA + FFN &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 w c D g l f k f a T c c V X L u i 0 / n J v 1 Y Y 0 = " &gt; A A A C B n i c b V D L S g M x F L 1 T X 7 W + R l 2 K E C y C q z J T i r o R i m 5 c V r Q P a I e S y W T a 0 M z D J C O U o S s 3 / o o b F 4 q 4 9 R v c + T e m 7 S j a e i D c w z n 3 c n O P G 3 M m l W V 9 G r m F x a X l l f x q Y W 1 9 Y 3 P L 3 N 5 p y C g R h N Z J x C P R c r G k n I W 0 r p j i t B U L i g O X 0 6 Y 7 u B j 7 z T s q J I v C G z W M q R P g X s h 8 R r D S U t f c v 0 Z n y E a d 2 w R 7 P 7 X 8 X b t m 0 S p Z E 6 B 5 Y m e k C B l q X f O j 4 0 U k C W i o C M d S t m 0 r V k 6 K h W K E 0 1 G h k 0 g a Y z L A P d r W N M Q B l U 4 6 O W O E D r X i I T 8 S + o U K T d T f E y k O p B w G r u 4 M s O r L W W 8 s / u e 1 E + W f O i k L 4 0 T R k E w X + Q l H K k L j T J D H B C W K D z X B R D D 9 V 0 T 6 W G C i d H I F H Y I 9 e / I 8 a Z R L 9 n G p c l U p V s + z O P K w B w d w B D a c Q B U u o Q Z 1 I H A P j / A M L 8 a D 8 W S 8 G m / T 1 p y R z e z C H x j v X + y 3 l k E = &lt; / l a t e x i t &gt; T v N S X H p d Q r k 9 O E i a c S + 6 8 = " &gt; A A A C K H i c d V D L S s N A F J 3 U V 6 2 v q E s 3 g 0 V w V R I p K o h Y 1 I X L C v Y B T S i T y a Q d O p n E m Y m l h H 6 O G 3 / F j Y g i 3 f o l T t s s b K s H B g 7 n 3 M O d e 7 y Y U a k s a 2 T k l p Z X V t f y 6 4 W N z a 3 t H X N 3 r y 6 j R G B S w x G L R N N D k j D K S U 1 R x U g z F g S F H i M N r 3 c z 9 h t P R E g a 8 Q c 1 i I k b o g 6 n A c V I a a l t X t 3 C S + h c O E m M h I j 6 0 H l M k A 8 d P + r z G e E / v 2 0 W r Z I 1 A V w k d k a K I E O 1 b b 7 r M E 5 C w h V m S M q W b c X K T Z F Q F D M y L D i J J D H C P d Q h L U 0 5 C o l 0 0 8 m h Q 3 i k F R 8 G k d C P K z h R f y d S F E o 5 C D 0 9 G S L V l f P e W P z L a y U q O H d T y u N E E Y 6 n i 4 K E Q R X B c W v Q p 4 J g x Q a a I C y o / i v E X S Q Q V r r b g i 7 B n j 9 5 k d R P S v Z p q X x f L l a u s z r y 4 A A c g m N g g z N Q A X e g C m o A g 2 f w C j 7 A p / F i v B l f x m g 6 m j O y z D 6 Y g f H 9 A w 2 5 p q 0 = &lt; / l a t e x i t &gt; D = " # " # &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 w c D g l f k f a T c cV X L u i 0 / n J v 1 Y Y 0 = " &gt; A A A C B n i c b V D L S g M x F L 1 T X 7 W + R l 2 K E C y C q z J T i r o R i m 5 c V r Q P a I e S y W T a 0 M z D J C O U o S s 3 / o o b F 4 q 4 9 R v c + T e m 7 S j a e i D c w z n 3 c n O P G 3 M m l W V 9 G r m F x a X l l f x q Y W 1 9 Y 3 P L 3 N 5 p y C g R h N Z J x C P R c r G k n I W 0 r p j i t B U L i g O X 0 6 Y 7 u B j 7 z T s q J I v C G z W M q R P g X s h 8 R r D S U t f c v 0 Z n y E a d 2 w R 7 P 7 X 8 X b t m 0 S p Z E 6 B 5 Y m e k C B l q X f O j 4 0 U k C W i o C M d S t m 0 r V k 6 K h W K E 0 1 G h k 0 g a Y z L A P d r W N M Q B l U 4 6 O W O E D r X i I T 8 S + o U K T d T f E y k O p B w G r u 4 M s O r L W W 8 s / u e 1 E + W f O i k L 4 0 T R k E w X + Q l H K k L j T J D H B C W K D z X B R D D 9 V 0 T 6 W G C id H I F H Y I 9 e / I 8 a Z R L 9 n G p c l U p V s + z O P K w B w d w B D a c Q B U u o Q Z 1 I H A P j / A M L 8 a D 8 W S 8 G m / T 1 p y R z e z C H x j v X + y 3 l k E = &lt; / l a t e x i t &gt; l a t e x i t s h a 1 _ b a s e 6 4 = " r G 4 6 Q 4 p 0 d T F i b o p I G + K U P H 8 H M u c = " &gt; A AA C K H i c d V D L S g M x F M 3 U V 6 2 v U Z d u g k V w V W a k q C B i U R c u K 9 g H d I a S y W T a 0 E x m T D K W M v R z 3 P g r b k Q U 6 d Y v M W 1 n Y V s 9 E D g 5 5 9 6 b 3 O P F j E p l W S M j t 7 S 8 s r q W X y 9 s b G 5 t 7 5 i 7 e 3 U Z J Q K T G o 5 Y J J o e k o R R T m q K K k a a s S A o 9 B h p e L 2 b s d 9 4 I k L S i D + o Q U z c E H U 4 D S h G S k t t 8 + o W X k L n w v G j P k d C R H 3 o P C b I h 0 4 S z 1 z / 8 9 t m 0 S p Z E 8 B F Y m e k C D J U 2 + a 7 n o W T k H C F G Z K y Z V u x c l M k F M W M D A t O I k m M c A 9 1 S E t T j k I i 3 X S y 6 B A e a c W H Q S T 0 4 Q p O 1 N 8 d K Q q l H I S e r g y R 6 s p 5 b y z + 5 b U S F Z y 7 K e V x o g j H 0 4 e C h E E V w X F q 0 K e C Y M U G m i A s q P 4 r x F 0 k E F Y 6 2 4 I O w Z 5 f e Z H U T 0 r 2 a a l 8 X y 5 W r r M 4 8 u A A H I J j Y I M z U A F 3 o A p q A I N n 8 A o + w K f x Y r w Z X 8 Z o W p o z s p 5 9 M A P j + w c P p 6 a t &lt; / l a t e x i t &gt; D = # " # "&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " k 3 q D h D q l 7 z J h 4 k o P 6 H L H S F Y d r b Y = " &gt; A A A C C n i c b V D L S g M x F M 3 4 r P U 1 6 t J N t A i u y o w U d V l 0 o 7 u K 9 g G d o W T S T B u a Z I Y k I 5 R h 1 m 7 8 F T c u F H H r F 7 j z b 8 y 0 I 2 j r g c D h n H P J v S e I G V X a c b 6 s h c W l 5 Z X V 0 l p 5 f W N z a 9 v e 2 W 2 p K J G Y N H H E I t k J k C K M C t L U V D P S i S V B P G C k H Y w u c 7 9 9 T 6 S i k b j T 4 5 j 4 H A 0 E D S l G 2 k g 9 + 8 D j S A 8 l T 6 9 F H i M Z 9 O C P d D u k o c 5 6 d s W p O h P A e e I W p A I K N H r 2 p 9 e P c M K J 0 J g h p b q u E 2 s / R V J T z E h W 9 h J F Y o R H a E C 6 h g r E i f L T y S k Z P D J K H 4 a R N E 9 o O F F / T 6 S I K z X m g U n m W 6 p Z L x f / 8 7 q J D s / 9 l I o 4 0 U T g 6 U d h w q C O Y N 4 L 7 F N J s G Z j Q x C W 1 O w K 8 R B J h L W p p W x K c G d P n i e t k 6 p 7 W q 3 d 1 C r 1 i 6 K O E t g H h + A Y u O A M 1 M E V a I A m w O A B P I E X 8 G o 9 W s / W m / U + j S 5 Y x c w e + A P r 4 x s d 7 Z s u &lt; / l a t e x i t &gt; Inverse Shift &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 8 6 k A B k r o B + 8 q D q s K d D U f X q H C X N c = " &gt; A A A B 9 X i c b V D L S g M x F M 3 U V 6 2 v q k s 3 w S K 4 K j N S 1 G X R j c u K 9 g H t W D J p p g 1 N M k N y R y l D / 8 O N C 0 X c + i / u / B s z 7 S y 0 9 U D g c M 6 9 3 J M T x I I b c N 1 v p 7 C y u r a + U d w s b W 3 v 7 O 6 V 9 w 9 a J k o 0 Z U 0 a i U h 3 A m K Y 4 I o 1 g Y N g n V g z I g P B 2 s H 4 O v P b j 0 w b H q l 7 m M T M l 2 S o e M g p A S s 9 9 C S B k Z b p 3 Y i H M O 2 X K 2 7 V n Q E v E y 8 n F Z S j 0 S 9 / 9 Q Y R T S R T Q A U x p u u 5 M f g p 0 c C p Y N N S L z E s J n R M h q x r q S K S G T + d p Z 7 i E 6 s M c B h p + x T g m f p 7 I y X S m I k M 7 G S W 0 i x 6 m f i f 1 0 0 g v P R T r u I E m K L z Q 2 E i M E Q 4 q w A P u G Y U x M Q S Q j W 3 W T E d E U 0 o 2 K J K t g R v 8 c v L p H V W 9 c 6 r t d t a p X 6 V 1 1 F E R + g Y n S I P X a A 6 u k E N 1 E Q U a f S M X t G b 8 + S 8 O O / O x 3 y 0 4 O Q 7 h + g P n M 8 f G V K S 6 g = = &lt; / l a t e x i t &gt; Shift &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p A j e V m G 8 I 4 u x s E V k E T 7 X c U O b P C U = " &gt; A A A B 8 X i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s x I q S 6 L b l x W 6 A v b o W T S T B u a Z I Y k I 5 S h f + H G h S J u / R t 3 / o 2 Z d h b a e i B w O O d e c u 4 J Y s 6 0 c d 1 v p 7 C x u b W 9 U 9 w t 7 e 0 f H B 6 V j 0 8 6 O k o U o W 0 S 8 U j 1 A q w p Z 5 K 2 D T O c 9 m J F s Q g 4 7 Q b T u 8 z v P l G l W S R b Z h Z T X + C x Z C E j 2 F j p c S C w m S i R t u b D c s W t u g u g d e L l p A I 5 m s P y 1 2 A U k U R Q a Q j H W v c 9 N z Z + i p V h h N N 5 a Z B o G m M y x W P a t 1 R i Q b W f L h L P 0 Y V V R i i M l H 3 S o I X 6 e y P F Q u u Z C O x k l l C v e p n 4 n 9 d P T H j j p 0 z G i a G S L D 8 K E 4 5 M h L L z 0 Y g p S g y f W Y K J Y j Y r I h O s M D G 2 p J I t w V s 9 e Z 1 0 r q p e v V p 7 q F U a t 3 k d R T i D c 7 g E D 6 6 h A f f Q h D Y Q k P A M r / D m a O f F e X c + l q M F J 9 8 5 h T 9 w P n 8 A 7 K S R G A = = &lt; / l a t e x i t &gt; T &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 r N P H m V 5 E w e M 6 q / d U G T / O L Y 3 I Y E = " &gt; A A A B 8 X i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s x I q S 6 L 3 b i s Y B / Y D i W T Z t r Q J D M k G a E M / Q s 3 L h R x 6 9 + 4 8 2 / M t L P Q 1 g O B w z n 3 k n N P E H O m j e t + O 4 W N z a 3 t n e J u a W / / 4 P C o f H z S 0 V G i C G 2 T i E e q F 2 B N O Z O 0 b Z j h t B c r i k X A a T e Y N j O / + 0 S V Z p F 8 M L O Y + g K P J Q s Z w c Z K j w O B z U S J t D k f l i t u 1 V 0 A r R M v J x X I 0 R q W v w a j i C S C S k M 4 1 r r v u b H x U 6 w M I 5 z O S 4 N E 0 x i T K R 7 T v q U S C 6 r 9 d J F 4 j i 6 s M k J h p O y T B i 3 U 3 x s p F l r P R G A n s 4 R 6 1 c v E / 7 x + Y s I b P 2 U y T g y V Z P l R m H B k I p S d j 0 Z M U W L 4 z B J M F L N Z E Z l g h Y m x J Z V s C d 7 q y e u k c 1 X 1 6 t X a f a 3 S u M 3 r K M I Z n M M l e H A N D b i D F r S B g I R n e I U 3 R z s v z r v z s R w t O P n O K f y B 8 / k D 0 s + R B w = = &lt; / l a t e x i t &gt; C &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " h h g 9 Z D 0 + V 7 f / N 5 c i l D X L 2 z u l X I c = " &gt; A A A B 8 X i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s x I q S 6 L L n R Z w T 6 w H U o m z b S h S W Z I M k I Z + h d u X C j i 1 r 9 x 5 9 + Y a W e h r Q c C h 3 P u J e e e I O Z M G 9 f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o r a N E E d o i E Y 9 U N 8 C a c i Z p y z D D a T d W F I u A 0 0 4 w u c n 8 z h N V m k X y w U x j 6 g s 8 k i x k B B s r P f Y F N m M l 0 t v Z o F x x q + 4 c a J V 4 O a l A j u a g / N U f R i Q R V B r C s d Y 9 z 4 2 N n 2 J l G O F 0 V u o n m s a Y T P C I 9 i y V W F D t p / P E M 3 R m l S E K I 2 W f N G i u / t 5 I s d B 6 K g I 7 m S X U y 1 4 m / u f 1 E h N e + S m T c W K o J I u P w o Q j E 6 H s f D R k i h L D p 5 Z g o p j N i s g Y K 0 y M L a l k S / C W T 1 4 l 7 Y u q V 6 / W 7 m u V x n V e R x F O 4 B T O w Y N L a M A d N K E F B C Q 8 w y u 8 O d p 5 c d 6 d j 8 V o w c l 3 j u E P n M 8 f 2 O O R C w = = &lt; / l a t e x i t &gt; G &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s k t 4 M k f R O M s t 7 E a y Z 6 D 0 K 7 P l 4 V I = " &gt; A A A B 9 X i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s y I q M u i G 5 c V 7 A P a s W T S t A 1 N Z o b k j l K G / o c b F 4 q 4 9 V / c + T d m 2 l l o 6 4 H A 4 Z x 7 u S c n i K U w 6 L r f T m F l d W 1 9 o 7 h Z 2 t r e 2 d 0 r 7 x 8 0 T Z R o x h s s k p F u B 9 R w K U L e Q I G S t 2 P N q Q o k b w X j m 8 x v P X J t R B T e 4 y T m v q L D U A w E o 2 i l h 6 6 i O N I q r V N k o 2 m v X H G r 7 g x k m X g 5 q U C O e q / 8 1 e 1 H L F E 8 R C a p M R 3 P j d F P q U b B J J + W u o n h M W V j O u Q d S 0 O q u P H T W e o p O b F K n w w i b V + I Z K b + 3 k i p M m a i A j u Z p T S L X i b + 5 3 U S H F z 5 q Q j j B H n I 5 o c G i S Q Y k a w C 0 h e a M 5 Q T S y j T w m Y l b E Q 1 Z W i L K t k S v M U v L 5 P m W d W 7 q J 7 f n V d q 1 3 k d R T i C Y z g F D y 6 h B r d Q h w Y w 0 P A M r / D m P D k v z r v z M R 8 t O P n O I f y B 8 / k D A / 6 S 3 A = = &lt; / l a t e x i t &gt; Patch &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x w T L Z B x k T o Y x A o g L h A z D 8 b V 5 B i 0 = " &gt; A A A C 1 3 i c j V H L S s N A F D 2 N r 1 p f t S 7 d B I v g q i Q i 6 r L o x o 1 Q w V b F l p J M x x q a F z M T U Y q 4 E 7 f + g F v 9 I / E P 9 C + 8 M 0 Z Q i + i E J G f O v e f M 3 H v 9 N A y k c p y X g j U 2 P j E 5 V Z w u z c z O z S + U F y s t m W S C 8 S Z L w k Q c + 5 7 k Y R D z p g p U y I 9 T w b 3 I D / m R P 9 j V 8 a M L L m S Q x I f q K u W d y O v H w V n A P E V U t 1 x p R 5 4 6 F 9 F w n 0 v J 4 z 4 X 1 9 1 y 1 a k 5 Z t m j w M 1 B F f l q J O V n t N F D A o Y M E T h i K M I h P E h 6 T u H C Q U p c B 0 P i B K H A x D m u U S J t R l m c M j x i B / T t 0 + 4 0 Z 2 P a a 0 9 p 1 I x O C e k V p L S x S p q E 8 g R h f Z p t 4 p l x 1 u x v 3 k P j q e 9 2 R X 8 / 9 4 q I V T g n 9 i / d Z + Z / d b o W h T N s m x o C q i k 1 j K 6 O 5 S 6 Z 6 Y q + u f 2 l K k U O K X E a 9 y g u C D O j / O y z b T T S 1 K 5 7 6 5 n 4 q 8 n U r N 6 z P D f D m 7 4 l D d j 9 O c 5 R 0 F q v u Z s 1 9 2 C j W t / J R 1 3 E M l a w R v P c Q h 1 7 a K B J 3 p d 4 w C O e r B P r x r q 1 7 j 5 S r U K u W c K 3 Z d 2 / A 4 P s l 2 w = &lt; / l a t e x i t &gt; An illustration of the messenger shift mechanism. Messenger tokens are first segmented into several groups (4 in the figure)and then shifted along the temporal axis with different stride (S) and direction (D) to exchange frame-level information. For every two successive messenger shift mechanisms, we apply an inverse shift operation. As shown in figure above, the shift direction (D) of each token group in the second shift mechanism (right) is exactly inverse to the first (medium). Messenger tokens are shown by green cubes, while blue cubes denote the patch tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>MST</cell><cell>FPS</cell><cell>AP</cell><cell>AP50</cell><cell>AP75</cell><cell>AR1</cell><cell>AR10</cell></row><row><cell>MaskTrack R-CNN [62]</cell><cell>ResNet-50</cell><cell></cell><cell>32.8</cell><cell>30.3</cell><cell>51.1</cell><cell>32.6</cell><cell>31.0</cell><cell>35.5</cell></row><row><cell>MaskTrack R-CNN [62]</cell><cell>ResNet-101</cell><cell></cell><cell>28.6</cell><cell>31.9</cell><cell>53.7</cell><cell>32.3</cell><cell>32.5</cell><cell>37.7</cell></row><row><cell>SipMask [6]</cell><cell>ResNet-50</cell><cell>?</cell><cell>34.1</cell><cell>33.7</cell><cell>54.1</cell><cell>35.8</cell><cell>35.4</cell><cell>40.1</cell></row><row><cell>SG-Net [31]</cell><cell>ResNet-50</cell><cell></cell><cell>?</cell><cell>34.8</cell><cell>56.1</cell><cell>36.8</cell><cell>35.8</cell><cell>40.8</cell></row><row><cell>SG-Net [31]</cell><cell>ResNet-101</cell><cell></cell><cell>?</cell><cell>36.3</cell><cell>57.1</cell><cell>39.6</cell><cell>35.9</cell><cell>43.0</cell></row><row><cell>CrossVIS [63]</cell><cell>ResNet-50</cell><cell>?</cell><cell>39.8</cell><cell>36.3</cell><cell>56.8</cell><cell>38.9</cell><cell>35.6</cell><cell>40.7</cell></row><row><cell>CrossVIS [63]</cell><cell>ResNet-101</cell><cell>?</cell><cell>35.6</cell><cell>36.6</cell><cell>57.3</cell><cell>39.7</cell><cell>36.0</cell><cell>42.0</cell></row><row><cell>STEm-Seg [2]</cell><cell>ResNet-50</cell><cell>?</cell><cell>3.0</cell><cell>30.6</cell><cell>50.7</cell><cell>33.5</cell><cell>31.6</cell><cell>37.1</cell></row><row><cell>STEm-Seg [2]</cell><cell>ResNet-101</cell><cell>?</cell><cell>?</cell><cell>34.6</cell><cell>55.8</cell><cell>37.9</cell><cell>34.4</cell><cell>41.6</cell></row><row><cell>MaskProp [3]</cell><cell>ResNet-50</cell><cell>?</cell><cell>?</cell><cell>40.0</cell><cell>?</cell><cell>42.9</cell><cell>?</cell><cell>?</cell></row><row><cell>MaskProp [3]</cell><cell>ResNet-101</cell><cell>?</cell><cell>?</cell><cell>42.5</cell><cell>?</cell><cell>45.6</cell><cell>?</cell><cell>?</cell></row><row><cell>MaskProp [3]</cell><cell>STSN-X-101</cell><cell>?</cell><cell>?</cell><cell>46.6</cell><cell>?</cell><cell>51.2</cell><cell>44.0</cell><cell>52.6</cell></row><row><cell>SeqMask R-CNN [27]</cell><cell>ResNet-50</cell><cell></cell><cell>?</cell><cell>40.4</cell><cell>63.0</cell><cell>43.8</cell><cell>41.1</cell><cell>49.7</cell></row><row><cell>SeqMask R-CNN [27]</cell><cell>ResNet-101</cell><cell></cell><cell>?</cell><cell>43.8</cell><cell>65.5</cell><cell>47.4</cell><cell>43.0</cell><cell>53.2</cell></row><row><cell>VisTR [57]</cell><cell>ResNet-50</cell><cell></cell><cell>51.1</cell><cell>36.2</cell><cell>59.8</cell><cell>36.9</cell><cell>37.2</cell><cell>42.4</cell></row><row><cell>VisTR [57]</cell><cell>ResNet-101</cell><cell></cell><cell>43.5</cell><cell>40.1</cell><cell>64.0</cell><cell>45.0</cell><cell>38.3</cell><cell>44.9</cell></row><row><cell>EfficientVIS [58]</cell><cell>ResNet-50</cell><cell>?</cell><cell>36.0</cell><cell>37.9</cell><cell>59.7</cell><cell>43.0</cell><cell>40.3</cell><cell>46.6</cell></row><row><cell>EfficientVIS [58]</cell><cell>ResNet-101</cell><cell>?</cell><cell>32.0</cell><cell>39.8</cell><cell>61.8</cell><cell>44.7</cell><cell>42.1</cell><cell>49.8</cell></row><row><cell>IFC [23]</cell><cell>ResNet-50</cell><cell>?</cell><cell>107.1</cell><cell>41.2</cell><cell>65.1</cell><cell>44.6</cell><cell>42.3</cell><cell>49.6</cell></row><row><cell>IFC [23]</cell><cell>ResNet-101</cell><cell>?</cell><cell>89.4</cell><cell>42.6</cell><cell>66.6</cell><cell>46.3</cell><cell>43.5</cell><cell>51.4</cell></row><row><cell>TeViT (ours)</cell><cell>MsgShifT</cell><cell></cell><cell>68.9</cell><cell>45.9</cell><cell>69.1</cell><cell>50.4</cell><cell>44.0</cell><cell>53.4</cell></row><row><cell>TeViT (ours)</cell><cell>MsgShifT</cell><cell>?</cell><cell>68.9</cell><cell>46.6</cell><cell>71.3</cell><cell>51.6</cell><cell>44.9</cell><cell>54.3</cell></row></table><note>Comparisons on YouTube-VIS-2019 dataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>CNN ? [62, 63] 28.6 48.9 29.6 26.5 33.8 SipMask ? [6, 63] 31.7 52.5 34.0 30.8 37.8 CrossVIS [63] 34.2 54.4 37.9 30.4 38.2 MaskTrack R-CNN ? [62, 63] 10.9 26.0 8.1 8.3 15.2 STEm-Seg ? [2, 40] 13.8 32.1 11.9 9.1 20.0 CrossVIS [63] 14.9 32.7 12.1 10.3 19.8</figDesc><table><row><cell>Methods</cell><cell cols="2">AP AP50 AP75 AR1 AR10</cell><cell>Methods</cell><cell>AP AP50 AP75 AR1 AR10</cell></row><row><cell>MaskTrack R-IFC [23]</cell><cell>35.2 57.2 37.5 ?</cell><cell>?</cell><cell>SipMask  ? [6, 63] CMaskTrack R-CNN  ? [40]</cell><cell>10.3 25.4 7.8 7.9 15.8 15.4 33.9 13.1 9.3 20.0</cell></row><row><cell>TeViT</cell><cell cols="2">37.9 61.2 42.1 35.1 44.6</cell><cell>TeViT</cell><cell>17.4 34.9 15.0 11.2 21.8</cell></row><row><cell cols="3">Table 2. Comparisons with previous VIS methods on YouTube-VIS-</cell><cell></cell><cell></cell></row><row><cell cols="3">2021 datasets. Methods with superscript " ?" are reported in [63].</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>? 0.47 67.6 44.0 43.0 52.7 ? 82.19 43.1 ?(+0.6) ? 0.71 67.2 47.8 43.5 52.4 ? 81.97 45.2 ?(+2.7) ? 0.85 68.9 50.2 44.0 53.0 ? ? 82.19 45.9 ?(+3.4) ? 0.58 69.1 50.4 44.0 53.4 Component-wise analysis on TeViT. MSM denotes the messenger shift mechanism and STQI denotes spatiotemporal query interaction. Without applying STQI implies only one MHSA is performed for query interaction within each frame (excluding Eq. 4). Fused Space-Time [57] 43.9 ?(+0.8) 69.5 48.4 42.9 52.0 Ours 45.9 ?(+2.7) 69.1 50.4 44.0 53.4</figDesc><table><row><cell>MSM STQI GFLOPs 81.97</cell><cell>AP ??AP 42.5 Interaction AP50 AP75 AR1 AR10 Spatial Only [18]</cell><cell>AP 43.1</cell><cell>AP50 AP75 AR1 AR10 67.2 47.8 43.5 52.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 8 .</head><label>8</label><figDesc>Impact of messenger token numbers. M indicates the number of messenger tokens. We increase M from 8 to 32 and observe the effects on final performance.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Manip.</cell><cell>AP</cell><cell>AP50 AP75</cell><cell>M</cell><cell>AP</cell><cell>AP50 AP75</cell><cell>AR1</cell><cell>AR10</cell></row><row><cell cols="2">None MHSA + FFN 44.5 ? 1.07 45.2 ? 0.85</cell><cell>68.9 50.2 69.2 49.3</cell><cell cols="3">None Conv MHSA + FFN 43.1 67.2 49.1 45.2 68.9 50.2 41.8 63.7 45.1</cell><cell cols="3">8 16 45.4 70.3 49.9 45.3 69.0 48.9 44.5 52.4 44.0 51.7</cell></row><row><cell>Shift</cell><cell cols="2">45.9 ? 0.58 69.1 50.4</cell><cell>Msg Shift</cell><cell cols="2">45.9 69.1 50.4</cell><cell cols="3">32 45.9 69.1 50.4 44.0 53.1</cell></row><row><cell cols="3">Table 6. Study of the manipulations upon</cell><cell cols="3">Table 7. Study of frame-level feature</cell><cell></cell><cell></cell></row><row><cell cols="3">messenger tokens. Our method obtains the</cell><cell cols="3">aggregation. Compared to other frame-</cell><cell></cell><cell></cell></row><row><cell cols="3">highest AP and a relatively stable perfor-</cell><cell cols="3">level feature manipulations, our messen-</cell><cell></cell><cell></cell></row><row><cell cols="2">mance (?AP) among all settings.</cell><cell></cell><cell cols="3">ger shift (Row 4) obtains the best results.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Our method achieves the best AP and outperforms None by 0.7 AP and MHSA + FFN by 1.4 AP. (2) Compared to conducting MHSA + FFN upon messenger tokens, our method obtains a more stable performance. MHSA + FFN shows more fluctuating final results (see ? AP in Tab. 6) while ours is much more stable. The results confirm that the randomized MHSA + FFN is unable to capture temporal context while our parameter-free shift operation works. Different manipulations on frame-level feature aggregation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 9 .Table 11 .Table 12 .</head><label>91112</label><figDesc>Effect of training clip length on AP. "T" indicates the number of frames for each video clip during training.Table 10. Study the impact of clip length and stride during inference phase. "T" and "S" indicates the clip length and overlapping stride respectively. Ours w/o. Eq. 4 78.1 36.8 78.3 38.8 38.3 46.2 Ours 76.8 41.7 67.8 44.8 41.3 49.9 Ours ? 76.8 42.3 67.6 44.0 43.0 52.7 Comparisons with ResNet-50 as backbone. ?(?0.4) 68.5 49.9 43.8 52.5 learnable random 45.6 ?(?0.3) 68.2 49.7 43.7 52.4 Revisiting messenger tokens in inference phase.</figDesc><table><row><cell>T</cell><cell>AP</cell><cell>AP50</cell><cell>AP75</cell><cell>AR1</cell><cell>AR10</cell><cell>T</cell><cell>S</cell><cell>AP</cell><cell>AP50</cell><cell>AP75</cell><cell>AR1</cell><cell>AR10</cell></row><row><cell>2</cell><cell>41.1</cell><cell>64.8</cell><cell>44.3</cell><cell>41.2</cell><cell>50.2</cell><cell>5 5</cell><cell>1 3</cell><cell>42.1 41.7</cell><cell>66.8 63.2</cell><cell>46.7 46.1</cell><cell>41.5 41.8</cell><cell>51.3 50.8</cell></row><row><cell>3</cell><cell>44.3</cell><cell>69.2</cell><cell>49.3</cell><cell>43.7</cell><cell>52.1</cell><cell>10</cell><cell>5</cell><cell>44.1</cell><cell>66.4</cell><cell>48.3</cell><cell>42.9</cell><cell>51.9</cell></row><row><cell>5</cell><cell>45.9</cell><cell>68.9</cell><cell>50.2</cell><cell>44.0</cell><cell>53.0</cell><cell>15 20</cell><cell>8 10</cell><cell>44.7 46.0</cell><cell>67.0 67.5</cell><cell>48.7 50.0</cell><cell>43.4 43.7</cell><cell>52.5 53.1</cell></row><row><cell>7</cell><cell>46.3</cell><cell>71.9</cell><cell>51.6</cell><cell>44.0</cell><cell>53.4</cell><cell>36</cell><cell>18</cell><cell>45.9</cell><cell>68.9</cell><cell>50.2</cell><cell>44.0</cell><cell>53.0</cell></row><row><cell>Method</cell><cell cols="2">MST FPS</cell><cell cols="3">AP AP50 AP75 AR1 AR10</cell><cell>Train</cell><cell cols="2">Inference</cell><cell>AP</cell><cell cols="3">AP50 AP75 AR1 AR10</cell></row><row><cell cols="2">VisTR [57]</cell><cell cols="4">51.1 36.2 59.8 36.9 37.2 42.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>IFC [23]</cell><cell>?</cell><cell cols="4">107.1 41.2 65.1 44.6 42.3 49.6</cell><cell cols="3">learnable learned</cell><cell>45.9</cell><cell cols="3">68.9 50.2 44.0 53.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>learnable</cell><cell></cell><cell>zero</cell><cell>45.5</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This work was in part supported by NSFC (No. 61876212 and No. 61733007) and CAAI-Huawei MindSpore Open Fund.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Stem-seg: Spatio-temporal embeddings for instance segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Athar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabarinath</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Classifying, segmenting, and tracking object instances in video with mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Is space-time attention all you need for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, 2021</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Sergi Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevis-Kokitsi</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00737</idno>
		<title level="m">The 2019 davis challenge on vos: Unsupervised multi-object segmentation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sipmask: Spatial information preservation for fast image and video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiale</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisham</forename><surname>Rao Muhammad Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Perpixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Solq: Segmenting objects by learning queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2021</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multiscale vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Msg-transformer: Exchanging local spatial information by manipulating messenger tokens</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiemin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">You only look at one sequence: Rethinking transformer in vision through object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bencheng</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiemin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Instances as queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shusheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Compfeat: Comprehensive feature aggregation for video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Shuffle transformer: Rethinking spatial shuffle for vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youcheng</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guozhong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03650</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Video instance segmentation using inter-frame communication transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sukjun</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miran</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning video instance segmentation with recurrent graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Johnander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emil</forename><surname>Brissman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.03911</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The hungarian method for the assignment problem. Naval research logistics quarterly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold W Kuhn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Video instance segmentation with a propose-reduce paradigm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaijia</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangbo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sg-net: Spatial granularity network for one-stage video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongfang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingjie</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13230</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Video swin transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Conditional detr for fast training convergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Depu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejia</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fausto</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed-Ahmad</forename><surname>Ahmadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mindspore</surname></persName>
		</author>
		<ptr target="https://github.com/mindspore-ai/mindspore.5" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Maya Zohar, and Dotan Asselmann. Video transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Neimark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Bar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">1st place solution for youtubevos challenge 2021: Video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thuy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan</forename><forename type="middle">N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chuong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayuki</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Yamazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yamanaka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.06649</idno>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Occluded video instance segmentation: Dataset and challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<editor>Xiang Bai, Serge Belongie, Alan Yuille, Philip Torr, and Song Bai</editor>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Segmenter: Transformer for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Strudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sparse r-cnn: End-to-end object detection with learnable proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Conditional convolutions for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fcos: A simple and strong anchor-free object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Scaling local self-attention for parameter efficient visual backbones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Mots: Multi-object tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berin</forename><surname>Balachandar Gnana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Sekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">End-toend video instance segmentation via spatial-temporal graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kean</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Pvtv2: Improved baselines with pyramid vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVM</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">End-to-end video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoshan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Efficient video instance segmentation via tracklet query and proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudhir</forename><surname>Yarram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayan</forename><surname>Eledath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Youtubevis dataset 2021 version</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="https://youtube-vos.org/dataset/vis,2021" />
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Crossover learning for fast online video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shusheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shusheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.11963</idno>
		<title level="m">Bin Feng, and Wenyu Liu. Tracking instances as queries</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Token shift transformer for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbin</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">K-net: Towards unified image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Vidtr: Video transformer without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biagio</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Marsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Tighe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2021. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
