<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Named Entity Recognition and Relation Extraction using Enhanced Table Filling by Contextualized Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youmi</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Hiraoka</surname></persName>
							<email>tatsuya.hiraoka@nlp.c.titech.ac.jpokazaki@c.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Named Entity Recognition and Relation Extraction using Enhanced Table Filling by Contextualized Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this study, a novel method for extracting named entities and relations from unstructured text based on the table representation is presented. By using contextualized word embeddings, the proposed method computes representations for entity mentions and longrange dependencies without complicated handcrafted features or neural-network architectures. We also adapt a tensor dot-product to predict relation labels all at once without resorting to history-based predictions or search strategies. These advances significantly simplify the model and algorithm for the extraction of named entities and relations. Despite its simplicity, the experimental results demonstrate that the proposed method outperforms the state-of-the-art methods on the CoNLL04 and ACE05 English datasets. We also confirm that the proposed method achieves a comparable performance with the state-of-the-art NER models on the ACE05 datasets when multiple sentences are provided for context aggregation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Named Entity Recognition (NER) <ref type="bibr" target="#b13">(Nadeau and Sekine, 2007;</ref><ref type="bibr" target="#b17">Ratinov and Roth, 2009</ref>) and Relation Extraction (RE) <ref type="bibr" target="#b25">(Zelenko et al., 2003;</ref><ref type="bibr" target="#b27">Zhou et al., 2005)</ref> are two major sub-tasks of Information Extraction (IE). Recent studies have reported advantages of solving these two tasks jointly in terms of both efficiency and accuracy <ref type="bibr" target="#b12">(Miwa and Sasaki, 2014;</ref><ref type="bibr" target="#b6">Li and Ji, 2014;</ref><ref type="bibr" target="#b5">Gupta et al., 2016;</ref><ref type="bibr" target="#b11">Miwa and Bansal, 2016;</ref><ref type="bibr" target="#b26">Zhang et al., 2017)</ref>. Compared with the pipelined approaches (Chan and <ref type="bibr" target="#b0">Roth, 2011)</ref>, models that jointly extract named entities (NE) and relations can capture dependencies between entities and relations.</p><p>Many existing studies cast joint extraction of NER and RE as a table-filling problem, where entity and relation labels are represented as cells in a single table <ref type="bibr" target="#b12">(Miwa and Sasaki, 2014;</ref><ref type="bibr" target="#b5">Gupta et al., 2016;</ref><ref type="bibr" target="#b26">Zhang et al., 2017)</ref>. As reported by these studies, table-filling is a promising approach for extracting both NE and relations. However, tablefilling approaches require feature engineering and search strategy, which is merely a representation of the label space of NER and RE. Previous work have designed complicated features to encode contexts and long-range dependencies between NE and relations. For example, <ref type="bibr" target="#b12">Miwa and Sasaki (2014)</ref> used hand-crafted syntactic features (e.g., the shortest path between two words in the syntactic tree) and <ref type="bibr" target="#b26">Zhang et al. (2017)</ref> extracted syntactic information using the encoder of a pre-trained syntactic parser. Authors in <ref type="bibr" target="#b12">Miwa and Sasaki (2014)</ref> explore decoding (search) strategies for filling in the table, based on history-based predictions. In addition, they explore six strategies to determine the order of filling for table cells. History-based predictions are also an obstacle for parallelizing label decoding.</p><p>To address the aforementioned issues, we present a novel yet simple method for NER and RE by enhancing table-filling approaches with pretrained BERT, named TablERT. We utilize BERT initialized with pre-trained weights for representing entity mentions and encoding long-range dependencies among entities to simplify feature engineering. Furthermore, the presented model enhances entity representations with span-based features. To reduce the burden of exploring searching strategies, we utilize a tensor dot-product to fill up cells of relation labels in the table all-at-once (instead of cell-by-cell with beam-search). This modification also simplifies the decoding process and improves decoding parallelism, completing RE with matrix and tensor operations.</p><p>This work uses two widely used benchmark datasets, namely, CoNLL04 <ref type="bibr" target="#b18">(Roth and Yih, 2004)</ref> and ACE05, both in English, for evaluating the models of both NER and RE. Experimental results  demonstrate that the proposed method achieves higher performance than previous state-of-the-art methods, including SpERT <ref type="bibr" target="#b4">(Eberts and Ulges, 2020)</ref> and DyGIE++  in addition to the conventional table-filling systems <ref type="bibr" target="#b12">(Miwa and Sasaki, 2014;</ref><ref type="bibr" target="#b26">Zhang et al., 2017)</ref>. We confirm that the tensor dot-product successfully predict relation labels at once without any special search strategy. Moreover, the proposed method attains comparable performance to the state-of-theart NER model DyGIE++ when providing multiple sentences as input for context aggregation. The source code is publicly available at https: //github.com/YoumiMa/TablERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Method</head><p>This study aims to extract NE and relation instances. Given a sequence of words w 1 w 2 ? ? ? w n (n is the number of words in the input), our goal is to extract relation triples in the form of (arg0 type0 , relation, arg1 type1 ). Here, type0 represents the NE type of the mention arg0; arg1 and type1 are defined analogously. We define E and R as label sets of named entities and relations, respectively.</p><p>The table representation <ref type="bibr" target="#b12">(Miwa and Sasaki, 2014)</ref> is employed for jointly recognizing NEs and relation instances. Formally, we define an n ? n upper triangular matrix Y , where a diagonal element Y i,i ? E (1 ? i ? n) represents an NE label for the word w i , and an off-diagonal element Y i,j ? R (1 ? i &lt; j ? n) represents a directed relation label between the words w i and w j . Following <ref type="bibr" target="#b26">Zhang et al. (2017)</ref>, we hard-code directions into relation labels R to avoid considering the lower triangular part of the table for RE. Our model can be seen as a mapping transforming a sequence of words w 1 w 2 ? ? ? w n to an upper triangular matrix Y . We denote an NE label as y i = Y i,i for simplicity. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates an example of a matrix Y for the input sentence, "Johanson Smith lives in London". Notably, relations are mapped from 1-dimensional word sequences to 2-dimensional matrix Y on entity-level. Further, each word inside an entity span is annotated with the corresponding relation label. Take the sentence in <ref type="figure" target="#fig_0">Figure 1</ref> as an example, for the NE "Johanson Smith" labeled as PERSON, relation ????? LIVEIN is labeled on both Y 1,5 and Y 2,5 corresponding to "Johanson" and "Smith," respectively.</p><p>This study is based on pre-trained BERT models to leverage contextual information in solving NER and RE. The proposed method stacks layers for NER and RE on the top of a BERT encoder. As illustrated in Figures 2 and 3, our method computes word representations from contextualized embeddings of sub-word tokens obtained by Byte-Pair-Encoding (BPE) (explained in Section 2.1), and performs NER (Section 2.2) and RE (Section 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Word Representations</head><p>BERT tokenizer uses WordPiece to split words (e.g., "Johanson") into sub-word tokens (e.g., "Johan" and "##son") with the aid of BPE. This technique is proved to be effective in reducing the vocabulary size and unknown words <ref type="bibr" target="#b1">(Devlin et al., 2019)</ref>. Since NEs are annotated at word level, we need its representations at word level during both training and predicting.</p><p>In this study, we compute a max-pooling of BERT embeddings of sub-word tokens composing the word as its representation 1 <ref type="bibr" target="#b8">(Liu et al., 2019)</ref>,</p><formula xml:id="formula_0">e w i = f (e t i,1 , e t i,2 , ? ? ? , e t i,s ).</formula><p>(1)</p><p>Here, we assume the following: the word w i comprises s sub-word tokens t i,1 , t i,2 , ? ? ? , t i,s ; e w i and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT [CLS]</head><p>Johanson Smith London lives  for the words "Johanson" and "lives" in detail. e t i,k are embeddings for the word w i and sub-word token t i,k , respectively; and f presents a maxpooling operation (henceforth).</p><formula xml:id="formula_1">! ! ! " O " " B-PER " ! L-PER O U-LOC</formula><formula xml:id="formula_2"># !" # "" Subtoken-level Embedding $$ %$ &amp;$ '$ $% # "!</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Named Entity Recognition</head><p>We use the BILOU (begin, inside, last, outside, unit-length) notation for representing spans of NEs <ref type="bibr" target="#b17">(Ratinov and Roth, 2009</ref>). We consider NER as a sequential labeling task, where each word w i in the input is labeled as y i (a diagonal element in Y ) in the BILOU notation. In this study, we enhance the existing architecture by using span features at previous timesteps. The use of span features is inspired by <ref type="bibr" target="#b26">Zhang et al. (2017)</ref>; the authors extracted span representations from bidirectional LSTM cells as features. Specifically, the model predicts an NE label for the word w i based on three features: (1) a representation e w i of the word w i , (2) embeddings l y i?1 of the label y i?1 at the previous timestep i ? 1, and (3) max-pooling of BERT embeddings of the previous NE span appearing at timesteps (first(i ? 1), ? ? ? , i ? 1). Here, first(i) denotes the timestep where the NE span including word i starts. For example, when processing the sentence shown in <ref type="figure" target="#fig_1">Figure 2</ref>, since the phrase "Johanson Smith" is labeled as an NE, we have first(1) = first(2) = 1. Similarly, since "lives" is a single non-entity word, we have first(3) = 3. In addition, when the timestep is one (i = 1), we assume the previous word and a unit-length outside (O) as the previous label, i.e., y 0 = O and w 0 = [CLS]. Following <ref type="bibr" target="#b26">Zhang et al. (2017)</ref>, when the previous word is labeled O, we assume that the previous span is a unit-length span, i.e., first(i ? 1) = i ? 1.</p><formula xml:id="formula_3">[CLS] as ? | | Word Representation K Q Tensor Dot-Product Softmax Classifier ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? Johanson Smith lives in London | | ! (#$%) ' (#$%) ( (#$%) | |</formula><p>We define the entity representation for predicting the label of current timestep i as the concatenation of three features described above,</p><formula xml:id="formula_4">h (ent) i = e w i ? l y i?1 ? f (e w first(i?1) , ? ? ? , e w i?1 ),</formula><p>(2) where ? stands for a vector concatenation.</p><p>We apply a fully connected layer followed by a softmax function ? to obtain the probability distribution across all possible NE labels at timestep i,</p><formula xml:id="formula_5">y i = ?(W (ent) h (ent) i + b (ent) ).</formula><p>( <ref type="formula">3)</ref> Here, W (ent) and b (ent) represent the matrix and the bias vector for a linear transformation. The vector? i represents the probability distribution over NE labels; we fill the element Y i,i (= y i ) with the NE label yielding the highest probability for y i . Thus, we perform NER by filling up diagonal elements Y i,i from i = 1 to n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Relation Extraction</head><p>We perform RE on top of the BERT encoder and entity spans recognized in Section 2.2. After the NER model fills up all diagonal elements in Y , the RE model predicts all off-diagonal elements in Y . We adapt a tensor dot-product to score each word pair along with the relation label distribution. The computation is similar to the multi-head selfattention mechanism <ref type="bibr" target="#b22">(Vaswani et al., 2017)</ref>, but our goal is not to compute attention weights over entity representations 2 . Our model utilizes features of entity spans and their NE labels to obtain relation representations for predicting relation labels. Let last(i) denote the timestep where the entity span containing the timestep i ends, analogous to first(i) defined in Section 2.2. For instance, for sentence delineated in <ref type="figure" target="#fig_1">Figure 2</ref>, we have last(1) = last(2) = 2. The entity-span feature z i (at timestep i) is computed using the representations of the constituent words in the entity span,</p><formula xml:id="formula_6">z i = f (e w first(i) , ? ? ? , e w i , ? ? ? , e w last(i) ). (4)</formula><p>To rephrase, z i is the max-pooling across word representations of the entity span starting at first(i) and ending at last(i). Embedding of the NE label l y i is then used as the entity-label feature at timestep i. Mathematically, the word representation, i.e., an input to the RE model, is a concatenation of the entity-span and entity-label feature,</p><formula xml:id="formula_7">h (rel) i = z i ? l y i .<label>(5)</label></formula><p>For each possible relation r ? R, we apply linear transformations parameterized by two matrices W</p><formula xml:id="formula_8">(q) r , W (k) r ? R datt?d rel and two bias vectors b (q) r , b (k) r ? R datt , q i,r = W (q) r h (rel) i + b (q) r ,<label>(6)</label></formula><formula xml:id="formula_9">k i,r = W (k) r h (rel) i + b (k) r ,<label>(7)</label></formula><p>where d rel denotes the number of dimensions of h (rel) i , and d att represents the number of dimensions after the transformations. q i,r ? R datt and k i,r ? R datt are query and key vectors, respectively, for the relation r at timestep i. As demonstrated, we map the word representation vector h (rel) i into the query and key spaces associated with the relation r.</p><p>After collecting both query and key vectors for all relations r ? R at all timesteps i ? {1, . . . , n}, we obtain two tensors Q ? R n?|R|?datt and K ? R n?|R|?datt . Slices of the tensors are,</p><formula xml:id="formula_10">Q i,r,: = q i,r = W (q) r h (rel) i + b (q) r ,<label>(8)</label></formula><formula xml:id="formula_11">K i,r,: = k i,r = W (k) r h (rel) i + b (k) r .<label>(9)</label></formula><p>We compute a probability distribution across all possible relations for every combination of i and 2 We also tried the deep bi-affine attention mechanism used in <ref type="bibr" target="#b3">Dozat and Manning (2017)</ref> and <ref type="bibr" target="#b14">Nguyen and Verspoor (2019)</ref>. However, we found it sufficient to use the tensor dot-product in the early experiments.</p><p>j (1 ? i &lt; j ? n), which is realized by the dotproduct of Q and K,</p><formula xml:id="formula_12">y i,j = ?(QK ) i,j,: .<label>(10)</label></formula><p>Here, (?) i,j,: denotes a slice (vector) of the tensor extracting the (i, j, * ) elements. Softmax function ? computes the probability distribution across all relation labels R. We fill in the cell Y i,j with the relation label yielding the highest probability for y i,j . In this way, the RE model predicts relation labels for all pairs of input words at once by computing Equation 10 on the top of NE labels and spans predicted by the NE module. Meanwhile, we replace Equation 5 with 11 during the training phase,</p><formula xml:id="formula_13">z i = e w i .<label>(11)</label></formula><p>The rationale behind this treatment is that Equation 5 might repeat the same pattern of parameter updates for multiple words within an entity span because of the max-pooling operation. In contrast, Equation 11 promotes different patterns of parameter updates for different words in an entity span 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training and Predicting</head><p>The objective of training is to minimize the sum of cross-entropy losses of NER (L (ent) ) and RE (L (rel) ), L = L (ent) + L (rel) .</p><p>The proposed NER model uses ground-truth NE labels and spans at training time, similar to <ref type="bibr" target="#b26">Zhang et al. (2017)</ref>. We perform a greedy search (from left to right) for predicting a label sequence 4 . Our RE model receives the predicted NE labels and spans from the NER model and then predicts relation labels based on these predictions.</p><p>Notably, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, if a phrase is labeled as an NE, relations sourcing from or pointing to the phrase will be mapped span-wise to the matrix Y . The mapping strategy helps us fully update parameters for different words inside an entity span during relation training. During relation prediction, we ignore inconsistency of label predictions among componential words by max-pooling as in Equation 4, which results in the same entity-span feature inside the span.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We assessed the performance for NER and RE of our approach TablERT on two widely used datasets: CoNLL04 <ref type="bibr" target="#b18">(Roth and Yih, 2004)</ref> and ACE05. In addition, the ability of TablERT to capture cross-sentence dependencies in NER is evaluated on CoNLL03 <ref type="bibr" target="#b21">(Tjong Kim Sang and De Meulder, 2003)</ref> and ACE05, which include markers for document boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CoNLL04</head><p>The dataset defines four entity types and five relation types. We report F1 scores for NER and RE adhering to the conventional evaluation scheme. The experiments followed the setup and data split of <ref type="bibr" target="#b5">Gupta et al. (2016)</ref> and <ref type="bibr" target="#b4">Eberts and Ulges (2020)</ref>, which are similar to those of <ref type="bibr" target="#b12">Miwa and Sasaki (2014)</ref> and <ref type="bibr" target="#b26">Zhang et al. (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACE05</head><p>We used the English corpus that encompasses seven coarse-grained entity types and six coarse-grained relation types. We followed the data splits, pre-processing, and task settings of <ref type="bibr" target="#b6">Li and Ji (2014)</ref> and <ref type="bibr" target="#b11">Miwa and Bansal (2016)</ref>. For evaluating NER, we regarded an entity mention as correct if its label and the headword of its span were identical to the ground truth. For evaluating RE, we report performance values computed by two different criteria to make them comparable with the previous work: ACE05 ? is indifferent towards incorrect predictions of NE labels, while ACE05 ? requires NE labels of relation arguments to be correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CoNLL03</head><p>The dataset contains four different entity types similar to CoNLL04 <ref type="bibr" target="#b18">(Roth and Yih, 2004)</ref>. We used this dataset to measure the performance of NER that considers cross-sentence contexts within a document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Settings</head><p>Our model is implemented in PyTorch <ref type="bibr" target="#b15">(Paszke et al., 2019)</ref> with HuggingFace Transformer package <ref type="bibr" target="#b24">(Wolf et al., 2019)</ref>, utilizing BERT BASE (cased) <ref type="bibr" target="#b1">(Devlin et al., 2019)</ref> as a pre-trained BERT model. We ran all experiments on a single GPU of NVIDIA Tesla V100 (16 GiB). We trained parameters of the NER and RE models as well as those in BERT (fine-tuning) during the training phase, with parameters other than pre-trained BERT initialized with the default initializer. We used the AdamW algorithm implemented in PyTorch for parameter updates <ref type="bibr" target="#b9">(Loshchilov and Hutter, 2019)</ref>.</p><p>Hyperparameters were tuned on the held-out development set of CoNLL04. Further, we merged the development and training sets of CoNLL04 for the final training and evaluation, following the procedure of <ref type="bibr" target="#b5">Gupta et al. (2016)</ref> and <ref type="bibr" target="#b4">Eberts and Ulges (2020)</ref>. Major hyperparameters are listed in Appendix A. We report mean values of all evaluation metrics following five runs on each dataset throughout the paper. <ref type="table">Table 1</ref> reports the performance of our method on the datasets, along with in several recent studies on joint NER and RE. On the CoNLL04 dataset, TablERT achieved comparable or slightly better performance in both NER and RE than SpERT, i.e., an existing state-of-the-art (SOTA) model. Another advantage of TablERT over SpERT is its stability in achieving higher performance. <ref type="table" target="#tab_4">Table 2</ref> reports the standard derivations (SDs) of F1 scores of NER and RE on the CoNLL04 test set achieved by the two models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Main Results</head><p>In addition, <ref type="table">Table 1</ref> indicates that TablERT outperformed existing work on ACE05. Regardless of the evaluation criteria for RE (ACE05 ? and ACE05 ? ), F1 scores of TablERT were approximately 1.0 point higher than those of previous SOTA models (DyGIE++ and Multi-turn QA).</p><p>TablERT ranked second for NER on ACE05 among previous studies, while DyGIE++ portrayed superior performance on NER. However, their method receives document-level contexts as input that make it incomparable with our model, i.e., trained only with sentence-level contexts. In addition, DyGIE++ utilized coreference information from OntoNotes (Pradhan et al., 2012). As we will see in Section 3.6 with <ref type="table" target="#tab_8">Table 5</ref>, the proposed method achieved comparable performance to Dy-GIE++ on NER with document-level context given to the input.</p><p>Additionally, a detailed error inspection of both NER and RE is shown in Appendix B. The error inspection aided us in categorizing two significant error types of RE sources, namely, a lack of external knowledge and global constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Test</head><p>To better understand the significance of our proposed method, we conducted ablation tests. New features were gradually removed from the model,  and the consequent performance drops were measured. Specifically, we ablated the features of the label embedding l y i?1 and the previous span f (e w first(i?1) , ? ? ? , e w i?1 ) from Equation 2. As can be seen from <ref type="table">Table 3</ref>, the removal of the features of previous spans from the model had the highest negative impact, whereas the removal of label embeddings at previous time steps had a relatively insignificant negative impact. This result validates our assumption that span-level features are beneficial for representing entities in both NER and RE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Prediction Order</head><p>Existing methods for jointly extracting entities and relations with the table-filling approach rely on history-based predictions, i.e., fill up the lower (or upper) triangular part of a  <ref type="table">Table 3</ref>: Ablation test of features used in this study on the CoNLL04 test set. "-Label" presents the performance when removing the label embeddings at previous timesteps from the model. "-Span" removes the features of the previous span. "-Both" presents the results when removing both of them.</p><p>pre-defined order <ref type="bibr" target="#b12">(Miwa and Sasaki, 2014;</ref><ref type="bibr" target="#b5">Gupta et al., 2016;</ref><ref type="bibr" target="#b26">Zhang et al., 2017)</ref>. These methods assume that earlier decisions help later decisions, which may involve long-range dependencies. In contrast, our model is free from prediction history for relation labels; it focuses on predicting them at once using a tensor dot-product.</p><p>A natural question is whether history-based predictions are useful for the proposed method or not?</p><p>To find the answer, we designed a variant of the RE model that utilized predicted results of cells to the left of and below a target cell. More specifically, we modified Equation 10 to make use of the embeddings of relation labels at (i, j ? 1) and (i + 1, j) when predicting a relation label for the element  <ref type="table">Table 4</ref>: Performance of the proposed method on the CoNLL04 test set with history-based predictions. "Once" stands for the proposed method that predicts all relation labels at once. "Seq" decides relation labels sequentially so that it can incorporate prediction results of cells to the left of and below a target.</p><p>(i, j), and scheduled predictions in ascending order of distance from the diagonal elements and from left-top to right-bottom. However, a significant improvement in the performance of the model is not observed even after several fine-tuning efforts. Experimental results on the CoNLL04 test set are shown in <ref type="table">Table 4</ref>. It is difficult to identify the reason for the experimental results, but the RE model might utilize long-range dependencies from the BERT encoder to make decisions. In addition, the history-based prediction increases the number of parameters and complexity of label predictions, by introducing extra parameters for relation embeddings and additional classifiers. It is potentially beneficial to try several prediction orders as in <ref type="bibr" target="#b12">Miwa and Sasaki (2014)</ref>, but the experimental results suggest that predicting relation labels at once is sufficient for the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Multi-Sentence NER</head><p>As described in Section 3.3, the proposed method could not outperform DyGIE++  on ACE05 NER, because of the unavailability of cross-sentence information such as coreferences. In this subsection, we describe how we eliminated the performance gap by merely providing multiple sentences into the model without modifying the architecture. Specifically, we split a document into segments of multiple sentences such that each segment was not longer than 256 sub-word tokens. This length restriction is introduced due to GPU memory capacity limitations. Assuming that each segment was a sequence of sub-word tokens from multiple sentences, we fed each segment to the model with multiple sentences separated by a special token [SEP], similar to <ref type="bibr" target="#b1">Devlin et al. (2019)</ref>. <ref type="table" target="#tab_8">Table 5</ref> shows the performance of NER models on the CoNLL03 and ACE05 with and without multi-sentence inputs. To better investigate the ef-fectiveness of our model, we use "BERT (our replication)" as the baseline. Our models then equip the BERT encoder with extra modules, as described in Section 2.2. We observe that multi-sentence inputs boosted the performance on both datasets, making our model outperform other models, including DyGIE++ and BERT <ref type="bibr" target="#b1">(Devlin et al., 2019)</ref>. By comparing the prediction results, we conclude that multi-sentence input improved predictions for multiple occurrences of the same entity, gathering contexts in different occurrences. Specific shreds of evidence with examples are shown in Appendix C.</p><p>Although we cannot apply this technique directly to RE (because the table size is O(n 2 )), we tested our RE model in a pipelined fashion by using predictions of NER with multi-sentence input on the ACE05 test set. However, we did not see a performance boost on RE. By analyzing predicted RE instances, we discovered that multi-sentence NER increased the overall performance by capturing coreferences, but it also failed to correctly label some of the NEs essential for RE. The observation emphasizes the importance of improving the design of the RE model and a better approach to combine sentence-level and document-level context during RE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Early studies formulated the task of jointly extracting entities and relations as a structured prediction with the global features and search algorithms. <ref type="bibr" target="#b6">Li and Ji (2014)</ref> presented an incremental algorithm for joint NER and RE with global features and inexact (beam-search) decoding. <ref type="bibr" target="#b12">Miwa and Sasaki (2014)</ref> proposed a table representation for entities and relations. Further, it investigated hand-crafted features and complex search heuristics on the table. <ref type="bibr" target="#b5">Gupta et al. (2016)</ref>   More recently, span-enumeration methods have been a popular approach for jointly extracting entities and relations <ref type="bibr" target="#b4">Eberts and Ulges, 2020)</ref>. In general, span enumeration methods consider possible entity spans for an input sentence with some criteria (e.g., the maximum number of words), choose likely spans using features extracted from the span candidates.  proposed a general framework of information extraction called DyGIE that can incorporate global information on a dynamic span graph.  further expanded the model to DyGIE++. The method receives multiple sentences from the same document as input and enumerates candidate spans for relation, coreference, and event identification. To update span representations of entities, they carefully designed strategies for dynamic graph construction and span refinement. <ref type="bibr" target="#b4">Eberts and Ulges (2020)</ref> also proposed an end-to-end RE model for extracting both entities and relations called SpERT. The method bases on pre-trained BERT models and enumerates candidates of entity spans. Using a negative sampling strategy for both NER and RE, the method classifies entity and relation candidates into positive and negative.</p><p>We aimed at solving the drawbacks of the tablefilling approach, e.g., complicated feature engineering and decoding algorithm. In our approach, feature engineering for NER was removed by using contextualized word representations and spanbased features. The proposed method utilized a tensor dot-product for filling in off-diagonal cells at once without using history-based predictions. Although the proposed architecture was different from the span-enumeration based approaches (DyGIE++ and SpERT), the experimental results demonstrated competitive or better performance than the span-enumeration based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper presented TablERT, a novel method for extracting NE and relations based on the table representation, making use of contextualized word embeddings for representing entity mentions. We applied tensor dot-product for predicting all the relation labels at once. The experimental results on the CoNLL04 and ACE05 dataset demonstrated that the proposed method outperformed not only the existing table-filling methods but also the SOTA methods based on pre-trained BERT models. We also confirmed that the method achieved comparable performance to the SOTA NER models on the ACE05 when multiple sentences were fed to the model.</p><p>In the future, we plan to explore an approach for incorporating global constraints in the RE model, which currently predicts all relation labels independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A: Major Hyper-Parameters</head><p>This section contains a list of major hyperparameters in our model, as shown in <ref type="table">Table 6</ref>. For both CoNLL04 and ACE05, we used the same hyperparameters with an exception to batch size, owing to the difference in the data scale. We applied dropout to e w i and f (e w first(i?1) , ? ? ? , e w i?1 ) in Equation 2 and z i in Equation 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter</head><p>Value # dims of token embeddings |e w | 768 # dims of label embeddings |l y | 50 # dims of relation attention d att 20 learning rate (BERT encoder) 5 ? 10 ?5 learning rate (others) 1 ? 10 ?3 dropout rate 0.3 warm-up period 0.2 total number of epochs 30 <ref type="table">Table 6</ref>: Hyper-parameter settings. We adopted a learning rate scheduler that increased learning rates linearly from 0 to 5 ? 10 ?5 in the warm-up period of 0.2 ? (total number of epochs), and then decreased learning rates using a cosine function.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example of the upper triangular matrix Y and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The Named Entity Recognition model. For clarity, we only show the calculation of entity hidden vectors h</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>F1) on the test sets of CoNLL04 and ACE05. ACE05 ? regards a relation prediction to be correct when both the relation label and head regions of two arguments are correct. ACE05 ? requires that NE labels of arguments are correct in addition to the evaluation criteria of ACE05 ? . Notably, NER scores of ACE05 ? and ACE05 ? are comparable because the difference in the evaluation criteria affects RE scores only.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>P</cell><cell>Entity R</cell><cell>F1</cell><cell>P</cell><cell>Relation R</cell><cell>F1</cell></row><row><cell></cell><cell>Miwa and Sasaki (2014)</cell><cell cols="6">81.2 80.2 80.7 76.0 50.9 61.0</cell></row><row><cell></cell><cell>Zhang et al. (2017)</cell><cell>-</cell><cell>-</cell><cell>85.6</cell><cell>-</cell><cell>-</cell><cell>67.8</cell></row><row><cell>CoNLL04</cell><cell>Multi-turn QA (Li et al., 2019)</cell><cell cols="6">89.0 86.6 87.8 69.2 68.2 68.9</cell></row><row><cell></cell><cell cols="7">SpERT (Eberts and Ulges, 2020) 88.3 89.6 88.9 73.0 70.0 71.5</cell></row><row><cell></cell><cell>TablERT (ours)</cell><cell cols="6">89.7 90.6 90.2 75.0 70.3 72.6</cell></row><row><cell></cell><cell>Li and Ji (2014)</cell><cell cols="6">85.2 76.9 80.8 68.9 41.9 52.1</cell></row><row><cell>ACE05 ?</cell><cell>Dixit and Al-Onaizan (2019) DyGIE++ (Wadden et al., 2019)</cell><cell cols="6">85.9 86.1 86.0 68.0 58.4 62.8 --88.6 --63.4</cell></row><row><cell></cell><cell>TablERT (ours)</cell><cell cols="6">87.8 88.2 88.0 70.9 61.9 66.1</cell></row><row><cell></cell><cell>Li and Ji (2014)</cell><cell cols="6">85.2 76.9 80.8 65.4 39.8 49.5</cell></row><row><cell></cell><cell cols="7">SPTree (Miwa and Bansal, 2016) 82.9 83.9 83.4 57.2 54.0 55.6</cell></row><row><cell>ACE05 ?</cell><cell>Zhang et al. (2017)</cell><cell>-</cell><cell>-</cell><cell>83.6</cell><cell>-</cell><cell>-</cell><cell>57.5</cell></row><row><cell></cell><cell>MRT (Sun et al., 2018)</cell><cell cols="6">83.9 83.2 83.6 64.9 55.1 59.6</cell></row><row><cell></cell><cell>Multi-turn QA (Li et al., 2019)</cell><cell cols="6">84.7 84.9 84.8 64.8 56.2 60.2</cell></row><row><cell></cell><cell>TablERT (ours)</cell><cell cols="6">87.8 88.2 88.0 67.0 58.5 62.4</cell></row><row><cell cols="3">SD Table 1: Micro-averaged precision (P), recall (R), and F1 score (Model NER RE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SpERT</cell><cell>0.378 0.857</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">TablERT (ours) 0.187 0.334</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Standard derivations (SD) of F1 scores of NER and RE on the CoNLL04 test set predicted by SpERT and this work (with five runs).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>table cell</head><label>cell</label><figDesc></figDesc><table><row><cell>Model</cell><cell>P</cell><cell>Entity R</cell><cell>F1</cell><cell>P</cell><cell>Relation R</cell><cell>F1</cell></row><row><cell>Full</cell><cell cols="6">89.7 90.5 90.1 74.7 70.8 72.7</cell></row><row><cell cols="7">-Label 89.7 90.5 90.1 74.1 70.6 72.3</cell></row><row><cell>-Span</cell><cell cols="6">89.5 90.6 90.0 74.3 69.6 71.9</cell></row><row><cell>-Both</cell><cell cols="6">89.4 90.2 89.8 73.5 69.7 71.3</cell></row><row><cell>-by-cell in a</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Results of NER on the CoNLL03 and ACE05 test sets. Values of<ref type="bibr" target="#b1">Devlin et al. (2019)</ref> and are reported scores. We included a baseline BERT, following the original study settings, which involves each word to be represented by its first sub-word token.</figDesc><table><row><cell>contextualized word representations for the sequen-</cell></row><row><cell>tial labeling problem. Liu et al. (2019) proposed</cell></row><row><cell>a deep transition architecture enhanced with the</cell></row><row><cell>global context and reported improvements on NER</cell></row><row><cell>and chunking tasks by using contextualized word</cell></row><row><cell>embeddings. Strakov? et al. (2019) also demon-</cell></row><row><cell>strated the effectiveness of the contextualized rep-</cell></row><row><cell>resentations on the architectures for nested named</cell></row><row><cell>entity recognition, where NE may overlap with</cell></row><row><cell>multiple labels assigned.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We examined the performance on the CoNLL04 development set by using (1) embedding of first sub-word token<ref type="bibr" target="#b1">(Devlin et al., 2019)</ref>; (2) mean-pooling of constituent sub-word tokens; (3) mean-pooling of constituent sub-word tokens and [CLS]; (4) max-pooling of constituent sub-word tokens. Among these, max-pooling worked the best.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We explore several combinations. For example, using Equation 5 for both training and predicting phases, we confirmed that the combination: (Equation 5 for prediction and Equation 11 for training), performed the best.4  The beam search decoding did not depict a definite performance improvement, which is consistent with the report in<ref type="bibr" target="#b11">Miwa and Bansal (2016)</ref>.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B: Error Inspection</head><p>This section contains descriptions for incorrect predictions of the proposed method to delineate future directions for improvement. <ref type="table">Table 7</ref> summarizes typical errors of the proposed method found in the CoNLL04 test set.</p><p>Incorrect NE span Cases where the NER model predicts slightly incorrect spans. Typical errors of this category involve adding/missing a nearby phrase of an NE span. Table 7 (a) is an example where the phrase "on Earth" can be interpreted as a prepositional phrase or a part of a proper noun.</p><p>Incorrect NE type Cases where the NER model predicts incorrect NE labels for entity mentions. These cases usually occur when an entity can be interpreted with different NE types. Table 7 (b) illustrates that "Charing Cross Hospital" can be categorized as ORGANIZATION if we look at the NE alone, but is actually annotated as LOCATION in the context (indicating the location of the event 'died').</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lack of Knowledge</head><p>Cases where the RE model fails to recognize implicit relations. In <ref type="table">Table 7</ref> (c), it is not so easy to recognize the relation instance (Livingston Loc , LocatedIn, Montana Loc ) only from the sentence without the knowledge about the entities 'Livingston' and 'Montana'. Fortunately, the RE model could predict the relation instance correctly in this example. However, it is even more difficult to infer the relation instance (Livingston Loc , LocatedIn, Rocky Mountains Loc ) from the text; we are not sure of the inclusion relation between "Rocky Mountain" and 'Livingston' without the knowledge about the entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lack of global constraints</head><p>Cases where the RE model could avoid incorrect RE instances with constraints. As shown in <ref type="table">Table 7</ref> (d), the model infers that the same person lives in two different places (Soviet and China). the proposed method cannot consider associations between relation predictions explicitly because relation labels are predicted independently of each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C: Predicted Examples for Multi-Sentence NER</head><p>This section contains several typical predicted examples showing the effectiveness of the multisentence NER model, as shown in <ref type="table">Table 8</ref>.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exploiting syntactico-semantic structures for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Seng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL)</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="551" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers) (NAACL)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Span-level model for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalpit</forename><surname>Dixit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1525</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5308" to="5314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep biaffine attention for neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Span-based joint entity and relation extraction with transformer pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Eberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Ulges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th European Conference on Artificial Intelligence (ECAI)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Table filling multi-task recurrent neural network for joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pankaj</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Andrassy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers (COLING)</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers (COLING)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2537" to="2547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Incremental joint extraction of entity mentions and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-1038</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="402" to="412" />
		</imprint>
	</monogr>
	<note>Long Papers) (ACL)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Entity-relation extraction as multi-turn question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiayu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duo</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1129</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1340" to="1350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">GCDT: A global context enhanced deep transition architecture for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinchao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1233</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2431" to="2441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A general framework for information extraction using dynamic span graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1308</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3036" to="3046" />
		</imprint>
	</monogr>
	<note>Long and Short Papers) (NAACL)</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">End-to-end relation extraction using LSTMs on sequences and tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1105</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1105" to="1116" />
		</imprint>
	</monogr>
	<note>Long Papers) (ACL)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modeling joint entity and relation extraction with table representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Sasaki</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1200</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1858" to="1869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A survey of named entity recognition and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linguisticae Investigationes</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-toend neural relation extraction using deep biaffine attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Verspoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st European Conference on Information Retrieval</title>
		<meeting>the 41st European Conference on Information Retrieval</meeting>
		<imprint>
			<publisher>ECIR</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32 (NIPS)</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Conll-2012 shared task: Modeling multilingual unrestricted coreference in ontonotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on EMNLP and CoNLL -Shared Task</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A linear programming formulation for global inference in natural language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Conference on Computational Natural Language Learning (CoNLL-2004) at HLT-NAACL 2004</title>
		<meeting>the Eighth Conference on Computational Natural Language Learning (CoNLL-2004) at HLT-NAACL 2004</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural architectures for nested NER through linearization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Strakov?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1527</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5326" to="5331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Extracting entities and relations with joint minimum risk training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changzhi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Chih</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewen</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1249</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien</forename><surname>De Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</title>
		<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Entity, relation, and event extraction with contextualized span representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulme</forename><surname>Wennberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1585</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5784" to="5789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R&amp;apos;emi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Brew</surname></persName>
		</author>
		<idno>abs/1910.03771</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Kernel methods for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Zelenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinatsu</forename><surname>Aone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Richardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1083" to="1106" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">End-to-end neural relation extraction with global optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohong</forename><surname>Fu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1182</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1730" to="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploring various knowledge in relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.3115/1219840.1219893</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">75</biblScope>
		</imprint>
	</monogr>
	<note>CoNLL03 Location (single), Organization (multi)</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Leading scorers: Charleroi -Eric Cleymans 18</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estudiantes</forename><surname>Madrid</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="34" to="35" />
		</imprint>
	</monogr>
	<note>Ron Ellis 18, Jacques Stas 14 Person (single), Organization (multi), Organization (gold</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Organization (multi), Organization (gold) North Korea has told American lawmakers it already has nuclear weapons</title>
		<imprint/>
	</monogr>
	<note>Person (single</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">They admitted to having just about completed the reprocessing of 8, 000 rods</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Person (multi), Person (gold)</title>
	</analytic>
	<monogr>
		<title level="j">Geographical Entity (single)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Table 8: Typical examples where the proposed NER model showed improvements with multi-sentence input. A boldface word presents a target for predicting the NE label, and an italic word is a coreference to the boldface word. NE labels shown on top of each example are the prediction from single-sentence input; that from multi-sentence input; and the ground-truth label. We can infer that the model with multi-sentence</title>
		<imprint/>
	</monogr>
	<note>Southern voters are &quot;children of Democrats who are not swayed by the same things. input made correct predictions, looking at coreferential words</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
