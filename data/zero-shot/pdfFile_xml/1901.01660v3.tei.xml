<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deeper and Wider Siamese Networks for Real-Time Visual Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhang</surname></persName>
							<email>zhipeng.zhang@nlpr.ia.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
							<email>houwen.peng@micrsoft.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences&amp;CASIA</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deeper and Wider Siamese Networks for Real-Time Visual Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Siamese networks have drawn great attention in visual tracking because of their balanced accuracy and speed. However, the backbone networks used in Siamese trackers are relatively shallow, such as AlexNet <ref type="bibr" target="#b17">[18]</ref>, which does not fully take advantage of the capability of modern deep neural networks. In this paper, we investigate how to leverage deeper and wider convolutional neural networks to enhance tracking robustness and accuracy. We observe that direct replacement of backbones with existing powerful architectures, such as ResNet <ref type="bibr" target="#b13">[14]</ref> and Inception <ref type="bibr" target="#b32">[33]</ref>, does not bring improvements. The main reasons are that 1) large increases in the receptive field of neurons lead to reduced feature discriminability and localization precision; and 2) the network padding for convolutions induces a positional bias in learning. To address these issues, we propose new residual modules to eliminate the negative impact of padding, and further design new architectures using these modules with controlled receptive field size and network stride. The designed architectures are lightweight and guarantee real-time tracking speed when applied to SiamFC [2]   and SiamRPN [20]. Experiments show that solely due to the proposed network architectures, our SiamFC+ and SiamRPN+ obtain up to 9.8%/5.7% (AUC), 23.3%/8.8% (EAO) and 24.4%/25.0% (EAO) relative improvements over the original versions [2, 20] on the OTB-15, VOT-16 and VOT-17 datasets, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual tracking is one of the fundamental problems in computer vision. It aims to estimate the position of an arbitrary target in a video sequence, given only its location in the initial frame. Tracking at real-time speeds plays a vital role in numerous vision applications, such as surveillance, robotics, and human-computer interaction <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>Recently, trackers based on Siamese networks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40]</ref> have drawn great attention due to their high speed and accuracy. However, the backbone network utilized in these trackers is still the classical AlexNet <ref type="bibr" target="#b17">[18]</ref>, * corresponding author Here, width refers to the number of branches in a module. The results are obtained using SiamFC <ref type="bibr" target="#b1">[2]</ref> with different backbone networks, through evaluation on  rather than modern deep neural networks that have proven more effective for feature embedding. To examine this issue, we replace the shallow backbone with deeper and wider networks, including VGG <ref type="bibr" target="#b28">[29]</ref>, Inception <ref type="bibr" target="#b32">[33]</ref> and ResNet <ref type="bibr" target="#b13">[14]</ref>. Unexpectedly, this straightforward replacement does not bring much improvement, and can even cause substantial performance drops when the network depth or width increases, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. This phenomenon runs counter to the evidence that increasing network depth and width is beneficial for elevating model capability <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>One intuitive reasoning is that these deeper and wider network architectures are primarily designed for image classification tasks, where the precise localization of the object is not paramount. To investigate the concrete reason, we analyze the Siamese network architecture and identify that the receptive field size of neurons, network stride and feature padding are three important factors affecting tracking accuracy. In particular, the receptive field determines the image region used in computing a feature. A larger receptive field provides greater image context, while a small one may not capture the structure of target objects. The network stride affects the degree of localization precision, especially for small-sized objects. Meanwhile, it controls the size of output feature maps, which affects feature discriminability and detection accuracy. Moreover, for a fully-convolutional architecture <ref type="bibr" target="#b1">[2]</ref>, the feature padding for convolutions induces a potential position bias in model training, such that when an object moves near the search range boundary, it is difficult to make an accurate prediction. These three factors together prevent Siamese trackers from benefiting from current deeper and more sophisticated network architectures.</p><p>In this paper, we address these issues by designing new residual modules and architectures that allow deeper and wider backbone networks to unleash their power in Siamese trackers. First, we propose a group of cropping-inside residual (CIR) units based on the "bottleneck" residual block <ref type="bibr" target="#b13">[14]</ref>. The CIR units crop out padding-affected features inside the block (i.e., features receiving padding signals), and thus prevent convolution filters from learning the position bias. Second, we design two kinds of network architectures, namely deeper and wider networks, by stacking the CIR units. In these networks, the stride and neuron receptive field are formulated to enhance localization precision. Finally, we apply the designed backbone networks to two representative Siamese trackers: SiamFC <ref type="bibr" target="#b1">[2]</ref> and SiamRPN <ref type="bibr" target="#b19">[20]</ref>. Experiments show that solely due to the proposed network architectures, the Siamese trackers obtain up to 9.8%/5.7%(AUC), 23.3%/8.8%(EAO) and 24.4%/25.0%(EAO) relative improvements over the original versions <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20]</ref> on the OTB-15, VOT-16 and VOT-17 datasets, respectively. In addition, the designed architectures are lightweight and permit the trackers to run at realtime speed.</p><p>The main cotributions of this work are twofold.</p><p>? We present a systematic study on the factors of backbone networks that affect tracking accuracy, and provides architectural design guidelines for the Siamese tracking framework.</p><p>? We design new deeper and wider network architectures for Siamese trackers, based on our proposed nopadding residual units. Experimental results demonstrate that the new architectures provide clear improvements over the baseline trackers. Code and models are available at https://github.com/ researchmm/SiamDW.</p><p>In the remainder of this paper, we first review background on Siamese tracking in Sec. 2. This is followed by an analysis of performance degradation in Sec. 3. Based on the analysis, we propose new residual modules and network architectures in Sec. 4. Experiments and comparisons are reported in Sec. 5. We end the paper with a discussion of related work and the conclusion in Sec. 6 and 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background on Siamese Tracking</head><p>Before analyzing the reasons for the performance degradation shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, we briefly review the fullyconvolutional Siamese tracker SiamFC <ref type="bibr" target="#b1">[2]</ref>, which serves as the basic framework discussed in this work. The standard Siamese architecture takes an image pair as input, comprising an exemplar image z and a candidate search image x. The image z represents the object of interest (e.g., an image patch centered on the target object in the first video frame), while x is typically larger and represents the search area in subsequent video frames. Both inputs are processed by a ConvNet ? with parameters ?. This yields two feature maps, which are cross-correlated as</p><formula xml:id="formula_0">f ? (z, x) = ? ? (z) ? ? (x) + b ? 1,<label>(1)</label></formula><p>where b ? 1 denotes a bias term which takes the value b ? R at every location. Eq. 1 amounts to performing an exhaustive search of the pattern z over the image x. The goal is to match the maximum value in response map f to the target location.</p><p>To achieve this goal, the network is trained offline with random image pairs (z, x) taken from training videos and the corresponding ground-truth label y. The parameters ? of the ConvNet are obtained by minimizing the following logistic loss over the training set:</p><formula xml:id="formula_1">arg min ? E (z,x,y) (y, f ? (z, x)).<label>(2)</label></formula><p>Previous methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42]</ref> commonly utilize the classic and relatively shallow AlexNet <ref type="bibr" target="#b17">[18]</ref> as the backbone network ? in this framework. In our work, we study the problem of how to design and leverage a more advanced ConvNet ? to learn an effective model ? that enhances tracking robustness and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Analysis of Performance Degradation</head><p>In this section, we analyze the underlying reasons for the performance degradation presented in <ref type="figure" target="#fig_0">Fig. 1</ref>. We conduct ablation experiments on the internal factors of network structures, and identify the ones most responsible for performance drops. We then propose a set of practical guidelines for network architecture design, aimed to alleviate the negative effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Analysis</head><p>Quantitative analysis. Performance degradation can directly be attributed to network structure, as it is the only setting that changes in the experiments of <ref type="figure" target="#fig_0">Fig. 1</ref>. Therefore, we first identify the structural differences among these network architectures 1 . As shown in Tab. 2, besides depth and width, there are several other internal network factors that differ among the networks, including stride (STR), padding (PAD), receptive field (RF) of neurons in the last layers, and output feature size (OFS).</p><p>To investigate the impact of these factors, we conduct an ablation study. We modify the structures of AlexNet, VGG,  In these cases, the optimal receptive field size is about 60%?80% of the input exemplar image z size (e.g. 91 vs 127). Intriguingly, this ratio is robust for various networks in our study, and it is insensitive to their structures. It illustrates that the size of RF is crucial for feature embedding in a Siamese framework. The underlying reason is that RF determines the image region used in computing a feature. A large receptive field covers much image context, resulting in the extracted feature being insensitive to the spatial location of target objects. On the contrary, a small one may not capture the structural information of objects, and thus it is less discriminative for matching. Therefore, only RF in a certain size range allows the feature to abstract the characteristics of the object, and its ideal size is closely related to the size of the exemplar image. For the output feature size, it is observed that a small size (OFS ? 3) does not benefit tracking accuracy. This is because small feature maps lack enough spatial structure description of target objects, and thus are not robust in image similarity calcula- tion. Moreover, as shown in Tab. 1 (| vs. } on AlexNet and VGG, { vs. | on Incep. and ResNet), we observe that network padding has a highly negative impact on the final performance. To examine this further, we conduct a qualitative experiment. Qualitative analysis. The Siamese networks feed pairs of exemplar and search images as training data, and learn a feature embedding for matching. If the networks contain padding operations, the embedding features of an exemplar image are extracted from the original exemplar image plus additional (zero)-padding regions. Differently, for the features of a search image, some of them are extracted only from image content itself, while some are extracted from image content plus additional (zero)-padding regions (e.g. the features near the border). As a result, there is inconsistency between the embeddings of target object appearing at different positions in search images, and therefore the matching similarity comparison degrades. <ref type="figure" target="#fig_1">Fig. 2</ref> presents a visualization example of such inconsistency-induced effect in the testing phase. It shows that when the target object moves to image borders, its peak does not precisely indicate the location of the target. This is a common case caused by tracker drifts, when the predicted target location is not precise enough in the previous frame.</p><formula xml:id="formula_2">Input ResNet| CIResNet-22 Input ResNet| CIResNet-22</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Guidelines</head><p>According to the above analysis, we summarize four basic guidelines to alleviate the negative impacts of structural factors in a network architecture.</p><p>? Siamese trackers prefer a relatively small network stride.</p><p>Network stride affects the overlap ratio of receptive fields  for two neighboring output features. It thereby determines the basic degree of localization precision. Therefore, when network depth increases, the stride should not increase accordingly. With regards to accuracy and efficiency, an empirically effective choice is to set the stride to 4 or 8.</p><formula xml:id="formula_3">(a) original (a ) CIR (b) original (b ) CIR-D (c) CIR-Inception (d) CIR-NeXt</formula><p>? The receptive field of output features should be set based on its ratio to the size of the exemplar image. An empirically effective ratio is 60%?80% for an exemplar image. A proper ratio allows the network to extract a set of features, each of which captures the information of different spatial parts of a target object. This leads the extracted features to be robust in calculating region similarity. Particularly, the maximum RF should not be larger than the exemplar image, otherwise the performance will drop significantly.</p><p>? Network stride, receptive field and output feature size should be considered as a whole when designing a network architecture. These three factors are not independent of one another. If one changes, the others will change accordingly. Considering them together can help the designed network to extract more discriminative features in a Siamese framework.</p><p>? For a fully convolutional Siamese matching network, it is critical to handle the problem of perceptual inconsistency between the two network streams. There are two feasible solutions. One is to remove the padding operations in networks, while the other is to enlarge the size of both input exemplar and search images, and then crop out padding-affected features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Deeper and Wider Siamese Networks</head><p>In this section, we design new modules, i.e. croppinginside residual (CIR) units, to eliminate the underlying position bias. Then, we build up deeper and wider backbone networks by stacking the new residual modules. The network stride and receptive field size are well controlled according to the guidelines. We further apply the designed networks to two representative Siamese trackers, i.e. SiamFC <ref type="bibr" target="#b1">[2]</ref> and SiamRPN <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Cropping-Inside Residual (CIR) Units</head><p>The residual unit <ref type="bibr" target="#b13">[14]</ref> is a key module in network architecture design due to its easy optimization and powerful representation. It consists of 3 stacked convolution layers and a shortcut connection that bypasses them, as shown in <ref type="figure" target="#fig_2">Fig. 3(a)</ref>. The three layers are 1?1, 3?3 and 1?1 convolutions, where the 1?1 layers are responsible for reducing and then restoring dimensions, leaving the 3?3 layer as a bottleneck with smaller input and output dimensions. This bottleneck convolution includes zero-padding of size 1, to ensure a compatible output size before the addition.</p><p>CIR Unit. As discussed in Sec. 3, network padding may introduce position bias in the Siamese framework. Hence, it is necessary to remove the padding in residual units when utilizing them to build a Siamese network. To this end, we augment the residual unit with a cropping operation, which is incorporated after the feature addition, as shown in <ref type="figure" target="#fig_2">Fig. 3(a )</ref>. The cropping operation removes features whose calculation is affected by the zero-padding signals.</p><p>Since the padding size is one in the bottleneck layer, only the outermost features on the border of the feature maps are cropped out. This simple operation neatly removes padding-affected features in residual unit.</p><p>Downsampling CIR (CIR-D) Unit. The downsampling residual unit is another key building block for network design. It is utilized to reduce the spatial size of feature maps while doubling the number of feature channels. Similar to the residual unit, the downsampling unit also contains padding operations, as shown in <ref type="figure" target="#fig_2">Fig. 3(b)</ref>. Thus, we also modify its structure to remove the negative effects caused by the padding. As shown in <ref type="figure" target="#fig_2">Fig. 3(b )</ref>, we change the convolutional stride from 2 to 1 within both the bottleneck layer and shortcut connection. Cropping is again inserted after the addition operation to remove the padding-affected features. Finally, max-pooling is employed to perform spatial downsampling of the feature map. The key idea of these modifications is to ensure that only the features influenced by padding are removed, while keeping the intrinsic block structure unchanged. If we were only to insert cropping after the addition operation, as done in the proposed CIR unit, without changing the position of downsampling, the features after cropping would not receive any signal from the outermost pixels in the input image. As the network depth increases, this would effectively cause even more image content to be removed, resulting in noisy/incomplete extracted features. CIR-Inception and CIR-NeXt Units. We further equip the CIR unit with a multi-branch structure, enabling it to be used in building wide networks. Similar to Inception <ref type="bibr" target="#b32">[33]</ref> and ResNeXt <ref type="bibr" target="#b38">[39]</ref>, we widen the CIR unit with multiple feature transformations, generating the CIR-Inception and CIR-NeXt modules as shown in <ref type="figure" target="#fig_2">Fig. 3(c-d)</ref>. Specifically, in the CIR-Inception structure, we insert a 1?1 convolution into the shortcut connection, and merge the features of the two branches by concatenation, rather than by addition. In CIR-ResNeXt, we split the bottleneck layer into 32 transformation branches, and aggregate them by addition. Moreover, for the downsampling units of CIR-Inception and CIR-NeXt, the modifications are the same as those in CIR-D ( <ref type="figure" target="#fig_2">Fig. 3(b )</ref>), where the convolution stride is reduced and max-pooling is added. These two multi-branch structures enable the units to learn richer feature representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Network Architectures</head><p>By stacking the above CIR units, we build up deeper and wider networks. The constructions follow our design guidelines. First, we determine the network stride. A stride of 8 is used to build a 3-stage network, while a stride of 4 is employed in constructing a 2-stage one. Then, we stack CIR units. We control the number of units and the position of downsampling units in each stage. The goal is to ensure that the receptive field size of neurons in the final layer lies within the derived range, i.e. 60%-80% of the exemplar image. Additionally, when network depth increases, the receptive field may exceed this range. Therefore, we halve the stride to 4 to control the receptive field.</p><p>Deeper Networks. We construct deeper networks using CIR and CIR-D units. The structures are similar to ResNet <ref type="bibr" target="#b13">[14]</ref>, but with different network stride, receptive field size, and building blocks. In Tab. 3, we present four deep cropping-inside residual networks, i.e. CIResNet-16, 19, 22 and 43. Since these networks have similar structure, we present details for just two of them: CIResNet-22 and CIResNet-43.</p><p>CIResNet-22 has 3 stages (stride=8) and consists of 22 weighted convolution layers. Except for the first 7?7 convolution, the others are all CIR units. A cropping operation (with size of 2) follows the 7?7 convolution to remove padding-affected features. The feature downsampling in the first two stages are performed by a convolution and a maxpooling of stride 2, following the original ResNet <ref type="bibr" target="#b13">[14]</ref>. In the third stage, downsampling is performed by the proposed CIR-D unit, which is located at the first block in this stage (4 blocks in total). When the feature map size is downsampled, the number of filters is doubled to increase feature discriminability. The spatial size of the output feature map is 5?5, with each feature receiving signals from a region of size 93?93 pixels on the input image plane, i.e. the corresponding size of receptive field.</p><p>We further increase network depth to 43 layers in building CIResNet-43. Because of its large depth, CIResNet-43 is designed with only 2 stages, to keep its receptive field size within the suggested range. In the second stage of CIResNet-43, there are 14 blocks, where the fourth one has a CIR-D unit for feature downsampling. It is worth noticing that CIResNet-43 almost reaches the maximum depth of backbone networks that can achieve real-time speed in the SiamFC <ref type="bibr" target="#b1">[2]</ref> framework. It has 6.07G FLOPs (multiplyadds) and runs at an average of ?35 fps in SiamFC framework on a GeForce GTX 1080 GPU.</p><p>Wider Networks. We construct two types of wide network architectures using CIR-Inception and CIR-NeXt units, respectively. Here, we only present a 22-layer structure as an example, since other wider networks are similar to this case. As presented in Tab. 3, the wide networks, i.e. CIResInception-22 and CIResNeXt-22, have similar structure to CIResNet-22 in terms of network stride, building block number and output feature size. But the network widths are increased by 2 and 32 times respectively, through the multi-branch building blocks. Also, the receptive field size becomes diverse (i.e. 13?93) in CIResInception-22 due to multi-branch concatenation, but the maximum size still remain within the suggested range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Applications</head><p>We apply the designed deeper and wider networks to two representative Siamese trackers: the classical SiamFC <ref type="bibr" target="#b1">[2]</ref> and the most recently proposed SiamRPN <ref type="bibr" target="#b19">[20]</ref>. In both of these two trackers, we replace the original shallow backbones, i.e. the 5-layer AlexNet <ref type="bibr" target="#b17">[18]</ref>, with our designed networks, which is the only modification to the original frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>This section presents the results of our deeper and wider Siamese networks on multiple benchmark datasets, with comparisons to the state-of-the-art tracking algorithms. Ablation studies are also provided to analyze the effects of the components in the proposed networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Details</head><p>Training. The parameters of our networks are initialized with the weights pre-trained on ImageNet <ref type="bibr" target="#b27">[28]</ref>. During training, we freeze the weights of the first 7?7 convolution layer, and gradually fine-tune other layers from back to front. We unfreeze the weights of the layers in each block (i.e. the proposed cropping-inside residual units) after every five training epochs. There are 50 epochs in total, the same as in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20]</ref>. The learning rates are decreased logarithmically from 10 ?3 /10 ?2 to 10 ?7 /10 ?5 for SiamFC and SiamRPN, respectively. The weight decay is set to 10 ?4 , and the momentum is set to 0.9 (for both SiamFC and SiamRPN). We use synchronized SGD <ref type="bibr" target="#b18">[19]</ref> on 4 GPUs, with each GPU hosting 32 images, hence the mini-batch size is 128 images per iteration.</p><p>The training image pairs for SiamFC are collected from the ImageNet VID dataset <ref type="bibr" target="#b27">[28]</ref>, while for SiamRPN, it is generated from ImageNet VID <ref type="bibr" target="#b27">[28]</ref> and Youtube-BB <ref type="bibr" target="#b26">[27]</ref>, which is the same as those in the original frameworks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20]</ref>. The size of an exemplar image is 127?127 pixels, while the size of a search image is 255?255 pixels.</p><p>Testing. Tracking follows the same protocols as in SiamFC <ref type="bibr" target="#b1">[2]</ref> and SiamRPN <ref type="bibr" target="#b19">[20]</ref>. The embedding ? ? (z) of the target object is computed once at the first frame, and then is continuously matched to subsequent search images ? ? (x). To handle scale variations, SiamFC searches for the object over three scales 1.0482 {?1,0,1} and updates the scale by linear interpolation with a factor of 0.3629 to provide damping. SiamRPN searches over only one scale since it employs proposal refinement to handle scale change. The penalty for large change of proposal size and aspect ratio is   The listed methods, such as EBT <ref type="bibr" target="#b40">[41]</ref>, LDP <ref type="bibr" target="#b23">[24]</ref>, nSAMF <ref type="bibr" target="#b21">[22]</ref>, TCNN <ref type="bibr" target="#b24">[25]</ref>, MLDF <ref type="bibr" target="#b35">[36]</ref>, CFWCR <ref type="bibr" target="#b35">[36]</ref> and CFCF <ref type="bibr" target="#b10">[11]</ref> are compared in VOT challenges <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17</ref>]. set to 0.439.</p><p>Our networks and trackers are implemented using Python 3.6 and PyTorch 0.3.1. The experiments are conducted on a PC with a GeForce GTX 1080 GPU and a Xeon E5 2.4GHz CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with Baselines</head><p>We first compare our deeper and wider networks to the baseline AlexNet in the SiamFC and SiamRPN frameworks. As presented in Tab. 4, on OTB-13, OTB-15 and VOT-17 datasets, our proposed networks outperform the baseline AlexNet. In particular, SiamFC equipped with a CIResNet.-22 backbone obtains relative improvements of 9.8% (AUC) and 24.4% (EAO) over the original AlexNet on OTB-2015 and VOT-17, respectively. Meanwhile, SiamRPN armed with CIResNet-22 achieves 4.4% and 23.3% relative gains. This verifies that our designed architectures resolve the performance degradation problem shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Also, it shows the effectiveness of our proposed CIR units for Siamese networks.</p><p>It is worth noting that when the depth of CIResNets increases from 16 to 22 layers, the performance improves accordingly. But when increasing to 43 layers, CIResNet does not obtain further gains. There are two main reasons. 1) The network stride is changed to 4, such that the overlap between the receptive fields of two adjacent features is large. Consequently, it is not as precise as networks with stride of 8 in object localization. 2) The number of output feature channels is halved, compared to the other networks in Tab. 3 (i.e. 256 vs. 512 channels). The overall parameter size is also smaller. These two reasons together limit the performance of CIResNet-43. Furthermore, wider networks also bring gains for Siamese trackers. Though CIResNeXt-22 contain more transformation branches, its model size is smaller (see <ref type="table">Tab.</ref> 3). Therefore, its performance is inferior to CIResIncep.-22 and CIResNet-22.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison to State-of-the-Art Trackers</head><p>We further compare our enhanced Siamese trackers to state-of-the-art tracking algorithms. We select some of the currently best performing trackers in general, as well as other recent Siamese trackers for comparison. Our enhanced trackers employ the best performing CIResNet-22 as the backbone, and are denoted as SiamFC+ and SiamRPN+. The comparison is conducted on five datasets: OTB-2013, OTB-2015, VOT15, VOT16 and VOT17.</p><p>OTB Benchmarks. The evaluation on OTB-2013 and OTB-2015 follows the standard protocols proposed in <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref>. Two metrics, i.e. precision and area under curve (AUC) of success plots, are used to rank the trackers. The results are reported within Tab. 5. It shows that our SiamFC+ and SiamRPN+ surpass other Siamese trackers, such as the recent proposed StructSiam <ref type="bibr" target="#b39">[40]</ref> and TriSiam <ref type="bibr" target="#b6">[7]</ref>. This demonstrates the effectiveness of our designed architecture. Moreover, compared with other state-of-the-art algorithms, such as ECO-HC <ref type="bibr" target="#b3">[4]</ref> and CFNet <ref type="bibr" target="#b34">[35]</ref>, our trackers are still superior in terms of precision and speed.  VOT Benchmarks. The evaluation on VOT benchmarks is performed by the official toolkit <ref type="bibr" target="#b16">[17]</ref>, in which accuracy (A), robustness (R) and expected average overlap (EAO) serve as the metrics.</p><p>VOT-15. We compare our SiamFC+ and SiamRPN+ with the state-of-the-art trackers on the vot-2015 challenge. The results are reported in <ref type="figure" target="#fig_3">Fig. 4 (top)</ref>. Our SiamRPN+ achieves the best result, slightly better than MDNet <ref type="bibr" target="#b25">[26]</ref>. Also, SiamRPN+ runs much faster than MDNet, which operates at ?1 fps. Compared to the baselines, i.e. SiamFC and SiamRPN, our deeper network enhanced trackers obtain 8.8% and 8.9% relative improvements, respectively.</p><p>VOT-16. The video sequences in VOT-16 are the same as those in VOT-15, but the ground-truth bounding boxes are precisely re-annotated. We compare our trackers to the top-10 trackers in the challenge. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>(middle), SiamRPN+ ranks first in terms of EAO. It surpasses CCOT <ref type="bibr" target="#b5">[6]</ref>, the champion of the vot-2016 challenge, by 3.9 points, as well as the most recent VITAL <ref type="bibr" target="#b30">[31]</ref> by 4.7 points. Moreover, SiamFC+ also improves over the baseline by a large margin, i.e. 6.0 points on EAO.</p><p>VOT-17. <ref type="figure" target="#fig_3">Fig. 4(bottom)</ref> shows the comparison with the trackers in the vot-2017 challenge. Our SiamRPN+ achieves an EAO of 3.01, slightly inferior to the best performing LSART tracker <ref type="bibr" target="#b31">[32]</ref>. However, SiamRPN+ runs at 150 fps, which is 150? faster than LSART. Compared to real-time trackers, SiamRPN+ ranks first in terms of accuracy and robustness. Surprisingly, even the plain SiamFC+ surpasses the real-time tracker champion of vot-2017 CSRDCF++[23] by 2.2 points. This further verifies the effectiveness of our deeper network architecture designed for Siamese trackers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation Study</head><p>In Tab. 6?8, we evaluate the effects of different factors in our designed networks on the VOT-16 dataset.</p><p>With vs. without CIR unit. The cropping-inside residual unit is a key component in our network architectures. To evaluate its impact, we replace it with the original residual unit <ref type="bibr" target="#b13">[14]</ref> in the networks. As shown in Tab. 6, this replacement causes remarkable performance drops, e.g. a degradation of 8.8 points from 0.301 to 0.213 on CIResNet.-22. It clearly verifies the importance of padding removal in the CIR unit, which essentially eliminates position bias in learning. The predicted heatmaps of CIResNet-22 in <ref type="figure" target="#fig_1">Fig. 2</ref> also prove this point.</p><p>With vs. without CIR-D unit. We compare three different downsampling settings in networks: 1) directly using the original downsampling residual unit, i.e. <ref type="figure" target="#fig_2">Fig. 3(b)</ref>, 2) inserting a cropping operation after addition in the downsampling residual unit, and 3) the proposed CIR-D unit, i.e. <ref type="figure" target="#fig_2">Fig. 3(b )</ref>. Tab. 7 presents the results. It shows that the first two settings are comparable, but inferior to the third one. This indicates our CIR-D unit is effective. In particular, the cropping introduced in the second setting does not bring improvements, since it removes parts of internal features (i.e. not the padding-affected features), resulting in information loss from the original input.</p><p>Impact of receptive field, feature size and stride. We tune the sizes of these factors and show their impacts on final performance. Specifically, we vary the convolutional kernel size in the last cropping-inside residual block to change the size of receptive field and output feature. Taking CIResNet-22 as an example, we vary the kernel size from 1 to 6, which causes the feature size to change from 7 to 2. To change the network stride, we replace one CIR unit with a CIR-D unit in the networks. Tab. 8 shows the results. We can observe that when RF becomes large, the performance drops significantly (i.e. from { to x in Tab. 8). The underlying reason is that a large RF covers much image context, resulting in the extracted feature being insensitive to the spatial location of the target object. For output features, it is observed that a small size (OFS ? 3) does not benefit accuracy. Also, a large network stride, i.e. 16, is not as precise as a medium sized one, i.e. 8. These results echo our analysis and guidelines presented at the beginning of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussions</head><p>Network Architectures. The problem addressed in this paper can be seen as a subtask of network architecture design, which mainly develops in two ways: making networks deeper <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b13">14]</ref> or wider <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b38">39]</ref>. To make networks deeper, ResNets <ref type="bibr" target="#b13">[14]</ref> introduce an identity mapping that makes training ultra deep networks possible. To make networks wider, GoogLeNet <ref type="bibr" target="#b32">[33]</ref> and its descendants employ an Inception module to introduce multiple feature transformations and thus enhance model representation capacity.</p><p>Our work takes advantage of these deep and wide network architectures, and modifies them to adapt effectively to Siamese networks. Two key principles for Siamese backbone design are presented. One is to remove padding operations inside network architectures, the other is to control the receptive field size and network stride. Both of them are shown to have significant impact on tracking performance. Moreover, this is the first work that systematically studies how to design robust backbone networks in visual tracking.</p><p>Siamese Trackers. Siamese trackers follow a tracking by similarity matching strategy. The pioneering works are SINT <ref type="bibr" target="#b33">[34]</ref> and SiamFC <ref type="bibr" target="#b1">[2]</ref>, which each employ a Siamese network to offline train a similarity metric between the object target and candidate image patches. A large amount of follow-up work <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b39">40]</ref> have been proposed, and they fall into two camps. One improves matching precision with high-level semantic information or a localization model <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b19">20]</ref>. The other enhances the offline Siamese model with online updates <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>There is a recent work also studying how to leverage deep networks for visual tracking <ref type="bibr" target="#b2">[3]</ref>. But it approaches the problem from the direction of data augmentation and feature fusion. Differently, our work studies how to design network architectures and successfully equips Siamese trackers with deeper and wider backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we design deep and wide network architectures for Siamese trackers. This is motivated by the observation that direct replacement of backbones with existing powerful networks does not bring improvements. We carefully study the key causes and determine that receptive field size, network padding and stride are crucial factors. Experiments on five benchmarks demonstrate the effectiveness of the proposed architectures, leading to competitive performance on five datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>AUC of success plot vs. network depth and width (indicated by W).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Visualization of position bias learnt in the model w/ and w/o padding (ResNet| in Tab. 2 vs. ours). (A) presents the target at the image center, while (B-D) show it moving to boundaries due to imprecise tracked position in the previous frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The proposed cropping-inside residual units. (a) and (b) are the original residual unit and downsampling unit, while (a ) and (b ) are our proposed ones. (c) and (d) are the proposed wide residual units. The grey arrows indicate the shortcut paths for easy information propagation, while the blue boxes highlight the differences from the original units. The letters 'p' and 's' indicate the padding size and stride, respectively. The settings of 'p' and 's' in (d) are the same as in (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Expected average overlap (EAO) plot for VOT-15, 16 and 17.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Analysis of network internal factors on AlexNet, VGG-10, Incep.-22 and ResNet-33. The numbers xrepresent different versions, in which the convolution kernel size, downsampling layer and padding are modified to show the trends. Details on the modifications are given in the supplementary material due to limited space. To better show the trends, we denote ?0 as the original RF size of the network. + and ? represent increasing and decreasing size over the originals.Max(127) represents the maximum effective RF, which is identical to the size of the exemplar image, i.e. 127 ? 127 pixels.<ref type="bibr" target="#b1">2</ref> For the Inception network, its RF size lies in a range. Here we only list the theoretically maximum size, to align with ResNet for comparison.</figDesc><table><row><cell cols="2"># NUM</cell><cell>x</cell><cell>y</cell><cell cols="2">z {</cell><cell>|</cell><cell cols="3">}~</cell><cell></cell><cell></cell><cell cols="2"># NUM x</cell><cell cols="2">y z</cell><cell>{</cell><cell cols="3">| }~</cell></row><row><cell cols="12">RF 1 Max(127) +24 +16 +8 ?0 (87) ?0 -8 -16 +16 +16</cell><cell>RF</cell><cell cols="7">+32 +16 +8 ?0 (91) ?0 -8 -16 +16 +16</cell></row><row><cell>STR</cell><cell></cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell cols="3">8 16 4</cell><cell>STR</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>8 16 4</cell></row><row><cell>OFS</cell><cell></cell><cell>1</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell cols="2">16 7</cell><cell>8</cell><cell>2</cell><cell>7</cell><cell>OFS</cell><cell>1</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell cols="2">16 6</cell><cell>7</cell><cell>2</cell><cell>6</cell></row><row><cell>PAD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PAD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Alex</cell><cell></cell><cell>0.56</cell><cell cols="17">0.57 0.60 0.60 0.61 0.55 0.59 0.58 0.55 0.59 ResNet 0.56 0.59 0.60 0.62 0.56 0.60 0.60 0.54 0.58</cell></row><row><cell>VGG</cell><cell></cell><cell>0.58</cell><cell cols="17">0.59 0.61 0.61 0.62 0.56 0.59 0.58 0.54 0.58 Incep. 2 0.58 0.60 0.61 0.63 0.58 0.62 0.61 0.56 0.59</cell></row><row><cell cols="11">Alex VGG-10 Incep.-16 Res.-17 Incep.-22 Res.-33</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RF</cell><cell>87</cell><cell>103</cell><cell></cell><cell>23~183</cell><cell></cell><cell>227</cell><cell cols="2">39~519</cell><cell>739</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>STR</cell><cell>8</cell><cell>8</cell><cell></cell><cell>8</cell><cell></cell><cell>8</cell><cell>16</cell><cell></cell><cell>16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>OFS</cell><cell>6</cell><cell>4</cell><cell></cell><cell>16</cell><cell></cell><cell>16</cell><cell>8</cell><cell></cell><cell>8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PAD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>W</cell><cell>1</cell><cell>1</cell><cell></cell><cell>4</cell><cell></cell><cell>1</cell><cell>4</cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">AUC 0.61</cell><cell>0.61</cell><cell></cell><cell>0.59</cell><cell></cell><cell>0.57</cell><cell>0.56</cell><cell></cell><cell>0.55</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Internal or 8 to 16, the performance drops significantly ( vs. z vs. on AlexNet and VGG, vs. y vs.</figDesc><table><row><cell>factors of different networks: receptive field</cell></row><row><cell>(RF) of neurons in the last layer of network, stride (STR), output</cell></row><row><cell>feature size (OFS), padding (PAD) and width (W). Since Inception</cell></row><row><cell>contains multiple branches in one block, its RF lies within a range.</cell></row><row><cell>Inception and ResNet, and expose the effects of the inter-</cell></row><row><cell>nal factors. As shown in Tab. 1, when network stride (STR)</cell></row><row><cell>increases from 4 on Incep. and ResNet). This illustrates that Siamese track-</cell></row><row><cell>ers prefer mid-level features (stride 4 or 8), which are more</cell></row><row><cell>precise in object localization than high-level features (stride</cell></row></table><note>? 16). For the maximum size of receptive field (RF), the optima lies in a small range. Specifically, for AlexNet, it ranges from 87-8 (Alex~) to 87+16 (Alexz) pixels; while for Incep.-22, it ranges from 91-16 (Incep.~) to 91+8 (In- cep.z) pixels. VGG-10 and ResNet-17 also exhibit similar phenomena.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Architectures of designed backbone networks for Siamese trackers. The CIR-D units are used in the first block of the 'conv3' stage, except for CIResNet-43, in which CIR-D is located in the fourth block.</figDesc><table><row><cell>Stage</cell><cell></cell><cell></cell><cell cols="3">CIResNet-16</cell><cell></cell><cell></cell><cell cols="3">CIResNet-19</cell><cell></cell><cell></cell><cell cols="3">CIResNet-22</cell><cell cols="4">CIResInception-22</cell><cell>CIResNeXt-22</cell><cell>CIResNet-43</cell></row><row><cell>conv1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">7?7, 64, stride 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">2?2 max pool, stride 2</cell></row><row><cell></cell><cell cols="2">?</cell><cell>1 ? 1, 64</cell><cell cols="2">?</cell><cell cols="2">?</cell><cell>1 ? 1, 64</cell><cell cols="2">?</cell><cell cols="2">?</cell><cell>1 ? 1, 64</cell><cell cols="2">?</cell><cell cols="2">?</cell><cell>1 ? 1, 64</cell><cell>?</cell><cell>?</cell><cell>1 ? 1, 64</cell><cell>?</cell><cell>?</cell><cell>1 ? 1, 64</cell><cell>?</cell></row><row><cell>conv2</cell><cell cols="2">?</cell><cell>3 ? 3, 64</cell><cell cols="2">? ? 1</cell><cell cols="2">?</cell><cell>3 ? 3, 64</cell><cell cols="2">? ? 2</cell><cell cols="2">?</cell><cell>3 ? 3, 64</cell><cell cols="2">? ? 3</cell><cell cols="2">?</cell><cell>3 ? 3, 64</cell><cell>? ? 3</cell><cell>?</cell><cell>3 ? 3, 64, C = 32</cell><cell>? ? 3</cell><cell>?</cell><cell>3 ? 3, 64</cell><cell>? ? 14</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">1 ? 1, 256</cell><cell></cell><cell></cell><cell></cell><cell cols="2">1 ? 1, 256</cell><cell></cell><cell></cell><cell></cell><cell cols="2">1 ? 1, 256</cell><cell></cell><cell></cell><cell cols="3">1 ? 1, 256</cell><cell>1 ? 1, 256</cell><cell>1 ? 1, 256</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">[1 ? 1, 64] ? 3</cell></row><row><cell></cell><cell>?</cell><cell cols="2">1 ? 1, 128</cell><cell></cell><cell>?</cell><cell>?</cell><cell cols="2">1 ? 1, 128</cell><cell></cell><cell>?</cell><cell>?</cell><cell cols="2">1 ? 1, 128</cell><cell></cell><cell>?</cell><cell>?</cell><cell cols="2">1 ? 1, 128</cell><cell>?</cell><cell>?</cell><cell>1 ? 1, 128</cell><cell>?</cell></row><row><cell>conv3</cell><cell>?</cell><cell cols="3">1 ? 1, 512 3 ? 3, 128</cell><cell>? ? 4</cell><cell>?</cell><cell cols="3">1 ? 1, 512 3 ? 3, 128</cell><cell>? ? 4</cell><cell>?</cell><cell cols="3">1 ? 1, 512 3 ? 3, 128</cell><cell>? ? 4</cell><cell>?</cell><cell cols="2">1 ? 1, 512 3 ? 3, 128</cell><cell>? ? 4</cell><cell>?</cell><cell>1 ? 1, 512 3 ? 3, 128, C = 32</cell><cell>? ? 4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">[1 ? 1, 128] ? 4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">cross correlation Eq. 1</cell></row><row><cell># RF</cell><cell></cell><cell></cell><cell>77</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>93</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">13?93</cell><cell>93</cell><cell>105</cell></row><row><cell># OFS</cell><cell></cell><cell></cell><cell>7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5</cell><cell>5</cell><cell>6</cell></row><row><cell># Params</cell><cell></cell><cell></cell><cell cols="3">1.304 M</cell><cell></cell><cell></cell><cell cols="3">1.374 M</cell><cell></cell><cell></cell><cell cols="3">1.445 M</cell><cell></cell><cell></cell><cell cols="2">1.695 M</cell><cell>1.417 M</cell><cell>1.010 M</cell></row><row><cell># FLOPs</cell><cell></cell><cell></cell><cell cols="3">2.43 G</cell><cell></cell><cell></cell><cell cols="3">2.55 G</cell><cell></cell><cell></cell><cell cols="3">2.65 G</cell><cell></cell><cell></cell><cell cols="2">2.71 G</cell><cell>2.52 G</cell><cell>6.07 G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Performance</figDesc><table><row><cell>of our network architectures in SiamFC</cell></row><row><cell>and SiamRPN. To compare with the original results reported</cell></row><row><cell>in [2, 17, 20], SiamFCs are evaluated on OTB-2013 and VOT-17,</cell></row><row><cell>while SiamRPNs are evaluated on OTB-2015 and VOT-17. The</cell></row><row><cell>speed (FPS) is measured on a GeForce GTX 1080 GPU.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Performance comparisons on five tracking benchmarks. Red, Green and Blue fonts indicate the top-3 trackers, respectively.</figDesc><table><row><cell>Tracker</cell><cell>Year</cell><cell cols="2">OTB-2013</cell><cell cols="2">OTB-2015</cell><cell></cell><cell>VOT15</cell><cell></cell><cell></cell><cell>VOT16</cell><cell></cell><cell></cell><cell>VOT17</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">AUC Prec. AUC Prec.</cell><cell>A</cell><cell>R</cell><cell>EAO</cell><cell>A</cell><cell>R</cell><cell>EAO</cell><cell>A</cell><cell>R</cell><cell>EAO</cell></row><row><cell>SRDCF [5]</cell><cell>2015</cell><cell>0.63</cell><cell>0.84</cell><cell>0.60</cell><cell>0.80</cell><cell cols="2">0.56 1.24</cell><cell>0.29</cell><cell cols="2">0.54 0.42</cell><cell>0.25</cell><cell cols="2">0.49 0.97</cell><cell>0.12</cell></row><row><cell>SINT [34]</cell><cell>2016</cell><cell>0.64</cell><cell>0.85</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Staple [1]</cell><cell>2016</cell><cell>0.60</cell><cell>0.80</cell><cell>0.58</cell><cell>0.78</cell><cell cols="2">0.57 1.39</cell><cell>0.30</cell><cell cols="2">0.54 0.38</cell><cell>0.30</cell><cell cols="2">0.52 0.69</cell><cell>0.17</cell></row><row><cell>SiamFC [2]</cell><cell>2016</cell><cell>0.61</cell><cell>0.81</cell><cell>0.58</cell><cell>0.77</cell><cell cols="2">0.53 0.88</cell><cell>0.29</cell><cell cols="2">0.53 0.46</cell><cell>0.24</cell><cell cols="2">0.50 0.59</cell><cell>0.19</cell></row><row><cell>ECO-HC [4]</cell><cell>2017</cell><cell>0.65</cell><cell>0.87</cell><cell>0.64</cell><cell>0.86</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.54</cell><cell>0.3</cell><cell>0.32</cell><cell cols="2">0.49 0.44</cell><cell>0.24</cell></row><row><cell>PTAV [8]</cell><cell>2017</cell><cell>0.66</cell><cell>0.89</cell><cell>0.64</cell><cell>0.85</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DSiam [12]</cell><cell>2017</cell><cell>0.64</cell><cell>0.81</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CFNet [35]</cell><cell>2017</cell><cell>0.61</cell><cell>0.80</cell><cell>0.59</cell><cell>0.78</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">StructSiam [40] 2018</cell><cell>0.64</cell><cell>0.88</cell><cell>0.62</cell><cell>0.85</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.26</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TriSiam [7]</cell><cell>2018</cell><cell>0.62</cell><cell>0.82</cell><cell>0.59</cell><cell>0.78</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.20</cell></row><row><cell>SiamRPN [20]</cell><cell>2018</cell><cell>-</cell><cell>-</cell><cell>0.64</cell><cell>0.85</cell><cell cols="2">0.58 1.13</cell><cell>0.35</cell><cell cols="2">0.56 0.26</cell><cell>0.34</cell><cell cols="2">0.49 0.46</cell><cell>0.24</cell></row><row><cell>SiamFC+</cell><cell>Ours</cell><cell>0.67</cell><cell>0.88</cell><cell>0.64</cell><cell>0.85</cell><cell>0.57</cell><cell>-</cell><cell>0.31</cell><cell cols="2">0.54 0.38</cell><cell>0.30</cell><cell cols="2">0.50 0.49</cell><cell>0.23</cell></row><row><cell>SiamRPN+</cell><cell>Ours</cell><cell>0.67</cell><cell>0.92</cell><cell>0.67</cell><cell>0.90</cell><cell>0.59</cell><cell>-</cell><cell>0.38</cell><cell cols="2">0.58 0.24</cell><cell>0.37</cell><cell cols="2">0.52 0.41</cell><cell>0.30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Ablation for residual unit vs. CIR unit on SiamFC.</figDesc><table><row><cell></cell><cell cols="3">CIResNet-20 CIResNet-22 CIResIncep.-22</cell></row><row><cell>Res. Unit</cell><cell>0.204</cell><cell>0.213</cell><cell>0.227</cell></row><row><cell>CIR Unit</cell><cell>0.271</cell><cell>0.301</cell><cell>0.282</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Ablation over different downsampling settings used in SiamFC. See main text for explanations.</figDesc><table><row><cell></cell><cell cols="3">CIResNet-20 CIResNet-22 CIResIncep.-22</cell></row><row><cell>Setting 1</cell><cell>0.264</cell><cell>0.292</cell><cell>0.266</cell></row><row><cell>Setting 2</cell><cell>0.259</cell><cell>0.287</cell><cell>0.275</cell></row><row><cell>CIR-D</cell><cell>0.271</cell><cell>0.301</cell><cell>0.282</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Analysis of network internal factors. .23 0.24 0.26 0.25 0.24 0.23 0.20 CIResNet-19 0.23 0.26 0.26 0.28 0.27 0.26 0.24 0.21 CIResNet-22 0.25 0.27 0.28 0.30 0.29 0.27 0.26 0.23 CIResIncep.-22 0.24 0.26 0.27 0.28 0.27 0.26 0.25 0.22</figDesc><table><row><cell># NUM</cell><cell>x</cell><cell>y</cell><cell>z</cell><cell>{</cell><cell>|</cell><cell cols="3">}~</cell></row><row><cell>RF</cell><cell cols="8">+24 +16 +8 ?0 (93) -8 -16 +16 +16</cell></row><row><cell>OFS</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>6</cell><cell>2</cell></row><row><cell>STR</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>4</cell><cell>16</cell></row><row><cell>CIResNet-16</cell><cell>0.22 0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that the network structures are slightly different from their original versions<ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33]</ref>, where the network stride and padding are modified according to SiamFC<ref type="bibr" target="#b1">[2]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This work was done in Microsoft Research Asia. Thanks to Steve Lin and Zhirong Wu for helpful discussions. Zhipeng Zhang is partly supported by NSFC U1803119, 2016QY01W0106 and JQ18018.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Staple: Complementary learners for real-time tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Miksik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1401" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="850" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06833</idno>
		<title level="m">Unveiling the power of deep tracking</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient convolution operators for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3" to="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning spatially regularized correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4310" to="4318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Beyond correlation filters: Learning continuous convolution operators for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="472" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Triplet loss in siamese network for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="459" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Parallel tracking and verifying: A framework for real-time and high accuracy visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision</title>
		<meeting>IEEE Int. Conf. Computer Vision<address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The thermal infrared visual object tracking vot-tir2015 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahlberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cehovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="76" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning background-aware correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Galoogahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1144" to="1152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Good features to correlate for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gundogdu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alatan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2526" to="2540" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning dynamic siamese network for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A twofold siamese network for real-time object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4834" to="4843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to track at 100 fps with deep regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="749" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The visual object tracking vot2016 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Et</surname></persName>
		</author>
		<idno>8Oct. 2016. 6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings, European Conference on Computer Vision (ECCV) workshops</title>
		<meeting>European Conference on Computer Vision (ECCV) workshops</meeting>
		<imprint>
			<biblScope unit="page" from="777" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The visual object tracking vot2017 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Et</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision Workshop (IC-CVW)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">00</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="8971" to="8980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hengel. A survey of appearance models in visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">58</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A scale adaptive kernel correlation filter tracker with feature integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="254" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Discriminative correlation filter with channel and spatial reliability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="8" to="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deformable parts correlation filters for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Luke?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">?</forename><surname>Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1849" to="1861" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Modeling and propagating cnns in a tree structure for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07242</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4293" to="4302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Youtube-boundingboxes: A large high-precision human-annotated data set for object detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mazzocchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7464" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visual tracking: An experimental survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04273</idno>
		<title level="m">Vital: Visual tracking via adversarial learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning spatial-aware regressions for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8962" to="8970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Siamese instance search for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1420" to="1429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">End-to-end representation learning for correlation filter based tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="5000" to="5008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visual tracking with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3119" to="3127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Online object tracking: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2411" to="2418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Object tracking benchmark. IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1834" to="1848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Structured siamese network for real-time visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="351" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Beyond local search: Tracking objects everywhere with instance-specific proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="943" to="951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Distractor-aware siamese networks for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="103" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning support correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
