<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Empirical Evaluation of Deep Learning on Highway Driving</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University ? Twitter ? Texas Instruments ? Baidu Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University ? Twitter ? Texas Instruments ? Baidu Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameep</forename><surname>Tandon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University ? Twitter ? Texas Instruments ? Baidu Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Kiske</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University ? Twitter ? Texas Instruments ? Baidu Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University ? Twitter ? Texas Instruments ? Baidu Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Pazhayampallil</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University ? Twitter ? Texas Instruments ? Baidu Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University ? Twitter ? Texas Instruments ? Baidu Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University ? Twitter ? Texas Instruments ? Baidu Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toki</forename><surname>Migimatsu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University ? Twitter ? Texas Instruments ? Baidu Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Royce</forename><surname>Cheng-Yue</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University ? Twitter ? Texas Instruments ? Baidu Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Mujica</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University ? Twitter ? Texas Instruments ? Baidu Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University ? Twitter ? Texas Instruments ? Baidu Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University ? Twitter ? Texas Instruments ? Baidu Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">An Empirical Evaluation of Deep Learning on Highway Driving</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Numerous groups have applied a variety of deep learning techniques to computer vision problems in highway perception scenarios. In this paper, we presented a number of empirical evaluations of recent deep learning advances. Computer vision, combined with deep learning, has the potential to bring about a relatively inexpensive, robust solution to autonomous driving. To prepare deep learning for industry uptake and practical applications, neural networks will require large data sets that represent all possible driving environments and scenarios. We collect a large data set of highway data and apply deep learning and computer vision algorithms to problems such as car and lane detection. We show how existing convolutional neural networks (CNNs) can be used to perform lane and vehicle detection while running at frame rates required for a real-time system. Our results lend credence to the hypothesis that deep learning holds promise for autonomous driving.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Since the DARPA Grand Challenges for autonomous vehicles, there has been an explosion in applications and research for self-driving cars. Among the different environments for self-driving cars, highway and urban roads are on opposite ends of the spectrum. In general, highways tend to be more predictable and orderly, with road surfaces typically wellmaintained and lanes well-marked. In contrast, residential or urban driving environments feature a much higher degree of unpredictability with many generic objects, inconsistent lanemarkings, and elaborate traffic flow patterns. The relative regularity and structure of highways has facilitated some of the first practical applications of autonomous driving technology. Many automakers have begun pursuing highway auto-pilot solutions designed to mitigate driver stress and fatigue and to provide additional safety features; for example, certain advanced-driver assistance systems (ADAS) can both keep cars within their lane and perform front-view car detection. Currently, the human drivers retain liability and, as such, must keep their hands on the steering wheel and prepare to control the vehicle in the event of any unexpected obstacle or catastrophic incident. Financial considerations contribute to a substantial performance gap between commercially available auto-pilot systems and fully self-driving cars developed by Google and others. Namely, today's self-driving cars are equipped with expensive but critical sensors, such as LIDAR, radar and high-precision GPS coupled with highly detailed maps.</p><p>In today's production-grade autonomous vehicles, critical sensors include radar, sonar, and cameras. Long-range vehicle detection typically requires radar, while nearby car detection can be solved with sonar. Computer vision can play an important a role in lane detection as well as redundant object detection at moderate distances. Radar works reasonably well for detecting vehicles, but has difficulty distinguishing between different metal objects and thus can register false positives on objects such as tin cans. Also, radar provides little orientation information and has a higher variance on the lateral position of objects, making the localization difficult on sharp bends. The utility of sonar is both compromised at high speeds and, even at slow speeds, is limited to a working distance of about 2 meters. Compared to sonar and radar, cameras generate a richer set of features at a fraction of the cost. By advancing computer vision, cameras could serve as a reliable redundant sensor for autonomous driving. Despite its potential, computer vision has yet to assume a significant role in today's selfdriving cars. Classic computer vision techniques simply have not provided the robustness required for production grade automotives; these techniques require intensive hand engineering, road modeling, and special case handling. Considering the seemingly infinite number of specific driving situations, environments, and unexpected obstacles, the task of scaling classic computer vision to robust, human-level performance would prove monumental and is likely to be unrealistic.</p><p>Deep learning, or neural networks, represents an alternative arXiv:1504.01716v3 [cs.RO] 17 Apr 2015 approach to computer vision. It shows considerable promise as a solution to the shortcomings of classic computer vision. Recent progress in the field has advanced the feasibility of deep learning applications to solve complex, real-world problems; industry has responded by increasing uptake of such technology. Deep learning is data centric, requiring heavy computation but minimal hand-engineering. In the last few years, an increase in available storage and compute capabilities have enabled deep learning to achieve success in supervised perception tasks, such as image detection. A neural network, after training for days or even weeks on a large data set, can be capable of inference in real-time with a model size that is no larger than a few hundred MB <ref type="bibr" target="#b8">[9]</ref>. State-of-the-art neural networks for computer vision require very large training sets coupled with extensive networks capable of modeling such immense volumes of data. For example, the ILSRVC data-set, where neural networks achieve top results, contains 1.2 million images in over 1000 categories. By using expensive existing sensors which are currently used for self-driving applications, such as LIDAR and mmaccurate GPS, and calibrating them with cameras, we can create a video data set containing labeled lane-markings and annotated vehicles with location and relative speed. By building a labeled data set in all types of driving situations (rain, snow, night, day, etc.), we can evaluate neural networks on this data to determine if it is robust in every driving environment and situation for which we have training data.</p><p>In this paper, we detail empirical evaluation on the data set we collect. In addition, we explain the neural network that we applied for detecting lanes and cars, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Recently, computer vision has been expected to player a larger role within autonomous driving. However, due to its history of relatively low precision, it is typically used in conjunction with either other sensors or other road models <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Cho et al. <ref type="bibr" target="#b2">[3]</ref> uses multiple sensors, such as LIDAR, radar, and computer vision for object detection. They then fuse these sensors together in a Kalman filter using motion models on the objects. Held et al. <ref type="bibr" target="#b3">[4]</ref>, uses only a deformable parts based model on images to get the detections, then uses road models to filter out false positives. Carafii et al. <ref type="bibr" target="#b5">[6]</ref> uses a WaldBoost detector along with a tracker to generate pixel space detections in real time. Jazayeri et al. <ref type="bibr" target="#b6">[7]</ref> relies on temporal information of features for detection, and then filters out false positives with a front-view motion model.</p><p>In contrast to these object detectors, we do not use any road or motion-based models; instead we rely only on the robustness of a neural network to make reasonable predictions. In addition, we currently do not rely on any temporal features, and the detector operates independently on single frames from a monocular camera. To make up for the lack of other sensors, which estimate object depth, we train the neural network to predict depth based on labels extracted from radar returns. Although the model only predicts a single depth value for each object, Eigen et al. have shown how a neural network can predict entire depth maps from single images <ref type="bibr" target="#b11">[12]</ref>. The network we train likely learns some model of the road for object detection and depth predictions, but it is never explicitly engineered and instead learns from the annotations alone.</p><p>Before the wide spread adoption of Convolutional Neural Networks (CNNs) within computer vision, deformable parts based models were the most successful methods for detection <ref type="bibr" target="#b12">[13]</ref>. After the popular CNN model AlexNet <ref type="bibr" target="#b8">[9]</ref> was proposed, state-of-the-art detection shifted towards CNNs for feature extraction <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Girshick et al. developed R-CNN, a two part system which used Selective Search <ref type="bibr" target="#b15">[16]</ref> to propose regions and AlexNet to classify them. R-CNN achieved state-of-the-art on Pascal by a large margin; however, due to its nearly 1000 classification queries and inefficient re-use of convolutions, it remains impractical for real-time implementations. Szegedy et al. presented a more scalable alternative to R-CNN, that relies on the CNN to propose higher quality regions compared to Selective Search. This reduces the number of region proposals down to as low as 79 while keeping the mAP competitive with Selective Search. An even faster approach to image detection called Overfeat was presented by Sermanet et al. <ref type="bibr" target="#b0">[1]</ref>. By using a regular pattern of "region proposals", Overfeat can efficiently reuse convolution computations from each layer, requiring only a single forward pass for inference.</p><p>For our empirical evaluation, we use a straight-forward application of Overfeat, due to its efficiencies, and combine this with labels similar to the ones proposed by Szegedy et al.. We describe the model and similarities in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. REAL TIME VEHICLE DETECTION</head><p>Convolutional Neural Networks (CNNs) have had the largest success in image recognition in the past 3 years <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. From these image recognition systems, a number of detection networks were adapted, leading to further advances in image detection. While the improvements have been staggering, not much consideration had been given to the real-time detection performance required for some applications. In this paper, we present a detection system capable of operating at greater than 10Hz using nothing but a laptop GPU. Due to the requirements of highway driving, we need to ensure that the system used can detect cars more than 100m away and can operate at speeds greater than 10Hz; this distance requires higher image resolutions than is typically used, and in our case is 640 ? 480. We use the Overfeat CNN detector, which is very scalable, and simulates a sliding window detector in a single forward pass in the network by efficiently reusing convolutional results on each layer <ref type="bibr" target="#b0">[1]</ref>. Other detection systems, such as R-CNN, rely on selecting as many as 1000 candidate windows, where each is evaluated independently and does not reuse convolutional results.</p><p>In our implementation, we make a few minor modifications to Overfeat's labels in order to handle occlusions of cars, predictions of lanes, and accelerate performance during inference. We will first provide a brief overview of the original implementation and will then address the modifications. Overfeat converts an image recognition CNN into a "sliding window" detector by providing a larger resolution image and transforming the fully connected layers into convolutional layers.</p><p>Then, after converting the fully connected layer, which would have produced a single final feature vector, to a convolutional layer, a grid of final feature vectors is produced. Each of the resulting feature vectors represents a slightly different context view location within the original pixel space. To determine the stride of this window in pixel space, it is possible to simply multiply the strides on each convolutional or pool layer together. The network we used has a stride size of 32 pixels. Each final feature vector in this grid can predict the presence of an object; once an object is detected, those same features are then used to predict a single bounding box through regression. The classifier will predict "no-object" if it can not discern any part of an object within its entire input view. This causes large ambiguities for the classifier, which can only predict a single object, as two different objects could can easily appear in the context view of the final feature vector, which is typically larger than 50% of the input image resolution.</p><p>The network we used has a context view of 355?355 pixels in size. To ensure that all objects in the image are classified at least once, many different context views are taken of the image by using skip gram kernels to reduce the stride of the context views and by using up to four different scales of the input image. The classifier is then trained to activate when an object appears anywhere within its entire context view. In the original Overfeat paper, this results in 1575 different context views (or final feature vectors), where each one is likely to become active (create a bounding box).</p><p>This creates two problems for our empirical evaluation. Due to the L2 loss between the predicted bounding box and actual bounding proposed by Sermanet et al., the ambiguity of having two valid bounding box locations to predict when two objects appear, is incorrectly handled by the network by predicting a box in the center of the two objects to minimize its expected loss. These boxes tend to cause a problem for the bounding box merging algorithm, which incorrectly decides that there must be a third object between the two ground truth objects. This could cause problems for an ADAS system which falsely believes there is a car where there is not, and emergency breaking is falsely applied. In addition, the merging algorithm, used only during inference, operates in O(n 2 ) where n is the number of bounding boxes proposed. Because the bounding box merging is not as easily parallelizable as the CNN, this merging may become the bottleneck of a real-time system in the case of an inefficient implementation or too many predicted bounding boxes.</p><p>In our evaluations, we use a mask detector as described in Szegedy et al. <ref type="bibr" target="#b9">[10]</ref> to improve some of the issues with Overfeat as described above. Szegedy et al. proposes a CNN that takes an image as input and outputs an object mask through regression, highlighting the object location. The idea of a mask detector is shown in <ref type="figure">Fig 2.</ref> To distinguish multiple nearby objects, different part-detectors output object masks, from which bounding boxes are then extracted. The detector they propose must take many crops of the image, and run multiple CNNs for each part on every crop. Their resulting implementation takes roughly 5-6 seconds per frame per class using a 12-core machine, which would be too slow for our application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2: mask detector</head><p>We combine these ideas by using the efficient "sliding window" detector of Overfeat to produce an object mask and perform bounding box regression. This is shown in <ref type="figure" target="#fig_1">Fig 3.</ref> In this implementation, we use a single image resolution of 640 ? 480 with no skip gram kernels. To help the ambiguity problem, and reduce the number of bounding boxes predicted, we alter the detector on the top layer to only activate within a 4 ? 4 pixel region at the center of its context view, as shown in the first box in <ref type="figure" target="#fig_1">Fig 3.</ref> Because it's highly unlikely that any two different object's bounding boxes appear in a 4 ? 4 pixel region, compared to the entire context view with Overfeat, the bounding box regressor will no longer have to arbitrarily choose between two valid objects in its context view. In addition, because the requirement for the detector to fire is stricter, this produces many fewer bounding boxes which greatly reduces our run-time performance during inference.</p><p>Although these changes helped, ambiguity was still a common problem on the border of bounding boxes in the cases of occlusion. This ambiguity results in a false bounding box being predicted between the two ground truth bounding boxes. To fix this problem, the bounding boxes were first shrunk by 75% before creating the detection mask label. This added the additional requirement that the center 4 ? 4-pixel region of the detector window had to be within the center region of the object before activating. The bounding box regressor however, still predicts the original bounding box before shrinking. This also further reduces the number of active bounding boxes as input to our merging algorithm. We also found that switching from L2 to L1 loss on the bounding box regressions results in better performance. To merge the bounding boxes together, we used OpenCV's efficient implementation of groupRectangles, which clusters the bounding boxes based on a similarity metric in O(n 2 ) <ref type="bibr" target="#b7">[8]</ref>.</p><p>The lower layers of our CNN we use for feature extraction is similar to the one proposed by Krizhevsky et al. <ref type="bibr" target="#b8">[9]</ref>. Our modifications to the network occurs on the dense layers which are converted to convolution, as described in Sermanet et al. <ref type="bibr" target="#b0">[1]</ref>. When using our larger image sizes of 640 ? 480 this changes the previous final feature response maps of size 1 ? 1 ? 4096 to 20 ? 15 ? 4096. As stated earlier, each of these feature vectors sees a context region of 355?355 pixels, and the stride between them is 32 ? 32 pixels; however, we want each making predictions at a resolution of 4 ? 4 pixels, which would leave gaps in our input image. To fix this, we use each 4096 feature as input to 64 softmax classifiers, which are arranged in an 8 ? 8 grid each predicting if an object is within a different 4 ? 4 pixel region. This allows for the 4096 feature vector to cover the full stride size of 32 ? 32 pixels; the end</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mask Detector Result</head><p>Bounding Box Regression Detector Context "Sliding Window" -coded such that the closest segments are red and the furthest ones are blue. Due to our data collection methods for lane labels, we are able to obtain ground truth in spite of objects that occlude them. This forces the neural network to learn more than a simple paint detector, and must use context to predict lanes where there are occlusions.</p><p>Similar to the vehicle detector, we use L1 loss to train the regressor. We use mini-batch stochastic gradient descent for optimization. The learning rate is controlled by a variant of the momentum scheduler <ref type="bibr" target="#b10">[11]</ref>. To obtain semantic lane information, we use DBSCAN to cluster the line segments into lanes. <ref type="figure">Fig 5 shows</ref> our lane predictions after DBSCAN clustering. Different lanes are represented by different colors. Since our regressor outputs depths as well, we can predict the lane shapes in 3D using inverse camera perspective mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL SETUP A. Data Collection</head><p>Our Research Vehicle is a 2014 Infiniti Q50. The car currently uses the following sensors: 6x Point Grey Flea3 cameras, 1x Velodyne HDL32E lidar, and 1x Novatel SPAN-SE Receiver. We also have access to the Q50 built-in Continental mid-range radar system. The sensors are connected to a Linux PC with a Core i7-4770k processor.</p><p>Once the raw videos are collected, we annotate the 3D locations for vehicles and lanes as well as the relative speed of all the vehicles. To get vehicle annotations, we follow the conventional approach of using Amazon Mechanical Turk to get accurate bounding box locations within pixel space. Then, we match bounding boxes and radar returns to obtain the distance and relative speed of the vehicles.</p><p>Unlike vehicles that can be annotated with bounding boxes, highway lane borders often need to be annotated as curves of various shapes. This makes frame-level labelling not only tedious and inefficient, but also prone to human errors. Fortunately, lane markings can be considered as static objects that do not change their geolocations very often. We follow the process descried in <ref type="bibr" target="#b4">[5]</ref> to create LIDAR maps of the environment using the Velodyne and GNSS systems. Using these maps, labeling is straight forward. First, we filter the 3D point clouds based on lidar return intensity and position to obtain the left and right boundaries of the ego-lane. Then, we replicate the left and right ego-lane boundaries to obtain initial guesses for all the lane boundaries. A human annotator inspects the generated lane boundaries and makes appropriate corrections using our 3D labelling tool. For completeness, we describe each of these steps in details.</p><p>1) Ego-lane boundary generation: Since we do not change lanes during data collection drives, the GPS trajectory of our research vehicle already gives a decent estimate of the shape of the road. We can then easily locate the ego-lane boundaries using a few heuristic filters. Noting that lane boundaries on highways are usually marked with retro-reflective materials, we first filter out low-reflectivity surfaces such as asphalt in our 3D point cloud maps and only consider points with high enough laser return intensities. We then filter out other reflective surfaces such as cars and traffic signs by only considering points whose heights are close enough the ground plane. Lastly, assuming our car drives close to the center of the lane, we filter out ground paint other than the ego-lane boundaries, such as other lane boundaries, car-pool or directional signs, by only considering markings whose absolute lateral distances from the car are smaller than 2.2 meters and greater than 1.4 meters. We can also distinguish the left boundary from the right one using the sign of the lateral distance. After obtaining the points in the left and right boundaries, we fit a piecewise linear curve similar to the GPS trajectory to each boundary.</p><p>2) Semi-automatic generation of multiple lane boundaries: We observe that the width of lanes during a single data collection run stays constant most of the time, with occasional exceptions such as merges and splits. Therefore, if we predefine the number of lanes to the left and right of the car for a single run, we can make a good initial guess of all the lane boundaries by shifting the auto-generated ego-lane boundaries laterally by multiples of the lane width. We will then rely on human annotators to fix the exception cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data Set</head><p>At the time of this writing our annotated data-set consists of 14 days of driving in the San Francisco Bay Area during the months of April-June for a few hours each day. The vehicle annotated data is sampled at 1 /3Hz and contains nearly 17 thousand frames with 140 thousand bounding boxes. The lane annotated data is sampled at 5Hz and contains over 616 thousand frames. During training, translation and 7 different perspective distortions are applied to the raw data sets. <ref type="figure" target="#fig_3">Fig 6</ref> shows an example image after perspective distortions are applied. Note that we apply the same perspective distortion </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results</head><p>The detection network used is capable of running at 44Hz using a desktop PC equipped with a GTX 780 Ti. When using a mobile GPU, such as the Tegra K1, we were capable of running the network at 2.5Hz, and would expect the system to run at 5Hz using the Nvidia PX1 chipset.</p><p>Our lane detection test set consists of 22 video clips collected using both left and right cameras during 11 different data collection runs, which correspond to about 50 minutes of driving. We evaluate detection results for four lane boundaries, namely, the left and right boundaries of the ego lane, plus the outer boundaries of the two adjacent lanes. For each of these lane boundaries, we further break down the evaluation by longitudinal distances, which range from 15 to 80 meters ahead of the car, spaced by 5 meters. Thus, there are at maximum 4 ? 14 = 56 positions at which we evaluate the detection results. We pair up the prediction and ground truth points at each of these locations using greedy nearest neighbor matching. True positives, false positives and false negatives are accumulated at every evaluation location in a standard way: A true positive is counted when the matched prediction and ground truth differ by less than 0.5 meter. If the matched prediction and ground truth differ by more than 0.5 meter, both false positive and false negative counts are incremented.  Nearby false positives can cause the largest problems for ADAS systems which could cause the system to needlessly apply the brakes. In our system, we found overpasses and shading effects to cause the largest problems. Two examples of these situations are shown in <ref type="figure" target="#fig_0">Fig 10.</ref> As a baseline to our car detector, we compared the detection results to the Continental mid-range radar within our data collection vehicle. While matching radar returns to ground truth bounding boxes, we found that although radar had nearly 100% precision, false positives were being introduced through errors in radar/camera calibration. Therefore, to ensure a fair comparison we matched every radar return to a ground truth bounding box even if IOU&lt; 0.5, giving our radar returns 100% precision. This comparison is shown in <ref type="figure" target="#fig_0">Fig 11,</ref> the F1 score for radar is simply the recall.</p><p>In addition to the bounding box locations, we measured the accuracy of the predicted depth by using radar returns as For a qualitative review of the detection system, we have uploaded a 1.5 hour video of the vehicle detector ran on our test set. This may be found at youtu.be/GJ0cZBkHoHc. A short video of our lane detector may also be found online at youtu.be/__f5pqqp6aM. In these videos, we evaluate the detector on every frame independently and display the raw detections, without the use of any Kalman filters or road models. The red locations in the video correspond to the mask detectors that are activated. This network was only trained on the rear view of cars traveling in the same direction, which is why cars across the highway barrier are commonly missed.</p><p>We have open sourced the code for the vehicle and lane <ref type="figure" target="#fig_0">Fig. 11</ref>: Radar Comparison to Vehicle Detector V. CONCLUSION By using Camera, Lidar, Radar, and GPS we built a highway data set consisting of 17 thousand image frames with vehicle bounding boxes and over 616 thousand image frames with lane annotations. We then trained on this data using a CNN architecture capable of detecting all lanes and cars in a single forward pass. Using a single GTX 780 Ti our system runs at 44Hz, which is more than adequate for real-time use. Our results show existing CNN algorithms are capable of good performance in highway lane and vehicle detection. Future work will focus on acquiring frame level annotations that will allow us to develop new neural networks capable of using temporal information across frames.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Sample output from our neural network capable of lane and vehicle detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>overfeat-mask result is a grid mask detector of size 160 ? 120 where each element is 4 ? 4 pixels which covers the entire input image of size 640 ? 480. A. Lane Detection The CNN used for vehicle detection can be easily extended for lane boundary detection by adding an additional class. Whereas the regression for the vehicle class predicts a five dimensional value (four for the bounding box and one for depth), the lane regression predicts six dimensions. Similar to the vehicle detector, the first four dimensions indicate the two end points of a local line segment of the lane boundary. The remaining two dimensions indicate the depth of the endpoints with respect to the camera. Fig 4 visualizes the lane boundary ground truth label overlaid on an example image. The green tiles indicate locations where the detector is trained to fire, and the line segments represented by the regression labels are explicitly drawn. The line segments have their ends connected to form continuous splines. The depth of the line segments are color</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :Fig. 5 :</head><label>45</label><figDesc>Example of lane boundary ground truth Example output of lane detector after DBSCAN clustering</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Image after perspective distortion to the ground truth labels so that they match correctly with the distorted image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig 7</head><label>7</label><figDesc>shows a visualization of this evaluation method on one image. The blue dots are true positives. The red dots are false positives, and the yellow ones are false negatives. Fig 8 shows the aggregated precision, recall and F1 score on all test videos.For the ego-lane boundaries, we obtain 100% F1 score up to 50 meters. Recall starts to drop fast beyond 65 meters, mainly because the resolution of the image cannot capture the width of the lane markings at that distance. For the adjacent lanes, recall is low for the nearest point because it is outside the field of view of the camera.The vehicle detection test set consists of 13 video clips collected from a single day, which corresponds to 1 hour</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :Fig. 8 :</head><label>78</label><figDesc>Left: lane prediction on test image. Right: Lane detection evaluated in 3D Lane detection results on different lateral lanes. (a) Ego-lane left border. (b) Ego-lane right border. (c) Left adjacent lane left border. (d) Right adjacent lane right border.and 30 mins of driving. The accuracy of the vehicle bounding box predictions were measured using Intersection Over Union (IOU) against the ground truth boxes from Amazon Mechanical Turk (AMT). A bounding box prediction matched with ground truth if IOU? 0.5. The performance of our car detection as a function of depth can be seen inFig 9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 :Fig. 10 :</head><label>910</label><figDesc>Car Detector Bounding Box Performance (a) FP: tree (b) FP: overpass Vehicle False Positives ground truth. The standard error in the depth predictions as a function of depth can be seen in Fig 12.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 12 :</head><label>12</label><figDesc>Car Detector Depth Performance detector online at github.com/brodyh/caffe. Our repository was forked from the original Caffe code base from the BVLC group<ref type="bibr" target="#b19">[20]</ref>.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This research was funded in part by Nissan who generously donated the car used for data collection. We thank our colleagues Yuta Yoshihata from Nissan who provided technical support and expertise on vehicles that assisted the research. In addition, the authors would like to thank the author of Overfeat, Pierre Sermanet, for their helpful suggestions on image detection.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Traffic and transport psychology: Theory and application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Talib</forename><surname>Rothengatter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique Carbonell Ed</forename><surname>Ed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of Traffic and Transport Psychology</title>
		<meeting><address><addrLine>Valencia, Spain. Pergamon</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier Science Inc</publisher>
			<date type="published" when="1996-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A multi-sensor fusion system for moving object detection and tracking in urban driving environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunggi</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2014" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
	<note>Robotics and Automation (ICRA)</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A probabilistic framework for car detection in images using context and scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Levinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Robotics and Automation (ICRA)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards fully autonomous driving: systems and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Levinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A system for real-time detection and tracking of vehicles from a single car-mounted camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Caraffi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>15th International IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vehicle detection and tracking in car video based on motion model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirali</forename><surname>Jazayeri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="583" to="595" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Intelligent Transportation Systems</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The opencv library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Doctor Dobbs Journal</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="120" to="126" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ilya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML-13)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML-13)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Pattern Analysis and Machine Intelligence</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Scalable, High-Quality Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1441</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><forename type="middle">Rr</forename><surname>Uijlings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01852</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
