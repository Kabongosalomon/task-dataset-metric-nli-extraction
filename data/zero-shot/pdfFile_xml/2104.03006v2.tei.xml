<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Librispeech Transducer Model with Internal Language Model Prior Correction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Zeyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Human Language Technology and Pattern Recognition</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<postCode>52062</postCode>
									<settlement>Aachen, Germany</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">AppTek GmbH</orgName>
								<address>
									<postCode>52062</postCode>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?</forename><surname>Merboldt</surname></persName>
							<email>andre.merboldt@rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Human Language Technology and Pattern Recognition</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<postCode>52062</postCode>
									<settlement>Aachen, Germany</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilfried</forename><surname>Michel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Human Language Technology and Pattern Recognition</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<postCode>52062</postCode>
									<settlement>Aachen, Germany</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">AppTek GmbH</orgName>
								<address>
									<postCode>52062</postCode>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schl?ter</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Human Language Technology and Pattern Recognition</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<postCode>52062</postCode>
									<settlement>Aachen, Germany</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">AppTek GmbH</orgName>
								<address>
									<postCode>52062</postCode>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
							<email>ney@cs.rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Human Language Technology and Pattern Recognition</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<postCode>52062</postCode>
									<settlement>Aachen, Germany</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">AppTek GmbH</orgName>
								<address>
									<postCode>52062</postCode>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Librispeech Transducer Model with Internal Language Model Prior Correction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: transducer</term>
					<term>language model integration</term>
					<term>speech recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present our transducer model on Librispeech. We study variants to include an external language model (LM) with shallow fusion and subtract an estimated internal LM. This is justified by a Bayesian interpretation where the transducer model prior is given by the estimated internal LM. The subtraction of the internal LM gives us over 14% relative improvement over normal shallow fusion. Our transducer has a separate probability distribution for the non-blank labels which allows for easier combination with the external LM, and easier estimation of the internal LM. We additionally take care of including the end-ofsentence (EOS) probability of the external LM in the last blank probability which further improves the performance. All our code and setups are published.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction &amp; Related Work</head><p>The recurrent neural network transducer (RNN-T) model <ref type="bibr">[1,</ref><ref type="bibr" target="#b0">2]</ref> is an end-to-end model which allows for time-synchronous decoding, which a more natural fit for many applications such as online recognition. Thus RNN-T and many variations has recently gained interest <ref type="bibr" target="#b1">[3]</ref><ref type="bibr" target="#b2">[4]</ref><ref type="bibr" target="#b3">[5]</ref><ref type="bibr" target="#b4">[6]</ref><ref type="bibr" target="#b5">[7]</ref><ref type="bibr" target="#b6">[8]</ref>.</p><p>In a Bayesian interpretation, a discriminative acoustic model pAM(y | x) can be combined with an external language model pLM(y) by p(y | x) = pAM(y | x) pAM(y) ? pAM(x) ? pLM(y) ? 1 p(x) .</p><p>In recognition, when searching for arg max y p(y | x), we can omit p(x) and pAM(x). In shallow fusion, pAM(y) is omitted as well. In the density ratio approach <ref type="bibr" target="#b7">[9]</ref>, pAM(y) is estimated by a separate language model trained on just the acoustic training transcriptions. In the hybrid autoregressive transducer (HAT) <ref type="bibr" target="#b4">[6]</ref>, pAM(y) is estimated directly based on the implicit internal LM (ILM) of pAM(y | x). The HAT model has a particular simple architecture which was designed such that there is a simple approximation for this ILM estimation by setting the encoder input to 0. We follow up on the ILM estimation approach and try some variations of the estimation. Using 0 as encoder input also works but we found some other variations to be better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Model</head><p>We follow a transducer variant as defined in <ref type="bibr" target="#b5">[7]</ref>. The whole model can be seen in <ref type="figure">Figure 1</ref>. Let x T 1 be the acoustic input features (MFCC in our case) of length T , and y S 1 some label Figure 1: Our transducer model with all dependencies. The decoder is unrolled over the alignment axis u. Compare to <ref type="bibr" target="#b5">[7]</ref>.</p><p>sequence of length S over labels ? (excluding blank ). We use byte pair encoding (BPE)-based subword units <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b9">11]</ref> with a vocabulary size of about 1000 labels 1 .</p><p>We have a multi-layer bidirectional LSTM <ref type="bibr" target="#b10">[12]</ref> encoder model with interchanged max-pooling in time to downscale the input to length T with factor 6. This results in h T 1 := Encoder(x T 1 ).</p><p>We define the probability for the label sequence y S 1 as</p><formula xml:id="formula_0">p(y S 1 | x T 1 ) := ? U 1 :y S 1 p(? U 1 | x T 1 ), p(? U 1 | x T 1 ) := U u=1 pu(?u | ? u?1 1 , x T 1 ),</formula><p>with alignment label ?u ? ? := ??{ }, and where ? U 1 : y S 1 is defined by the label topology A. Specifically, we use alignment length U = T + S and allow all alignment label sequences ? U 1 which match the sequence y S 1 after removing all blanks , also notated as A(? U 1 ) = y S 1 . This defines an alignment between h and y as can be seen in <ref type="figure" target="#fig_0">Figure 2</ref>. Our decoder model defines the probability distribution over labels ?u as</p><formula xml:id="formula_1">pu(?u | ...) := pu(?tu=1 | ...), ?u = , pu(?tu=0 | ...) ? qu(?u | ...), ?u ? ? pu(?tu | ...) := ?(? FFemit(z fast u )), ?tu = 1 ?( FFemit(z fast u )), ?tu = 0 qu(?u | ...) := softmax?(FF?(z fast u )), ?u ? ? z fast u := Readout(ht u , z slow su ) z slow su := SlowRNN(y su?1 1 )</formula><p>where ? is the sigmoid function and ?t ? {0, 1}, where ?tu = 0 means that we emit a new non-blank label (?su = 1) and ?tu = 1 means that we proceed forward in the time dimension without emitting a non-blank label (?su = 0). Thus ?tu = 1 can be understand as a reinterpretation of the blank label . Then we have a separate probability distribution q over the labels ? (excluding ). FFemit, FF? are linear transformations, Readout is a linear transformation with maxout activation, and SlowRNN is an LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Training</head><p>The loss is defined as</p><formula xml:id="formula_2">L := ? log p(y S 1 | x T 1 ) = ? log ? U 1 :y S 1 p(? U 1 | x T 1 ).</formula><p>As we use the simplified transducer model where Readout does not depend on ?u?1, we can efficiently calculate the exact full sum over all alignments ? U 1 : y S 1 and do not need the maximum approximation <ref type="bibr" target="#b5">[7]</ref>.</p><p>We use zoneout <ref type="bibr" target="#b11">[13]</ref> for the SlowRNN and optionally recurrent weight dropout <ref type="bibr" target="#b12">[14]</ref> for the encoder BLSTMs.</p><p>We use the Adam optimizer <ref type="bibr" target="#b13">[15]</ref> with learning rate scheduling based on cross validation scores. Additionally, we reset the learning rate back to the initial value after a larger number of epochs, after the model already converged, and start over with the learning rate scheduling. We train for 128 epochs. The long amount of training had a huge effect on the overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pretraining</head><p>We use a pretraining scheme where we schedule multiple aspects of the training:</p><p>? We grow the encoder from 3 layers with 500 dim. up to 6 layers with 1000 dimensions <ref type="bibr" target="#b14">[16]</ref>. ? We increase the dropout rates. ? We use curriculum learning and start with shorter sequences initially. ? We use linear learning rate warmup from 0.0001 to 0.001. ? We use a higher initial time reduction factor 20 in the encoder and reduce it to the final factor 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Distributed Multi-GPU Training</head><p>Our distributed training implementation uses independent trainer (worker) instances per GPU. Each worker independently loads the dataset. To make sure that every worker uses a different part of the dataset, it is common to use striding. Striding has the disadvantage that it is very IO intensive in this setting where every worker loads the dataset independently and often becomes the bottleneck. So we came up with the idea to use a different random seed for the shuffling of the dataset for every worker, to replace the striding. This greatly improved the IO in our case and made the training much faster. Additionally, every worker independently trains an own copy of the model for multiple update steps, until the models get synchronized by averaging the parameters over all workers. We made the further improvement that we do not synchronize after a fixed number of steps, but instead after a fixed time interval. A fixed number of steps implies that the training is always as slow as the slowest worker, and variations in the runtime often lead to some workers being slower than others even on same hardware. Synchronizing after a fixed time interval does not have this problem, while being more stochastic.</p><p>We synchronize only after 100 seconds to reduce the communication between workers. The workers can potentially be on different computing nodes and might need to communicate over network, which can result in 1-2 seconds for the synchronization. We train on either 8 or 16 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Decoding &amp; Language Model Combination</head><p>Our beam search decoding tries to find the sequence?? 1 given</p><p>x T 1 which maximizes the probability, i.e. specifically</p><p>x T 1 ??,?? 1 := arg max</p><formula xml:id="formula_3">S,y S 1 log p(y S 1 | x T 1 ) ? A ? arg max U,? U 1 log p(? U 1 | x T 1 )</formula><p>We perform alignment-synchronous decoding, i.e. all hypotheses are in the same alignment step u when being pruned <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b15">17]</ref>. We merge hypotheses by summing their scores when they correspond to the same word sequence after BPE-merging. The training recipe for our BPE-10K LSTM LM <ref type="bibr" target="#b16">[18]</ref> has been adapted for the new BPE-1k label set but otherwise no changes have been made. Shallow fusion (SF) <ref type="bibr" target="#b17">[19]</ref> is a loglinear combination of the log-scores of external LM and ASR model scores during the recognition process, with scale ? for the LM and scale ? for the acoustic (non-blank) label probability q, while we do not add an own scale for p(?t). Specifically, we use the score</p><formula xml:id="formula_4">log p SF u (?u | ...) := ? ? ? ? ? ? ? ? ? log pu(?tu=1 | ...), ?u = , log pu(?tu=0 | ...) + ? ? log qu(?u | ...) + ? ? log pLM(?u | ...), ?u ? ? .</formula><p>We experimented with fixing the label scale at ? = 1 or ? = 1 ? ?. Inspired by <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b18">20]</ref> we also tried to subtract the internal LM log score. It assumes that we can factorize our model into a language and acoustic model. Although our model is not directly formulated as such, we can approximate the internal language model. For that we used the estimated score log pILM as shown in Section 4.1, where we use the average of encoder features in the time dimension.</p><formula xml:id="formula_5">log p SF-ILM u (?u | ...) := ? ? ? ? ? ? ? ? ? ? ? ? ? log pu(?tu=1 | ...), ?u = , log pu(?tu=0 | ...) + ? ? log qu(?u | ...) + ? ? log pLM(?u | ...) ? ? ? log pILM(?u | ...), ?u ? ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Internal LM Estimation</head><p>The transducer is trained on audio-text pairs but learns an implicit prior model on the text. This is explicitly given by the context dependency on previous labels. In this transducer case, the SlowRNN is also explicitly modeled such that it models the most important part of this prior as it operates only on the textonly part and runs label-synchronous. This prior is an implicit internal LM in our acoustic model</p><formula xml:id="formula_6">pprior(y) = x pAM(y | x) ? p(x)</formula><p>which can not be calculated efficiently in general. To approximate the internal LM, we replace the encoder input to the rest of the model (Readout). We either use a 0 vector or the encoder mean (avg). The mean is computed over the time dimension for each sequence separately. <ref type="bibr" target="#b0">2</ref> We evaluate the estimated internal LM on text-only data. In <ref type="table" target="#tab_0">Table 1</ref>, the BPE-level perplexities (PPL) are shown and compared against the LSTM LM which was trained only on text data, but without any overlap to the audio transcriptions <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">EOS Modelling</head><p>In contrast to language models or attention models, transducers and models with explicit time modeling do not have to model the end-of-sentence/sequence explicitly with an additional token (denoted as eos ). Instead the search ends when all input frames have been consumed. However, for LM integration, when only considering actual output symbols, the information about when the sequence should end is not considered. This additional information is usually ignored in the literature, however it provides valuable information to the search process.</p><p>Our approach is to combine the LM EOS probability with pu(?t=1) ( ) in the last time frame (tu = T ) because that <ref type="bibr" target="#b0">2</ref> We also tested several other variants but got mixed inconclusive results. In another work <ref type="bibr" target="#b20">[21]</ref>, we investigate variants on the ILM estimation in more detail for attention-based encoder-decoder models. </p><formula xml:id="formula_7">:= ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? log p u (?t=1 | ...), ?u = , tu?1 &lt; T ? eos log p u (?t=1 | ...) + ? eos log pLM( eos | ...), ?u = , tu?1 = T log p u (?t=0 | ...) + ? ? log qu(?u | ...) + ? ? log pLM(?u | ...) ? ? ? log pILM(?u | ...), ?u ? ?</formula><p>Usually ? eos = ? eos = 0.5 yielded good performance, although it was not tuned properly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We perform experiments on LibriSpeech <ref type="bibr" target="#b21">[22]</ref>. Our model training and decoding is implemented in RETURNN <ref type="bibr" target="#b22">[23]</ref>, based on TensorFlow <ref type="bibr" target="#b23">[24]</ref>. The distributed multi-GPU training is implemented with Horovod <ref type="bibr" target="#b24">[25]</ref>. We make use of Mingkun Huang's warp-transducer loss implementation 3 . Our decoder uses the builtin RETURNN features for stochastic variables and searches over ?u. This uses GPU-based batched one-pass decoding with the external LM and internal LM subtraction. We publish all the configuration files needed to reproduce the experiments <ref type="bibr" target="#b2">4</ref> . We have a variety of different exponent scales for our loglinear modeling, as well as additional parameters for EOSmodeling. Label scale ? which is set to either ? = 1 or ? = 1 ? ?, the emission model scale ?, and the scales for external and internal LM ?, ?, respectively. Additionally for EOS-modeling ? eos and ? eos is used, although it was fixed to ? eos = ? eos = 0.5. The scaling factors ? and ? have to be tuned jointly on a held-out dataset, as can be seen in <ref type="figure" target="#fig_1">Fig. 3</ref>, with ? = 1 ? ?. They were tuned separately for each subset dev-clean and dev-other. Results for LM integration are presented in <ref type="table" target="#tab_1">Table 2 and in Table 3</ref> with additional EOS-modeling. With shallow fusion of just the LM we already see a significant WER improvement by over 22% relative. When additionally subtracting the internal LM, a further significant improvement is observed by over 14% relative over the shallow fusion.  The avgILM estimation seems to be better than 0except on testother. The effect of EOS gives us 7% relative improvement. We also test a stronger Transformer LM in <ref type="table" target="#tab_2">Table 3</ref> (perplexities in <ref type="table" target="#tab_0">Table 1</ref>) and see further improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Error Analysis</head><p>One of the sources of errors when looking at an entire system are errors the model made when its prediction was wrong. We look at the percentages of substitution, deletion, and insertion errors of the word error rate (WER). Especially interesting is the comparison between different models and their respective LM integration. Also of interest are how long the hypothesized sentences are, relative to the reference transcription. The transducer seems to model the hypothesis length better than hybrid (without rescoring) and attention-based models, although adding an external LM seems to help the attention model. Overall we can see that introducing the external LM helps with substitutions and insertion errors, while the deletions actually increase. In comparison to the attention-based model, the transducer model has significantly less insertion errors, but more deletion errors, relative to the overall WER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions &amp; Future Work</head><p>The subtraction of the ILM helped to improve the model by a lot (over 14% relative) over the already strong shallow fusion. The EOS modeling also helped (7% relative). We noticed that all recognition experiments are very sensitive to the LM/ILM  As future work, we plan to study the effect of the label unit and to test simple characters and other subword variations, similar to <ref type="bibr" target="#b27">[28]</ref>. The encoder model might get improvements by more recent advancements <ref type="bibr" target="#b3">[5]</ref>. The decoder can be extended as well <ref type="bibr" target="#b5">[7]</ref>. We also can potentially improve the ILM estimation. Finally, we expect to get improvements by min. WER training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head><p>We thank Yingbo Gao for providing us with the Transformer LM on our BPE-1k label set. This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement n o 694537, project "SEQCLAS"). The work reflects only the authors' views and the European Research Council Executive Agency (ERCEA) is not responsible for any use that may be made of the information it contains. This work was partly funded by the Google Focused Award "Pushing the Frontiers of ASR: Training Criteria and Semi-Supervised Learning".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">References</head><p>[1] A. Graves, "Sequence transduction with recurrent neural networks," Preprint arXiv:1211.3711, 2012.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Unrolled label topology with allowed vertical transitions, with a highlighted path for the sequence A(" th e s i s ") = "thesis". The terminal node is marked in the top-right corner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Tuning of LM scales of a transducer with EOSmodeling. The baseline without an external LM has 8.66% WER on dev-other with a beam size of 24. ? = 1 ? ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Perplexity and WER measurements on Librispeech devother of a transducer model. Note that the BPE-level (1k units) perplexity is evaluated without the EOS token, since the transducer has no explicit end-of-sequence symbol. Compared are both setting the encoder (h) to 0 and to the mean over the timedimension (avg). The LSTM and Trafo-LM are trained on textonly data without overlap to the audio transcriptions<ref type="bibr" target="#b21">[22]</ref>.</figDesc><table><row><cell>Model</cell><cell>Epochs</cell><cell>Perplexity h = 0 avg</cell><cell>WER [%]</cell></row><row><cell>Transducer BPE-1K</cell><cell>8 16 32 64 133</cell><cell cols="2">82.76 67.47 36.41 49.32 38.89 17.16 45.13 32.86 11.85 46.53 31.94 9.69 47.05 31.37 8.92</cell></row><row><cell>LSTM LM</cell><cell>20</cell><cell>15.40</cell><cell>?</cell></row><row><cell>Trafo LM</cell><cell>39</cell><cell>14.44</cell><cell>?</cell></row><row><cell cols="3">determines the EOS in the transducer.</cell><cell></cell></row><row><cell>log p SF-ILM+EOS</cell><cell></cell><cell></cell><cell></cell></row></table><note>u (?u | ...)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>We investigate the effect of LM integration for the model with either shallow fusion (SF) or additional internal LM (ILM) subtraction. All experiments were conducted with a fixed beam-size 24 and without EOS-modeling, the LSTM-LM has a BPE-1K level perplexity of 15.4 on dev-other.</figDesc><table><row><cell>LM</cell><cell>LM Integration Method</cell><cell>Label scale ?</cell><cell>WER [%] dev test clean other clean other</cell></row><row><cell>-LSTM</cell><cell>-SF SF-ILM(avg)</cell><cell>? = 1 ? = 1 ? ?</cell><cell>3.22 8.76 3.30 8.70 2.53 6.79 2.66 6.99 2.47 6.50 2.57 6.70 2.29 5.63 2.36 6.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>We investigate the effect of LM integration for the model with either shallow fusion (SF) or additional internal LM (ILM) subtraction. All experiments were conducted with a fixed beam-size 24 and EOS-modeling (last blank frame), the LSTM-LM has a BPE-1K level perplexity of 15.4 on dev-other. InFig. 3the heat map for the joint tuning over ?OTHER and ?OTHER. ? eos = ? eos = 0.5.</figDesc><table><row><cell>LM</cell><cell>LM Integration Method</cell><cell>Label scale ?</cell><cell>WER [%] dev test clean other clean other</cell></row><row><cell>-</cell><cell>-</cell><cell>? = 1</cell><cell>3.20 8.66 3.28 8.60</cell></row><row><cell>LSTM</cell><cell>SF SF-ILM(avg)</cell><cell>? = 1 ? = 1 ? ?</cell><cell>2.52 6.69 2.65 6.85 2.45 6.35 2.55 6.68 2.26 5.49 2.42 5.91</cell></row><row><cell>Trafo</cell><cell>SF SF-ILM(avg) SF-ILM(0)</cell><cell>? = 1 ? ?</cell><cell>2.41 6.29 2.52 6.56 2.17 5.28 2.23 5.74 2.22 5.32 2.25 5.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>We investigate the different type of word errors on ILM(avg) 80.90 9.06 10.04 5.63 SF-ILM(avg) +EOS 79.81 10.15 10.04 5.49 Trafo 80.45 9.55 10.00 5.28 scales. The long training time also had a huge effect on the final performance.</figDesc><table><row><cell cols="5">various models. Hybrid [26], Attention [27], and Transducer</cell></row><row><cell cols="5">(ours). With either shallow fusion (SF) or additional internal</cell></row><row><cell cols="5">LM (ILM) subtraction. For log-linear combination in the trans-</cell></row><row><cell cols="3">ducer case, ? = 1 ? ?.</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>LM</cell><cell cols="2">LM integration</cell><cell>Edit operations [%] WER Sub. Del. Ins. [%]</cell></row><row><cell cols="2">Attention None LSTM</cell><cell></cell><cell>-SF</cell><cell>80.40 6.96 12.65 9.93 79.42 5.68 14.90 7.50</cell></row><row><cell cols="2">Hybrid 4-gr.</cell><cell></cell><cell>SF</cell><cell>75.75 12.98 11.27 9.37</cell></row><row><cell>Transd.</cell><cell>None LSTM</cell><cell>SF-</cell><cell>-SF</cell><cell>82.52 7.55 9.93 8.76 79.10 10.93 9.97 6.50</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In earlier work on attention-based encoder decoder models, we used 10k BPE labels for Librispeech. However, because of computation time and memory constraints, we reduced it to 1k for the transducer model.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/HawkAaron/warp-transducer 4 https://github.com/rwth-i6/returnn-experiments/ tree/master/2021-transducer</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Transformer transducer: A streamable speech recognition model with Transformer encoders and RNN-T loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="7829" to="7833" />
			<date type="published" when="2020" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">ContextNet: Improving convolutional neural networks for automatic speech recognition with global context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.03191</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Conformer: Convolution-augmented Transformer for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08100</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Variani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Hybrid autoregressive transducer (HAT),&quot; in ICASSP</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A new training pipeline for an improved neural transducer,&quot; in Interspeech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Merboldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schl?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-10" />
			<pubPlace>Shanghai, China</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Phoneme based neural transducer for large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schl?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2021-05" />
		</imprint>
	</monogr>
	<note>submitted to</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A density ratio approach to language model fusion in end-to-end automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Variani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ASRU</publisher>
			<biblScope unit="page" from="434" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improved training of end-to-end attention models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schl?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<meeting><address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Zoneout: Regularizing RNNs by randomly preserving hidden activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kram?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pezeshki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">R</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v28/wan13.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research</title>
		<meeting>the 30th International Conference on Machine Learning, ser. Machine Learning Research<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2013-06-19" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A comprehensive analysis on attention models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Merboldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schl?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interpretability and Robustness in Audio, Speech, and Language (IRASL) Workshop</title>
		<meeting><address><addrLine>NeurIPS, Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Alignment-length synchronous decoding for RNN transducer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>T?ske</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="7804" to="7808" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Advancing neural language modeling in automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<ptr target="http://publications.rwth-aachen.de/record/789081" />
		<imprint>
			<date type="published" when="2020-05" />
			<pubPlace>Aachen, Germany</pubPlace>
		</imprint>
		<respStmt>
			<orgName>RWTH Aachen University, Computer Science Department, RWTH Aachen University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On using monolingual corpora in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>G?l?ehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="137" to="148" />
			<date type="published" when="2017-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Internal language model estimation for domain-adaptive end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="243" to="250" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Investigating methods to improve language model integration for attention-based encoder-decoder ASR models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeineldeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Glushko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schl?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>submitted to Interspeech 2021</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lib-riSpeech: an ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="5206" to="5210" />
			<date type="published" when="2015" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">RETURNN as a generic flexible neural toolkit with application to translation and speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Alkhouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Assoc. for Computational Linguistics</title>
		<meeting><address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tensorflow</forename><surname>Development Team</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>software available from tensorflow.org. [Online</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Horovod: fast and easy distributed deep learning in TensorFlow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Del</forename><surname>Balso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05799</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">RWTH ASR systems for LibriSpeech: Hybrid vs attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>L?scher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kitza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schl?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-09" />
			<biblScope unit="page" from="231" to="235" />
			<pubPlace>Interspeech, Graz, Austria</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A comparison of Transformer and LSTM encoder decoder models for ASR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schl?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ASRU</title>
		<imprint>
			<biblScope unit="page" from="8" to="15" />
			<date type="published" when="2019-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A systematic comparison of grapheme-based vs. phoneme-based label units for encoder-decoder-attention models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeineldeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schl?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.09336</idno>
		<ptr target="http://arxiv.org/abs/2005.09336" />
		<imprint>
			<date type="published" when="2020-11" />
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note>submitted to ICASSP 2021</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
