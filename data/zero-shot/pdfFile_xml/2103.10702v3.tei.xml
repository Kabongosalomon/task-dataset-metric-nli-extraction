<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ClawCraneNet: Leveraging Object-level Relation for Text-based Video Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Zhejiang University Baidu Research ReLER</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Zhejiang University Baidu Research ReLER</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Zhejiang University Baidu Research ReLER</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Zhejiang University Baidu Research ReLER</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ClawCraneNet: Leveraging Object-level Relation for Text-based Video Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text-based video segmentation is a challenging task that segments out the natural language referred objects in videos. It essentially requires semantic comprehension and fine-grained video understanding. Existing methods introduce language representation into segmentation models in a bottom-up manner, which merely conducts vision-language interaction within local receptive fields of ConvNets. We argue such interaction is not fulfilled since the model can barely construct region-level relationships given partial observations, which is contrary to the description logic of natural language/referring expressions. In fact, people usually describe a target object using relations with other objects, which may not be easily understood without seeing the whole video. To address the issue, we introduce a novel top-down approach by imitating how we human segment an object with the language guidance. We first figure out all candidate objects in videos and then choose the refereed one by parsing relations among those high-level objects. Three kinds of object-level relations are investigated for precise relationship understanding, i.e., positional relation, text-guided semantic relation, and temporal relation. Extensive experiments on A2D Sentences and J-HMDB Sentences show our method outperforms state-of-the-art methods by a large margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the significant progresses achieved on computer vision and natural language processing, novel tasks requiring a joint understanding of both visual and linguistic modalities emerge recently, e.g., visual question answering <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b2">3]</ref>, image <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b40">41]</ref> and video captioning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b37">38]</ref>, vision-dialog navigation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b50">51]</ref> and so on. Inspired by such great success, Gavrilyuk et al. <ref type="bibr" target="#b9">[10]</ref> introduces a challenging task of text-based video segmentation, which takes a video and a natural language description as inputs, and prospects a set of segmentation masks for the referent. Certain solutions for tackling this task lie in a comprehensively understanding of the visual and linguistic information with a fine-grained Corresponding models could not correctly identify the high-level relation merely based on local perceptive fields, and directly leads to an ambiguous prediction. (b) Our top-down pipeline first performs feature extraction for objects and then model the crucial relation information based on a high-level sensation, leading to better segmentation masks by conducting multi-modal retrieving. Vividly, we analogize the process of retrieving a visual object with the linguistic query as playing a claw crane machine.</p><p>analysis of video contents. Accordingly, several recent works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b31">32]</ref> proposed to capitalize on the pixel-level visual language relationship for video comprehension. These techniques are approached by a bottom-up paradigm, which mainly focuses on multi-modal feature fusion and pixel-level relation establishment to generate the segmentation masks directly. While some effectiveness has been achieved, these attempts generally lack the sensation of object-level information and relation, which is crucial for understanding semantic information, especially for multi-modal comprehension <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> (a), methods with low-level sensation could only formulate local region-level relationship, which is insufficient for modeling semantic relationship with natural language description logic. Thus the bottom-up approaches would inevitably introduce noisy re-lationship modeling and inaccurate object comprehension, leading to ambiguous segmentation results.</p><p>In this paper, we propose a novel method to tackle the above issue. We are partially inspired by the human visual cognitive system that humans preferentially direct attention towards meaningful entities (object-orientated) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b38">39]</ref>, and then extracts structured information on how entities relate to/interact with each other (relation-based) <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>Based on the foundation, we propose a top-down pipeline to mimic how humans localize the referent with language guidance, i.e., first finding candidates and then parsing relations. Specifically, by applying an off-the-shelf instance segmentation module to find out candidate objects, we could tackle the Text-based Video Segmentation problem in an object-level cross-modal retrieval manner. To our best knowledge, this is the first attempt to tackle the textbased video segmentation problem from the top-down view.</p><p>We further explore three kinds of object-level relations in our top-down pipeline, i.e., positional relation, semantic relation, and temporal relation.</p><p>Firstly, we propose a relative position encoding module to encode spatial information of each candidate object. On the basis of absolute position, we further consider the relative ranked index for each object according to its coordinates. The relative index addresses the spatial relations that are common in natural language descriptions, e.g., "the second guy from the left".</p><p>Secondly, we propose a language-guided object attention module to construct semantic relations, which directly highlights the referring entities. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, related objects (e.g., "guy" and "ball") will exchange information according to responsiveness to relational expressions (e.g., "kicking"). After relation-aware language comprehension, a particular referent feature (e.g., "guy") would contain rich semantic evidence including relationship (e.g., "kicking a ball") and attribute (e.g., "in black").</p><p>Thirdly, we investigate the temporal relationship between inter-frame objects by a merge-by-track diagram. Particularly, with a multi-object tracking strategy, we perform inter-frame object association based on similarities, and build temporally related tracks with the Hungarian algorithm <ref type="bibr" target="#b23">[24]</ref>. Finally, the final prediction is performed according to the average of confidence scores in each track.</p><p>In this way, we obtain a visual embedding with rich individual and mutual information, which would facilitate a correct language-to-vision corresponding in the complex video context. Vividly, these visual objects are like wellpackaged dolls displayed in a claw crane machine, and the linguistic description performs as a claw looking for the shiniest doll. Our network just explicitly formulates the retrieving pipeline, and that's why we named our network ClawCraneNet. The main contributions are as follows:</p><p>? We propose a novel top-down pipeline that tackles the text-based video segmentation task in a retrieval manner.</p><p>? We explicitly investigate three kinds of object-level relations to progressively construct discriminative visual embedding, i.e., relative positional relation, crossobject semantic relation, and inter-frame temporal relation.</p><p>? The proposed method significantly outperforms stateof-the-art methods on two popular text-guided video segmentation datasets, i.e., A2D, and J-HMDB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Referring Image Segmentation</head><p>Referring expression segmentation aims at precisely localizing the entity referred by a natural language expression with a pixel-level segmentation mask. The bottomup methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b44">45]</ref> mainly construct a multi-modal feature, then generate referring masks after some refinement progress. Most state-of-theart works conduct the structures of fully convolutional network (FCN) <ref type="bibr" target="#b26">[27]</ref> to generate the pixel-level segmentation mask. At first, Hu et al. <ref type="bibr" target="#b13">[14]</ref> directly leverage the concatenation of visual and linguistic features from CNN and LSTM to construct multi-modal feature and generate the final mask. Later, several techniques are incorporated into this field , e.g. multi-modal LSTM <ref type="bibr" target="#b25">[26]</ref>, image-to-word attention <ref type="bibr" target="#b46">[47]</ref>, dynamic filter <ref type="bibr" target="#b29">[30]</ref>, and adversarial learning <ref type="bibr" target="#b32">[33]</ref> or cycle-consistency <ref type="bibr" target="#b6">[7]</ref> between referring expression and its reconstructed caption. Recently, to explore the relationship between multi-modal features <ref type="bibr" target="#b41">[42]</ref> and further model the structural context, Hu et al. <ref type="bibr" target="#b13">[14]</ref> propose a bidirectional cross-modal attention module to emphasize visual guidance on linguistic features. Huang et al. <ref type="bibr" target="#b15">[16]</ref> utilize a graph-based structure to progressively exploits different types of words in the expression. Different from these works which focus on low-level feature comprehension. Inspired by the human vision system, e.g. finding the referring objects then parsing the relation, we investigate object-level feature retrieving as another alternative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Top-down Text-based Object Grounding</head><p>The existing top-down methods <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b3">4]</ref> mainly leverage the pre-trained detector, e.g., Mask R-CNN <ref type="bibr" target="#b10">[11]</ref>, to generate object proposals, and then rank the boxlevel objects according to similarity score among visonlanguage embeddings. With the same attempts, we tend to follow the same top-down strategy, utilize an off-theshelf instance segmentation method to perceive candidate objects. However, different from existing methods which mainly realize the box-level object feature matching as the main task and consider the segmentation mask as a byproduct of the modular comprehension procedure by simply replacing the output heads. With the object feature constructed on bounding boxes, these methods would not handle the occlusions among objects in a video, especially for crowded scenes. In this work, we try to directly model the object feature based on fine-grained segmentation masks to learn more discriminative object features. Additionally, we further explore multi-modal relationship modeling among high-level visual object features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Text-based Video Segmentation</head><p>Certain success has been achieved in referring image segmentation. Beyond image domain, the temporal coherence of referred video objects is still waiting to be explored. Recently, Gavrilyuk et al. <ref type="bibr" target="#b9">[10]</ref> extend Actor-Action Dataset (A2D) with human-annotated sentences and introduce the challenging task of actor and action video segmentation from referring expressions. They adopt language guided dynamic convolution filters to fuse the multi-modal feature. Since then, bottom-up methods have sprung up. Wang et al. <ref type="bibr" target="#b36">[37]</ref> utilize asymmetric attention mechanisms to facilitate visual guided linguistic feature learning. Later, they <ref type="bibr" target="#b35">[36]</ref> extend vanilla dynamic convolution with a context modulated dynamic convolution kernel. Ning et al. <ref type="bibr" target="#b31">[32]</ref> convert spatial relations to terms of direction and range for better linguistic spatial formulation. McIntosh et al. <ref type="bibr" target="#b30">[31]</ref> introduce a capsule-based approach for better capturing the relationship between multi-modal features. For further mining continuous temporal information, they extend the A2D dataset with annotations for all frames.</p><p>In this work, with the concrete objects obtained with the instance segmentation module, we explicitly exploit temporal coherence among all frames in a video including annotated key-frames and unlabeled frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Top-Down Pipeline</head><p>Previous work <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b31">32]</ref> tackles Text-based Video Segmentation task from a bottom-up pixel-level way. Differently, we view it as a cross-modal retrieval problem by decomposing the task into two stages. The first stage is to find out all potential candidate objects and their masks in the video, and the second one is to populate information among objects and select the best matched candidate given the referring sentence.</p><p>Suppose a video V has M frames, where each frame f i contains k i candidate objects {x 1 i , x 2 i , ..., x ki i }. Our target is to retrieve a set of referring objects T * = {x * 1 , x * 2 , ..., x * M } in a video by a natural language referring sentence L. The sentence L is composed of a sequence of words (w 1 , w 2 , ..., w N l ), where N l is the length of input sentence.</p><p>The task is taking the language description L to retrieve the target object track T from a video. Linguistic Embedding Construction. The language expression L is first processed via a bi-LSTM <ref type="bibr" target="#b16">[17]</ref>, where the hidden states {h 1 , h 2 , ? ? ? h N l } are further encoded by a self-guided attention module. In particular, the linguistic embedding L can be obtained by:</p><formula xml:id="formula_0">L = MLP( N l i=1 ? i h i ),<label>(1)</label></formula><p>where ? is the word-level attention weights that calculated by ? i = softmax(fc(h i )) and MLP denotes the multilayer perception. The self-guided attention module introduces a flexible way for the language encoder to focus on keywords and reduce the negative impact caused by sentence truncation or padding. Following we illustrate our designed top-down pipeline for this task.</p><p>Mask Out Foreground Objects. To localize objects in videos, we first build an instance segmentation model by considering the visual content only. Specifically, we use CondInst <ref type="bibr" target="#b34">[35]</ref> as our backbone, and train the model using all the object masks. Then we apply the instance segmentation model on each frame and detect and segment all the foreground objects as candidates. Note that we do not exploit additional data/annotations via the instance segmentation model. Denote the object segmentation masks in frame f i as {o j } Nv j=1 , where N v is the number of candidates. Individual Object features. We then obtain the j-th individual object feature v j by max-pooling on mask-cropped feature map extracted from the visual CNN model. Formally, the process could be achieved by,</p><formula xml:id="formula_1">v j = MLP(Max(F v o j )),<label>(2)</label></formula><p>where is element-wise multiplication, Max stands for global max pooling and F v denotes feature map of the entire frame. During training, the CNN model is updated in an end-to-end manner. Via Eq. 2, we build individual object feature v j by applying its instance segmentation mask o j on top of the CNN feature map. Find Visual-Linguistic Match. From the top-down perceptive, the final target is to select a best match among all candidate objects given the language input. Thus we train our model to maximize the matching score between referring object track and the language representation by,</p><formula xml:id="formula_2">T * = arg max M i=1 S(v * i , L).<label>(3)</label></formula><p>Therefore, the core of the problem is to learn a proper visual object embedding that distinguishes the target objects from others. In the later sections, details about how to learn discriminative multi-modal embeddings are introduced. </p><formula xml:id="formula_3">? Matrix Multiplication ? ? Video Clip " # # $ # % # &amp; ' $ ' % ' &amp; ( ) # ) * ) +</formula><p>, -, . , / , 0 <ref type="figure">Figure 2</ref>. The framework of our proposed ClawCraneNet. As a top-down pipeline, objects are first perceived by an off-the-shelf instance segmentation module, and then selected by finding best visual-semantic match. During this process, we populate information among objects by performing three kinds of relation formulation module, i.e., positional relation, text-guided semantic relation, and temporal relation. We then utilize linguistic embedding to retrieve the final prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Object-level Visual Embedding Construction</head><p>As many entities exist in a visual scene, semantic information of describing entity categories is not enough for distinguishing them. Therefore, it is natural to populate information among all the candidates to facilitate retrieving progress. In this section, on the basis of individual object representation, we further leverage three kinds of objectlevel relation, i.e., positional relation, text-guided semantic relation, and inter-frame temporal relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Positional Relation Module.</head><p>Positional information is crucial in depicting a object in images/videos. We design a Positional Relation Module (PRM) to encode the object-level spatial information with</p><formula xml:id="formula_4">p i = (x i min , y i min , x i max , y i max , x i c , y i c , w i , h i , r x i , r y i ) , where (x i min , y i min ), (x i max , y i max ), (x i c , y i c )</formula><p>, w i and h i are the normalized top-left coordinates, bottom-right coordinates, center coordinates, width and height of the smallest circumscribed box of segment o i , respectively. The last two dimensions r x i and r y i are the normalized relative position index according to the x-axis and y-axis coordinates. Then, spatial enhanced object features V i is calculated from:</p><formula xml:id="formula_5">V i = v i + W p (p i ),<label>(4)</label></formula><p>where W p is a learnable matrix. With the explicitly modeling of relative position information, our network earns the ability to handle the referring like "the second from the left" which is hard for low-level networks to infer with pixel-level spatial encoding only. We show by experiments that even with such slight guidance, our model does learn to comprehend relative spatial descriptions. During the training phase, to enhance the model to be position-aware, we randomly horizontal flip the frame image and swap the corresponding direction textual descriptions (e.g., changing from "right" to "left").</p><p>Text-guided Semantic Relation Module. On formulating intra-frame object-level relationships, a simple idea is to employ the common-used vanilla attention module as a relation formulator. However, vanilla attention boosts information exchange naively based on feature similarity, which is hard to distinguish with the within-modal similarity. To diminish the gap, we introduce text guidance into the semantic relation module. As the referring example illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, comparing with directly forming the relationship between "Guy" and "Ball", it is easier for the network to infer the "Ball" if it has already known that the "Guy" is "Kicking" something. Based on the aforementioned motivation, we devise a Text-guided Semantic Relation Module (TSRM) to leverage relational expression in language description. As illustrated in <ref type="figure">Figure 3</ref>, TSRM takes concatenated object features f V = (V 1 , V 2 , ..., V Nv ) and the sentence L as inputs. TSRM first learns self-guided weights for representing relationship-aware linguistic feature f t by,</p><formula xml:id="formula_6">f o = W o (Concat(f V , f t )),<label>(5)</label></formula><p>where Concat(, ) represents the concatenation operation along the channel axis, W o is a learnable matrix. Next in relation stage, the original visual feature is utilized to query the multi-modal feature. Given the query f q = W q f V , the key f k = W k f o , and the value f v = W v f o , the process to get text-guided object features F o = (V r 1 , V r 2 , ..., V r Nv ) could be formulated as,</p><formula xml:id="formula_7">F o = f V + softmax( f q f T k ? d k )f v ,<label>(6)</label></formula><p>where V r i is the relation enhanced object feature. During  <ref type="figure">Figure 3</ref>. Illustration of the text-guided semantic relation module. ?: Matrix Multiplication; c : Matrix Concatenation; Three boxes with different colors stand for three different objects. Self-guided linguistic context L is used as a guidance to infer the relationship between object features fV . the procedure, each object could earn query-focused global context information especially when there is a strong response between the visual object and linguistic description. Temporal Relation Module. To deal with blurry or complicated scenes in a video, a natural idea is to borrow the confident judgment from a clear scene for tackling hard scenes. In this part, we employ a tracking-based strategy to meet this purpose. Particularly, cross-frame objects are associated based on visual similarities to form a track. Given any two object (x i , x j ) from adjacent frames, the association similarity S s is formulated as follows:</p><formula xml:id="formula_8">(/ 0 , 1 0 ) (/ 2 , 1 2 ) (1 2 ) (C, ) (/ 0 , 1 2 ) (/ 0 , 1 0 + 1 2 ) (/ 0 , 1 0 ) (/ 0 , 1 4 ) (/ 0 , 1 4 ) (/ 0 , / 0 ) * 5 (/ 0 , 1 0 ) 6++78+9)8(* ; , * &lt; , * 0 )</formula><formula xml:id="formula_9">S s (x i , x j ) = S c (V r i , V r j ) + ? * U (x i , x j ),<label>(7)</label></formula><p>where S c denotes the cosine similarity and U represents the mask IoU. Following the multi-object tracking strategy in <ref type="bibr" target="#b43">[44]</ref>, we maintain several active tracks initialized from the first frame by treating each candidate object as an exclusive track. For each frame, we compute the visual similarity between the all active tracks and all candidate object embeddings in the current frame according to Eq. 7. The association procedure is allowed, only when the visual similarity is greater than a threshold ? and the active track would be ended if it is not updated for ? matching rounds. The Hungarian algorithm <ref type="bibr" target="#b23">[24]</ref> is applied to perform multiobject matching. After the aforementioned procedure, unassigned segments will start new tracks and repeat the object association until the final frame. Since the target object may appear or disappear in internal frames, it's reasonable to allow intermittent tracks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training and Inference</head><p>Once all the comprehension procedure has been done, the final set of visual embeddings {V r 1 , V r 2 , ? ? ? , V r Nv } and linguistic embedding L are obtained. We calculate the cosine similarity between the linguistic embedding and each visual candidate, V r i T L. Here we enforce all vectors to be L2-normalized feature embeddings, i.e., ||V r i || = 1, ||L|| = 1. We adopt the contrastive learning loss for optimizing the model,</p><formula xml:id="formula_10">s i = exp(V r i T L/? ) Nv j=1 exp(V r j T L/? ) ,<label>(8)</label></formula><formula xml:id="formula_11">loss = ? log(s gt ),<label>(9)</label></formula><p>where s gt is the matching score of the ground-truth object, ? is a temperature parameter that controls the concentration level of the distribution. Higher ? leads to a softer probability distribution. We set ? = 0.1 in our experiments. During the inference phase, our network first extracts multi-modal embeddings for each frame. Then, Temporal Relation Module is conducted to obtain the candidate tracks. The final track is retrieved by choosing the candidate track with the highest matched candidate.</p><p>Besides, as a prerequisite for the object association step, visual embeddings belonging to the same object are implicitly pulled together since they are all expected to be close with the same linguistic embedding. More explanations are conducted in supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Criteria</head><p>We conduct our experiments on two extended datasets: A2D Sentences and J-HMDB Sentences. These datasets are released in <ref type="bibr" target="#b9">[10]</ref> by additionally providing corresponding human natural descriptions on original A2D <ref type="bibr" target="#b42">[43]</ref> and J-HMDB <ref type="bibr" target="#b18">[19]</ref> respectively. A2D Sentences contains 3782 videos in total with 8 action classes performed by 7 actor classes. Each video in A2D has 3 to 5 frames annotated with pixel-level actor-action segmentation masks. Besides, it contains 6,655 sentences corresponding to actors and their actions. Following settings in <ref type="bibr" target="#b36">[37]</ref>, we split the whole dataset into 3017 training videos, 737 testing videos, and 28 unlabeled videos. J-HMDB Sentences contains 928 short videos with 928 corresponding sentences describing 21 different action classes. Pixel-wise 2D articulated human puppet masks are provided for evaluating segmentation performance.</p><p>The proposed method is evaluated with the criteria of Intersection-over-Union (IoU) and precision. The overall IoU computes the ratio of the total intersection area divided by the total union area over testing samples. The mean IoU is the averaged IoU over all samples, which treats samples of different sizes equally. We also measure preci-sion@K which considers the percentage of testing samples whose IoU scores are higher than threshold K at 5 different </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Our network is built on a one-stage instance segmentation method named CondInst <ref type="bibr" target="#b34">[35]</ref> for balanced performance and speed. It could be replaced with any other instance segmentation network. This model is initialized from ResNet101 <ref type="bibr" target="#b11">[12]</ref> pre-trained on ImageNet <ref type="bibr" target="#b7">[8]</ref> and further trained exclusively on A2D <ref type="bibr" target="#b42">[43]</ref>. Note that we do not leverage any additional data/annotations when building the instance segmentation module.</p><p>For visual and linguistic feature extractor, we adopt ResNet50 <ref type="bibr" target="#b11">[12]</ref> model pre-trained on ImageNet <ref type="bibr" target="#b7">[8]</ref> as visual backbone and bi-LSTM <ref type="bibr" target="#b16">[17]</ref> as text encoder. All input frames are resized to 320 ? 320. Following the settings in <ref type="bibr" target="#b9">[10]</ref>, the maximum length of sentences is set to 20 and the dimension of word vector is 1000. We employ the hidden states of bi-LSTM <ref type="bibr" target="#b16">[17]</ref> as sentence features with a dimension of 2000. The word embeddings are initialized with one-hot vectors without any pre-trained weights applied. The cross frame entity association threshold ? is set to 0.8 by default. Training is done with Adam optimizer <ref type="bibr" target="#b22">[23]</ref> with an initial learning rate of 0.0001, and a scheduler that waits for 2 epochs after loss stagnation to reduce the learning rate by a factor of 10. The batch size is 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-Art Methods</head><p>We compare our ClawCraneNet with other state-of-theart text-based video segmentation models following the set-tings in <ref type="bibr" target="#b9">[10]</ref> on the two datasets, i.e., A2D Sentences and J-HMDB Sentences. The comparison results are demonstrated in <ref type="table">Table 1 and Table 2</ref>. First on A2D Sentences, we evaluate <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25]</ref> pre-trained on ReferIt dataset <ref type="bibr" target="#b21">[22]</ref> and then fine-tuned version on A2D sentences. Other methods including ours are trained on A2D Sentences exclusively. As shown in <ref type="table">Table 1</ref>, with the help of object-level relation comprehension, our approach achieves state-of-the-art performance on most metrics with a remarkable margin, especially at higher IoU thresholds. On P @0.8, our method outperforms the SOTA by a large margin of 16.7%. Moreover, we bring 6.8% improvement on Mean IoU and 10.4% in mAP over SOTA respectively, which directly proves the effectiveness of our method. In spite of such obvious achievement on mean IoU, we get relatively poor performance on overall IoU. Owing to the special favor of large objects, overall IoU lacks the perception for smaller objects which is crucial for reflecting model performance. It seems ClawCraneNet not only captures obvious larger objects but also learns the object-level semantic context for distinguishing small objects. Besides, we found our method is also efficient (high FPS) compared to other bottom-up methods.</p><p>On J-HMDB Sentences, for fair comparisons, we follow the setting in <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b9">10]</ref>, and evaluate our model pre-trained on A2D sentences without any additional finetuning, which is kept the same as other compared methods. Our approach significantly outperforms previous state-ofthe-art methods on all metrics considered. For the result of P @0.9, one possible reason is that the ground truth masks from J-HMDB Sentences are generated from puppets. It's hard to fit the data distribution with a model trained from precise segmentation masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head><p>Effectiveness of Top-Down Pipeline. We first investigate the effectiveness of our designed Top-Down Pipeline. We evaluated the basic top-down pipeline (segment-embedretrieve pipeline), which ignores all the relation information among candidate objects and removes all relation-based modules. The results are reported the results in <ref type="table" target="#tab_2">Table 3</ref>. Compared to state-of-the-art bottom-up methods shown in <ref type="table">Table 1</ref>, our top-down pipeline achieves significantly better performances, especially on high precision predictions. Impact of Instance Segmentation Modules. In our experiments, for better trade-off between time cost and performance, we employ a one-stage segmentation method <ref type="bibr" target="#b34">[35]</ref>. The performance of our ClawCraneNet with different instance segmentation methods is shown in <ref type="table">Table 4</ref>. We use CondInst(R-101-FPN) as the off-the-shelf instance segmentation module by default. With weaker instance segmentation models, we still shows competitive performance compared with bottom-up approaches. Impact of Positional Relation Module. As shown in <ref type="table" target="#tab_2">Table 3</ref>, we have tried to adding different positional encoding methods, and achieved significant improvements compared to the basic top-down pipeline. We can conclude that position information is very useful for the top-down framework of text-based video segmentation task. In addition, we found our full Positional Relation Module (PRM) achieves better performances compared to absolute and relative position encoding methods. The reason is that absolute position encoding lacks the sensation for relative description like "the second from left" and thus it is hard for exclusively relative encoding to balance the weights for formulating absolute information and relative information. Impact of Text-guided Semantic Relation Module. We further evaluate effectiveness of the proposed TSRM. As shown in <ref type="table" target="#tab_2">Table 3</ref> (5-6 row), the vanilla object-level selfattention does not benefit the performance. But certain improvement occurs when introducing text guide to formulate object-level relations. A possible reason is that the plain visual-based relation module cannot correctly gather relational information by just measuring context similarities. But with more linguistic information contained, concrete relations could be formulated, leading to a positive impact on the performance. Impact of Temporal Relation Module. By fully utilizing the temporal coherence, our ClawCraneNet with temporal relation module (the last row in <ref type="table" target="#tab_2">Table 3</ref>) further enhanced outperforms all the other models which validate the effects of the design. Conclusively, these results confirm the merits of the object-level relation formulation again.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative Analysis</head><p>We would like to investigate the internal mechanism in ClawCraneNet by analyzing qualitative results. Compared with the bottom-up method ((b) of <ref type="figure">Figure 4</ref>, <ref type="figure">Figure 5</ref>), our top-down pipeline shows reasonable segmentation results, while the other messes up the relational information and lead to ambiguous foreground masks. As shown in <ref type="figure">Figure 4</ref> (c) and (d), when introducing the text-guided semantic relation module, our network learns to capture mutual information of corresponding objects. Visualization examples of text-guided attention weights are shown in <ref type="figure">Figure 4</ref> (f). The comparisons between complete ClawCraneNet and alternative structures are illustrated in <ref type="figure">Figure 5</ref>. Only a part of the objects are labeled with language description in A2D Sentences, and we illustrate the unmentioned objects in <ref type="figure">Figure 5</ref> with purple masks. Without the relative position module, the model tends to focus on objects that match the absolute position description "left", resulting in a wrong prediction. Ourtemporal relation module helps to achieve temporal consistency among frames, and correct the misunderstanding of the previous module ( <ref type="figure">Figure 5 (e)</ref>). In con-   <ref type="table" target="#tab_2">Table 3</ref>). (e) Results of our full ClawCraneNet. clusion, these visualized results show the effectiveness of our top-down design and the object-relational modules in ClawCraneNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a novel ClawCraneNet following the segment-comprehend-retrieve strategy for the first attempt of introducing object-level relation into text-based video segmentation field. Different from previous bottomup methods, our ClawCraneNet maximizes the semantic in-formation flow between object-level features by fully investigating the relationship between intra-frame and interframe objects, i.e., positional relation, text-guided semantic relation, and inter-frame temporal relation. Evaluations on commonly used benchmark datasets demonstrate that ClawCraneNet surpasses all the state-of-the-art methods by large margins.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Analysis of Puppet Mask in J-HMDB</head><p>In this section, we give a brief explanation about the poor performance of P @0.9 on J-HMDB Sentences <ref type="bibr" target="#b18">[19]</ref> dataset (Line 8 in <ref type="table">Table 2</ref>). As shown in <ref type="figure" target="#fig_5">Figure 6</ref>, ground-truth masks from J-HMDB Sentences <ref type="bibr" target="#b18">[19]</ref> are performed by puppets which leads to inconsistency between the segmentation mask and the actual object. Since the evaluated model is trained from precise segmentation masks on A2D Sentences <ref type="bibr" target="#b42">[43]</ref>, it is hard for ClawCraneNet to fit the data distribution of J-HMDB without fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Analysis of Object Embedding</head><p>We visualize predicted embeddings of 12 language queries for all candidate objects on a randomly selected video in A2D Sentences <ref type="bibr" target="#b42">[43]</ref> validation set and use t-SNE <ref type="bibr" target="#b28">[29]</ref> to embed visual object embeddings (256-dim) into a 2D space. As shown in <ref type="figure" target="#fig_6">Figure 7</ref>, embeddings belonging to different objects have clearly distinguishable margins, confirming that ClawCraneNet learns discriminative object embeddings which meets the prerequisites of temporal relation module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. More Details about Predicted Results</head><p>In <ref type="figure" target="#fig_7">Figure 8</ref>, we detail the predictions of our ClawCraneNet. In each sub-figure, we give the original RGB frame and language query as input, then plot the predicted results of our ClawCraneNet and ACGA <ref type="bibr" target="#b36">[37]</ref>. All candidates refers to candidate objects perceived by the instance segmentation module.</p><p>Thanks to the object-level comprehension of ClawCraneNet, reasonable results with clear boundaries are generated. Specifically, compared to bottom-up methods which mainly focus on salient objects, our network could entirely perceive inconspicuous objects and distinguish them with semantic context. Some failure cases are shown in <ref type="figure" target="#fig_8">Figure 9</ref>, it is still hard for ClawCraneNet to handle some visual ambiguity, i.e., objects moving at high speed <ref type="figure" target="#fig_8">(Figure 9 (a)</ref>) or objects in the mirror <ref type="figure" target="#fig_8">(Figure 9 (c)</ref>). An interesting observation is that with ambiguous language description like Man in red running, multiple fitting objects can be highlighted with higher similarity scores, as shown in <ref type="figure" target="#fig_8">Figure 9</ref> (b). To give more examples, we supply some results in form of videos within the supplementary material.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>""Figure 1 .</head><label>1</label><figDesc>Guy in all black kicking a ball" "Guy in all black kicking a ball" "Guy in all black kicking a ball" "Guy in all black kicking a ball" Guy in all black kicking a ball" (a) Previous bottom-up methods mainly perform semantic relationship formulation at the pixel level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Query: 1 .</head><label>1</label><figDesc>Guy in all black kicking a ball 2. Ball flying into the sky 3. A man in green tshirt is giving a pass</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 . 1 . Man in uniform walking on the bottom left 2 .Figure 5 .</head><label>4125</label><figDesc>Qualitative results of text-based video segmentation. We show three language query, and draw the corresponding segmentation results using the same query color. As queries 1 and 3 are predicted on a single object, the left most object in the first row of (c) is covered with both red (query 1) and green (query 3). (a) Original frames. (b) Results of the bottom-up method<ref type="bibr" target="#b36">[37]</ref>. (c) Results of our basic top-down pipeline (row 1 inTable 3).(d) Results of our full model. (e) Ground truth. (f) Visualization of attention weights in our TSRM. Query: Man in uniform walking at last in the second row from left 3. Large guy is walking in the middle of the street 4. Bottom right man in uniform walking on the street 5Visualization results of a complex video. An object is covered with different colors if it is referred by more than one queries, e.g., the left most object in the first row of (c). (a) Original video frame. (b) Results of the bottom-up method [37]. (c) Results of our top-down pipeline. (d) Results of the PRM-enhanced top-down model (row 4 in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative comparison between the predicted mask by ClawCraneNet and the ground-truth puppet mask.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>T-SNE Visualizations of learned object embeddings from ClawCraneNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Qualitative results of text-based video segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Failure cases of text-based video segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>"Guy in all black kicking a ball" Word Embedding Bi-LSTM Res Net Instance Segmentation Object Feature Construction Temporal Relation Module Positional Relation Module</head><label></label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Text-guided Semantic Relation Module</cell></row><row><cell></cell><cell>Language-guided</cell><cell>Excitation</cell></row><row><cell>MLP</cell><cell>Language-guided</cell><cell>Excitation</cell></row><row><cell>MLP</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2</head><label>12</label><figDesc>P@0.6 P@0.7 P@0.8 P@0.9 0.5:0.95 Overall Mean Hu et al. Comparison with state-of-the-art methods on the A2D Sentences using IoU and Precision@K as metrics.</figDesc><table><row><cell>Methods</cell><cell cols="2">P@0.5 [14] ECCV16 34.8</cell><cell>23.6</cell><cell cols="2">Overlap 13.3</cell><cell>3.3</cell><cell>0.1</cell><cell>mAP 13.2</cell><cell>47.4</cell><cell>IoU</cell><cell>35.0</cell><cell>FPS -</cell></row><row><cell cols="2">Li et al. [25] CVPR17</cell><cell>38.7</cell><cell>29.0</cell><cell></cell><cell>17.5</cell><cell>6.6</cell><cell>0.1</cell><cell>16.3</cell><cell>51.5</cell><cell></cell><cell>35.4</cell><cell>-</cell></row><row><cell cols="2">Gavrilyuk et al. [10] CVPR18</cell><cell>53.8</cell><cell>43.7</cell><cell></cell><cell>31.8</cell><cell>17.1</cell><cell>2.1</cell><cell>26.9</cell><cell>57.4</cell><cell></cell><cell>48.1</cell><cell>-</cell></row><row><cell cols="2">Wang et al. [37] ICCV19</cell><cell>55.7</cell><cell>45.9</cell><cell></cell><cell>31.9</cell><cell>16.0</cell><cell>2.0</cell><cell>27.4</cell><cell>60.1</cell><cell></cell><cell>49.0 8.64</cell></row><row><cell cols="2">McIntosh et al. [31] CVPR20</cell><cell>52.6</cell><cell>45.0</cell><cell></cell><cell>34.5</cell><cell>20.7</cell><cell>3.6</cell><cell>30.3</cell><cell>56.8</cell><cell></cell><cell>46.0</cell><cell>-</cell></row><row><cell cols="2">Wang et al. [36] AAAI20</cell><cell>60.7</cell><cell>52.5</cell><cell></cell><cell>40.5</cell><cell>23.5</cell><cell>4.5</cell><cell>33.3</cell><cell>62.3</cell><cell></cell><cell>53.1 7.18</cell></row><row><cell cols="2">Ning et al. [32] IJCAI20</cell><cell>63.4</cell><cell>57.9</cell><cell></cell><cell>48.3</cell><cell>32.2</cell><cell>8.3</cell><cell>38.8</cell><cell>66.1</cell><cell></cell><cell>52.9 5.42</cell></row><row><cell>Ours</cell><cell></cell><cell>70.4</cell><cell>67.7</cell><cell></cell><cell>61.7</cell><cell>48.9</cell><cell>17.1</cell><cell>49.4</cell><cell>63.1</cell><cell></cell><cell>59.9 9.27</cell></row><row><cell cols="2">Methods</cell><cell cols="10">Overlap P@0.5 P@0.6 P@0.7 P@0.8 P@0.9 0.5:0.95 Overall Mean mAP IoU</cell></row><row><cell cols="2">Hu et al. [14] ECCV16</cell><cell>63.3</cell><cell cols="2">35.0</cell><cell>8.5</cell><cell>0.2</cell><cell>0.0</cell><cell>17.8</cell><cell cols="2">54.6</cell><cell>52.8</cell></row><row><cell cols="2">Li et al. [25] CVPR17</cell><cell>57.8</cell><cell cols="2">33.5</cell><cell>10.3</cell><cell>0.6</cell><cell>0.0</cell><cell>17.3</cell><cell cols="2">52.9</cell><cell>49.1</cell></row><row><cell cols="2">Gavrilyuk et al. [10] CVPR18</cell><cell>71.2</cell><cell cols="2">51.8</cell><cell>26.4</cell><cell>3.0</cell><cell>0.0</cell><cell>26.7</cell><cell cols="2">55.5</cell><cell>57.0</cell></row><row><cell cols="2">Wang et al. [37] ICCV19</cell><cell>75.6</cell><cell cols="2">56.4</cell><cell>28.7</cell><cell>3.4</cell><cell>0.0</cell><cell>28.9</cell><cell cols="2">57.6</cell><cell>58.4</cell></row><row><cell cols="2">McIntosh et al. [31] CVPR20</cell><cell>67.7</cell><cell cols="2">51.3</cell><cell>28.3</cell><cell>5.1</cell><cell>0.0</cell><cell>26.1</cell><cell cols="2">53.5</cell><cell>55.0</cell></row><row><cell cols="2">Wang et al. [36] AAAI20</cell><cell>74.2</cell><cell cols="2">58.7</cell><cell>31.6</cell><cell>4.7</cell><cell>0.0</cell><cell>30.1</cell><cell cols="2">55.4</cell><cell>57.6</cell></row><row><cell cols="2">Ning et al. [32] IJCAI20</cell><cell>69.1</cell><cell cols="2">57.2</cell><cell>31.9</cell><cell>6.0</cell><cell>0.1</cell><cell>29.4</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell>Ours</cell><cell></cell><cell>88.0</cell><cell cols="2">79.6</cell><cell>56.6</cell><cell>14.7</cell><cell>0.2</cell><cell>43.3</cell><cell cols="2">64.4</cell><cell>65.5</cell></row></table><note>. Comparison with state-of-the-arts on the J-HMDB Sentences dataset.IoU thresholds and calculate mean average precision over 0.50:0.05:0.95 [10].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation studies on A2D Sentences. PE indicates Position Encoding. Positional Relation Module, Text-guided Semantic Relation Module, and Temporal Relation Module are abbreviated as "PRM", "TSRM", and "TRM", respectively.</figDesc><table><row><cell></cell><cell>Methods</cell><cell></cell><cell cols="7">Overlap P@0.5 P@0.6 P@0.7 P@0.8 P@0.9 0.5:0.95 Overall Mean mAP IoU</cell></row><row><cell></cell><cell cols="2">Top-Down Pipeline</cell><cell>64.4</cell><cell>62.1</cell><cell>56.9</cell><cell>45.3</cell><cell>16.4</cell><cell>45.6</cell><cell>58.4</cell><cell>55.1</cell></row><row><cell></cell><cell>+ Absolute PE</cell><cell></cell><cell>66.8</cell><cell>64.2</cell><cell>58.7</cell><cell>46.9</cell><cell>16.4</cell><cell>47.0</cell><cell>60.6</cell><cell>56.9</cell></row><row><cell></cell><cell>+ Relative PE</cell><cell></cell><cell>66.3</cell><cell>63.9</cell><cell>60.2</cell><cell>46.9</cell><cell>16.3</cell><cell>46.9</cell><cell>60.2</cell><cell>56.7</cell></row><row><cell></cell><cell>+ PRM</cell><cell></cell><cell>67.2</cell><cell>64.5</cell><cell>58.9</cell><cell>47.2</cell><cell>16.4</cell><cell>47.2</cell><cell>61.1</cell><cell>57.4</cell></row><row><cell></cell><cell cols="2">+ PRM + Vanilla-attention</cell><cell>67.5</cell><cell>64.6</cell><cell>59.2</cell><cell>47.3</cell><cell>16.6</cell><cell>47.3</cell><cell>61.0</cell><cell>57.6</cell></row><row><cell></cell><cell>+ PRM + TSRM</cell><cell></cell><cell>68.6</cell><cell>66.0</cell><cell>60.2</cell><cell>48.1</cell><cell>16.8</cell><cell>48.3</cell><cell>62.3</cell><cell>58.6</cell></row><row><cell></cell><cell cols="2">+ PRM + TSRM + TRM</cell><cell>70.4</cell><cell>67.7</cell><cell>61.7</cell><cell>48.9</cell><cell>17.1</cell><cell>49.4</cell><cell>63.1</cell><cell>59.9</cell></row><row><cell>Methods</cell><cell>Backbone</cell><cell cols="3">mAP 0.5:0.95 Overall Mean IoU</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CondInst</cell><cell>R-101-FPN</cell><cell>49.4</cell><cell>63.1</cell><cell>59.9</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CondInst</cell><cell>R-50-FPN</cell><cell>48.3</cell><cell>62.7</cell><cell>59.4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Mask R-CNN R-50-FPN</cell><cell>48.1</cell><cell>62.4</cell><cell>58.8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Table 4. Impact of Instance Segmentation Modules. With weaker</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">modules, we still get better performance than SOTAs.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">and Anton van den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niko</forename><surname>S?nderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3674" to="3683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">G3raphground: Graph-based language grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanjun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4281" to="4290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">How people look at pictures: a study of the psychology and perception in art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><forename type="middle">Thomas</forename><surname>Buswell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">See-through-text grouping for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding-Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Chen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwann-Tzong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyng-Luh</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7454" to="7463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Referring expression object segmentation with caption-aware consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Wen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">263</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Video captioning with attention-based lstm and semantic consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianli</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2045" to="2055" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Actor and action video segmentation from a sentence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirill</forename><surname>Gavrilyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="5958" to="5966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Meaning-based guidance of attention in scenes as revealed by meaning maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor R</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hayes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="743" to="747" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Segmentation from natural language expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bi-directional relationship inferring network for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="4424" to="4433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Referring image segmentation via cross-modal progressive comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaofei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luoqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="10488" to="10497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Linguistic structure guided context modeling for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaofei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sansi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faxi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="59" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Person-centered cognition: The presence of people in a visual scene promotes relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David A Kalkstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaacov</forename><surname>Hackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trope</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Social Psychology</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Neural correlates of actual and predicted memory formation. Nature neuroscience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Ching</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John De</forename><surname>Gabrieli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1776" to="1783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Referitgame: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahar</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="787" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold W Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval research logistics quarterly</title>
		<imprint>
			<date type="published" when="1955" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Tracking by natural language specification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6495" to="6503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recurrent multimodal interaction for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1271" to="1280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dynamic multimodal instance segmentation guided by natural language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Margffoy-Tuay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Botero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arbel?ez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visual-textual capsule routing for text-based video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Mcintosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yogesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="9942" to="9951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Polar relative positional encoding for video-language segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<editor>IJ-CAI</editor>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Referring image segmentation by generative adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikui</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1333" to="1344" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Video object grounding using semantic roles in language description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arka</forename><surname>Sadhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="10417" to="10427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Conditional convolutions for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020. 3</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Context modulated dynamic networks for actor and action video segmentation with language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="12152" to="12159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Asymmetric cross-guided attention network for actor and action video segmentation from natural language query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Video captioning via hierarchical reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4213" to="4222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Five factors that guide attention in visual search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeremy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd S</forename><surname>Wolfe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Phrasecut: Language-based image segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="10216" to="10225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Decoupled novel object captioner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dual attention matching for audio-visual event localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Can humans fly? action understanding with multiple classes of actors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shao-Hang</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Segment as points for efficient online multi-object tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liusheng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A fast and accurate onestage approach to visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4683" to="4693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Grounding-tracking-integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCSVT</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cross-modal self-attention network for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linwei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrigank</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10502" to="10511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanzeng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4651" to="4659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Mattnet: Modular attention network for referring expression comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1307" to="1315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Where does it exist: Spatio-temporal video grounding for multi-form sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huasheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianli</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10668" to="10677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Vision-dialog navigation by exploring cross-modal memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengda</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohuan</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingqian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10730" to="10739" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
