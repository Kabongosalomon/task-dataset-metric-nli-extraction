<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GENESIS-V2: Inferring Unordered Object Representations without Iterative Refinement</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
							<email>martin@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Applied AI Lab</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oiwi</forename><forename type="middle">Parker</forename><surname>Jones</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Applied AI Lab</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Posner</surname></persName>
							<email>ingmar@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Applied AI Lab</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GENESIS-V2: Inferring Unordered Object Representations without Iterative Refinement</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Advances in unsupervised learning of object-representations have culminated in the development of a broad range of methods for unsupervised object segmentation and interpretable object-centric scene generation. These methods, however, are limited to simulated and real-world datasets with limited visual complexity. Moreover, object representations are often inferred using RNNs which do not scale well to large images or iterative refinement which avoids imposing an unnatural ordering on objects in an image but requires the a priori initialisation of a fixed number of object representations. In contrast to established paradigms, this work proposes an embedding-based approach in which embeddings of pixels are clustered in a differentiable fashion using a stochastic stick-breaking process. Similar to iterative refinement, this clustering procedure also leads to randomly ordered object representations, but without the need of initialising a fixed number of clusters a priori. This is used to develop a new model, GENESIS-V2, which can infer a variable number of object representations without using RNNs or iterative refinement. We show that GENESIS-V2 performs strongly in comparison to recent baselines in terms of unsupervised image segmentation and object-centric scene generation on established synthetic datasets as well as more complex real-world datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Reasoning about discrete objects in an environment is foundational to how agents perceive their surroundings and act in it. For example, autonomous vehicles need to identify and respond to other road users (e.g. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>) and robotic manipulation tasks involve grasping and pushing individual objects (e.g. <ref type="bibr" target="#b2">[3]</ref>). While supervised methods can identify selected objects (e.g. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>), it is intractable to manually collect labels for every possible object category. Furthermore, we often desire the ability to predict, or imagine, how a collection of objects might behave (e.g. <ref type="bibr" target="#b5">[6]</ref>). A range of works have thus explored unsupervised segmentation and object-centric generation in recent years (e.g. ). These models are often formulated as variational autoencoders (VAEs) <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref> which allow the joint learning of inference and generation networks to identify objects in images and to generate scenes in an object-centric fashion (e.g. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28]</ref>).</p><p>Moreover, such models require a differentiable mechanism for separating objects in an image. While some works use spatial transformer networks (STNs) <ref type="bibr" target="#b38">[39]</ref> to process crops that contain objects (e.g. <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>), others directly predict pixel-wise instance segmentation masks (e.g. <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref>). The latter avoids the use of fixed-size sampling grids which are ill-suited for objects of varying size. Instead, object representations are inferred either by iteratively refining a set of randomly initialised representations (e.g. <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref>) or by using a recurrent neural networks (RNN) (e.g. <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>). One particularly interesting model of the latter category is <ref type="bibr">GENESIS</ref>  <ref type="bibr" target="#b16">[17]</ref> can perform both scene segmentation and generation by capturing relationships between objects with an autoregressive prior.</p><p>As noted in Novotny et al. <ref type="bibr" target="#b39">[40]</ref>, however, using RNNs for instance segmentation requires processing high-dimensional inputs in a sequential fashion which is computationally expensive and does not scale well to large images with potentially many objects. We also posit that recurrent inference is not only problematic from a computational point of view, but that it can also inhibit the learning of object representations by imposing an unnatural ordering on objects. In particular, we argue that this leads to different object slots receiving gradients of varying magnitude which provides a possible explanation for models collapsing to a single object slot during training, unless the flexibility of the model is restricted (see <ref type="bibr" target="#b17">[18]</ref>). While iterative refinement instead infers unordered object representations, it requires the a priori initialisation of a fixed number of object slots even though the number of objects in an image is unknown.</p><p>In contrast, our work takes inspiration from the literature on supervised instance segmentation and adopts an instance colouring approach (e.g. <ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref>) in which pixel-wise embeddings-or colours-are clustered into attention masks. Typically, either a supervised learning signal is used to obtain cluster seeds (e.g. <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>) or clustering is performed as a non-differentiable post-processing operation (e.g. <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>). Neither of these approaches is suitable for unsupervised, end-to-end learning of segmentation masks. We hence develop an instance colouring stick-breaking process (IC-SBP) to cluster embeddings in a differentiable fashion. This is achieved by stochastically sampling cluster seeds from the pixel embeddings to perform a soft grouping of the embeddings into a set of randomly ordered attention masks. It is therefore possible to infer object representations both without imposing a fixed ordering or performing iterative refinement.</p><p>Inspired by <ref type="bibr">GENESIS</ref>  <ref type="bibr" target="#b16">[17]</ref>, we leverage the IC-SBP to develop GENESIS-V2, a novel model that learns to segment objects in images without supervision and that uses an autoregressive prior to generate scenes in an interpretable, object-centric fashion. GENESIS-V2 is comprehensively benchmarked against recent prior art <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24]</ref> on established synthetic datasets-ObjectsRoom <ref type="bibr" target="#b43">[44]</ref> and ShapeStacks <ref type="bibr" target="#b44">[45]</ref>-where it performs strongly in comparison to several recent baselines. We also evaluate GENESIS-V2 on more challenging real-world images from the Sketchy <ref type="bibr" target="#b45">[46]</ref> and the MIT-Princeton Amazon Picking Challenge (APC) 2016 Object Segmentation datasets <ref type="bibr" target="#b46">[47]</ref>, where it also achieves promising results. Code and pre-trained models are available at https://github.com/applied-ai-lab/genesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Unsupervised models for learning object representations are typically formulated either as autoencoders (e.g. <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref>) or generative adversarial networks (GANs) (e.g. <ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref>). Typical GANs are able to generate images, but lack an associated inference mechanism and often suffer from training instabilities (see e.g. <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49]</ref>). A comprehensive review and discussion of the subject is provided in Greff et al. <ref type="bibr" target="#b49">[50]</ref>.</p><p>In order to infer object representations, STNs <ref type="bibr" target="#b38">[39]</ref> can explicitly disentangle object location by cropping out a rectangular region from an input, allowing object appearance to be modelled in a canonical pose (e.g. <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>). This operation, however, relies on a fixed-size sampling grid which is not well-suited if objects vary broadly in terms of scale. In addition, gradients are usually obtained via bi-linear interpolation and are therefore limited to the extent of the sampling grid which can impede training: for example, if the sampling grid does not overlap with any object, then its location cannot be updated in a meaningful way. In contrast, purely segmentation based approaches <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref>) often use RNNs (e.g. <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>) or iterative refinement (e.g. <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref>) to infer object representations from an image. Other works either use a fixed number of slots <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> or group pixels in a non-differentiable fashion <ref type="bibr" target="#b26">[27]</ref>. RNN based models need to learn a fixed strategy that sequentially attends to different regions in an image, but this imposes an unnatural ordering on objects in an image. Avoiding such a fixed ordering leads to a routing problem. One way to address this is by randomly initialising a set of object representations and iteratively refining them. More broadly, this is also related to Deep Set Prediction Networks <ref type="bibr" target="#b50">[51]</ref>, where a set is iteratively refined in a gradient-based fashion. The main disadvantage of iterative refinement is that it is necessary to initialise a fixed number of clusters a priori, even though ideally we would like the number of clusters to be input-dependent. This is directly facilitated by the proposed IC-SBP. The IC-SBP and GENESIS-V2 are in this respect also related to the Stick-Breaking VAE <ref type="bibr" target="#b51">[52]</ref> which uses a stochastic number of latent variables, but does not attempt to explicitly capture the object-based structure of visual scenes.</p><p>Unlike some other works which learn unsupervised object representations from video sequences (e.g. <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>), our work considers the more difficult task of learning such representations from individual images alone. GENESIS-V2 is most directly related to <ref type="bibr">GENESIS</ref>  <ref type="bibr" target="#b16">[17]</ref> and SLOT-ATTENTION <ref type="bibr" target="#b23">[24]</ref>. Like GENESIS, the model is formulated as a VAE to perform both object segmentation and object-centric scene generation, whereby the latter is facilitated by an autoregressive prior. Similar to SLOT-ATTENTION, the model uses a shared convolutional encoder to extract a feature map from which features are pooled via an attention mechanism to infer object representations with a random ordering. In contrast to SLOT-ATTENTION, however, the attention masks are obtained with a parameter-free clustering algorithm that does not require iterative refinement or a predefined number of clusters. Both GENESIS and SLOT-ATTENTION are only evaluated on synthetic datasets. In this work, we use synthetic datasets for quantitative benchmarking, but we also perform experiments on two more challenging real-world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GENESIS-V2</head><p>An image x of height H, width W , with C channels, and pixel values in the interval [0, 1] is considered to be a three-dimensional tensor x ? [0, 1] H?W ?C . This work is only concerned with RGB images where C = 3, but other input modalities with a different number of channels could also be considered. Assigning individual pixels to object-like scene components can be formulated as obtaining a set of object masks ? ? [0, 1] H?W ?K with k ? i,j,k = 1 for all pixel coordinate tuples (i, j) in an image, where K is the number of scene components. Inspired by prior works (e.g. <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22]</ref>) and identical to Engelcke et al. <ref type="bibr" target="#b16">[17]</ref>, this is achieved by modelling the image likelihood p ? (x|z 1:K ) as an SGMM of the form</p><formula xml:id="formula_0">log p ? (x | z 1:K ) = H i=1 W j=1 C c=1 log K k=1 ? i,j,k (z 1:K ) N (? i,j,c (z k ), ? 2 x ) .<label>(1)</label></formula><p>The parameters ? of the model are learned ? x is a fixed standard deviation that is shared across object slots. The summation in Equation <ref type="formula" target="#formula_0">(1)</ref> implies that the likelihood is permutation-invariant to the order of the object representations z 1:K (see e.g. <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54]</ref>) provided that ? i,j,k (z 1:K ) is also permutation-invariant. This allows the generative model to accommodate for a variable number of object representations.</p><p>To segment objects in images and to generate synthetic images in an object-centric fashion requires the formulation of appropriate inference and generative models, i.e. q ? (z 1:K | x) and p ? (x | z 1:K ) p ? (z 1:K ), respectively, where ? are also learnable parameters. In the generative model, it is necessary to model relationships between object representations to facilitate the generation of coherent scenes. Inspired by GENESIS <ref type="bibr" target="#b16">[17]</ref>, this is facilitated by an autoregressive prior</p><formula xml:id="formula_1">p ? (z 1:K ) = K k=1 p ? (z k | z 1:k?1 ) .<label>(2)</label></formula><p>GENESIS uses two sets of latent variables to encode object masks and appearances separately. In contrast, GENESIS-V2 uses one set of latent variables z 1:K to encode both, which increases parameter sharing. The graphical model of GENESIS-V2 is shown next to related models in Appendix A.</p><p>While GENESIS relies on a recurrent mechanism in the inference model to predict segmentation masks, GENESIS-V2 instead infers latent variables without imposing a fixed ordering and assumes object latents z 1:K to be conditionally independent given an input image x, i.e., q ? (z 1:K |x) = k q ? (z k |x). Specifically, GENESIS-V2 first extracts an encoding with a deterministic UNet backbone. This encoding is used to predict a map of semi-convolutional pixel embeddings ? ? R H?W ?D ? (see <ref type="bibr" target="#b39">[40]</ref>). Semi-convolutional embeddings are introduced in Novotny et al. <ref type="bibr" target="#b39">[40]</ref> to facilitate the prediction of unique embeddings for multiple objects of identical appearance. The embeddings are computed by performing an element-wise addition of pixel coordinates to two dimensions of the embeddings. In this work, we let the pixel coordinates be in the interval [?1, 1] relative to the image centre. Subsequently, the IC-SBP converts the embeddings into a set of normalised attention masks m ? [0, 1] H?W ?K with k m i,j,k = 1 via a distance kernel ?. The spatial structure of the embeddings should induce the attention masks to be spatially localised, but this is not a hard constraint. In addition, we derive principled initialisations for the scaling-factor of different IC-SBP distance kernels ? in Section 3.2. <ref type="figure">Figure 1</ref>: GENESIS-V2 overview. The image x is passed into a deterministic backbone. The resulting encoding is used to compute the pixel embeddings ? which are clustered into attention masks m 1:K by the IC-SBP. Features are pooled according to these attention masks to infer the object latents z 1:K . These are decoded into the object masks ? 1:K and reconstructed components x 1:K .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IC-SBP</head><p>Inspired by Locatello et al. <ref type="bibr" target="#b23">[24]</ref>, GENESIS-V2 uses the attention masks m 1:K to pool a feature vector for each scene component from the deterministic image encoding. A set of object latents z 1:K is then computed from these feature vectors. This set of latents is decoded in parallel to compute the statistics of the SGMM in Equation <ref type="formula" target="#formula_0">(1)</ref>. The object masks ? are normalised with a softmax operator. The inference and generation models can be jointly trained as a VAE as illustrated in <ref type="figure">Figure 1</ref>. Further architecture details are described in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Instance Colouring Stick-Breaking Process</head><p>The IC-SBP is a stochastic, differentiable algorithm that clusters pixel embeddings ? ? R H?W ?D ? into a variable number of soft attention masks m ? [0, 1] H?W ?K . Intuitively, this is achieved by (1) sampling the location (i, j) of a pixel that has not been assigned to a cluster yet, (2) creating a soft or hard cluster according to the distance of the embedding ? i,j at the selected pixel location to all other pixel embeddings according to a kernel ?, and (3) repeating the previous two steps until all pixels are explained or some form of stopping condition is reached. Crucially, the stochastic selection of pixel embeddings as cluster seeds leads to a set of randomly ordered soft clusters. Due to its conceptual similarity, the method derives its name from more formal stick-breaking process formulations as can, e.g., be found in Yuan et al. <ref type="bibr" target="#b10">[11]</ref> or Nalisnick and Smyth <ref type="bibr" target="#b51">[52]</ref>.</p><p>The IC-SBP is described more formally in Algorithm 1. Specifically and inspired by Burgess et al.</p><p>[16], a scope s ? [0, 1] H?W is initialised to a matrix of ones 1 H?W to track the degree to which pixels have been assigned to clusters. In addition, a matrix of seed scores is created once by sampling from a uniform distribution c ? U (0, 1) ? R H?W to perform the stochastic selection of pixel embeddings. At each iteration, a single embedding vector ? i,j is selected at the spatial location (i, j) which corresponds to the argmax of the element-wise multiplication of the seed scores and the current scope. This ensures that cluster seeds are sampled from pixel embeddings that have not yet been assigned to clusters. An alpha mask ? k ? [0, 1] H?W is computed as the distance between the cluster seed embedding ? i,j and all individual pixel embeddings according to a distance kernel ?. The output of the kernel ? is one if two embeddings are identical and decreases to zero as the distance between a pair of embeddings increases. The associated attention mask m k is obtained by the element-wise multiplication of the alpha masks by the current scope to ensure that the final set of attention masks is normalised. The scope is then updated by an element-wise multiplication with the complement of the alpha masks. This process is repeated until a stopping condition is satisfied, at which point the final scope is added as an additional mask to explain any remaining pixels.</p><p>In this work, we restrict ourselves to soft cluster assignment, leading to continuous attention masks with values in m k ? [0, 1] H?W . Unless the attention masks take binary values, several executions of the algorithm will lead to slightly different masks for individual objects. If the mask values are discrete and exactly equal to zero or one, however, then the set of cluster seeds and the set of attention masks are uniquely defined apart from their ordering. This can be inferred from the fact that if at each step of the IC-SBP produces a discrete mask, then embeddings associated with this mask cannot Algorithm 1: Instance Colouring Stick-Breaking Process</p><formula xml:id="formula_2">Input: embeddings ? ? R H?W ?D ? Output: masks m 1:K with m k ? [0, 1] H?W Initialise: masks m = ?, scope s = 1 H?W , seed scores c ? U (0, 1) ? R H?W while not StopCondition(m) do i, j = argmax(s c); ? = DistanceKernel(?, ? i,j ); m.append(s ?); s = s (1 ? ?); end m.append(s)</formula><p>be sampled as cluster seeds later on due to the masking of the seed scores by the scope. A different cluster with an associated discrete mask is therefore created at every step until all embeddings are uniquely clustered. Another interesting modification would be to use continuous masks while making the output of the IC-SBP permutation-equivariant with respect to the ordering of the cluster seeds. This could be achieved either by directly using the cluster seeds for downstream computations or by separating the mask normalisation from the stochastic seed selection. While the SBP formulation facilitates the selection of a diverse set of cluster seeds, the masks could be normalised separately after the cluster seeds are selected by using a softmax operation, for example. An investigation of these ideas is left for future work.</p><p>In contrast to GENESIS, the stochastic ordering of the masks implies that it is not possible for GENESIS-V2 to learn a fixed sequential decomposition strategy. While this does not strictly apply to the last mask which is set equal to the remaining scope, we find empirically that models learn a strategy where the final scope is either largely unused or where it corresponds to a generic background cluster with foreground objects remaining unordered as desired. Unlike as in iterative refinement where a fixed number of clusters needs to be initialised a priori, the IC-SBP can infer a variable number of object representations by using a heuristic that considers the current set of attention masks at every iteration in Algorithm 1. While we use a fixed number of K masks during training for efficient parallelism on GPU accelerators, we demonstrate that a flexible number of masks can be extracted at test time with minimal impact on segmentation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Kernel Initialisation with Semi-Convolutional Embeddings</head><p>For semi-convolutional embeddings to be similar according to a distance kernel ?, the model needs to learn to compensate for the addition of the relative pixel coordinates. It can achieve this by predicting a delta vector for each embedding to a specific pixel location, for example to the centre of the object that the embedding belongs to. A corollary of this is that if embeddings are equal to the relative pixel coordinates with the other dimensions being zero, then clustering embeddings based on their relative distances results in "blob-like", spatially localised masks. In this work, we make use of this property to derive a meaningful initialisation for free parameters in the distance kernel ? of the IC-SBP. Established candidates for ? from the literature are the Gaussian ? G <ref type="bibr" target="#b39">[40]</ref>, Laplacian ? L <ref type="bibr" target="#b39">[40]</ref>, and Epanechnikov ? E kernels <ref type="bibr" target="#b54">[55]</ref> with</p><formula xml:id="formula_3">? G = exp ? ||u?v|| 2 ? G , ? L = exp ? ||u?v|| ? L , ? E = max 1 ? ||u?v|| 2 ? E , 0 ,<label>(3)</label></formula><p>whereby u and v are two embeddings of equal dimension. Each kernel contains a scaling factor ? {G,L,E} ? R + . By initialising the model at the beginning of training so that the embeddings are equal to the relative pixel coordinates with the other dimensions being zero, then ? {G,L,E} can be initialised so that the initial attention masks are similarly-sized circular patches. In particular, we initialise these scaling factors as</p><formula xml:id="formula_4">? ?1 G = K ln 2, ? ?1 L = ? K ln 2, ? ?1 E = K/2 ,<label>(4)</label></formula><p>which is derived and illustrated in Appendix C. After initialisation, ? {G,L,E} is jointly optimised along with the other learnable parameters of the model as in Novotny et al. <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training</head><p>Following Engelcke et al. <ref type="bibr" target="#b16">[17]</ref>, GENESIS-V2 is trained by minimising the GECO objective <ref type="bibr" target="#b55">[56]</ref>, which can be written as a loss function of the form</p><formula xml:id="formula_5">L g = E q ? (z|x) [? ln p ? (x | z)] + ? g ? KL[q ? (z | x) || p ? (z)] .<label>(5)</label></formula><p>The relative weighting factor ? g ? R + is updated at every training iteration separately from the model parameters according to</p><formula xml:id="formula_6">? g = ? g ? e ?(C?E) with E = ? g ? E + (1 ? ? g ) ? E q ? (z|x) [? ln p ? (x | z)] .<label>(6)</label></formula><p>E ? R is an exponential moving average of the negative image log-likelihood, ? g ? [0, 1] is a momentum factor, ? ? R + is a step size hyperparameter, and C ? R is a target reconstruction error. Intuitively, the optimisation decreases the weighting of the KL (Kullback-Leibler) regularisation term as long as the reconstruction error is larger than the target C. The weighting of the KL term is increased again once the target is satisfied.</p><p>In some applications, a practitioner might only require segmentation masks in which case, so having to reconstruct the entire input would be rather inefficient. While we observed that the attention masks m are correlated to the object masks ?, they do not align as closely with object boundaries. We conjecture that this is a consequence of the large receptive field of the UNet backbone which spatially dilates information about objects. Consequently, we also conduct experiments with an additional auxiliary mask consistency loss that encourages attention masks m and object masks ? to be similar. This leads to a modified loss function of the form</p><formula xml:id="formula_7">L g = E + ? g ? KL[q ? (z | x) || p ? (z)] + KL[m || nograd(?)] ,<label>(7)</label></formula><p>in which m and ? are interpreted as pixel-wise categorical distributions. Preliminary experiments indicated that stopping the gradient propagation through the object masks ? helps to achieve segmentation quality comparable to using the original loss function in Equation <ref type="formula" target="#formula_5">(5)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>This section presents results on two simulated datasets-ObjectsRoom <ref type="bibr" target="#b43">[44]</ref> and ShapeStacks <ref type="bibr" target="#b44">[45]</ref>-as well as two real-world datasets-Sketchy <ref type="bibr" target="#b45">[46]</ref> and APC <ref type="bibr" target="#b46">[47]</ref>-which are described in Appendix D. GENESIS-V2 is compared against three recent baselines: GENESIS <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr">MONET</ref>  <ref type="bibr" target="#b15">[16]</ref>, and SLOT-ATTENTION <ref type="bibr" target="#b23">[24]</ref>. Even though SLOT-ATTENTION is trained with a pure reconstruction objective and is not a generative model, it is an informative and strong baseline for unsupervised scene segmentation. The other models are trained with the GECO objective <ref type="bibr" target="#b55">[56]</ref> following the protocol from Engelcke et al. <ref type="bibr" target="#b16">[17]</ref> for comparability. We refer to MONET trained with GECO as MONET-G to avoid conflating the results with the original settings. Further training details are described in Appendix E.</p><p>Following prior works (e.g. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref>), segmentation quality is quantified using the Adjusted Rand Index (ARI) <ref type="bibr" target="#b56">[57]</ref> and the Mean Segmentation Covering (MSC). The MSC is derived from Arbelaez et al. <ref type="bibr" target="#b57">[58]</ref> and described in detail in Engelcke et al. <ref type="bibr" target="#b16">[17]</ref>. These are by default computed using pixels belonging to ground truth foreground objects (ARI-FG and MSC-FG). Similar to Greff et al. <ref type="bibr" target="#b21">[22]</ref>, these are averaged over 320 images from respective test sets. We also report the Evidence Lower Bound (ELBO) averaged over 320 test images as a measure how well the generative models are able to fit the data. Generation quality is measured using the Fr?chet Inception Distance (FID) <ref type="bibr" target="#b58">[59]</ref> which is computed from 10,000 samples and 10,000 test set images using the implementation from Seitzer <ref type="bibr" target="#b59">[60]</ref>. When models are trained with multiple random seeds, we always show qualitative results for the seed that achieves the highest ARI-FG. In terms of the two real-world datasets, there are only ground truth segmentation masks available for the APC data, with the caveat that there is only a single foreground object per image. In this case, when computing the segmentation metrics from foreground pixels alone, the ARI-FG could be trivially maximised by assigning all image pixels to the same component and the MSC-FG would be equal to the largest IOU between the predicted masks and the foreground objects. When considering all pixels, the optimal solution for both metrics is to have exactly one set of pixels assigned to the foreground object and another set of pixels being assigned to the background. While acknowledging that segmenting the background as a single component is arguably not the only valid way of segmenting the background, we report the ARI and MSC using all pixels instead to develop a sense of how well foreground objects are separated from the background.   <ref type="figure" target="#fig_0">Figure 2</ref> shows qualitative results on ShapeStacks and additional qualitative results are included in Appendix F. GENESIS-V2 cleanly separates the foreground objects from the background and the reconstructions. Interestingly, GENESIS-V2 is the only model that segments the entire background as a single component. We conjecture that this behaviour is a consequence of the fact that the background structures are of finite variety. As a result, even when viewing only a fraction of the background, it is possible for the model to largely predict the appearance of the rest of the background. The KL regularisation during training encourages efficient compression of the inputs, which penalises redundant information between slots and might thus explain this behaviour. SLOT-ATTENTION is trained without KL regularisation and the decoders of MONET-G as well GENESIS are possibly not flexible enough to reconstruct the entire background as a single component (see Appendix B for architecture details).</p><p>In terms of quantitative performance, <ref type="table" target="#tab_0">Table 1</ref> summarises the segmentation results on ObjectsRoom and ShapeStacks. GENESIS-V2 outperforms the two generative baselines GENESIS and MONET-G across datasets on all metrics, showing that the IC-SBP is indeed suitable for learning object-centric representations. GENESIS-V2 outperforms the non-generative SLOT-ATTENTION baseline in terms of the ARI-FG on both datasets. SLOT-ATTENTION manages to achieve a better mean MSC-FG. The standard deviation of the MSC-FG values is much larger, though, which indicates training is not as stable. While the ARI-FG indicates the models ability to separate objects, it does not penalise the undersegmentation of objects (see <ref type="bibr" target="#b16">[17]</ref>). The MSC-FG, in contrast, is an IOU based metric and sensitive to the exact segmentation masks. We conjecture that SLOT-ATTENTION manages to predict slightly more accurate segmentation masks given that is trained on a pure reconstruction objective and without KL regularisation, thus leading to a slightly better mean MSC-FG. A set of ablations for GENESIS-V2 is also included in Appendix F.  We also examine whether the IC-SBP can indeed be used to extract a variable number of object representations. This is done by terminating the IC-SBP according to a manual heuristic and setting the final mask to the remaining scope. Specifically, we terminate the IC-SBP when the sum of the current attention mask values is smaller than 70 pixels for ObjectsRoom and 20 pixels for ShapeStacks. A larger threshold is used for the former as the attention masks tend to be more dilated (see Appendix F). The average number of used slots, the Mean Absolute Error (MAE) to the ideal number of slots, and segmentation metrics are when using a fixed number and a variable number slots after training are summarised in <ref type="table" target="#tab_1">Table 2</ref>. On both datasets, allowing for a flexible number requires fewer steps and achieves a smaller MAE. This is incurred, though, at a drop in segmentation performance. When training GENESIS-V2 with the auxiliary mask loss as in Equation <ref type="formula" target="#formula_7">(7)</ref> on ShapeStacks, the average number of steps and the MAE further decrease at no impact on the segmentation metrics. On ObjectsRoom, the auxiliary mask loss appeared to deteriorate the learning of good object segmentations and the associated results are therefore not included.</p><p>In terms of density estimation and scene generation, <ref type="table" target="#tab_2">Table 3</ref> summarises the ELBO values and FID scores for GENESIS-V2 and the baselines on ObjectsRoom and ShapeStacks. 2 GENESIS achieves a slightly better ELBO than GENESIS-V2 on ObjectsRoom, but GENESIS-V2 performs significantly better on ShapeStacks. We hypothesise that GENESIS benefits here from having a more flexible, autoregressive posterior than GENESIS-V2. Regarding scene generation, GENESIS-V2 consistently performs best with a particularly significant improvement on ShapeStacks. Results on ShapeStacks, however, are not as good as for ObjectsRoom, which is likely caused by the increased visual complexity of the images in the ShapeStacks dataset. Qualitative results for scene generation are shown in <ref type="figure">Figures 3 and 4</ref>. Both GENESIS and GENESIS-V2 produce reasonable samples after training on ObjectsRoom. For ShapeStacks, samples from GENESIS contain significant distortions. In comparison, samples from GENESIS-V2 are more realistic, but also still show room for improvement. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Real-World Applications</head><p>After validating GENESIS-V2 on two simulated datasets, this section present experiments on the Sketchy <ref type="bibr" target="#b45">[46]</ref> and APC datasets <ref type="bibr" target="#b46">[47]</ref>; two significantly more challenging real-world datasets collected in the context of robot manipulation. Due to the long training time on these datasets, each model is only trained with a single random seed. While this makes it infeasible to draw statistically strong conclusions, the aim of this section is to provide an indication and early exploration of how these models fare on more complex real-world datasets. Reconstructions and segmentations after training GENESIS-V2 on Sketchy and APC are shown in <ref type="figure" target="#fig_2">Figures 5 and 6</ref>. For Sketchy, it can be seen that GENESIS-V2 and SLOT-ATTENTION disambiguate the individual foreground objects and the robot gripper fairly well. SLOT-ATTENTION produces slightly more accurate reconstructions, which is likely facilitated by the pure reconstruction objective that the model is trained with. However, GENESIS-V2 is the only model that separates foreground objects from the background in APC images. Environment conditions in Sketchy are highly controlled and SLOT-ATTENTION appears to be unable to handle the more complex conditions in APC images. Nevertheless, GENESIS-V2 also struggles to capture the fine-grained details and oversegments one of the foreground objects into several parts, leaving room for improvement in future work. Additional qualitative results are included in Appendix F.   --</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GENESIS-V2</head><p>208.1 245.6 <ref type="table" target="#tab_3">Table 4</ref> reports the ARI and MSC scores computed from all pixels GENESIS-V2 and the baselines. GENESIS-V2 stands out in terms of both metrics, corroborating that GENESIS-V2 takes a valuable step towards learning unsupervised object-representations from real-world datasets. FID scores for generated images are summarised in <ref type="table" target="#tab_4">Table 5</ref> and qualitative results are included in Appendix F. GENESIS-V2 achieves the best FID on Sketchy, but it is outperformed by GENESIS on APC. Both models consistently outperform MONET-G. All of the FID scores, however, are fairly large which is not surprising given the much higher visual complexity of these images. It is therefore difficult to draw strong conclusions from these beyond a rough sense of sample quality. Further work is required to generate high-fidelity images after training on real-world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This work develops GENESIS-V2, a novel object-centric latent variable model of scenes which is able to both decompose visual scenes into semantically meaningful constituent parts while at the same time being able to generate coherent scenes in an object-centric fashion. GENESIS-V2 leverages a differentiable clustering algorithm for grouping pixel embeddings into a variable number of attention masks which are used to infer an unordered set of object representations. This approach is validated empirically on two established simulated datasets as well as two additional real-world datasets. The results show that GENESIS-V2 takes a step towards learning better object-centric representations without labelled supervision from real-world datasets.</p><p>In terms of future work, there is still room for improvement in terms of reconstruction, segmentation, and sample quality. It would also be interesting to investigate the ability of GENESIS-V2 and the IC-SBP to handle out-of-distribution images that contain more objects than seen during training. Moreover, several interesting variations of the IC-SBP were also already described in Section 3.1. Additional promising avenues include the extension of GENESIS-V2 to learn object representations from video (see e.g. <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>) or to leverage recent advances in hierarchical latent variable models such as Nouveau VAEs (NVAEs) <ref type="bibr" target="#b60">[61]</ref>.  <ref type="figure">Figure 7</ref>: Graphical model of GENESIS-V2 compared to a standard VAE <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref>, MONET <ref type="bibr" target="#b15">[16]</ref>, IODINE <ref type="bibr" target="#b21">[22]</ref>, and GENESIS <ref type="bibr" target="#b16">[17]</ref>. N denotes the number of refinement iterations in IODINE. GENESIS and GENESIS-V2 capture correlations between object slots with an autoregressive prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Graphical Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B GENESIS-V2 Architecture Details</head><p>The GENESIS-V2 architecture consists of four main components: a deterministic backbone, the attention and object pooling module, the component decoders, and an optional autoregressive prior which are described in detail below.</p><p>Backbone GENESIS-V2 uses a UNet <ref type="bibr" target="#b61">[62]</ref> encoder similar to the attention network in the reimplementation of MONET in Engelcke et al. <ref type="bibr" target="#b16">[17]</ref> with <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr">128,</ref><ref type="bibr">128,</ref><ref type="bibr">128]</ref> filters in the encoder and the reverse in the decoder. Each convolutional block decreases or increases the spatial resolution by a factor of two and there are two hidden layers with 128 units each in between the encoder and the decoder. The only difference to the UNet implementation in Engelcke et al. <ref type="bibr" target="#b16">[17]</ref> is that the instance normalisation (IN) layers <ref type="bibr" target="#b62">[63]</ref> are replaced with group normalisation (GN) layers <ref type="bibr" target="#b63">[64]</ref> to preserve contrast information. The number of groups is set to eight in all such layers which is also referred to as a GN8 layer. The output of this backbone encoder is a feature map e ? R H?W ?De with D e = 64 output channels and spatial dimensions that are equal to the height and width of the input image.</p><p>Attention and Object Pooling Following feature extraction, an attention head computes pixelwise semi-convolutional embeddings ? with eight channels, i.e. D ? = 8, as in Novotny et al. <ref type="bibr" target="#b39">[40]</ref>. The attention head consists of a 3 ? 3 Conv-GN8-ReLU block with 64 filters and a 1 ? 1 semiconvolutional layer. The pixel embeddings are clustered into K attention masks m 1:K using the IC-SBP. A Gaussian kernel ? G is used unless noted otherwise. A feature head consisting of a 3 ? 3 Conv-GN8-ReLU block with 64 filters and a 1 ? 1 convolution with 128 filters refines the encoder output e to obtain a new feature map f ? R H?W ?D f with D f = 128. Similar to Locatello et al. <ref type="bibr" target="#b23">[24]</ref>, the attention masks m 1:K are used to pool feature vectors from the feature map by multiplying the feature map with an individual attention mask and summing across the spatial dimensions. Each pooled feature vector is normalised by dividing by the sum of the attention mask values plus a small epsilon value to avoid numerical instabilities. Finally, a posterior head uses layer normalisation <ref type="bibr" target="#b64">[65]</ref> followed by a fully-connected ReLU block with 128 units and a second fully-connected layer to compute the sufficient statistics of the individual object latents z 1:K with z k ? R 64 from pooled feature vector.</p><p>Component Decoders Following Greff et al. <ref type="bibr" target="#b21">[22]</ref> and Locatello et al. <ref type="bibr" target="#b23">[24]</ref>, the object latents are decoded by separate decoders with shared weights to parameterise the sufficient statistics of the SGMM in Equation <ref type="bibr" target="#b0">(1)</ref>. Each decoded component has four channels per pixel. The first three channels contain the RGB values and the fourth channel contains the unnormalised segmentation logits which are normalised across scene components using a softmax operator. Again following Locatello et al. <ref type="bibr" target="#b23">[24]</ref>, the first layer is a spatial broadcasting module as introduced in Watters et al. <ref type="bibr" target="#b65">[66]</ref> which is designed to facilitate the disentanglement of the independent factors of variation in a dataset. An additional advantage of spatial broadcasting is that it requires a smaller number of parameters than a fully-connected layer when upsampling a feature vector to a specific spatial resolution. The spatial broadcasting module is followed by four 5 ? 5, stride-2 deconvolutional GN8-ReLU layers with 64 filters to retrieve the full image resolution before a final 1 ? 1 convolution which computes the four output channels. The use of stride-2 deconvolutional layers should make the GENESIS-V2 decoder more flexible compared to the counterparts used in MONET-G and GENESIS, which broadcast higher resolution and use stride-1 convolutions for decoding (see also <ref type="bibr" target="#b17">[18]</ref>.</p><p>Autoregressive Prior Identical to GENESIS <ref type="bibr" target="#b16">[17]</ref>, the autoregressive prior for scene generation is implemented as an LSTM <ref type="bibr" target="#b66">[67]</ref> followed by a fully-connected linear layer with 256 units to infer the sufficient statistics of the prior distribution for each component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Kernel Initialisation</head><p>Assume a maximum of K scene components to be present in an image and that model is initialised so that the pixel embeddings are equal to the relative pixel coordinates with the other dimensions being zero at the beginning of training. For each initial mask to cover approximately the same area of an image, further assume that the circular isocontours of the kernels are packed into an image in a square fashion. Using linear relative pixel coordinates in [?1, 1] and dividing an image into K equally sized squares, each square has a side-length of 2/ ? K. Let the mask value decrease to 0.5 at the intersection of the square and the circular isocontour, i.e., at a distance of 1/ ? K from the centre of the kernel as illustrated in <ref type="figure" target="#fig_4">Figure 8</ref>. Solving this for each kernel in Equation <ref type="formula" target="#formula_3">(3)</ref> leads to ? 0, 1/</p><formula xml:id="formula_8">? K = 0.5 ?? ? ?1 G = K ln 2, ? ?1 L = ? K ln 2, ? ?1 E = K/2 .<label>(8)</label></formula><p>Examples of the initial masks obtained when running the IC-SBP with the proposed initialisations are illustrated in <ref type="figure" target="#fig_5">Figure 9</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Datasets</head><p>We evaluate GENESIS-V2 on simulated images from ObjectsRoom <ref type="bibr" target="#b43">[44]</ref> and ShapeStacks <ref type="bibr" target="#b44">[45]</ref> as well as real-world images from Sketchy <ref type="bibr" target="#b45">[46]</ref> and APC <ref type="bibr" target="#b46">[47]</ref>. ObjectsRoom and ShapeStacks are well established in the context of this work and we follow the same preprocessing procedures as used in Engelcke et al. <ref type="bibr" target="#b16">[17]</ref> and Engelcke et al. <ref type="bibr" target="#b17">[18]</ref>. As in these works, the default number of object slots is set to K = 7 and K = 9 for ObjectsRoom and ShapeStacks, respectively, across all models. This work is the first to train and evaluate models that aim to learn object representations without supervision on Sketchy and APC. We therefore developed our own preprocessing and training/validation/test splits, which are described in detail below. The exact splits that were used will be released along with the code for reproducibility.</p><p>Sketchy The Sketchy dataset <ref type="bibr" target="#b45">[46]</ref> is designed for off-policy reinforcement learning (RL), providing episodes showing a robotic arm performing different tasks that involve three differently coloured shapes (blue, red, green) or a cloth. The dataset includes camera images from several viewpoints, depth images, manipulator joint information, rewards, and other meta-data. The dataset is quite considerable in size and takes about 5TB of storage in total. We ease the computational and storage demands by only using a subset of this dataset. Specifically, we use the high-quality demonstrations from the "lift-green" and "stack-green-on-red" tasks corresponding to a total of 395 episodes, 10% of which are set aside as validation and test sets each. Sketchy also contains episodes from a task that involves lifting a cloth and an even larger number of lower-quality demonstrations that offer a wider coverage of the state space. We restrict ourselves to the high-quality episodes that involve the manipulation of solid objects. The number of high-quality episodes alone is already considerable and we want to evaluate whether the models can separate multiple foreground objects. From these episodes, we use the images from the front-left and front-right cameras which show the arm and the foreground objects without obstruction.  <ref type="figure" target="#fig_6">Figure 10</ref>. The default number of object slots is set to K = 10 across all models to give them sufficient flexibility to discover different types of solutions. APC For their entry to the 2016 Amazon Picking Challenge (APC), the MIT-Princeton team created and released an object segmentation training set, showing a single challenge object either on a shelf or in a tray <ref type="bibr" target="#b46">[47]</ref>. The raw images are first resized so that the shorter image side has a length of 128 pixels. The centre 128-by-128 pixels are then extracted to remove uninteresting pixels belonging to the background. Example images after processing are shown in <ref type="figure" target="#fig_7">Figure 11</ref>. For each object, there exists a set of scenes showing the object in different poses on both the shelf and in the red tray. For each scene, there are images taken from different camera viewpoints. We select 10% of the scenes at random to be set aside for validation and testing each so that scenes between the training, validation, and test sets do not overlap. The resulting training, validation, and test sets consist of 109,281; 13,644; and 13,650 images, respectively. As for Sketchy, the default number of object slots is set to K = 10 to provide enough flexibility for models to discover different types of solutions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Training Details</head><p>Models apart from SLOT-ATTENTION are trained with the protocol from Engelcke et al. <ref type="bibr" target="#b16">[17]</ref> for comparability, which minimises the GECO objective <ref type="bibr" target="#b55">[56]</ref> using the Adam optimiser <ref type="bibr" target="#b67">[68]</ref>, a learning rate of 10 ?4 , a batch size of 32, and 500,000 training iterations. The Gaussian standard deviation ? x in Equation <ref type="formula" target="#formula_0">(1)</ref> is set to 0.7 and GECO reconstruction goal is set to a negative log-likelihood value per pixel and per channel of 0.5655 for the simulated datasets and the APC dataset. For Sketchy, a GECO goal of 0.5645 was found to lead to better segmentations and was used instead. As in Engelcke et al. <ref type="bibr" target="#b16">[17]</ref>, the GECO hyperparameters are set to ? g = 0.99, ? = 10 ?5 when C ? E and ? = 10 ?4 otherwise. ? g is initialised to 1.0 and clamped to a minimum value of 10 ?10 . For experiments with the auxiliary mask consistency loss in Equation <ref type="formula" target="#formula_7">(7)</ref>, we found that an initial high weighting of the mask loss inhibits the learning of good segmentations, so in these experiments ? g is initialised to 10 ?10 instead. We refer to MONET trained with GECO as MONET-G to avoid conflating the results with the original settings from Burgess et al. <ref type="bibr" target="#b15">[16]</ref>. <ref type="bibr">SLOT</ref> F Additional results <ref type="table" target="#tab_7">Table 6</ref> shows a set of ablations for GENESIS-V2 in terms of segmentation performance. A first set of experiments is conducted with an independent prior, the three different distance kernels described in Section 3.2, and semi-convolutional embeddings. The Gaussian kernel appears to perform most robustly and is therefore selected for all other experiments. A second set of experiments is conducted in which models are trained with an auto-regressive prior and either with a semi-convolutional or a standard convolutional output layer for obtaining pixel embeddings. Both the auto-regressive prior and the semi-convolutional operation improve segmentation performance.   <ref type="figure">Figure 13</ref>: Applying GENESIS-V2 several times to the same images from the ObjectsRoom dataset with three different random seeds shows that the model produces similar reconstructions and segmentations for each seed, but foreground objects are allocated to different slots as indicated by the segmentation colours.  <ref type="figure">Figure 15</ref>: Applying GENESIS-V2 several times to the same images from the ShapeStacks dataset with three different random seeds shows that the model produces similar reconstructions and segmentations for each seed, but components are allocated to different slots as indicated by the segmentation colours.   <ref type="figure" target="#fig_5">Figure 19</ref>: APC samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Potential Negative Societal Impacts</head><p>GENESIS-V2 is a generative model. Generative models can potentially be used spread disinformation by generating synthetic images for manipulative purposes. At this point in time, however, GENESIS-V2 is only able to generate plausible images when training on simulated images with limited visual complexity. A direct application of this method for malicious purposes is therefore unlikely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Third-Party Assets</head><p>GENESIS-V2 is implemented using PyTorch <ref type="bibr" target="#b68">[69]</ref>. In addition to various Python packages, we make use of several third-party assets: The datasets are publicly available under open-source licenses and consent was therefore not explicitly requested. To the best of our knowledge, none of the datasets contain personally identifiable information or offensive content.</p><formula xml:id="formula_9">?</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>GENESIS-V2 learns better reconstructions and segmentations on ShapeStacks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :Figure 5 :</head><label>345</label><figDesc>ObjectsRoom samples. ShapeStacks samples. In contrast to MONET-G and GENESIS, GENESIS-V2 as well as SLOT-ATTENTION are able to learn reasonable object segmentations on the more challenging Sketchy dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>GENESIS-V2 is the only model that separates the foreground objects in images from APC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Illustration of packing K = 4 circular kernels into a square image and linear relative pixel coordinates in [?1, 1], resulting in circular isocontours of radius 1/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Initial masks obtained when running the IC-SBP with different randomly sampled seed scores, using the initialisations in Equation(8)and K = 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :</head><label>10</label><figDesc>128-by-128 crops as used for training, extracted from the front-left and front-right cameras of a single image from the Sketchy dataset [46]. Showing from left to right: centre, top-left, top-centre, top-right, bottom-left, bottom-centre, and bottom-right crops.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :</head><label>11</label><figDesc>Examples from the APC dataset<ref type="bibr" target="#b46">[47]</ref> after cropping and resizing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 :</head><label>12</label><figDesc>ObjectsRoom reconstructions and segmentations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 14 :</head><label>14</label><figDesc>ShapeStacks reconstructions and segmentations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 17 :</head><label>17</label><figDesc>APC reconstructions and segmentations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 18 :</head><label>18</label><figDesc>Sketchy samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Kabra et al. [44] (Apache-2.0 License): ObjectsRoom dataset, ? Groth et al. [45] (GPL-3.0 License): ShapeStacks dataset, ? Cabi et al. [46] (Apache-2.0 License): Sketchy dataset, ? Zeng et al. [47] (BSD-2-Clause License): APC dataset, ? Engelcke et al. [17, 18] (GPL-3.0 License): Implementation of GENESIS and MONET-G, ? Locatello et al. [24] (Apache-2.0 License): Implementation of SLOT-ATTENTION, ? Seitzer [60] (Apache-2.0 License): FID computation in PyTorch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">ObjectsRoom</cell><cell cols="2">ShapeStacks</cell></row><row><cell>Model</cell><cell>Generative</cell><cell>ARI-FG</cell><cell>MSC-FG</cell><cell>ARI-FG</cell><cell>MSC-FG</cell></row><row><cell>MONET-G</cell><cell>Yes</cell><cell cols="4">0.54?0.00 0.33?0.01 0.70?0.04 0.57?0.12</cell></row></table><note>Means and standard deviations of the segmentation metrics from three seeds. Bold values in the first half of the table indicate the best values for the generative models; bold values in the second half indicate any better values achieved by the additional non-generative baseline.GENESIS Yes 0.63?0.03 0.53?0.07 0.70?0.05 0.67?0.02GENESIS-V2 Yes 0.85?0.01 0.59?0.01 0.81?0.01 0.67?0.01SLOT-ATTENTION No 0.79?0.02 0.64?0.13 0.76?0.01 0.70?0.05 4.1 Benchmarking in Simulation</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Means and standard deviations of the segmentation metrics from three seeds for GENESIS-V2 with a fixed or flexible number of object slots. Highlighting follows an analogous scheme as inTable 1.</figDesc><table><row><cell>Dataset</cell><cell>Training</cell><cell>Slots</cell><cell>Avg. K ?</cell><cell>MAE ?</cell><cell>ARI-FG ? MSC-FG ?</cell></row><row><cell cols="2">ObjectsRoom No mask loss</cell><cell>Fixed</cell><cell>7.0?0.0</cell><cell cols="2">3.3?0.0 0.85?0.01 0.59?0.01</cell></row><row><cell></cell><cell></cell><cell>Flexible</cell><cell>5.0?0.9</cell><cell cols="2">1.7?0.9 0.84?0.01 0.51?0.10</cell></row><row><cell>ShapeStacks</cell><cell>No mask loss</cell><cell>Fixed</cell><cell>9.0?0.0</cell><cell cols="2">4.4?0.0 0.81?0.01 0.67?0.01</cell></row><row><cell></cell><cell></cell><cell>Flexible</cell><cell>6.3?0.3</cell><cell cols="2">1.9?0.3 0.77?0.02 0.63?0.01</cell></row><row><cell></cell><cell cols="2">With mask loss Fixed</cell><cell>9.0?0.0</cell><cell cols="2">4.4?0.0 0.81?0.01 0.68?0.00</cell></row><row><cell></cell><cell></cell><cell>Flexible</cell><cell>5.7?0.2</cell><cell cols="2">1.1?0.02 0.81?0.01 0.68?0.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Means and standard deviations of ELBO values and FID scores from three seeds.</figDesc><table><row><cell>Model</cell><cell cols="2">ObjectsRoom</cell><cell cols="2">ShapeStacks</cell></row><row><cell></cell><cell>ELBO ?</cell><cell>FID ?</cell><cell>ELBO ?</cell><cell>FID ?</cell></row><row><cell>MONET-G</cell><cell cols="4">-7217?19 205.7?7.6 -7268?19 197.8?5.2</cell></row><row><cell>GENESIS</cell><cell>-7023?2</cell><cell cols="3">62.8?2.5 -7082?15 186.8?18.0</cell></row><row><cell>SLOT-ATT.</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GENESIS-V2</cell><cell>-7040?2</cell><cell>52.6?2.7</cell><cell>-7019?2</cell><cell>112.7?3.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Segmentation metrics on APC.</figDesc><table><row><cell></cell><cell>ARI MSC</cell></row><row><cell>MONET-G</cell><cell>0.11 0.48</cell></row><row><cell>GENESIS</cell><cell>0.04 0.29</cell></row><row><cell>SLOT-ATT.</cell><cell>0.03 0.25</cell></row><row><cell>GENESIS-V2</cell><cell>0.55 0.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>FID scores on Sketchy and APC.</figDesc><table><row><cell></cell><cell cols="2">Sketchy APC</cell></row><row><cell>MONET-G</cell><cell>294.3</cell><cell>269.3</cell></row><row><cell>GENESIS</cell><cell>241.9</cell><cell>183.2</cell></row></table><note>SLOT-ATT.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>The raw images have a resolution of 600-by-960 pixels. To remove uninteresting pixels belonging to the background, 144 pixels on the left and right are cropped away for both camera views, the top 71 and bottom 81 pixels are cropped away for the front-left view, and the top 91 and bottom 61 are cropped away for the front-right view, resulting in a 448-by-672 crop. From this 448-by-672 crop, seven square crops are extracted to obtain a variety of views for the models to learn from. The first crop corresponds to the centre 448-by-448 pixels. For the other six crops, the top and bottom left, centre, and right squares of size 352 are extracted. Finally, we resize these crops to a resolution of 128-by-128 to reduce the computational demands of training the models. This leads to a total of 337,498 training; 41,426 validation; and 41,426 test images. Examples of images obtained with this preprocessing procedure are shown in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>-ATTENTION is trained using the official reference implementation with default hyperparameters. Training on 64-by-64 images from ObjectsRoom and ShapeStacks takes around two days with a single NVIDIA Titan RTX GPU. Similarly, training on 128-by-128 images from Sketchy and APC takes around eight days.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>GENESIS-V2 ablations showing means and standard deviations from three seeds. Highlighting follows an analogous scheme as in Table 1. 79?0.01 0.47?0.17 0.79?0.01 0.67?0.00 No ? L Yes 0.74?0.08 0.48?0.20 0.79?0.01 0.67?0.01 No ? E Yes 0.78?0.01 0.34?0.08 0.78?0.01 0.66?0.01 Yes ? G Yes 0.84?0.01 0.58?0.03 0.81?0.00 0.68?0.01 Yes ? G No 0.79?0.05 0.59?0.02 0.60?0.38 0.56?0.21</figDesc><table><row><cell>ObjectsRoom</cell><cell>ShapeStacks</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that in contrast to the MONET-G training objective, the decoded mask distribution rather than the deterministic attention masks are used to compute the reconstruction likelihood in the ELBO calculation for MONET-G. The KL divergence between the two mask distributions as used in the training objective is not part of this ELBO calculation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">(a) MONET-G (b) GENESIS (c) GENESIS-V2</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Are We Ready for Autonomous Driving? The KITTI Vision Benchmark Suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The Cityscapes Dataset for Semantic Urban Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep Object-Centric Representations for Generalizable Robot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Coline</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Greedy Hierarchical Variational Autoencoders for Large-Scale Video Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suraj</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Martin-Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06362</idno>
		<title level="m">Efficient Inference in Occlusion-Aware Generative models of Images</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Infer, Repeat: Fast Scene Understanding with Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Sm Ali Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theophane</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Szepesvari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-Objects Generation with Amortized Structural Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spatially Invariant Unsupervised Object Detection with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Crawford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative Modeling of Infinite Occluded Objects for Compositional Scene Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SPACE: Unsupervised Object-Oriented Scene Representation via Spatial Attention and Decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Fu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skand</forename><surname>Vishwanath Peri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sequential Attend, Infer, Repeat: Generative Modelling of Moving Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SCALOR: Generative World Models with Scalable Object Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepehr</forename><surname>Janghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><forename type="middle">De</forename><surname>Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative Neurosymbolic Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Kabra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lerchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Monet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11390</idno>
		<title level="m">Unsupervised Scene Decomposition and Representation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oiwi</forename><forename type="middle">Parker</forename><surname>Adam R Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parker</forename><surname>Oiwi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Posner</surname></persName>
		</author>
		<title level="m">Reconstruction Bottlenecks in Object-Centric Generative Models. ICML Workshop on Object-Oriented Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tagger: Deep Unsupervised Perceptual Grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tele</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural Expectation Maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Sjoerd Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Relational Neural Expectation Maximization: Unsupervised Discovery of Objects and their Interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sjoerd Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-Object Representation Learning with Iterative Variational Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rapha?l</forename><surname>Lopez Kaufmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishab</forename><surname>Kabra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Entity Abstraction in Visual Model-Based Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishi</forename><surname>Veerapaneni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Co-Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Janner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning (CoRL)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Object-Centric Learning with Slot Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravindh</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stacked Capsule Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Adam R Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to Manipulate Individual Objects in an Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning Physical Graph Representations from Visual Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaofei</forename><surname>Bear</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhu</forename><surname>Mrowca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aran</forename><surname>Alter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Nayebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Titas</forename><surname>Anciukevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Christoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Henderson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00642</idno>
		<title level="m">Object-Centric Image Generation with Factored Depths, Locations, and Appearances</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A Case for Object Compositionality in Deep Generative Models of Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Sjoerd Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS Workshop on Modeling the Physical World: Learning, Perception, and Control</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised Object Segmentation by Redrawing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micka?l</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Arti?res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Emergence of Object Segmentation in Perturbed Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bielski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11369</idno>
		<title level="m">Object Discovery with a Copy-Pasting GAN</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samaneh</forename><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayna</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><forename type="middle">Darrell</forename><surname>Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.07560</idno>
		<title level="m">Learning Image-Conditional Binary Composition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">BlockGAN: Learning 3D Object-aware Scene Representations from Unlabelled Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thu</forename><surname>Nguyen-Phuoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Liang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niloy</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">RELATE: Physically Plausible Multi-Object Scene Synthesis Using Structured Latent Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Ehrhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aron</forename><surname>Monszpart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niloy</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Stochastic Backpropagation and Approximate Inference in Deep Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Spatial Transformer Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Semi-Convolutional Operators for Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10277</idno>
		<title level="m">Semantic Instance Segmentation via Deep Metric Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep Watershed Transform for Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Bert De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02551</idno>
		<title level="m">Semantic Instance Segmentation with a Discriminative Loss Function</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Kabra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Lopez Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
		<ptr target="https://github.com/deepmind/multi-object-datasets/" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Multi-Object Datasets</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">ShapeStacks: Learning Vision-Based Physical Intuition for Generalised Object Stacking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fabian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Scaling Data-Driven Robotics with Reward Sketching and Batch Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serkan</forename><surname>Cabi</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">G?mez</forename><surname>Colmenarejo</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Novikov</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ksenia</forename><surname>Konyushkova</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rae</forename><surname>Jeong</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Zolna</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Budden</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mel</forename><surname>Vecerik</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi-view Self-supervised Deep Learning for 6D Pose Estimation in the Amazon Picking Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Ting</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><surname>Walker</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Large Scale GAN Training for High Fidelity Natural Image Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Sjoerd Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.05208</idno>
		<title level="m">On the Binding Problem in Artificial Neural Networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep Set Prediction Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam Pr?gel-</forename><surname>Bennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Stick-Breaking Variational Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov, and Alexander Smola. Deep Sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">On the Limitations of Representing Functions on Sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Wagstaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael A</forename><surname>Osborne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Recurrent Pixel Embedding for Instance Grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00597</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Taming VAEs. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Comparing Partitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phipps</forename><surname>Arabie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Classification</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="193" to="218" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Contour Detection and Hierarchical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Seitzer</surname></persName>
		</author>
		<ptr target="https://github.com/mseitzer/pytorch-fid" />
		<title level="m">pytorch-fid: FID Score for PyTorch</title>
		<imprint>
			<date type="published" when="2020-08" />
		</imprint>
	</monogr>
	<note>Version 0.1.1</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">NVAE: A Deep Hierarchical Variational Autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Instance Normalization: The Missing Ingredient for Fast Stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Group Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer Normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lerchner</surname></persName>
		</author>
		<title level="m">Spatial Broadcast Decoder: A Simple Architecture for Learning Disentangled Representations in VAEs. ICLR Workshop on Learning from Limited Data</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Automatic Differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
