<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RELLIS-3D Dataset: Data, Benchmarks and Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Osteen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maggie</forename><surname>Wigness</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikanth</forename><surname>Saripalli</surname></persName>
						</author>
						<title level="a" type="main">RELLIS-3D Dataset: Data, Benchmarks and Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic scene understanding is crucial for robust and safe autonomous navigation, particularly so in off-road environments. Recent deep learning advances for 3D semantic segmentation rely heavily on large sets of training data, however existing autonomy datasets either represent urban environments or lack multimodal off-road data. We fill this gap with RELLIS-3D, a multimodal dataset collected in an off-road environment, which contains annotations for 13,556 LiDAR scans and 6,235 images. The data was collected on the Rellis Campus of Texas A&amp;M University, and presents challenges to existing algorithms related to class imbalance and environmental topography. Additionally, we evaluate the current state of the art deep learning semantic segmentation models on this dataset. Experimental results show that RELLIS-3D presents challenges for algorithms designed for segmentation in urban environments. This novel dataset provides the resources needed by researchers to continue to develop more advanced algorithms and investigate new research directions to enhance autonomous navigation in off-road environments. RELLIS-3D is available at https://github.com/unmannedlab/RELLIS-3D</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Autonomous navigation systems that rely solely on LiDAR and an inertial navigation system (INS) have been shown to perform poorly in off-road environments <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. These systems leverage geometric information but lack higher-level semantic understanding of the environment that could make path planning and navigation more efficient. For example, bushes are identified as obstacles by LiDAR, but for larger platforms, these parts of the environment may actually be traversable. The importance of semantic awareness has led to the emergence of visual perception systems that directly feed information to navigation systems to complement depth sensors <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[4]</ref>, <ref type="bibr" target="#b5">[5]</ref>, <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b7">[7]</ref>. LiDAR provides highly accurate 3D information, is not affected by varying illumination, and adds reflectance information to characterize object surfaces uniquely, whereas cameras provide dense color and texture information to obtain fine-grained semantic information.</p><p>Semantic segmentation for indoor and urban scenes has made great advancements using the numerous large-scale datasets available in these domains <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b9">[9]</ref>, <ref type="bibr" target="#b10">[10]</ref>, <ref type="bibr" target="#b11">[11]</ref>, <ref type="bibr" target="#b13">[12]</ref>, <ref type="bibr" target="#b14">[13]</ref>. Compared with urban road scenes and indoor environments, off-road environments have unstructured class boundaries, uneven terrain, strong textures, and irregular features that preclude the direct transfer of models between the different types of environments. Also, there are large differences in class distributions across distinct off-road environments. Although there are some off-road datasets available <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b15">[14]</ref>, <ref type="bibr" target="#b16">[15]</ref>, the autonomous navigation research community still lacks a multi-modal dataset with a large number of ground truth annotations for developing reliable autonomous robot systems in off-road environments.</p><p>To address the need for multi-modal data resources to advance autonomous navigation in off-road environments, we present RELLIS-3D, a novel dataset captured from a Clearpath Robotics Warthog platform (shown in <ref type="figure" target="#fig_0">Fig. 1</ref>). All data were recorded in the Ground Research facility on the Rellis Campus of Texas A&amp;M University, which is an offroad environment that includes different runways, aprons, terrain, forested areas, bushes, pastures, and lakes.</p><p>The RELLIS-3D dataset is comprised of a large set of raw sensor data synchronized with Precision Time Protocol (PTP), including color camera images, laser scans, highprecision global positioning measurements, inertial measurement from a combined Global Positioning and Inertial Navigation System (GPS/INS), and depth images from a 3D stereo camera. By including the full set of raw autonomy data, we facilitate additional algorithms, such as those that fuse visual and inertial/depth data, to be developed and tested without any new data collection.</p><p>Beyond the raw sensor data, RELLIS-3D will be released with ground truth annotations for a subset of the LiDAR </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Several datasets have been published in the last decade for scene understanding research for autonomous vehicles and robots. A large majority of existing data represent urban environments with mostly on-road navigation. Of these datasets, most only include 2D annotations (e.g., bounding boxes or region masks) for RGB camera images. Examples include CamVid <ref type="bibr" target="#b9">[9]</ref>, Cityscapes <ref type="bibr" target="#b10">[10]</ref>, Mapillary Vistas <ref type="bibr" target="#b19">[18]</ref>, D2-City <ref type="bibr" target="#b20">[19]</ref>, and BDD100k <ref type="bibr" target="#b22">[20]</ref>. However, RGB cameras are not the only sensors used in autonomous driving, therefore several multi-modal datasets have also been published, e.g., KITTI <ref type="bibr" target="#b23">[21]</ref>, nuScenes <ref type="bibr" target="#b11">[11]</ref> and A2D2 <ref type="bibr" target="#b24">[22]</ref>. And the SemanticKITTI <ref type="bibr" target="#b25">[23]</ref> further enrich KITTI by adding a large amount of semantic annotation for its LiDAR subset.</p><p>For off-road autonomous navigation research to mirror the progress made for operating in urban environments, high quality data resources must be made available to the research community. Yet, there are comparatively far fewer off-road datasets than those from urban environments. <ref type="table" target="#tab_0">Table I</ref> outlines the existing off-road datasets available and their data and annotation characteristics. RUGD <ref type="bibr" target="#b15">[14]</ref> is an RGB image dataset for semantic segmentation in off-road environments with a rich ontology and large set of ground truth annotations but lacks multiple modalities. The Freiburg Forest dataset <ref type="bibr" target="#b16">[15]</ref> provides multi-modal, multi-spectral image databut lacks additional sensor modalities, e.g., LiDAR, and only 366 images have ground truth annotations. YCOR <ref type="bibr" target="#b0">[1]</ref> provides both image and point cloud data, but only images are annotated. NREC Agricultural Person-Detection Dataset is a dataset for person detection in off-road environments, but only provides bounding box annotations for a single class. Dabbiru et al. <ref type="bibr" target="#b18">[17]</ref>, <ref type="bibr" target="#b26">[24]</ref> present a framework that generates simulated labeled point cloud data and trains a Convolution Neural Network (CNN) for LiDAR semantic segmentation, which could provide a route for sim2real transfer learning with real data such as RELLIS-3D. There is a clear gap in the available data for off-road navigation and RELLIS-3D fills this gap by providing a full stack of multi-modal sensor data and multi-modal annotation with a rich ontology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SENSOR SETUP AND CALIBRATION A. Sensors</head><p>Our sensor setup is illustrated in <ref type="figure" target="#fig_0">Fig.1</ref>  Hz IMU In addition to the sensor suite, our Warthog platform has two computers. The navigation computer is responsible for the robotic control, while the vision computer is devoted to data collection and sensor processing. The two computers communicate through Ethernet connection and are synchronized using PTP. Both computers run Ubuntu Linux (64 bit) and ROS Kinetic to collect the incoming data streams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Synchronization</head><p>We use PTP to synchronize the sensors throughout the computer and sensor network. The vision computer is synchronized with the navigation computer, Ouster Li-DAR, Stereo Camera, and RGB Camera. Unfortunately, the GPS/IMU system cannot be synchronized with PTP, however the GPS/IMU system provides updates at 100 Hz. In this case, we complete the synchronization by chosing the GPS/IMU information with the closest timestamp to the LiDAR and camera timestamp for a particular frame. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Camera Calibration</head><p>We assume our cameras fit a pin-hole projection model in which a 3D scene is projected on an image plane using a perspective transform. In addition to the standard projection parameters expressed by a calibration matrix K, real lenses also exhibit radial and tangential distortions given by distortion coefficients contained in a vector D. For this dataset, we use the ROS Camera Calibrator 1 library for intrinsic camera calibration. This package is a ROS wrapper for the camera calibration functionality provided in OpenCV <ref type="bibr" target="#b28">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. LiDAR &amp; Camera Calibration</head><p>The knowledge of the extrinsic calibration between sensors is of paramount importance for fusing information from all sensing modalities. Most perception and state estimation algorithms assume the extrinsic calibration to be known a-priori. We determine the extrinsic calibration between cameras and 3D-LiDARs using <ref type="bibr" target="#b29">[26]</ref>, <ref type="bibr" target="#b30">[27]</ref>, <ref type="bibr" target="#b31">[28]</ref>.</p><p>As described in the aforementioned works, in the absence of ground truth, we verify our algorithms by comparing the estimated parameters against the given factory stereo calibration and by projecting points lying on the edges of the planar target in the LiDAR frame on the camera image and calculating the mean line re-projection errors (MLRE). MLRE is an independent evaluation metric since the calibration algorithms described in <ref type="bibr" target="#b29">[26]</ref> and <ref type="bibr" target="#b30">[27]</ref> do not use it as a residual in their respective optimizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DATASET</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Description</head><p>RELLIS-3D includes five traversal sequences, and was collected on three non-paved trails of the Ground Research facility on the Rellis Campus of Texas A&amp;M University. 1 http://wiki.ros.org/camera_calibration Three sequences were recorded on the first trail that was covered with bushes and sparse trees. These sequences differ in the direction the robot is moving on the trail, and the day the data was collected. Another sequence captured the environment of the second trail that passes a pasture and traverses a forested area. The last sequence is recorded on a hill that is surrounded by a lake and highway. Sequences were collected by teleoperating the robot to follow the trail, and each sequence includes around fives minutes of data.</p><p>With the goal of providing multi-modal data to enhance autonomous off-road navigation, we defined an ontology of object and terrain classes, which largely derives from the RUGD dataset <ref type="bibr" target="#b15">[14]</ref> but also includes unique terrain and object classes not present in RUGD. Specifically, sequences from this dataset includes classes such as mud, man-made barriers, and rubble piles. Additionally, this dataset provides a finer-grained class structure for water sources, i.e., puddle and deep water, as these two classes present different traversability scenarios for most robotic platforms. Overall, 20 classes (including void class) are present in the data. The full ontology can be seen in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Annotations</head><p>Pixel-wise image annotations were provided by Appen 2 , a company that leverages crowdsourcing for training data annotation to be used for a variety of AI/ML tasks. To ensure annotation consistency, work was assigned to trained annotators, and a single annotator was assigned a sequence of frames from a single video sequence. Annotations from the crowdsourcing platform also underwent several rounds of in-house verification to correct missing, incorrect, or inconsistent labels. We downsample the camera stream to 5Hz for ground truth labeling, and do not duplicate annotations  for which the robot is stationary, resulting in a total of 6, 235 images with pixel-wise annotations.</p><p>Point-wise annotation for 3D point clouds is initialized by using the camera-LiDAR calibration to project the more than 6, 000 image annotations onto point clouds. Using the 3D point cloud annotation application provided by SemanticKITTI <ref type="bibr" target="#b25">[23]</ref>, annotations are refined for the multiple overlapped LiDAR scans. LiDAR scan alignment is crucial to obtain quality annotations and although our system has a highly accurate INS there are still map inconsistencies. To address this issue, we first register and loop close the sequences using a SLAM system <ref type="bibr" target="#b32">[29]</ref> and output each scan's position based on the SLAM results. Using this process the 13, 556 scans received full point-wise annotations, where each scan includes up to 13, 056 points. <ref type="figure" target="#fig_3">Figures 3 and 4</ref> show the class distribution breakdown for image and point cloud annotations, respectively. The class distribution among both modalities is highly imbalanced. For image annotations, sky, grass, tree, and bushes make up 94% of the total labeled pixels. Among the LiDAR data, grass, tree, and bushes make up 80% of the total point labels. Differences in resolution, viewing angles, and sensor mechanism leads to the divergence in label distributions between the image and point cloud data. For example, because of the sensor mechanism, LiDAR is unable to detect sky, but because the LiDAR has a 360 ? viewing angle it picks up more person labels than the imagery since human operators usually followed the robot during data collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Dataset Statistics</head><p>The non-uniform class distribution present in RELLIS-3D is common among datasets used for semantic segmentation <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b19">[18]</ref>, <ref type="bibr" target="#b25">[23]</ref>. Moreover, class imbalance is a problem that perception algorithms will encounter upon deployment. Although imbalanced class distributions exist in almost all current available urban datasets, the overall class distributions across distinct urban datasets are quite similar <ref type="bibr" target="#b25">[23]</ref>, <ref type="bibr" target="#b19">[18]</ref>. However, the class imbalances in off-road environments are highly dependent on the particular environment, and the class imbalance within a dataset is more severe, making the semantic segmentation of rare classes more challenging than for urban environments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. BENCHMARKS, EVALUATION METRICS AND EXPERIMENTS A. Evaluation Metrics</head><p>For semantic segmentation, we evaluate algorithm performance with the widely used mean intersection-over-union (mIoU) metric <ref type="bibr" target="#b33">[30]</ref>, given by</p><formula xml:id="formula_0">mIoU = 1 C C c=1 T P c T P c + F P c + F N c<label>(1)</label></formula><p>where T P c , F P c and F N c represent the number of true positive, false positive and false negative predictions for class c, and C is the number of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Image Semantic Segmentation</head><p>We provide an evaluation of 2D image semantic segmentation on our dataset using two state-of-the-art architectures: HRNETV2+OCR <ref type="bibr" target="#b34">[31]</ref>, <ref type="bibr" target="#b35">[32]</ref> and Gated-SCNN <ref type="bibr" target="#b36">[33]</ref>.</p><p>HRNETv2+OCR uses the High-Resolution Network (HRNet) <ref type="bibr" target="#b34">[31]</ref> as its backbone and the Object-Contextual Representations (OCR) model <ref type="bibr" target="#b35">[32]</ref> to explore the objectcontextual representation of each pixel. HRNet maintains high-resolution representations throughout the whole model, unlike most other backbones that downsample input first and then upsample the features. OCR improves the pixel representation by aggregating the pixel features lying in the object region.</p><p>Gated-SCNN <ref type="bibr" target="#b36">[33]</ref> is a two-stream CNN architecture for semantic segmentation that explicitly processes shape information in a separate branch, yielding a classical segmentation stream and a parallel shape stream. The architecture uses the higher-level activation in the classical stream to predict semantic segmentation and the low-level activation in the shape stream to abstract the shape information.</p><p>For this experiment, we split RELLIS-3D into a training set with 3,302 images, a validation set with 983 images, and a testing set with 1,672 images. When creating these splits, we tried to keep the training set large, while creating a testing set that was diverse including both similar and dissimilar scenarios seen in the training set. The image experiment uses 19 classes 3 (including void) for training and testing.  <ref type="table" target="#tab_0">Table II</ref> and <ref type="figure">Fig. 5</ref> shows the results of the image segmentation evaluation, which fall short of expectation. The Gated-SCNN model achieved only 52.92% mIoU, while it reached 74.7% mIoU <ref type="bibr" target="#b36">[33]</ref> on Cityscapes. HRNet+OCR achieved 51.55% mIoU, but reached 81.1% mIoU on Cityscapes. We believe the performance degradation is mainly caused by our dataset's serious class imbalance. This is supported by comparing <ref type="figure" target="#fig_3">Fig. 3 and Fig. 5</ref>. Notice that the classes log, pole, water, and building have the lowest IoUs, and also represent the classes with fewest labels. These incorrect detections are of importance as they affect navigation decisions; for example, water is not traversable, unlike puddle, and human-made poles in an off-road area might provide warning information. Beyond the imbalance, the off-road environment's unclear boundaries also cause problems for both algorithms and human labelers. For humans, the indefinite boundaries make annotation difficult as compared with an urban roadway. The GSCNN algorithm utilizes boundary information to help perform segmentation, but this might lead to performance degradation for classes such as bush, grass, and trees with unclear boundaries (see <ref type="table" target="#tab_0">Table II</ref>). For these classes, perfect boundary segmentation might not be necessary, but this problem inspires us to design new algorithms that can focus on the boundaries of specific classes, such as human-made objects and water.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. LiDAR Point Cloud Semantic Segmentation</head><p>There are two types of deep learning methods for LiDAR Point Cloud: point-based methods and the projective method <ref type="bibr" target="#b38">[34]</ref>. We provide an evaluation of point cloud semantic segmentation on our dataset using two state-of-the-art architectures: SalsaNext <ref type="bibr" target="#b39">[35]</ref> and KPConv <ref type="bibr" target="#b40">[36]</ref>.</p><p>SalsaNext is an uncertainty-aware semantic segmentation model for full 3D LiDAR point cloud in real-time. SalsaNext has an encoder-decoder architecture and works on projected LiDAR point cloud data. The encoder introduces a new residual dilated convolution stack with gradually increasing receptive fields, while the decoder uses the pixel-shuffle layer to recover the resolution instead of deconvolution or upsampling layers.</p><p>Kernel Point Convolution (KPConv) is a specified, designed point cloud convolution operation that is more flexible than fixed grid convolution. KPConv uses a series of local, 3D convolution kernels to apply to the input points close to them, using a k-d tree to find nearby points. The regular subsampling strategy in the paper makes the KPconv operation more efficient and robust to varying densities. In the experiment, we use the KP-FCNN architecture <ref type="bibr" target="#b40">[36]</ref> for semantic segmentation.</p><p>For the point cloud experiment, we follow the same data splits as for the image data. The training set has 7,800 scans, the validation set has 2,413 scans, and the testing set has 3,343 scans. Because the LiDAR scans are unable to establish points for classes such as sky, or objects far away (e.g., buildings in RELLIS-3D), the point cloud experiments use only the 15 classes with annotations (including void) for training and testing. <ref type="table" target="#tab_0">Table II</ref> and <ref type="figure">Fig. 5</ref> show the results of the point cloud semantic segmentation. SalsaNext achieved 43.07% mIoU and KPConv achieved 19.07% mIoU, which is far under their performance on SemanticKITTI dataset, which was 59.5% mIoU and 58.8%, respectively. The imbalance phenomena in the point cloud dataset challenges these algorithms as well. Compared with SalsaNext, the degradation of KPConv is more obvious. We believe that the extremely imbalanced and unstructured features of our dataset mainly cause the degradation. While training, the KPConv model does not learn on the whole LiDAR scan but instead on sampled neighborhoods of selected points. During training, there are two sampling strategies: random sampling and sampling based on the label distribution. The second strategy tried to mitigate the imbalanced distribution problem, but this attempt only increases results by 0.6% mIoU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Discussion</head><p>From the experimental results, we see that extreme class imbalance in off-road environments presents challenges for current semantic segmentation algorithms. For off-road environments tasks, the objects that are only visible in a small number of frames can be important for autonomous decision making and navigation planning. New algorithms might consider knowledge transfer from on-road datasets and fusion between two environments. Compared with the LiDAR point cloud, RGB images are more dense and contain more visually rich features to learn from, resulting in better performance for the image-based models on the semantic segmentation task. However, LiDAR has its own advantages and we provide examples of why this modality is critical to pair with RGB data for autonomous navigation. <ref type="figure">Figure 6</ref> shows an example frame from a sequence with a hill that is covered by bushes. Using both point cloud and image semantic understanding, the robot could recognize bushes successfully and decide whether to navigate among them, but a wrong navigation decision might be made without considering the 3D information. Using geometry-based methods alone, the bushes might be classified as non-traversable obstacle, but using semantic information a traversal decision This example inspires us to consider new possible geometrybased ontology definitions, to complement existing semantic defintions. Currently, semantic labels are defined on the classes of objects but do not contain more abstract semantic information such as traversable/non-traversable, or up/downhill. While some geometric supplemental information can be directly measured or inferred from measurement, some apriori properties (e.g., traversable vs non-traversable) could prove beneficial for autonomous navigation tasks. Regardless, we believe that inertial as well as visual and depth information are vital for autonomous off-road tasks, and that the different data types complement each other. For example, <ref type="figure" target="#fig_6">Figure 7</ref> shows an example where a fence was behind bushes and very difficult to detect with image data or even single LiDAR scans. By registering scans over a small time window, the integrated LiDAR scans can successfully detect the fence, which could prevent incorrect or even hazardous navigation decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. SUMMARY AND FUTURE WORK</head><p>We introduce RELLIS-3D, a large-scale multimodal dataset with pixel-wise and point-wise annotations representative of off-road environments for semantic segmentation. The dataset presents challenges related to imbalanced class distributions, and unstructured features common in off-road environments. We show that performance of current stateof-the-art deep learning models degrades significantly on RELLIS-3D, indicating the uniqueness of off-road navigation and the need to further semantic segmentation research in these unstructured environments. We plan to extend RELLIS-3D in future work by including higher-order semantic labels such as traversable, non-traversable, up-hill and down-hill.</p><p>Since the data was collected all in a single outdoor facility, maintaining diversity across the train/test/val set is a challenge. As we expand the dataset to different offroad environments, the environmental diversity will be greatly improved. In addition, as we provide high accurate GPS data and stereo images in our datasets, we will investigate adding odometry benchmarks for RELLIS-3D and explore using semantic information to improve the odometry estimation results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Warthog Platform Configuration. Illustration of the dimensions and mounting positions of the sensors with respect to the robot body. (Units: cm)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Ground truth annotations examples provided in the RELLIS-3D dataset. Images are densely annotated with pixel-wise labels from 20 different visual classes. LiDAR scans are point-wise labeled with the same ontogloy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>c e v e h i c l e o b j e c t p o l e w a t e r a s p h a l t b u i l d i n g d i r t r u b b l e</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Image Label distribution. The sky, grass, tree and bush constitute the major classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Point Cloud Label distribution. The grass, tree, and bush also dominate the population.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :Fig. 6 :</head><label>56</label><figDesc>Confusion matrix. The y and x axis numbers represent classes ids (1: sky; 2: grass; 3: tree; 4: bush; 5: concrete; 6: mud; 7: person; 8: puddle; 9: rubble; 10: barrier; 11: log; 12: fence; 13: vehicle; 14: object; 15: pole; 16: water; 17: asphalt;18: building; 19: void.) Note that the sky label is omitted for the point cloud algorithm confusion matrices. Hill Example (a) RGB Image, (b) Image Label, (c) Single Scan, (d) Multiple Scans could be made using higher level autonomy components.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Fence Example (a) RGB Image, (b) Image Label, (c) Single Scan, (d) Registered Scans. Note the clear difference in visibility of the fence class (shown in purple) between image and point clouds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Overview of off-road datasets. Ours is by far the largest dataset with multiple modalities.</figDesc><table><row><cell>Name</cell><cell>Sensors</cell><cell># Annotations 1</cell><cell># Classs 2</cell><cell>Annotation Type</cell><cell>Modality</cell></row><row><cell>RUGD [14]</cell><cell>camera</cell><cell>7546</cell><cell>24</cell><cell>pixel-wise</cell><cell>RGB</cell></row><row><cell>DeepScene [15]</cell><cell>camera</cell><cell>366</cell><cell>6</cell><cell>pixel-wise</cell><cell>RGB, Depth, NIR, NRG, NDVI, EVI</cell></row><row><cell>Pezzementi et al [16]</cell><cell>camera</cell><cell>95000</cell><cell>1</cell><cell>bounding box</cell><cell>RGB</cell></row><row><cell>YCOR [1]</cell><cell>camera</cell><cell>1076</cell><cell>8</cell><cell>pixel-wise</cell><cell>RGB</cell></row><row><cell>Dabbiru et al [17]</cell><cell>simLiDAR</cell><cell>2743</cell><cell>6</cell><cell>point-wise</cell><cell>Point Cloud</cell></row><row><cell>Ours</cell><cell>camera, LiDAR</cell><cell>6235/13556</cell><cell>20</cell><cell>pixel/point-wise</cell><cell>RGB, Point Cloud</cell></row><row><cell></cell><cell cols="5">1 Number of images/scans annotated, 2 Number of classes annotated.</cell></row><row><cell cols="4">scans and RGB camera images. To the best of the authors'</cell><cell></cell><cell></cell></row><row><cell cols="4">knowledge, RELLIS-3D represents the first multi-modal off-</cell><cell></cell><cell></cell></row><row><cell cols="4">road navigation dataset with synchronized raw sensor data</cell><cell></cell><cell></cell></row><row><cell cols="4">and a large number of ground truth annotations. The major</cell><cell></cell><cell></cell></row><row><cell cols="4">contributions of this dataset can be summarized as follows:</cell><cell></cell><cell></cell></row><row><cell cols="4">? We release five sequences of synchronized sensor data</cell><cell></cell><cell></cell></row><row><cell cols="4">captured while driving in off-road environments in</cell><cell></cell><cell></cell></row><row><cell cols="4">Robot Operating System (ROS) bag format, including</cell><cell></cell><cell></cell></row><row><cell cols="4">RGB camera images, LiDAR point clouds, a pair of</cell><cell></cell><cell></cell></row><row><cell cols="4">stereo images, high-precision GPS measurement, and</cell><cell></cell><cell></cell></row><row><cell>IMU data.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">We establish a benchmark by defining training, valida-</cell><cell></cell><cell></cell></row><row><cell cols="4">tion, and testing splits, and provide an initial analysis</cell><cell></cell><cell></cell></row><row><cell cols="4">using state-of-the-art image and point cloud semantic</cell><cell></cell><cell></cell></row><row><cell cols="4">segmentation algorithms. These results demonstrate the</cell><cell></cell><cell></cell></row><row><cell cols="4">challenges of semantic segmentation of off-road data</cell><cell></cell><cell></cell></row><row><cell cols="4">and help identify open areas of research that RELLIS-</cell><cell></cell><cell></cell></row><row><cell>3D can help advance.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>? Across the five sequences of collected data we provide 6,235 pixel-wise image annotations, and semantic labels for 13,556 full LiDAR point cloud scans.?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Single image (20 classes) / scan (16 classes) for all baselines on test set. 90.20 80.53 76.76 84.22 43.29 89.48 73.94 62.03 54.86 0.00 39.52 41.54 46.44 9.51 0.72 33.25 4.60 51.55 gscnn 97.02 84.95 78.52 70.33 83.82 45.52 90.31 71.49 66.03 55.12 2.92 41.86 46.51 54.64 6.90 0.94 44.18 11.47 52.92 salsanext -64.74 79.04 72.90 75.27 9.58 83.17 23.20 5.01 75.89 18.76 16.13 23.12</figDesc><table><row><cell>models</cell><cell>sky</cell><cell>grass</cell><cell>tree</cell><cell>bush</cell><cell>concrete</cell><cell>mud</cell><cell>person</cell><cell>puddle</cell><cell>rubble</cell><cell>barrier</cell><cell>log</cell><cell>fence</cell><cell>vehicle</cell><cell>object</cell><cell>pole</cell><cell>water</cell><cell>asphalt</cell><cell>building</cell><cell>mean</cell></row><row><cell cols="15">hrnet+OCR 96.94 -</cell><cell cols="2">56.26 0.00</cell><cell>-</cell><cell>-</cell><cell>43.07</cell></row><row><cell>kpconv</cell><cell>-</cell><cell cols="12">56.41 49.25 58.45 33.91 0.00 81.20 0.00 0.00 0.00 0.00 0.40 0.00</cell><cell>-</cell><cell cols="2">0.00 0.00</cell><cell>-</cell><cell>-</cell><cell>19.97</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://appen.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We omit the dirt class in this evaluation because it is extremely sparse in the annotations.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Real-Time Semantic Mapping for Autonomous Off-Road Navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-W</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uenoyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; F</forename><surname>Serv</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The DARPA LAGR program: Goals, challenges, methodology, and phase I results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Krotkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perschbacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pippine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sullivan</surname></persName>
		</author>
		<idno type="DOI">http:/doi.wiley.com/10.1002/rob.20161</idno>
		<ptr target="http://doi.wiley.com/10.1002/rob.20161" />
	</analytic>
	<monogr>
		<title level="j">Journal of Field Robotics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11-12</biblScope>
			<biblScope unit="page" from="945" to="973" />
			<date type="published" when="2006-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Winning the DARPA grand challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</title>
		<imprint>
			<biblScope unit="volume">4213</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<idno type="DOI">https:/link.springer.com/chapter/10.1007/11871842{_}4</idno>
		<ptr target="https://link.springer.com/chapter/10.1007/11871842{}4" />
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer Verlag</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Autonomous off-road navigation using stereo-vision and laser-rangefinder fusion for outdoor obstacles detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marin-Plaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>De La Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium, Proceedings</title>
		<imprint>
			<publisher>Augus. Institute of Electrical and Electronics Engineers Inc</publisher>
			<date type="published" when="2016-08" />
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="page" from="104" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stereo-based tree traversability analysis for autonomous off-road navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huertas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rankin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -Seventh IEEE Workshop on Applications of Computer Vision, WACV 2005</title>
		<meeting>-Seventh IEEE Workshop on Applications of Computer Vision, WACV 2005</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="210" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Obstacle detection and terrain classification for autonomous off-road navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manduchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Castano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talukder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthies</surname></persName>
		</author>
		<ptr target="http://www.gao.gov/new.items/d01311.pdf" />
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="102" />
			<date type="published" when="2005-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-camera visual SLAM for off-road navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fu</surname></persName>
		</author>
		<ptr target="https://linkinghub.elsevier.com/retrieve/pii/S0921889019308711" />
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page">103505</biblScope>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep Multi-Modal Object Detection and Semantic Segmentation for Autonomous Driving: Datasets, Methods, and Challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Haase-Schutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hertlein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Glaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Timm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wiesbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2020-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semantic object classes in video: A high-definition ground truth database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<ptr target="http://mi.eng.cam.ac.uk/research/projects/VideoRec/" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Cityscapes Dataset for Semantic Urban Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1604.01685" />
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-12" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">nuScenes: A multimodal dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">E</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<ptr target="http://arxiv.org/abs/1903.11027" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Nathan Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scene parsing through ADE20K dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<ptr target="http://groups.csail.mit.edu/vision/datasets/ADE20K/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings -30th IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>-30th IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5122" to="5130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A rugd dataset for autonomous navigation and visual perception in unstructured outdoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wigness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep multispectral semantic scene understanding of forested environments using multimodal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Comparing apples and oranges: Off-road pedestrian detection on the National Robotics Engineering Center agricultural person-detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pezzementi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tabor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wellington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Herman</surname></persName>
		</author>
		<idno type="DOI">http:/doi.wiley.com/10.1002/rob.21760</idno>
		<ptr target="http://doi.wiley.com/10.1002/rob.21760" />
	</analytic>
	<monogr>
		<title level="j">J. F. Robot</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="545" to="563" />
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">LiDAR Data Segmentation in Off-Road Environment Using Convolutional Neural Networks (CNN)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dabbiru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Goodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Scherrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carruth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SAE Tech. Pap</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<date type="published" when="2020-04" />
			<publisher>SAE International</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The Mapillary Vistas Dataset for Semantic Understanding of Street Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<publisher>October. Institute of Electrical and Electronics Engineers Inc</publisher>
			<date type="published" when="2017-12" />
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="5000" to="5009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">D$?2$-City: A Large-Scale Dashcam Video Dataset of Diverse Traffic Scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<ptr target="http://arxiv.org/abs/1904.01975" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1805.04687" />
		<imprint>
			<date type="published" when="2018-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kassahun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahmudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ricou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Durgesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hauswald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>M?hlegg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>J?nicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mirashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Savani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vorobiov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oelker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garreis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schuberth</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2004.06320" />
		<title level="m">A2D2: Audi Autonomous Driving Dataset</title>
		<imprint>
			<date type="published" when="2020-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/CVF International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE/CVF International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Enabling Off-Road Autonomous Navigation-Simulation of LIDAR in Dense Vegetation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Goodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Doude</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carruth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">154</biblScope>
			<date type="published" when="2018-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<ptr target="http://www.mdpi.com/2079-9292/7/9/154" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The OpenCV Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dr. Dobb&apos;s Journal of Software Tools</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Extrinsic calibration of a 3d-lidar and a camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saripalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Experimental evaluation of 3d-lidar camera extrinsic calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Osteen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saripalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Msg-cal: Multi-sensor graph-based calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Osteen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3660" to="3667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">VoxNet: A 3D Convolutional Neural Network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Conf. Intell. Robot. Syst</title>
		<imprint>
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page" from="922" to="928" />
			<date type="published" when="2015-12" />
			<publisher>Decem. Institute of Electrical and Electronics Engineers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The Pascal Visual Object Classes Challenge: A Retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Object-Contextual Representations for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1909.11065" />
		<imprint>
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Gated-SCNN: Gated shape CNNs for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Takikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<publisher>October. Institute of Electrical and Electronics Engineers Inc</publisher>
			<date type="published" when="2019-10" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="5228" to="5237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<ptr target="http://arxiv.org/abs/1907.05740" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep Learning for 3D Point Clouds: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<ptr target="https://github.com/QingyongHu/SoTA-Point-Cloud" />
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">SalsaNext: Fast, Uncertainty-aware Semantic Segmentation of LiDAR Point Clouds for Autonomous Driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cortinhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzelepis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">E</forename><surname>Aksoy</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2003.03653" />
		<imprint>
			<date type="published" when="2020-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">KPConv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1904.08889" />
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<publisher>October. Institute of Electrical and Electronics Engineers Inc</publisher>
			<date type="published" when="2019-10" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="6410" to="6419" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
