<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Localization with Sampling-Argmax</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Chen</surname></persName>
							<email>chentong1023@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujing</forename><surname>Lou</surname></persName>
							<email>louyujing@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
							<email>yonglu_li@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
							<email>lucewu@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Localization with Sampling-Argmax</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Soft-argmax operation is commonly adopted in detection-based methods to localize the target position in a differentiable manner. However, training the neural network with soft-argmax makes the shape of the probability map unconstrained. Consequently, the model lacks pixel-wise supervision through the map during training, leading to performance degradation. In this work, we propose sampling-argmax, a differentiable training method that imposes implicit constraints to the shape of the probability map by minimizing the expectation of the localization error. To approximate the expectation, we introduce a continuous formulation of the output distribution and develop a differentiable sampling process. The expectation can be approximated by calculating the average error of all samples drawn from the output distribution. We show that sampling-argmax can seamlessly replace the conventional soft-argmax operation on various localization tasks. Comprehensive experiments demonstrate the effectiveness and flexibility of the proposed method. Code is available at https://github.com/Jeff-sjtu/sampling-argmax.</p><p>Prior work [37] attempts to shape the probability map by introducing hand-crafted regularizations. The variance regularization encourages the variance of the probability map to get close to the predefined variance. The Gaussian regularization forces the probability map to resemble a Gaussian distribution. We argue that these variants are overconstrained. The hand-crafted constraints are not 35th</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Localizing the target position from the input is a fundamental task in the field of computer vision. Common approaches to localization can be divided into two categories: regression-based and detection-based. Detection-based methods show superiority over regression-based methods and demonstrate impressive performance on a wide variety of tasks <ref type="bibr" target="#b47">[51,</ref><ref type="bibr" target="#b39">43,</ref><ref type="bibr" target="#b45">49,</ref><ref type="bibr" target="#b12">16,</ref><ref type="bibr" target="#b20">24,</ref><ref type="bibr" target="#b14">18,</ref><ref type="bibr" target="#b22">26,</ref><ref type="bibr" target="#b17">21,</ref><ref type="bibr" target="#b37">41,</ref><ref type="bibr" target="#b23">27,</ref><ref type="bibr" target="#b36">40]</ref>. Probability maps (also referred to as heat maps) are predicted in detection-based methods to indicate the likelihood of the target position. The position with the highest probability is retrieved from the probability map with the argmax operation. However, the argmax operation is not differentiable and suffers from quantization error. For accurate localization and end-to-end learning, soft-argmax <ref type="bibr" target="#b8">[12,</ref><ref type="bibr" target="#b7">11]</ref> is proposed as an approximation of argmax. It has found a wide range of applications in human pose estimation <ref type="bibr" target="#b39">[43,</ref><ref type="bibr" target="#b26">30,</ref><ref type="bibr" target="#b27">31,</ref><ref type="bibr" target="#b40">44]</ref>, facial landmark localization <ref type="bibr" target="#b14">[18,</ref><ref type="bibr" target="#b25">29,</ref><ref type="bibr" target="#b5">9]</ref>, stereo matching <ref type="bibr" target="#b47">[51,</ref><ref type="bibr" target="#b18">22,</ref><ref type="bibr" target="#b6">10]</ref> and object keypoint estimation <ref type="bibr" target="#b36">[40]</ref>.</p><p>Nevertheless, the mechanism of training networks with soft-argmax is rarely studied. The conventional training strategy is to minimize the error between the output coordinate from soft-argmax and the ground truth position. However, this strategy is deficient since it only provides constraints to the expectation of the probability map, not to its shape. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, these two maps have the same mean values, but the bottom one is more concentrated. In well-calibrated probability maps, positions that locate closer to the ground truth have higher probabilities. Reliable confidence scores of localization results could be provided, which is essential in unconstrained real-world applications and downstream tasks. Besides, imposing constraints on the probability map can provide supervised pixel-wise gradients and facilitate the learning process. always correct in different cases. For example, the underlying shape of the probability map is not necessarily Gaussian, and the underlying variance might change as the input changes. Imposing the model to learn a fixed-variance Gaussian distribution might degrade the model performance. In this work, we present sampling-argmax, a novel training method to obtain well-calibrated probability maps and improve the localization accuracy. To constrain the shape of the map, we replace the objective function of minimizing "the error of the expectation" with minimizing "the expectation of the error". In this way, the network is encouraged to generate higher probabilities around the ground truth position.</p><p>A natural way to estimate the expectation is by calculating the probability-weighted sum of the errors at all grid positions. However, we find that the gradient has high variance, and the model is hard to train. To address this issue, we choose to approximate the expectation by sampling. The expectation of the error is calculated as the mean error of all samples. Therefore, the sampling process should be differentiable for end-to-end learning.</p><p>In our work, we show that the likelihood of the target position can be modelled in the continuous space with a mixture distribution. Samples can be drawn from the mixture distribution by three steps: i) generate categorical weights from the probability map; ii) draw samples from sub-distributions; iii) obtain a sample by the category-weighted sum. The benefit of using mixture distribution is that differentiable sampling from arbitrary continuous distributions can be resolved by differentiable sampling from categorical distributions, which is less challenging and can be addressed by off-the-shelf discrete sampling methods.</p><p>Sampling-argmax is simple and effective. With out-of-the-box settings, it can be integrated into methods that using soft-argmax operation. To study its effectiveness, we conduct experiments on a variety of localization tasks. Quantitative results demonstrate the superiority of sampling-argmax against soft-argmax and its variants. In summary, the contributions of this work are threefold:</p><p>? We propose sampling-argmax for improving detection-based localization methods. By minimizing "the expectation of the error", the network generates well-calibrated probability maps and obtains higher localization accuracy. ? We show the output likelihood can be formulated as a mixture distribution and develop a differentiable sampling pipeline. ? Comprehensive experiments show that sampling-argmax is effective and can be flexibly generalized to different localization tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminary</head><p>Given a learned discrete probability map ?, the value ? yi indicates the probability of the predicted target appearing at y i . A direct way to localize the target is taking the position with the maximum likelihood. However, this approach is non-differentiable, and the output is discrete, which impedes end-to-end training and brings quantization errors. Soft-argmax is an elegant approximation to address these issues:? = soft-argmax ? = i ? yi y i .</p><p>(1)</p><p>Notice that ? is a normalized distribution and the soft-argmax operation calculates the probabilityweighted sum, which is equivalent to taking the expectation of the probability map ?. A conventional way to train the model with the soft-argmax operation is minimizing the distance between the expectation and the ground truth:</p><formula xml:id="formula_0">L = d(y t , E y [y]) ? d(y t , i ? yi y i ),<label>(2)</label></formula><p>where y t denotes the ground truth position and d(?, ?) denotes the distance function, e.g. 1 distance. We refer to this objective function as "the error of the expectation".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>The conventional detection-based method with soft-argmax only supervises the expectation of the probability map. The shape of the distribution remains unconstrained. In well-calibrated probability maps, the positions closer to the ground truth should have higher probabilities. To this end, we proposed a new objective function that optimizes "the expectation of the error" instead of "the error of the expectation". In particular, the objective function is formulated as:</p><formula xml:id="formula_1">L = E y [d(y t , y)].<label>(3)</label></formula><p>The learned distribution tends to allocate high probabilities around the ground truth to minimize the entire loss. In this way, the shape of the probability map is implicitly constrained.</p><p>Discrete Distribution. The probability map ? predicted by the neural network is discrete. Similar to the soft-argmax operation, the expectation of error can be approximated by calculating the probability-weighted sum of the errors at all grid positions:</p><formula xml:id="formula_2">L = E y [d(y t , y)] ? i ? yi d(y t , y i ).<label>(4)</label></formula><p>This approximation treats the distribution of the target position as a discrete distribution. The target only appears at the grid positions, i.e. at position y i with the probability ? yi .</p><p>However, because the underlying target lies in a continuous space, modelling the distribution as a discrete distribution is not accurate. </p><formula xml:id="formula_3">? ? L = ? ? E y [d(y t , y)] = i d(y t , y i )? ? ? yi = i d(y t , y i )? yi ? ? log ? yi = E y [d(y t , y)? ? log ? y ].<label>(5)</label></formula><p>Notice that the form of the gradient is similar to the score function estimator (SF), which is alternatively called the REINFORCE estimator <ref type="bibr" target="#b41">[45]</ref>. SF estimator is known to have very high variance and is slow to converge. Therefore, using the discrete approximation for training is not a good solution. This challenge prompts us to explore a better approximation to calculate the expectation of the error.</p><p>In the following parts, we present sampling-argmax to estimate the expectation of the error by sampling. We first develop a continuous approximation to the distribution of the target position (Section 3.1). Then we propose a differentiable sampling method (Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Continuous Mixture Distribution</head><p>A differentiable process is necessary to estimate the expectation by sampling. However, since the underlying probability density functions can vary among different input images, it is challenging to draw samples from arbitrary distributions differentiably. In this work, we present a unified method by formulating the target distribution as a mixture distribution.</p><p>Let p(y) denotes the underlying density function of the target position, which is defined within the boundary of the input image, i.e. y ? [0, W ]. As illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>(a), the interval [0, W ] can be divided into n subintervals. The density function can be partitioned into shapes in the subintervals. We could use regular shape (rectangles, triangles, Gaussian functions) in subintervals to form the entire function (as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>(b-c)).</p><p>Formally, given a finite set of probability density functions {f 1 (y), f 2 (y), ? ? ? , f n (y)} and weights {w 1 , w 2 , ? ? ? , w n } such that w i ? 0 and w i = 1, the mixture density function p(y) is formulated as a sum: Here, we can leverage the discrete probability map ? to represent the mixture weights, i.e. w i = ? yi . In the context of signal processing, the original function can be perfectly reconstructed if the sample rate (the distance between two adjacent grid points) satisfies the Nyquist-Shannon sampling theorem. However, in our case, the sub-function f i (y) must be a probability density function, i.e. it has the non-negative values, and its integral over the entire space is equal to 1. Therefore, with these restrictions, the original function p(y) cannot be perfectly reconstructed. For approximation, we study three different types of standard density functions below.</p><formula xml:id="formula_4">p(y) = n i=1 w i f i (y).<label>(6)</label></formula><formula xml:id="formula_5">+ + + ? ? ? w i * f i (y) w i * f i (y) (a) (b) (c) = = =</formula><p>Uniform Basis. For the uniform basis, the sub-function f i (y) is a uniform distribution centred at the position y i :</p><formula xml:id="formula_6">f i (y) = 1 c , y ? [y i ? c 2 , y i + c 2 ], 0, otherwise,<label>(7)</label></formula><p>where c is the distance between two adjacent grid points.</p><p>Triangular Basis. For the triangular basis, the sub-function f i (y) is a triangular distribution:</p><formula xml:id="formula_7">f i (y) = ? ? ? 1 c 2 (y ? y i ) + 1 c , y ? [y i ? c, y i ), ? 1 c 2 (y ? y i ) + 1 c , y ? [y i , y i + c), 0, otherwise.<label>(8)</label></formula><p>For all y, there exist grid points y i and y i+1 that satisfy y ? [y i , y i+1 ]. Therefore, we have p(y)</p><formula xml:id="formula_8">= w i f i (y)+w i+1 f i+1 (y) = wi+1?wi c 2 (y ?y i )+ wi c ,</formula><p>which is the linear interpolation of w i and w i+1 . In other words, using triangular bases is equivalent to the linear interpolation of the discrete probability map.</p><p>Gaussian Basis. For the Gaussian basis, f i (y) is the Gaussian function:</p><formula xml:id="formula_9">f i (y) = 1 ? ? 2? exp ? 1 2 ( y ? y i ? ) 2 .<label>(9)</label></formula><p>where ? denotes the standard deviation. We set ? = c by default in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Differentiable Sampling</head><p>In this part, we present how to draw a sample from the mixture distribution. We first study the non-differentiable process and then present the differentiable approximation.</p><p>Non-differentiable Process. As illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>(a), the non-differentiable sampling process can be divided into two steps: i) determine which sub-distribution the sample comes from; ii) draw a sample from the selected sub-distribution. In the first step, the sub-distribution can be selected by drawing a random variable from a categorical distribution. The categorical distribution is indicated by the predicted probability map ?. The sub-distribution f i (y) is chosen with the probability ? yi .</p><p>(a) non-differentiable sampling </p><formula xml:id="formula_10">(b) differentiable sampling 0 0 0 0 0 0 0 0 0 1 Y =? k sample from the categorical distribution ? ? ? ? ? ? ? ? ? ? ? ? y 1 ? f 1 (y)? k ? f k (y)? n ? f n (y)</formula><formula xml:id="formula_11">Y = i? i?i Gumbel-softmax y 1 ? f 1 (y)? k ? f k (y)? n ? f n (y)</formula><p>probability ? ? ? ? ? ? ? ? ? ? ? ? step 1 step 2 ? step 3 weighted sum There are a number of methods to draw samples from the categorical distribution. Here, we introduce the Gumbel-Max trick <ref type="bibr" target="#b10">[14,</ref><ref type="bibr" target="#b29">33]</ref>:</p><formula xml:id="formula_12">z = one_hot_max i [g i + log ? i ],<label>(10)</label></formula><p>where g 1 , ? ? ? , g n are i.i.d samples drawn from Gumbel(0, 1), and the sample z is a one-hot vector with the value 1 in the maximum categorical column.</p><p>In the second step, sampling from the standard basis function is easy to implement. This step is independent of the predicted probability map ?. Therefore, the key to differentiable sampling from the mixture distribution is to make the first step differentiable.</p><p>Differentiable Process. The differentiable sampling process consists of three steps. In the first step, we adopt the Gumbel-softmax <ref type="bibr" target="#b16">[20]</ref> operation to sample the categorical weight from the probability map. Gumbel-softmax is a continuous and differentiable approximation of the Gumbel-Max trick. We can obtain an (n ? 1)-dimensional simplex? ? ?:</p><formula xml:id="formula_13">? i = exp ((g i + log ? i )/? ) n k=1 exp ((g k + log ? k )/? ) ,<label>(11)</label></formula><p>where? = {? 1 , ? ? ? ,? n } and? i denotes the sampled weight of the sub-distribution f i (y). As the softmax temperature ? approaches 0, the simplex? becomes one-hot, and its distribution becomes identical to the categorical distribution ?.</p><p>In the second step, we draw a sample? i from every sub-distribution f i (y). Note that the sampled weight is not completely one-hot. Therefore, we obtain the final sample? in the third step by adding all samples together with the sampled weight?:</p><formula xml:id="formula_14">Y = n i? i?i .<label>(12)</label></formula><p>This process is illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>(b). With the reparameterization trick, the sample? is computed as a deterministic function of the probability map ? and the independent random variables. The randomness of the sampling process is transferred to the variable g 1 , ? ? ? , g n . We denote the sampling process as? = s(?, ), where = {g 1 , ? ? ? , g n } follows the multivariate Gumbel(0, 1) distribution. The gradient from the expected error to the model parameters ? is derived as:</p><formula xml:id="formula_15">? ? E y [d(y t , y)] = ? ? E [d(y t , s(?, ))] = E ?d ?s ?s ?? ?? ?? .<label>(13)</label></formula><p>As we see, the gradient of the continuous sampling process is easy to compute via backpropagation. Therefore, we can relax the objective function by calculating the average error of the samples drawn from the mixture distribution. The objective function is written as:</p><formula xml:id="formula_16">L = E y?p(y) [d(y t , y)] ? 1 N s Ns k=1 d(y t ,? k ) = 1 N s Ns k=1 d(y t , s(?, k )),<label>(14)</label></formula><p>where N s denotes the number of samples. In the testing phase, no randomness is introduced, and sampling-argmax degrades to soft-argmax.</p><p>While the sampling process is differentiable, the sample? does not follow the original mixture distribution p(y) for non-zero temperature. For small temperatures, the distribution of? is close to p(y), but the variance of the gradients is large. There is a tradeoff between small temperatures and large temperatures. In our experiments, we start at a high temperature and anneal to a small temperature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Variants of Soft-Argmax. Nibali et al. <ref type="bibr" target="#b33">[37]</ref> introduced hand-crafted regularization to constrain the shape of the probability map.</p><p>Variance Regularization. Variance regularization is to control the variance of the probability map. It pushes the variance of the probability map close to the target variance ? 2 t :</p><formula xml:id="formula_17">L var = Var(?) ? ? 2 t 2 2 ,<label>(15)</label></formula><p>where the target variance ? 2 t is a hyperparameter and the variance of the probability map Var(?) is approximated in a discrete manner, i.e. Var(?) = i ? yi (y i ? k ? y k y k ) 2 .</p><p>Distribution Regularization. Distribution regularization is to impose strict regularization on the appearance of the heatmap to directly encourage a certain shape. Specifically, <ref type="bibr" target="#b33">[37]</ref> forces the probability map to resemble a Gaussian distribution by minimizing the Jensen-Shannon divergence between ? and target discrete Gaussian distribution:</p><formula xml:id="formula_18">L JS = D JS (? N (E(y), ? 2 t )).<label>(16)</label></formula><p>Unlike them, our objective function does not set pre-defined hyperparameters for the shape of the map, which makes it general and flexible in applying to various applications.</p><p>Other works <ref type="bibr" target="#b17">[21,</ref><ref type="bibr" target="#b20">24]</ref> study how to localize target with soft-argmax in different situations. Joung et al. <ref type="bibr" target="#b17">[21]</ref> proposed sinusoidal soft-argmax for cylindrical probabilities map. Lee et al. <ref type="bibr" target="#b20">[24]</ref> proposed kernel soft-argmax to make the results less susceptible to multi-modal probability map. Our work is compatible with these methods by applying the sinusoidal function to the grid positions or multiplying the Gaussian kernel before obtaining the probability map.</p><p>Differentiable Sampling. Differentiable sampling for a discrete random variable has been studied for a long time. Maddison et al. <ref type="bibr" target="#b28">[32]</ref> and Jang et al. <ref type="bibr" target="#b16">[20]</ref> concurrently proposed the idea of using a softmax of Gumbel as relaxation for differentiable sampling from discrete distributions. Ko?isk? et al. <ref type="bibr" target="#b19">[23]</ref> relaxed the discrete sampling by drawing symbols from a logistic-normal distribution rather than drawing from softmax. In this work, unlike previous methods that study discrete distributions, we focus on continuous distributions. We propose a relaxation of continuous sampling by formulating the target distribution as a mixture distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We validate the benefits of the proposed sampling-argmax with experiments on a variety of localization tasks, including human pose estimation, retina segmentation and object keypoint estimation. Additional experiments on facial landmark localization are provided in appendix. Sampling-argmax is compared with the conventional soft-argmax and the variants that using additional auxiliary loss <ref type="bibr" target="#b33">[37]</ref>. Training details of all tasks are provided in the supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">2D Human Pose Estimation from RGB</head><p>We first evaluate the proposed sampling-argmax in 2D human pose estimation. In 2D human pose estimation, the probability map is a typical representation to localize body keypoints. The experiments are conducted on the large-scale in-the-wild 2D human pose benchmark -COCO Keypoint <ref type="bibr" target="#b24">[28]</ref>. Significant progress has been achieved in this field <ref type="bibr" target="#b42">[46,</ref><ref type="bibr" target="#b38">42,</ref><ref type="bibr" target="#b32">36,</ref><ref type="bibr" target="#b30">34]</ref>. We adopt the standard model SimplePose <ref type="bibr" target="#b42">[46]</ref> for experiments. We follow the standard metric of COCO Keypoint and use mAP over 10 OKS (object keypoint similarity) thresholds for evaluation.</p><p>As shown in <ref type="table" target="#tab_1">Table 1</ref>, the proposed sampling-argmax significantly outperforms the soft-argmax operation and its variants. Soft, Soft w/V.R. and Soft w/D.R correspond to conventional softargmax, soft-argmax with variance regularization and distribution regularization, respectively. Samp. Uni., Tri. and Gau. correspond to sampling-argmax with uniform, triangular and Gaussian basis, respectively. The triangular basis brings 5.3 mAP improvement (relative 8.2%) to the original softargmax operation. Besides, we find the auxiliary losses degrade the model performance in COCO Keypoint. Number of Samples. In our method, the differentiable sampling process is utilized to approximate the expectation of the error. As the number of samples increases, the approximation will be closer to the underlying expectation. To study how the number of samples affects the final results, we compare the performance of the models that trained with different numbers of samples. In <ref type="table" target="#tab_2">Table 2</ref>, we report the results with N s = {1, 5, 10, 30, 50}. It shows that a large number of samples might improve the performance but not necessary. Training the model with only one sample can still obtain high performance while saving computation resources.  Correlation with Prediction Correctness. For a well-calibrated probability map, the shape of the map could reflect the uncertainty of the regression output. When encountering challenging cases, the probability map would have a large variance, resulting in a lower peak value. In other words, the peak value establishes the correlation with the prediction correctness. To demonstrate the probability map trained with sampling-argmax is better-calibrated, we calculate the Pearson correlation coefficient between the peak value and the prediction correctness. The correctness is represented by the OKS between the predicted pose and the ground-truth pose. <ref type="table" target="#tab_3">Table 3</ref> compares the correlation with prediction correctness among different methods. It shows that sampling-argmax has a much stronger correlation to the correctness than other methods. Compared to the soft-max operation, samplingargmax with the triangular bases brings 85.4% relative improvement. It demonstrates that training with sampling-argmax can obtain a more reliable probability map, which is essential to real-world applications and downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">3D Human Pose Estimation from RGB</head><p>We further evaluate the proposed sampling-argmax on Human3.6M <ref type="bibr" target="#b15">[19]</ref>, an indoor benchmark for 3D human pose estimation. The 3D probability map is adopted to represent the likelihoods for joints in the discrete 3D space. We adopt the model architecture of prior work <ref type="bibr" target="#b39">[43]</ref>. Following previous methods <ref type="bibr" target="#b34">[38,</ref><ref type="bibr" target="#b39">43,</ref><ref type="bibr" target="#b31">35,</ref><ref type="bibr" target="#b21">25]</ref>, MPJPE and PA-MPJPE <ref type="bibr" target="#b9">[13]</ref> are used as the evaluation metrics. Comparisons with baselines are shown in <ref type="table" target="#tab_4">Table 4</ref>. The proposed sampling-argmax provides consistent performance improvements. Different from the experiments on COCO Keypoint, the variance regularization provides performance improvements in Human3.6M.  <ref type="bibr" target="#b12">[16]</ref> proposes a regression method to regress the boundary and obtain the sub-pixel surface positions. One-dimensional probability maps are leveraged to model the position distribution of the surface in each column. In the testing phase, the soft-argmax method is used to infer the final surface positions. The entire surface can be reconstructed by connecting the surface positions in all columns.</p><p>The experiments are conducted on the multiple sclerosis and healthy controls dataset (MSHC) <ref type="bibr" target="#b13">[17]</ref>. Mean absolute distance (MAD) and standard deviation (Std. Dev.) are used as evaluation metrics. Quantitative results are reported in <ref type="table" target="#tab_5">Table 5</ref>. It shows that sampling-argmax achieve superior performance to other methods, while the auxiliary losses also provide performance improvements. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Supervised Object Keypoint Estimation from Point Clouds</head><p>Detecting aligned 3D object keypoints from point clouds has a wide range of applications on object tracking, shape retrieval and robotics. Probability maps are adopted to localize the semantic keypoints. Different from the RGB input, the probability map indicates the pointwise score of the input point cloud, not the grid position of an image. The distances between the adjacent point-pairs are different. Besides, point clouds are unordered, and each point has a different number of neighbours. Therefore, it is hard to directly apply the uniform bases or linear interpolation, which requires a constant adjacent distance. Fortunately, the Gaussian basis can be adopted. In the experiment, we set the standard deviation ? of the Gaussian bases to 0.01, which is the average adjacent point distance in the input point clouds. PointNet++ <ref type="bibr" target="#b35">[39]</ref> is adopted as the backbone network. The experiments are conducted on the large-scale object keypoint dataset -KeypointNet <ref type="bibr" target="#b44">[48]</ref>. The percentage of correct keypoints (PCK) <ref type="bibr" target="#b43">[47]</ref> is adopted for evaluation. The error distance threshold is set to 0.01. <ref type="table" target="#tab_6">Table 6</ref> shows the quantitative results on 16 categories. It shows that the proposed sampling-argmax is also effective on the non-grid input data. <ref type="table" target="#tab_6">Table 6</ref> also compare the results of sampling-argmax with different numbers of samples. It is seen that N s = 30 leads to the best average performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Unsupervised Object Keypoint Estimation from Point Clouds</head><p>We then evaluate the proposed method on object keypoint estimation in the context of unsupervised learning. The autoencoder framework is adopted to estimate the keypoint in an unsupervised manner. The encoder first estimates the 3D keypoints, and the decoder reconstructs the object point clouds from the estimated keypoints. We follow the state-of-the-art method <ref type="bibr" target="#b36">[40]</ref> that generates 3D keypoints with the soft-argmax operation for differentiable and end-to-end learning. The soft-argmax is replaced with sampling-argmax, where the Gaussian bases with the standard deviation ? = 0.01 are used.</p><p>The experiments are conducted on KeypointNet <ref type="bibr" target="#b44">[48]</ref>. Unlike supervised learning, the semantic of each predicted keypoint is unknown in unsupervised methods. Therefore, the PCK metric is not applicable. For evaluation, we adopt the dual alignment score (DAS) following the previous method <ref type="bibr" target="#b36">[40]</ref>. <ref type="table" target="#tab_7">Table 7</ref> reports the performance comparison with other methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Discussion</head><p>Although the variants of soft-argmax can bring improvements in some cases, they need laborious tuning of parameters, such as the weight of the regularization term and the variance of the target distribution. The best parameters for different tasks are different. Besides, the best parameters for variance regularization and distribution regularization is also different, which increases the effort needed for the process of parameters tunning. In our experiment, we tune the loss weight ranging from 0.1 to 10 and the variance ranging from 1 to 5 for each task. After laborious tuning, the performances of these variants are still not consistent across different tasks and they are inferior to the performance of our method, while our method is out-of-the-box and free from parameters tuning. Therefore, we think our method is effective and general to different cases.</p><p>In addition to a more accurate localization performance, sampling-argmax can predict well-calibrated probability maps and provide more reliable confidence scores. COCO Keypoint uses the mAP metric to evaluate multi-person pose estimation. Thus reliable confidence scores could also improve the performance. In other datasets, the metric only reflects the localization performance and ignore the importance of confidence scores. In many real-world applications and downstream tasks, a reliable confidence score is very important and necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose sampling-argmax, an operation for improving the detection-based localization. Sampling-argmax implicitly imposes shape constraints to the predicted probability map by optimizing "the expectation of error". With the continuous formulation and differentiable sampling, sampling-argmax can seamlessly replace the conventional soft-argmax operation. We show that sampling-argmax is effective and flexible by conducting comprehensive experiments on various localization tasks.</p><p>Retina Segmentation from OCT We follow the model architecture of <ref type="bibr" target="#b12">[16]</ref>. The input image is resized to 128 ? 1024. The learning rate is set to 1 ? 10 ?4 at first and reduced by a factor of 10 at the 10th and the 20th epoch. We use the Adam solver and train for 30 epochs, with a mini-batch size of 2 and 1 GPU. The split of training, validation and test sets follows the settings of the previous method <ref type="bibr" target="#b12">[16]</ref>. We set the target variance ? 2 t to 4, the loss weight of variance regularization to 1, and the loss weight of distributions regularization to 1 to achieve the best results after tuning.</p><p>Supervised Object Keypoint Estimation from Point Clouds We adopt PointNet++ <ref type="bibr" target="#b35">[39]</ref> as the backbone network. The output of the last layer is a per-point probability map for each keypoint. The input point cloud consists of 2048 points represented by their Euclidean coordinates sampled from a normalized object, and the indexes of keypoints are given. The learning rate is set to 1 ? 10 ?3 and halved every 10 epochs. We use Adam solver and train for 100 epochs with a mini-batch size of 8 on one GPU for each category. We set the target variance ? 2 t to 4, the loss weight of variance regularization to 1, and the loss weight of distributions regularization to 0.01 to achieve the best results after tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised Object Keypoint Estimation from Point Clouds</head><p>The learning rate is set to 1 ? 10 ?3 and halved every 10 epochs. We use the Adam solver and train for 50 epochs, with a mini-batch size of 8 and one GPU for each category. We set the target variance ? 2 t to 4, the loss weight of variance regularization to 1, and the loss weight of distributions regularization to 0.01 to achieve the best results after tuning.</p><p>Facial Landmark Localization from RGB ResNet-18 <ref type="bibr" target="#b11">[15]</ref> is adopted as the backbone network. The head network consists of 3 deconvolution layers and a 1 ? 1 convolution layer. The input image is resized to 256 ? 256. The learning rate is set to 1 ? 10 ?3 at first and reduced by a factor of 10 at the 10th and 20th epoch. We use the Adam solver and train for 30 epochs, which a mini-batch size of 32 and 4 GPUs in total. We set the target variance ? 2 t to 4, the loss weight of variance regularization to 1, and the loss weight of distributions regularization to 0.1 to achieve the best results after tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Broader Impact</head><p>In this work, we propose sampling-argmax to improve the ability of machines to understand target positions in input data. Current methods usually adopt computationally expensive models to improve the localization accuracy, which could cost many financial and environmental resources. We partly alleviate this issue by presenting a simple yet effective method. Furthermore, our method is an improvement of existing capabilities but does not introduce a radically new capability in machine learning. Thus our contribution is unlikely to facilitate misuse of technology that is already available to anyone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Limitation and Future Work</head><p>In our method, the underlying density function of the target position is approximated by a mixture of sub-distributions. By comparing the performance of the three proposed bases, we see that a more accurate reconstruction of the underlying function leads to better results. Theoretically, the underlying density function cannot be perfectly reconstructed since the proposed basis distributions are fixed. To address this limitation, learnable sub-distributions could be adopted in future works. For example, normalizing flow models can be leveraged to predict sub-distribution at each position according to the corresponding features. In this way, the sub-distributions are no longer fixed, and the mixture distribution has the potential to precisely reconstruct the underlying distribution and further improve the model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Data Acquisition</head><p>In our experiments, we use five different datasets, including COCO Keypoint <ref type="bibr" target="#b24">[28]</ref>, Human3.6M <ref type="bibr" target="#b15">[19]</ref>, MSHC <ref type="bibr" target="#b13">[17]</ref>, KeypointNet <ref type="bibr" target="#b44">[48]</ref> and MTFL <ref type="bibr" target="#b46">[50]</ref>. These public datasets do not contain personally identifiable information or offensive content. Human3.6M Human3.6M dataset is licensed under <ref type="bibr" target="#b2">[5]</ref>. To obtain the data, we register and download it from its official website <ref type="bibr" target="#b1">[4]</ref>.</p><p>MSHC MSHC dataset is publicly available, and no license is specified. We download the data from its official website <ref type="bibr">[7]</ref>.</p><p>KeypointNet KeypointNet dataset is publicly available, and no license is specified. We download the data from its official website <ref type="bibr" target="#b3">[6]</ref>.</p><p>MTFL MTFL dataset is publicly available, and no license is specified. We download the data from its official website <ref type="bibr" target="#b0">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Facial Landmark Localization from RGB</head><p>We further evaluate the proposed sampling-argmax on the facial landmark localization dataset MTFL <ref type="bibr" target="#b46">[50]</ref>. Absolute error and relative error (normalized by the two-eye distance) are adopted as evaluation metrics. Quantitative results are reported in <ref type="table" target="#tab_8">Table 8</ref>. Consistent with the experiments on other tasks, sampling-argmax provides performance improvement to facial landmark localization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Visualization of learned probability maps</head><p>We show the predicted probability maps of soft-argmax and sampling-argmax in <ref type="figure">Figure 4</ref>. It shows that soft-argmax is prone to predict multi-modal distribution, while the proposed sampling-argmax predicts better-calibrated probability maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Qualitative Results</head><p>Qualitative results on six tasks are shown in <ref type="figure" target="#fig_5">Figure 5</ref>, 6, 7, 8, 9 and 10.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Top: an unconstrained probability map. Bottom: a wellcalibrated probability map. These two maps have different shapes but a same mean value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Representing the continuous distribution as a mixture distribution. (a) The original probability density function can be viewed as the sum of n sub-functions. Each sub-function can be replaced by standard density functions with proper weights to approximate the original function. (b) Approximate the original function by replacing the sub-functions with uniform distribution. (c) Approximate the original function by replacing the sub-function with the triangular distribution, which is equivalent to the linear interpolation of the discrete weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>0.01 0.02 0.1 0.3 0.45 0.05 0.05 0.01 0.01</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the sampling process. (a) The non-differentiable process: i) select a sub-distribution by categorical sampling; ii) draw samples from the selected sub-distribution. (b) The differentiable process: i) approximate the categorical sampled weights by Gumbel-softmax; ii) draw samples from all sub-distribution; iii) add all samples together with the sampled weights. Reparameterization allows gradients to flow from the sample to the probability map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>COCO</head><label></label><figDesc>Keypoint COCO Keypoint dataset is licensed under the Creative Commons Attribution 4.0 License [2]. The images and annotations are publicly available. We download the images and annotations from its official website [1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results of 2D human pose estimation on COCO Keypoint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative results of 3D human pose estimation on Human3.6M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :Figure 8 :Figure 9 :</head><label>789</label><figDesc>Qualitative results of retina segmentation on MSHC. Airplane Laptop Qualitative results of supervised model on KeypointNet. Laptop Airplane Qualitative results of unsupervised model on KeypointNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Qualitative results of facial landmark localization on MTFL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Quantitative results on COCO Keypoint. Soft Soft w/ V.R. Soft w/ D.R. Samp. Uni. Samp. Tri. Samp. Gau.</figDesc><table><row><cell>64.5 84.7 mAP@0.75 ? 70.9 mAP ? mAP@0.5 ?</cell><cell>60.6 81.5 65.7</cell><cell>55.6 77.8 60.8</cell><cell>68.2 87.2 75.0</cell><cell>69.8 87.9 76.2</cell><cell>68.3 87.3 75.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of different sample numbers.</figDesc><table><row><cell>N s</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>30</cell><cell>50</cell></row><row><cell cols="6">Samp. Uni. 67.8 67.8 67.9 68.2 68.1</cell></row><row><cell>Samp. Tri.</cell><cell cols="5">69.7 69.7 69.6 69.8 69.8</cell></row><row><cell cols="6">Samp. Gau. 68.1 68.1 68.2 68.3 68.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Correlation testing.</figDesc><table><row><cell>Method</cell><cell>Corr. ?</cell></row><row><cell>Soft</cell><cell>0.233</cell></row><row><cell>Soft w/ V.R.</cell><cell>0.158</cell></row><row><cell cols="2">Soft w/ D.R. 0.082</cell></row><row><cell>Samp. Uni.</cell><cell>0.394</cell></row><row><cell>Samp. Tri.</cell><cell>0.432</cell></row><row><cell>Samp. Gau.</cell><cell>0.423</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Quantitative results on Human3.6M. Soft Soft w/ V.R. Soft w/ D.R. Samp. Uni. Samp. Tri. Samp. Gau.</figDesc><table><row><cell>50.4 PA-MPJPE ? 39.5 MPJPE ?</cell><cell>49.7 39.2</cell><cell>51.9 41.4</cell><cell>49.6 39.1</cell><cell>49.5 39.1</cell><cell>50.9 39.0</cell></row><row><cell cols="2">5.3 Retina Segmentation from OCT</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Using optical coherence tomography (OCT) to obtain 3D retina images is widely used in the clinic. A major goal of analyzing retinal OCT images is retinal layer segmentation. Previous work</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Quantitative results on MSHC dataset. Soft Soft w/ V.R. Soft w/ D.R. Samp. Uni. Samp. Tri. Samp. Gau.</figDesc><table><row><cell>3.08 Std. Dev. ? 0.281 MAD ?</cell><cell>0.743 0.114</cell><cell>0.746 0.108</cell><cell>0.735 0.101</cell><cell>0.744 0.100</cell><cell>0.740 0.104</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Quantitative results of supervised learning on KeypointNet dataset, reported as PCK (higher is better). Air. Bat. Bed Bot. Cap Car Cha. Gui. Hel. Kni. Lap. Mot. Mug Ska. Tab. Ves. Avg Soft 64.9 43.6 44.0 53.9 8.3 40.2 37.2 45.5 4.9 43.8 46.6 40.8 23.9 27.7 53.9 32.6 38.2 Soft w/ V.R. 64.1 41.6 39.2 53.2 12.5 38.3 37.7 44.5 3.7 39.8 52.8 44.0 24.9 25.6 54.4 30.7 37.9 Soft w/ D.R. 63.2 42.7 43.9 55.8 16.7 42.2 38.6 43.2 4.9 42.4 48.9 41.9 26.8 28.2 54.0 30.3 39.0 Samp. Gau. (N s = 1) 65.0 43.0 41.2 53.6 6.2 43.4 38.7 42.5 6.2 45.4 50.6 43.5 26.3 37.5 51.6 33.3 39.3 Samp. Gau. (N s = 5) 65.1 42.4 43.8 54.7 12.5 43.2 37.1 44.6 1.9 45.4 46.6 44.7 29.7 26.7 54.6 31.4 39.0 Samp. Gau. (N s = 10) 64.0 45.5 41.7 58.6 20.8 40.9 37.0 43.4 3.7 45.7 48.3 46.4 18.2 34.4 53.5 32.3 39.7 Samp. Gau. (N s = 30) 64.3 45.1 47.5 58.4 6.2 44.6 39.2 45.4 6.2 45.8 48.7 43.4 29.9 30.4 54.1 28.8 39.9</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Quantitative results of unsupervised learning on KeypointNet dataset, reported as DAS (higher is better). 52.8 54.7 63.4 70.9 56.1 61.6 50.3 82.4 59.8 71.7 65.3 85.1 38.1 62.3 Soft w/ D.R. 47.9 35.5 47.3 46.1 58.3 65.5 60.9 35.3 47.6 69.3 64.1 55.0 45.9 44.2 57.6 28.8 50.6 Samp. Gau. (N s = 1) 73.9 53.8 63.5 43.9 67.0 69.3 77.7 46.6 59.1 55.9 87.8 59.0 67.0 66.2 80.3 36.4 62.9 Samp. Gau. (N s = 5) 73.1 54.0 61.9 48.4 64.4 67.0 81.1 50.7 55.2 50.1 87.5 58.2 58.9 65.9 77.9 41.2 62.2 Samp. Gau. (N s = 10) 73.9 58.8 61.7 46.2 60.9 68.6 72.0 53.6 56.5 48.1 91.6 59.8 68.8 65.8 83.5 34.9 62.8 Samp. Gau. (N s = 30) 71.2 56.7 60.0 51.0 58.4 64.1 83.8 47.6 61.8 47.8 91.3 55.5 68.5 70.6 81.7 37.5 63.0</figDesc><table><row><cell>Ves. Avg</cell></row></table><note>Air. Bat. Bed Bot. Cap Car Cha. Gui. Hel. Kni. Lap. Mot. Mug Ska. Tab.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Quantitative results on MTFL dataset. Soft Soft w/ V.R. Soft w/ D.R. Samp. Uni. Samp. Tri. Samp. Gau.</figDesc><table><row><cell>Abs. Err ? 3.18 Rel. Err ? 7.25</cell><cell>3.16 7.22</cell><cell>3.15 7.20</cell><cell>3.00 6.86</cell><cell>2.98 6.82</cell><cell>2.94 6.96</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t to 4, the loss weight of variance regularization to 1, and the loss weight of distributions regularization to 0.1 to achieve the best results after tuning.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In the supplemental document, we elaborate on the training settings (Appendix A), the broader impact of our work (Appendix B), limitation and future work (Appendix C), descriptions of the utilized datasets (Appendix D), experiments on facial landmark localization (Appendix E), comparison between the learned distribution of soft-argmax and sampling-argmax(Appendix F), and qualitative results (Appendix G).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Training Details</head><p>2D Human Pose Estimation from RGB We adopt SimplePose <ref type="bibr" target="#b42">[46]</ref> for experiments. The model is trained and evaluated on COCO Keypoint <ref type="bibr" target="#b24">[28]</ref>. ResNet-50 <ref type="bibr" target="#b11">[15]</ref> is adopted as the backbone network. The input image is resized to 256 ? 192. The learning rate is set to 1 ? 10 ?3 at first and reduced by a factor of 10 at the 90th epoch and the 120th epoch. We use the Adam solver and train for 140 epochs, with a mini-batch size of 32 per GPU and 8 1080Ti GPUs in total. For comparison with the auxiliary losses, we set the target variance ? 2 t to 4, the loss weight of variance regularization to 1, and the loss weight of distributions regularization to 0.1 to achieve the best results after tuning.</p><p>3D Human Pose Estimation from RGB We follow the model architecture of Integral Pose <ref type="bibr" target="#b39">[43]</ref>. ResNet-50 <ref type="bibr" target="#b11">[15]</ref> is adopted as the backbone network. The input image is resized to 256 ? 256. The learning rate is set to 1 ? 10 ?3 at first and reduced by a factor of 10 at the 90th and 120th epoch. We use the Adam solver and train for 140 epochs, with a mini-batch size of 16 per GPU and 8 1080Ti GPUs in total. Following the settings of previous works <ref type="bibr" target="#b39">[43,</ref><ref type="bibr" target="#b31">35]</ref>, we mix Human3.6M and MPII <ref type="bibr" target="#b4">[8]</ref> data for training. Each mini-batch consists of half 2D and half 3D samples. Five subjects (S1, S5, S6, S7, S8) are used for training and two subjects (S9, S11) for evaluation. We set the target variance ? 2 Checklist 1. For all authors...</p><p>(a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? <ref type="bibr">[Yes]</ref> We claim that the proposed sampling-argmax can help the model obtain well-calibrated probability maps and improve the localization accuracy. In our experiments, we validate the localization accuracy of sampling-argmax across six tasks. We also demonstrate that the probability maps are well-calibrated by conducting correlation testing. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Facial landmark detection by deep multi-task learning</title>
		<ptr target="http://mmlab.ie.cuhk.edu.hk/projects/TCDCN.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
		<ptr target="http://vision.imar.ro/human3.6m/description.php" />
		<imprint/>
	</monogr>
	<note>6m dataset</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
		<ptr target="http://vision.imar.ro/human3.6m/eula.php" />
		<imprint/>
	</monogr>
	<note>6m license agreement</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keypointnet</surname></persName>
		</author>
		<ptr target="https://github.com/qq456cvb/KeypointNet" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention-driven cropping for very high resolution facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashanth</forename><surname>Chandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thabo</forename><surname>Beeler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deeppruner: Learning efficient stereo matching via differentiable patchmatch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivam</forename><surname>Duggal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep spatial autoencoders for visuomotor learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to linearize under uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generalized procrustes analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gower</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Statistical theory of extreme values and some practical applications: a series of lectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emil</forename><forename type="middle">Julius</forename><surname>Gumbel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">US Government Printing Office</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fully convolutional boundary regression for retina oct segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Carass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><forename type="middle">D</forename><surname>Jedynak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiv</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saidha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><forename type="middle">L</forename><surname>Calabresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Retinal layer parcellation of optical coherence tomography images: Data resource for multiple sclerosis and healthy controls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Carass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiv</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saidha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><forename type="middle">L</forename><surname>Calabresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prince</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Data in brief</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving landmark localization with semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Human3.6m: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cylindrical convolutional networks for joint object detection and viewpoint estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghun</forename><surname>Joung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ig-Jae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayk</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saumitro</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semantic parsing with semi-supervised sequential autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom??</forename><surname>Ko?isk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?bor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sfnet: Learning object-aware semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junghyup</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dohyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Human pose regression with residual log-likelihood estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ailing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11025" to="11034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Crowdpose: Efficient crowded scenes pose estimation and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10863" to="10872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hybrik: A hybrid analytical-neural inverse kinematics solution for 3d human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3383" to="3393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Grand challenge of 106-point facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinglu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibin</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqin</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICMEW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">and Hedi Tabia. 2d/3d pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Diogo C Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Human pose regression by combining indirect part detection and contextual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Diogo C Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tabia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Chris J Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A* sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Evopose2d: Pushing the boundaries of 2d human pose estimation using neuroevolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Mcnally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanav</forename><surname>Vats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mcphee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.08446</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Camera distance-aware top-down approach for 3D multi-person pose estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Posefix: Model-agnostic general human pose refinement network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiden</forename><surname>Nibali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Prendergast</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07372</idno>
		<title level="m">Numerical coordinate regression with convolutional neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Coarse-tofine volumetric prediction for single-image 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Skeleton merger: an unsupervised aligned keypoint detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoxi</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengrong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning to orient surfaces by self-supervised spherical cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Spezialetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marlon</forename><surname>Marcon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luciano</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuele</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hmor: Hierarchical multi-person ordinal relations for monocular multi-person 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="242" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Syncspeccnn: Synchronized spectral cnn for 3d shape segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingwen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Keypointnet: A large-scale 3d keypoint dataset aggregated from numerous human annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujing</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep imitation learning for complex manipulation tasks from virtual reality teleoperation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoe</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Jow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICRA</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unsupervised learning of stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2017. Soft-Argmax Sampling-Argmax Figure 4: Visualization of the learned distribution. Left: Soft-Argmax. Right: Sampling-Argmax</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
