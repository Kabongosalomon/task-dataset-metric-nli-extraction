<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-10-08">8 Oct 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Laboratory</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanglu</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Laboratory</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Huang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-10-08">8 Oct 2021</date>
						</imprint>
					</monogr>
					<note>Preprint. UNINET: UNIFIED ARCHITECTURE SEARCH WITH CONVOLUTION, TRANSFORMER, AND MLP</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, transformer and multi-layer perceptron (MLP) architectures have achieved impressive results on various vision tasks. A few works investigated manually combining those operators to design visual network architectures, and can achieve satisfactory performances to some extent. In this paper, we propose to jointly search the optimal combination of convolution, transformer, and MLP for building a series of all-operator network architectures with high performances on visual tasks. We empirically identify that the widely-used strided convolution or pooling based down-sampling modules become the performance bottlenecks when the operators are combined to form a network. To better tackle the global context captured by the transformer and MLP operators, we propose two novel context-aware down-sampling modules, which can better adapt to the global information encoded by transformer and MLP operators. To this end, we jointly search all operators and down-sampling modules in a unified search space. Notably, Our searched network UniNet (Unified Network) outperforms state-of-theart pure convolution-based architecture, EfficientNet, and pure transformer-based architecture, Swin-Transformer, on multiple public visual benchmarks, ImageNet classification, COCO object detection, and ADE20K semantic segmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Convolutional Neural Networks (CNN) dominate the learning of visual representations and show effectiveness on various downstream tasks, including image classification, object detection, semantic segmentation, etc. Recently, convolution-free backbones show impressive performances on image classification <ref type="bibr" target="#b4">(Deng et al., 2009)</ref>. Vision Transformer (ViT) <ref type="bibr" target="#b5">(Dosovitskiy et al., 2020)</ref> firstly shows that pure Transformer architecture can attain state-of-the-art performance when trained on largescale datasets (e.g. ImageNet-21k, JFT-300M). Data-efficient image Transformers (DeiT) <ref type="bibr" target="#b24">(Touvron et al., 2021b)</ref> is competitive with CNNs when trained with ImageNet only. MLP-Mixer <ref type="bibr" target="#b22">(Tolstikhin et al., 2021)</ref> introduced a pure multi-layer perceptron (MLP) architecture that can almost match ViT's performance without using the time-consuming attention mechanism. However, the recent transformer or mixer-based architectures are still manually designed with human experience. The different operators <ref type="bibr">(convolution, attention, MLP-mixer, etc.)</ref> have different properties, and how to properly assemble them to form networks is still under investigation. The underlying optimal architectures might be quite different for different dataset sizes and computational budgets. On the one hand, convolutions in CNNs are locally connected and their weights are input-independent, which makes it effective at extracting low-level representations and efficient under the low-data regime. On the other hand, the attention operations in the Transformer capture long-range dependency, and the attention weights are dynamically dependent on the input representations. Hence, it requires a significant amount of data and computational resources. There were recent papers on attempting to manually combine the different types of operators <ref type="bibr" target="#b3">(d'Ascoli et al., 2021;</ref> to form hybrid convolution-transformer visual networks, which, however, lack proper design principles and did not show promising performance. We conducted a pilot study on attempting to directly stack different operators to form networks. As shown in <ref type="table" target="#tab_1">Table 1</ref>  of different operators achieve even worse result than the original ViT with the only self-attention operator. In this paper, we search the combination of convolution, transformer, and MLP operators, trying to assemble those operators to create novel and high-performance visual network architectures. To the best of our knowledge, we are the first to automatically search the assembly of all types of operators to form novel network architectures. As different operators have distinct characteristics, it is non-trivial to merge them into a super-net that can achieve superior performance.</p><p>In addition, we find that the widely used down-sampling modules, such as strided convolution or max pooling, actually become the bottlenecks that hinder the searched architectures to achieve optimal performance. This might be derived from the fact that the different operations searched in our architectures, have distinct characteristics. One of the biggest differences is the receptive field. Convolution has local receptive field and is effective at capturing local features, which welly preserve the spatial structure after transformation. However, attention and MLP-mixer have global receptive field, and each pixel in the output feature map is a weighted sum of all spatial locations, which may destroy the spatial structure. Therefore, the locally-correlated down-sampling module is no longer suitable. To address the above issue, we propose three types of down-sampling modules (DSM), which satisfy different operation combinations. Besides the traditional Local-DSM (L-DSM) that are modeled as strided convolution, we propose Local-Global-DSM (LG-DSM) and Global-DSM (G-DSM) to conduct down-sample based on global context. Both LG-DSM and G-DSM use attention mechanisms to gather global features, but the ways to generate query features for calculating the attention weights are different to capture different characteristics of the context.</p><p>We jointly search the operation combination and down-sampling module and network size in a unified search space. We propose a novel scheme to automatically search both the building blocks and the proposed down-sampling modules in a joint manner. To find the optimal architecture, we use Reinforcement Learning approach to optimize accuracy and computation cost simultaneously. Our searched architecture, named UniNet (Unified Network), achieves strong results on various vision tasks. Our UniNet architectures achieve better performance than state-of-the-art pure convolution architecture, EfficientNet, and pure transformer architecture, Swin Transformer. For instance, on ImageNet, our UniNet-B3 is able to achieve 83.7% and 85.2% top-1 accuracy with 4.2G and 23.2G FLOPs respectively. On COCO <ref type="bibr" target="#b9">(Lin et al., 2014)</ref>, UniNet-B3 achieves 47.9 mAP with 50M parameters, which is also better than the recent local transformer-based architecture Swin-T.</p><p>In summary, our contributions are as follows:</p><p>1. We are the first to jointly search the optimal combination of convolution, transformer, and MLP to identify high-performance visual neural networks. 2. We reveal the traditional down-sampling module becomes the bottleneck of performance when combining different types of operators. We propose context-aware down-sampling modules, and search them with general processing operations in a joint manner. 3. Our searched hybrid architecture, UniNet, outperforms previous pure convolution architectures EfficientNet and pure transformer architecture Swin Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>Convolution, Transformer, and MLP. A host of ConvNets have been proposed to push forward the state-of-the-art computer vision approaches such as <ref type="bibr" target="#b8">(He et al., 2016;</ref><ref type="bibr" target="#b18">Szegedy et al., 2015;</ref>. Despite the numerous CNN models, their basic operations, convolution, are the same.</p><p>Recently, <ref type="bibr" target="#b5">Dosovitskiy et al. (2020)</ref> proposed a pure transformer based image classification model ViT, which achieves impressive performance on ImageNet benchmark. DeiT <ref type="bibr" target="#b24">(Touvron et al., 2021b)</ref> reveals that well-trained ViT can obtain a better performance-speed trade-off than ConvNets. PVT <ref type="bibr" target="#b27">(Wang et al., 2021b)</ref> proposes a pyramid vision transformer, which can be easily transferred to other downstream tasks. On the other hand, recent papers are attempting to use only MLP as the building block. MLP-Mixer <ref type="bibr" target="#b22">(Tolstikhin et al., 2021)</ref> and ResMLP <ref type="bibr" target="#b23">(Touvron et al., 2021a)</ref> show that a pure MLP architecture can also achieve near state-of-the-art performance.</p><p>Combination of different operators. Besides, another line of works tries to combine different operators to form a new network. CvT  propose to incorporate self-attention and convolution by generating Q, K, and V in self-attention with convolution. CeiT  replace the original patchy stem with convolutional stem and add depthwise convolution to FFN layer, which obtains fast convergence and better performance. <ref type="bibr">ConViT (d'Ascoli et al., 2021)</ref> tries to unify convolution and self-attention with gated positional self-attention and is more sample-efficient than self-attention. Previous works use a different form to combine those operators and get promising results, but requires tedious manual design, lacking effective guidelines. In this work, we propose a unified search space, in which we can search the combination of different operations automatically.</p><p>Down-sampling module. In ConvNets, the down-sampling module (DSM) is implemented with strided-Conv or pooling. As DSM breaks the shift invariant of convolution, Zhang (2019) propose anti-aliased DSM to keep it. Besides, a line of works tries to preserve more information when downsampling with a learnable or dynamic kernel <ref type="bibr" target="#b7">(Gao et al., 2019;</ref><ref type="bibr" target="#b13">Saeedan et al., 2018;</ref><ref type="bibr" target="#b26">Wang et al., 2021a)</ref>. Most of their approaches are down-sampling based on local context, which we show is not suitable for our unified network. In our work, we propose context-aware DSM and jointly search with operation combinations, which guarantees better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">UNIFIED ARCHITECTURE SEARCH</head><p>As discussed in previous works <ref type="bibr" target="#b3">(d'Ascoli et al., 2021)</ref>, an appropriate combination of convolution and transformer operators can lead to performance improvements. However, the previous approaches  only adopt convolution in self-attention or feed-forward network (FFN) sub-layers and stack them repeatedly. Their approaches did not fully explore their combinations to take advantage of their different characteristics. Besides, effective guidelines of properly assembling those operators are missing in prior works, which is important for designing new architectures.</p><p>Prior art <ref type="bibr" target="#b26">(Wang et al., 2021a;</ref><ref type="bibr" target="#b31">Zhang, 2019)</ref> show that the down-sampling module plays an important role in visual tasks. Most previous approaches adopt hand-crafted downsampling operations, i.e. strided convolution, max-pooling, or avg-pooling, to down-sample the feature map based on only the local context. However, these operations are specifically designed for ConvNets, and might not be suitable to the transformer or MLP based architectures, which capture representation globally.</p><p>In this paper, we jointly search the combination of convolution, transformer, and MLP operators, trying to assemble the operators to create novel and high-performance visual network architectures.</p><formula xml:id="formula_0">? ! ? " Input image Output GOP DSM GOP GOP DSM ? GOPs: ? Convolution ? Transformer ? MLP DSMs: ? L-DSM ? G-DSM ? LG-DSM ? # Size: ? Repeats ? Channels ? Expansion</formula><p>Unified Search Space: <ref type="figure">Figure 2</ref>: Unified Architecture Search. We jointly search the all operators and down-sampling modules and network size in a unified search space.</p><p>For better transform features across different operator blocks, we proposed context-aware downsampling modules. We jointly search the operators, down-sampling modules, and network size in a unified search space. In contrast, previous Neural Architecture Search (NAS) works achieved stateof-the-art performances mainly via searching the network sizes. We show that our proposed unified architecture search can achieve very promising performance with optimal all-operator network architectures.</p><p>In the remaining parts of the section, we firstly present how to properly combine different operators into a unified search space and search them jointly. We then present the challenge of incorporating down-sampling modules with different operators, and present our proposed context-aware downsampling module. Finally, we will introduce our UniNet architecture and NAS pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MODELING CONVOLUTION, ATTENTION, MLP WITH A UNIFIED SEARCHABLE FORM</head><p>Recently, transformer and MLP based architectures are able to achieve comparable performance with convolution-based ones on general visual tasks. To achieve better performance, it is intuitive to assemble all the types of operators to build high-performance all-operator networks. Actually, a few works <ref type="bibr" target="#b3">d'Ascoli et al., 2021)</ref> have been studied to empirically combine convolution and self-attention. However, manually search of network architectures is quite time-consuming and cannot ensure optimal performances with different computational budgets.</p><p>We introduce a unified search space that contains General Operators (GOPs, including convolution, transformer, and MLP), and then search for the optimal combination of those operators jointly. Compared with the prior art, we use a unified form to characterize each operator. Specifically, we use inverted residual <ref type="bibr" target="#b14">(Sandler et al., 2018)</ref> to model a general operator block, which first expands the input channel c to a larger size ec, and later projects the ec channels back to c for residual connection. The e is denoted as the expansion ratio, which is usually a small integer number, e.g., 4. The general operator block is modeled as</p><formula xml:id="formula_1">y = x + Operation(x),<label>(1)</label></formula><p>where Operation can be convolution, MLP, or self-attention operators, and x, y represent input and output features, respectively. For convolution, we place the convolution operation inside the bottleneck <ref type="bibr" target="#b14">(Sandler et al., 2018)</ref>, which can be expressed as</p><formula xml:id="formula_2">Operation(x) = Proj ec?c (Conv(Proj c?ec (x))).<label>(2)</label></formula><p>The Conv operation can be either regular convolution or depth-wise convolution (DWConv) <ref type="bibr" target="#b2">(Chollet, 2017)</ref>, and the Proj represents a linear projection. For self-attention and MLP, operating on the large bottleneck feature map can be quilt slow. Following previous works <ref type="bibr" target="#b5">(Dosovitskiy et al., 2020;</ref><ref type="bibr" target="#b22">Tolstikhin et al., 2021)</ref>, we separate them from the bottleneck for computation efficiency, and the Proj is implemented inside the FFN <ref type="bibr" target="#b25">(Vaswani et al., 2017)</ref> sub-layer. Each transformer block has a query-key-value attention sub-layer and an FFN sub-layer. For MLP block, we implement with transpose-FFN-transpose like <ref type="bibr" target="#b22">Tolstikhin et al. (2021)</ref>.</p><formula xml:id="formula_3">y = y + FFN(y ),<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">y = x + SA(x) or x + MLP(x), FFN(y ) = Proj ec?c (Proj c?ec (y )),<label>(4)</label></formula><p>where SA can be either vanilla self-attention or local self-attention LSA, and MLP refers to tokenmixing operation.</p><p>There are two main advantages of representing the different operators in a unified format and search space: (1) We can characterize each operator with the same set of configuration hyper-parameters except for the operation type, e.g. expansion rate and channel size. As a result, the search space for the operator is much simplified, which can speed up the search process.</p><p>(2) Under the same network size configuration, each operator block has a similar computation cost. The comparison between different operator combinations is fairer, which is extremely important for NAS .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">CONTEXT-AWARE DOWN-SAMPLING MODULES</head><p>As discussed in Section 3.1, the down-sampling module (DSM) plays an important role in visual tasks. In addition to hand-crafted DSM (i.e. max-pooling or avg-pooling), a few works <ref type="bibr" target="#b13">(Saeedan et al., 2018;</ref><ref type="bibr" target="#b7">Gao et al., 2019;</ref><ref type="bibr" target="#b26">Wang et al., 2021a</ref>) tried to preserve more information when downsampling with learnable or dynamic kernel. Most of their approaches are down-sampling based on local context, which suits for ConvNets well. However, in our unified search space, operators with different receptive filed can be assembled unrestrictedly to form a novel architecture, where the local context may be destroyed and therefore the previous approaches will fail. <ref type="figure">Figure 3</ref>: Structures of the context-aware down-sampling modules. In this paper, we propose context-aware DSM, which is instanced with L-DSM, LG-DSM, and G-DSM. The main difference between those DSMs is the context used when down-sampling. For L-DSM, only local context is involved, which fits ConvNets well as shown in previous works. For G-DSM, only global context is used to down-sample, which may fit other operations, e.g., transformer. The LG-DSM combines the characteristics of L-DSM and G-DSM. It uses both local and global context to downsample. Our intuition is that one of the largest dissimilarities of different operators is the receptive field. Transformer and MLP naturally have global receptive filed, while convolution has local receptive field, e.g., 3 ? 3. When combining those operators, there is no single optimal DSM that satisfies all scenarios.</p><formula xml:id="formula_5">Input Output Conv2d, s2 (a) L-DSM Input Multi-Head Attention Conv2d s2 Q K V Output (b) LG-DSM Input Multi-Head Attention Conv1d s2 Q K V Output (c) G-DSM</formula><p>The proposed DSMs are visualized in <ref type="figure">Figure 3</ref>. To down-sample based on global cues, we utilize the self-attention mechanism to capture global context, which is missed by the prior art. However, vanilla self-attention directly applies a linear transformation on input x, which is not applicable to down-sample a feature map. To address this issue, we replace the original linear transformation of a query with down-sampling operations. Specifically, for G-DSM, we use Conv1D with stride 2 to down-sample the query. To note that, there is no direct local context preserved after the transformation of G-DSM. For LG-DSM, we first reshape the flattened token sequences back to the spatial grid and apply Conv2D with stride 2 to down-sample the query, and then flatten the query back to calculate the attention weights.</p><p>Compared with previous works, which mainly try to improve ConvNets, our proposed DSMs are not designed for a specific architecture. Our motivation is that the optimal DSM is not fixed for different operators. For example, the optimal DSM may be L-DSM for ConvNets, but G-DSM for transformer. As thousands of operator combinations will be trained in our NAS process, it is unfeasible to decide which DSM to use by hand. To obtain the optimal architecture, we jointly search DSMs with the General Operators. An interesting result in our experiments is that our searched DSM matches our assumption. In our searched optimal architecture, L-DSM is used between operators with local receptive field while LG-DSM is favored by operators with global receptive field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">UNINET ARCHITECTURE</head><p>As shown in recent studies, combining different operators  can bring performance improvements. Most previous approaches only repeatedly stack the same operation in the whole architecture and use different channels in different stages. These approaches don't permit stage diversity, which we show is crucial for achieving high accuracy.</p><p>On the contrary, in our UniNet, the operators are not fixed but searched from the unified search space. We construct our UniNet architecture in a multi-stage fashion, which can be easily transferred to downstream tasks. Between two successive stages, one of our proposed DSMs will be inserted to reduce the spatial dimension. We jointly search the GOP and DSM per stage separately. The GOP could be different for different stages but repeated in one stage, which can greatly reduce the search space size as pointed out before . The overall architecture and search space are shown in <ref type="figure">Figure 2</ref>.</p><p>Thanks to our unified form of GOPs, the network size of each stage can be configured with repeat number r, channel size c, and expansion ratio e. To obtain better computation-accuracy trade-off, we jointly search the network size with the GOP and DSM. For GOP, we search for convolution, transformer, MLP and their promising variants, i.e., {SA, LSA, Conv, DWConv, MLP}, as mentioned in Section 3.2; for e, we search for {2, 3, 4, 5, 6}. Following previous work, we search the network size based on a given architecture, e.g. EfficientNet , and the channels and repeats will be changed according to it. For c and r, we search for {0.5, 0.75, 1.0, 1.25, 1.5} and {-2, -1, 0, 1, 2} respectively. Suppose we partition the network into K stages, and each stage has a sub search space of size S. Then the total search space is S N . In our implementation, K is set to 5 and S equals 125. As a result, our search space size is about 10 16 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">SEARCH ALGORITHM</head><p>We utilize Reinforcement Learning to search for the optimal architecture automatically. Concretely, we follow previous work  and map an architecture in the search space to a list of tokens, which are determined by a sequence of actions generated by an RNN. The RNN is optimized using PPO algorithm <ref type="bibr" target="#b15">(Schulman et al., 2017)</ref> by maximizing the expected reward. In our implementation, we simultaneously optimize accuracy and the theoretical computation cost (FLOPs). To handle the multi-objective optimization problem, we use a weighted product customized as  to approximate Pareto optimal. For one sampled architecture m, the reward is formulated as</p><formula xml:id="formula_6">r(m) = a(m) ? t f (m) ? ,</formula><p>where function a(m) and f (m) return the accuracy and the FLOPs of m, t is the target FLOPs, and ? is a weight factor that balances the accuracy and computation cost.</p><p>During the search process, thousands of combinations of GOPs and DSMs are trained on a proxy task with the same setting, which gives us a fair comparison between those combinations. When the RNN converges, the top-k architectures with the highest reward will be trained with full setting, and the top-performing one will be kept for model scaling and transferring to other downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL SETUP</head><p>To find the optimal architecture in our search space, we directly search on a large dataset, ImageNet. We reserve 50K images from the training set as a validation set. For each sampled architecture, we train it for 5 epochs. After that, we calculate the reward of the architecture with its FLOPs and the accuracy on the validation set. We set the target FLOPs t and weight factor ? in reward function to 550M and -0.07 respectively. During the search process, totally 2K models are trained on the proxy task. After that, we fully train the top 5 architectures on ImageNet and preserve the top-performing one for model scaling and transferring to other downstream tasks.</p><p>For regular ImageNet training, we mostly follow the training strategy in DeiT <ref type="bibr" target="#b24">(Touvron et al., 2021b)</ref>, except that we use small augmentation for small models and heave augmentation for large models as shown by <ref type="bibr" target="#b17">Steiner et al. (2021)</ref>. When the input resolution exceeds 256 ? 256, we train our models with 256 ? 256 and finetune on the large resolution for training efficiency. Besides, we also transfer UniNet to other downstream tasks, e.g., object detection on COCO and semantic segmentation on ADE20K. For COCO training, we use the typical framework Mask R-CNN and train on wildly-used 1x (12 epochs) and 3x (36 epochs) schedules. For ADE20K training, we use the UperNet framework and train with the same setting as <ref type="bibr" target="#b11">Liu et al. (2021b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">MAIN RESULTS</head><p>In this section, we firstly present our searched UniNet architecture. We then show the performance of the scaled UniNets on classification, object detection, and semantic segmentation.     or manual designed architecture <ref type="bibr" target="#b6">(Gao et al., 2021)</ref>, which show the effectiveness of our proposed unified architecture search.</p><p>Most previous transformer based architectures usually outperform convolution based architecture in high FLOPs region, but underperform in low FLOPs region. To prove the effectiveness of our searched UniNet, we scale up and down our search UniNet-B0. The FLOPs number varies from 0.56G to 23.2G, which covers both the mobile and large settings. We utilize the compound scaling  to scale depth, width, and resolution simultaneously. An exception is that we increase image resolution more slowly, which is more efficient as <ref type="bibr" target="#b0">Bello et al. (2021)</ref> shows. For object detection and semantic segmentation, we pick UniNet-B1 and UniNet-B3 and use them as the feature extractors of detection and segmentation frameworks. We compare our UniNet with other convolution or transformer based architectures. For COCO object detection, we use Mask-RCNN framework and compare the performance under 1x and 3x schedules. For ADE20K semantic segmentation we use UperNet framework and report mIoU (%) for different architectures under same the training setting. <ref type="table">Table 4</ref>, our searched UniNet consistent outperforming convolution based ResNet <ref type="bibr" target="#b8">(He et al., 2016)</ref> and transformer based PVT <ref type="bibr" target="#b27">(Wang et al., 2021b)</ref> or Swin-Transformer <ref type="bibr" target="#b11">(Liu et al., 2021b)</ref>. UniNet-B1 achieves 40.5 AP@box, which is 3.8 points better than PVT-Tiny but with 15% fewer parameters. UniNet-B4 achieves 45.6 AP@box with 1x schedule and 47.9 AP@box with 3x schedule, which is 3.4 points and 1.9 points better than Swin-T respectively. For ADE20K semantic segmentation, we achieve 49% mIoU with 59M parameters. Compared with transformer based Swin-T, our UniNet outperforms 4.5% mIoU with similar parameters. Besides, compared with convolution based ResNet101, we get 4.1% higher mIoU with 31% fewer parameters. All the results show the generalization of our searched UniNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">IMAGENET CLASSIFICATION PERFORMANCE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ABLATIVE STUDIES AND ANALYSIS</head><p>In this section, we study the impact of joint search of General Operations, and discuss the importance of context-aware down-sampling modules (DSMs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">SINGLE OPERATOR VS. GENERAL OPERATORS</head><p>Previous work  focus on the network size search, which uses single operator, convolution, as the main feature extractor. In comparison, we jointly search the combination of different General Operators (GOPs), i.e., convolution, transformer, MLP, and their promising variants. To verify the importance of GOPs, we remove MLP and transformer and their variants from our search Model #FLOPs (G) #Params (M) Top-1 Acc.</p><p>UniNet-B0 0.56 11.5 79.1</p><p>UniNet-B0 w/ Conv-Only 0.59 11.0 77.7 space, and re-run the search experiment under the same setting. After search, we fully train the top-5 architectures with highest reward on ImageNet, and report the best performance.</p><p>As shown in  When combining different operators into a unified network, the traditional down-sampling module, which is mainly implemented with strided-conv or pooling, could be sub-optimal. To verify the effectiveness of our proposed context-aware DSMs, we replace the DSMs of our search UniNet with one fixed DSM, and compare their performance under the same training setting.</p><p>As shown in <ref type="table">Table 6</ref>, our searched UniNet consistently outperforms its variants that use single fixed DSM in all stages. Although we see that using G-DSM or LG-DSM in all stages brings more computation and parameters, the performance does not become better. The result emphasizes the importance of our joint search of GOPs and DSMs.</p><p>Besides, we transfer our proposed DSMs to other popular transformer based architectures, e.g. Swin-Transformer <ref type="bibr" target="#b11">(Liu et al., 2021b)</ref> and PVT <ref type="bibr" target="#b27">(Wang et al., 2021b)</ref>. Both Swin and PVT have 4 stages. Borrowing the result from our searched UniNet, we use L-DSM for the first two stages while LG-DSM for the latter two stages. As shown in <ref type="table" target="#tab_8">Table 7</ref>, our proposed DSMs improve PVT-Tiny and Swin-T for 2.4% and 0.4% respectively. To note that, PVT uses a strided-conv to down-sample. As discussed in Section 3.3, it is harmful to the main operator in PVT, transformer, which has global receptive field. On the contrary, our proposed DSMs are able to down-sample based on both local and global context, and can greatly improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we propose to jointly search the combination of convolution, transformer, and MLP. We empirically identify that the widely-used strided convolution or pooling based down-sampling modules become the performance bottlenecks when the operators are combined to form a network. To further improve the performance, we propose context-aware down-sampling modules and jointly search them with all operators. Our searched UniNet outperforms state-of-the-art pure convolutionbased architecture, EfficientNet, and pure transformer-based architecture, Swin-Transformer, on Im-ageNet classification, COCO object detection, and ADE20K semantic segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, however, the straightforward stacking Preprint. ImageNet top-1 accuracy vs FLOPs. All models are trained with ImageNet-1k dataset. Our UniNet-B5 achieve 85.2% with 23.2G FLOPs, outperforming NFNet-F1 and BoTNet-T7 with 35% and 49% fewer FLOPs respectively.</figDesc><table><row><cell cols="2">(a) Comparison with ConvNet</cell><cell></cell><cell cols="2">(b) Comparison with transformer or hybrid</cell></row><row><cell>Figure 1: Model</cell><cell cols="4">Configuration #Params (M) #FLOPs (G) Top-1 Acc.</cell></row><row><cell>ViT</cell><cell>12 T</cell><cell>22</cell><cell>4.6</cell><cell>78.0</cell></row><row><cell>MLP-Mixer</cell><cell>18 M</cell><cell>23</cell><cell>4.7</cell><cell>76.8</cell></row><row><cell>ViT-MLP</cell><cell>7 T + 7 M</cell><cell>22</cell><cell>4.5</cell><cell>76.5</cell></row><row><cell>MLP-ViT</cell><cell>7 M + 7 T</cell><cell>22</cell><cell>4.5</cell><cell>77.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>ImageNet top-1 accuracy of different operator combinations. T and M refer to transformer block and mlp-mixer block respectively. Different block numbers are used to keep them comparable.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>UniNet performance on ImageNet. All UniNet models are trained with ImageNet dataset with 1.28m images. C, T, and H denotes convolution, transformer, and hybrid architecture respectively. ?: all EfficientNetV2 models are trained with progressive learning.</figDesc><table><row><cell>Model</cell><cell>Family</cell><cell>Input Size</cell><cell>#FLOPs (G)</cell><cell></cell><cell cols="2">#Params (M)</cell><cell>Top-1 Acc.</cell></row><row><cell>EffNet-B0 (Tan &amp; Le, 2019)</cell><cell>C</cell><cell>224</cell><cell>0.39</cell><cell></cell><cell>5.3</cell><cell>77.1</cell></row><row><cell>EffNetV2-B0  ? (Tan &amp; Le, 2021)</cell><cell>C</cell><cell>240</cell><cell>0.7</cell><cell></cell><cell>7.4</cell><cell>78.7</cell></row><row><cell>DeiT-Tiny (Touvron et al., 2021b)</cell><cell>T</cell><cell>224</cell><cell>1.3</cell><cell></cell><cell>5.7</cell><cell>72.2</cell></row><row><cell>PVT-Tiny (Wang et al., 2021b)</cell><cell>T</cell><cell>224</cell><cell>1.9</cell><cell></cell><cell cols="2">13.2</cell><cell>75.1</cell></row><row><cell>ConViT-Ti+ (d'Ascoli et al., 2021)</cell><cell>H</cell><cell>224</cell><cell>2</cell><cell></cell><cell>10</cell><cell>76.7</cell></row><row><cell>UniNet-B0</cell><cell>H</cell><cell>160</cell><cell>0.56</cell><cell></cell><cell cols="2">11.9</cell><cell>79.1</cell></row><row><cell>EffNet-B2 (Tan &amp; Le, 2019)</cell><cell>C</cell><cell>260</cell><cell>1</cell><cell></cell><cell>9.2</cell><cell>80.1</cell></row><row><cell>EffNetV2-B1  ? (Tan &amp; Le, 2021)</cell><cell>C</cell><cell>260</cell><cell>1.2</cell><cell></cell><cell>8.1</cell><cell>79.8</cell></row><row><cell>RegNetY-4G (Radosavovic et al., 2020)</cell><cell>C</cell><cell>224</cell><cell>4</cell><cell></cell><cell cols="2">20.6</cell><cell>80</cell></row><row><cell>DeiT-Small (Touvron et al., 2021b)</cell><cell>T</cell><cell>224</cell><cell>4.3</cell><cell></cell><cell>22</cell><cell>79.8</cell></row><row><cell>PVT-Small (Wang et al., 2021b)</cell><cell>T</cell><cell>224</cell><cell>3.8</cell><cell></cell><cell cols="2">24.5</cell><cell>79.8</cell></row><row><cell>UniNet-B1</cell><cell>H</cell><cell>192</cell><cell>0.99</cell><cell></cell><cell>14</cell><cell>80.4</cell></row><row><cell>EffNet-B3 (Tan &amp; Le, 2019)</cell><cell>C</cell><cell>300</cell><cell>1.8</cell><cell></cell><cell>12</cell><cell>81.6</cell></row><row><cell>EffNetV2-B3  ? (Tan &amp; Le, 2021)</cell><cell>C</cell><cell>300</cell><cell>3</cell><cell></cell><cell>14</cell><cell>82.1</cell></row><row><cell>Swin-T (Liu et al., 2021b)</cell><cell>T</cell><cell>224</cell><cell>4.5</cell><cell></cell><cell>29</cell><cell>81.3</cell></row><row><cell>CvT-13-NAS (Wu et al., 2021)</cell><cell>H</cell><cell>224</cell><cell>4.1</cell><cell></cell><cell>18</cell><cell>82.2</cell></row><row><cell>ConViT-B+ (d'Ascoli et al., 2021)</cell><cell>H</cell><cell>224</cell><cell>30</cell><cell></cell><cell>152</cell><cell>82.5</cell></row><row><cell>UniNet-B2</cell><cell>H</cell><cell>224</cell><cell>2.4</cell><cell></cell><cell cols="2">22.5</cell><cell>82.7</cell></row><row><cell>EffNet-B4 (Tan &amp; Le, 2019)</cell><cell>C</cell><cell>380</cell><cell>4.2</cell><cell></cell><cell>19</cell><cell>82.9</cell></row><row><cell>NFNet-F0 (Brock et al., 2021)</cell><cell>C</cell><cell>256</cell><cell>12.4</cell><cell></cell><cell cols="2">71.5</cell><cell>83.6</cell></row><row><cell>Swin-B (Liu et al., 2021b)</cell><cell>T</cell><cell>224</cell><cell>15.4</cell><cell></cell><cell>88</cell><cell>83.5</cell></row><row><cell>CvT-21 (Wu et al., 2021)</cell><cell>H</cell><cell>384</cell><cell>24.9</cell><cell></cell><cell>32</cell><cell>83.3</cell></row><row><cell>UniNet-B3</cell><cell>H</cell><cell>256</cell><cell>4.2</cell><cell></cell><cell cols="2">31.8</cell><cell>83.7</cell></row><row><cell>UniNet-B4</cell><cell>H</cell><cell>256</cell><cell>9.9</cell><cell></cell><cell cols="2">73.5</cell><cell>84.2</cell></row><row><cell>EffNet-B7 (Tan &amp; Le, 2019)</cell><cell>C</cell><cell>600</cell><cell>37</cell><cell></cell><cell>66</cell><cell>84.3</cell></row><row><cell>EffNetV2-M  ? (Tan &amp; Le, 2021)</cell><cell>C</cell><cell>480</cell><cell>24</cell><cell></cell><cell>54</cell><cell>85.1</cell></row><row><cell>NFNet-F2 (Brock et al., 2021)</cell><cell>C</cell><cell>352</cell><cell>62.6</cell><cell></cell><cell cols="2">193.8</cell><cell>85.1</cell></row><row><cell>BoTNet-T7 (Srinivas et al., 2021)</cell><cell>T</cell><cell>384</cell><cell>45.8</cell><cell></cell><cell cols="2">75.1</cell><cell>84.7</cell></row><row><cell>UniNet-B5</cell><cell>H</cell><cell>384</cell><cell>23.2</cell><cell></cell><cell cols="2">73.5</cell><cell>85.2</cell></row><row><cell>Table 2: 5.1 THE SEARCHED UNINET</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Stage</cell><cell cols="2">Operator GOP</cell><cell>DSM</cell><cell>Network Size e c r</cell></row><row><cell></cell><cell>0</cell><cell cols="2">DWConv</cell><cell cols="2">L-DSM</cell><cell>4 48</cell><cell>2</cell></row><row><cell></cell><cell>1</cell><cell cols="2">DWConv</cell><cell cols="2">L-DSM</cell><cell>6 80</cell><cell>4</cell></row><row><cell></cell><cell>2</cell><cell cols="2">DWConv</cell><cell cols="2">L-DSM</cell><cell>3 128 4</cell></row><row><cell></cell><cell>3</cell><cell cols="5">Transformer LG-DSM 2 128 4</cell></row><row><cell></cell><cell>4</cell><cell cols="5">Transformer LG-DSM 5 256 8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: UniNet-B0 architecture. GOP and DSM</cell></row><row><cell>represent General Operators and down-sampling</cell></row><row><cell>module respectively. DWConv and Transformer</cell></row><row><cell>are are described in Section 3.2.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>shows our searched UniNet-B0 archi-</cell></row><row><cell>tecture. Our searched architecture has the fol-</cell></row><row><cell>lowing characteristics: (1) UniNet uses DW-</cell></row><row><cell>Conv at early stages, while uses transformer</cell></row><row><cell>at later stages. (2) UniNet chooses L-DSM</cell></row><row><cell>to down-sample in the DWConv stage, while</cell></row><row><cell>uses LG-DSM for transformer. Surprisingly,</cell></row><row><cell>the searched operator combination somewhat</cell></row><row><cell>matches previous empirical finds</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2</head><label>2</label><figDesc>presents the performance comparison of our searched UniNet and other architectures, including convolution based, transformer based, and hybrid architectures. Our search UniNet has better accuracy and computation efficiency than previous ConvNets, transformer, or hybrid architectures. Object detection, instance segmentation, and semantic segmentation performance on the COCO val2017 and ADE20K val set. All UniNet models are pretrained on the ImageNet-1K dataset.</figDesc><table><row><cell cols="7">As shown in Table 2, under mobile setting, our UniNet-B0 achieves 79.1% top-1 accuracy with</cell></row><row><cell cols="7">0.56G FLOPs, outperforming EfficientNetV2-B0 (Tan &amp; Le, 2021) with less FLOPs. In the middle</cell></row><row><cell cols="7">FLOPs region, our UniNet-B3 achieves 83.7% top-1 accuracy with 4.2G FLOPs, which outper-</cell></row><row><cell cols="7">forms pure convolution based EfficientNet-B4, pure transformer based Swin-B, and hybrid architec-</cell></row><row><cell cols="7">ture CvT-21. For larger models, our UniNet-B5 achieve 85.2% with 23.2G FLOPs, outperforming</cell></row><row><cell cols="7">NFNet-F2 and BoTNet-T7 with 63% and 49% fewer FLOPs respectively. Figure 1 further visualizes</cell></row><row><cell cols="3">the comparison with on accuracy and FLOPs.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">5.3 OBJECT DETECTION AND SEMANTIC SEGMENTATION PERFORMANCE</cell><cell></cell></row><row><cell>Backbone</cell><cell>#Params (M) Det/Seg</cell><cell cols="4">Mask R-CNN 1x AP@box AP@mask AP@box AP@mask Mask R-CNN 3x</cell><cell>UperNet mIoU (%)</cell></row><row><cell>ResNet18</cell><cell>31/ -</cell><cell>34.0</cell><cell>31.2</cell><cell>36.9</cell><cell>33.6</cell><cell>-</cell></row><row><cell>ResNet50</cell><cell>44/ -</cell><cell>38.0</cell><cell>34.4</cell><cell>41.0</cell><cell>37.1</cell><cell>-</cell></row><row><cell>PVT-Tiny</cell><cell>33/ -</cell><cell>36.7</cell><cell>35.1</cell><cell>39.8</cell><cell>37.4</cell><cell>-</cell></row><row><cell>UniNet-B1</cell><cell>28/38</cell><cell>40.5</cell><cell>37.5</cell><cell>44.4</cell><cell>40.1</cell><cell>42.7</cell></row><row><cell>ResNet101</cell><cell>63/86</cell><cell>40.4</cell><cell>36.4</cell><cell>42.8</cell><cell>38.5</cell><cell>44.9</cell></row><row><cell>PVT-Small</cell><cell>44/ -</cell><cell>40.4</cell><cell>37.8</cell><cell>43.0</cell><cell>39.9</cell><cell>-</cell></row><row><cell>Swin-T</cell><cell>48/60</cell><cell>42.2</cell><cell>39.1</cell><cell>46.0</cell><cell>41.6</cell><cell>44.5</cell></row><row><cell>UniNet-B3</cell><cell>50/59</cell><cell>45.6</cell><cell>41.6</cell><cell>47.9</cell><cell>43.3</cell><cell>49.0</cell></row><row><cell>Table 4:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Performance on ImageNet of UniNet with different search settings. Conv-Only represents search UniNet with convolution operator only.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc>, our joint search of GOPs outperforms Conv-Only search by a large margin. The result verifies the effectiveness of our joint search of GOPs, which can combine the characteristics of different operators. To note that, the DSMs are also searched in the Conv-Only search experiment. However, the top-performing architecture chooses L-DSM in all its stages.6.2 FIXED VS. CONTEXT-AWARE DOWN-SAMPLING MODULE Performance on ImageNet of UniNetwith different DSMs. To note that, the result of traditional strided-conv based down-sampling module is shown in row 2.</figDesc><table><row><cell>Model UniNet w/ L-DSM w/ G-DSM w/ LG-DSM Table 6: Model #FLOPs (G) #Params (M) Top-1 Acc. 0.56 11.5 79.1 0.54 11.3 78.5 0.77 12.7 76.8 0.72 14.1 78.9 PVT-Tiny w/ DSM Swin-T w/ DSM</cell><cell>#FLOPs (G) 1.9 2.0 4.5 4.7</cell><cell>#Params (M) 13.2 14.3 29.0 30.0</cell><cell>Top-1 Acc. 75.1 77.5 81.2 81.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Performance comparison on Ima-geNet of different backbones when equipped with our proposed DSMs.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07579</idno>
		<title level="m">Revisiting resnets: Improved training and scaling strategies</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">High-performance large-scale image recognition without normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06171</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>St?phane D&amp;apos;ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulio</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sagun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10697</idno>
		<title level="m">Convit: Improving vision transformers with soft convolutional inductive biases</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Container: Context aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.01401</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lip: Local importance-based pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziteng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3355" to="3364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangting</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanglu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.11694</idno>
		<title level="m">Fnas: Uncertainty-aware fast neural architecture search</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10428" to="10436" />
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Doll?r</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Detail-preserving pooling in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faraz</forename><surname>Saeedan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Goesele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9108" to="9116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mo-bilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bottleneck transformers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16519" to="16529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">How to train your vit? data, augmentation, and regularization in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10270</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnetv2</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.00298</idno>
		<title level="m">Smaller models and faster training</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01601</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03404</idno>
		<title level="m">Resmlp: Feedforward networks for image classification with data-efficient training</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Carafe++: Unified content-aware reassembly of features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15808</idno>
		<title level="m">Cvt: Introducing convolutions to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.14881</idno>
		<title level="m">Piotr Doll?r, and Ross Girshick. Early convolutions help transformers see better</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Incorporating convolution designs into visual transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaopeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aojun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11816</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Making convolutional networks shift-invariant again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
