<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 1 Weakly Supervised Person Re-ID: Differentiable Graphical Learning and A New Benchmark</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangcong</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xujie</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengtao</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 1 Weakly Supervised Person Re-ID: Differentiable Graphical Learning and A New Benchmark</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Weakly Supervised Learning</term>
					<term>Person Re- identification</term>
					<term>Graphical Neural Networks</term>
					<term>Visual Surveillance</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Person re-identification (Re-ID) benefits greatly from the accurate annotations of existing datasets (e.g., CUHK03</p><p>[1] and Market-1501 [2]), which are quite expensive because each image in these datasets has to be assigned with a proper label. In this work, we ease the annotation of Re-ID by replacing the accurate annotation with inaccurate annotation, i.e., we group the images into bags in terms of time and assign a bag-level label for each bag. This greatly reduces the annotation effort and leads to the creation of a large-scale Re-ID benchmark called SYSU-30k. The new benchmark contains 30k individuals, which is about 20 times larger than CUHK03 (1.3k individuals) and Market-1501 (1.5k individuals), and 30 times larger than ImageNet (1k categories). It sums up to 29,606,918 images. Learning a Re-ID model with bag-level annotation is called the weakly supervised Re-ID problem. To solve this problem, we introduce a differentiable graphical model to capture the dependencies from all images in a bag and generate a reliable pseudo label for each person image. The pseudo label is further used to supervise the learning of the Re-ID model. When compared with the fully supervised Re-ID models, our method achieves state-of-the-art performance on SYSU-30k and other datasets. The code, dataset, and pretrained model will be available at https://github.com/wanggrun/SYSU-30k.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>an example of strong annotation while (b) is an example of a weak annotation. During testing, there is no difference between the fully and weakly supervised Re-ID problems, i.e., they both aim at finding the best-matching image for a given person image, as shown in (c).</p><p>An alternative way to create a Re-ID benchmark is to replace image-level annotations with bag-level annotations. Suppose that there is a short video containing many person images; we do not need to know who is in each image. A cast of characters is enough. Here, the exact person ID of each image is called the image-level label ( <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>), and the cast of characters is called the bag-level label ( <ref type="figure" target="#fig_0">Fig. 1  (b)</ref>). Based on our experience, collecting bag-level annotations is approximately three times faster/cheaper than collecting image-level annotations. Once the dataset has been collected, we train Re-ID models with the bag-level annotations. We call this the weakly supervised Re-ID problem.</p><p>Formally, with strong supervision, the supervised learning task is to learn f : X ? Y from a training set {(x 1 , y 1 ), ? ? ? , (x i , y i ), ? ? ? }, where x i ? X is a person image and y i ? Y is its exact person ID. By contrast, the weakly supervised learning task here is to learn f : B ? L from a training set {(b 1 , l 1 ), ? ? ? , (b j , l j ), ? ? ? }, where b j ? B is a bag of person images, i.e., b j = {x j1 , x j2 , ? ? ? , x jp }; and l j ? L is its bag-level label, i.e., l j = {y j1 , y j2 , ? ? ? , y jq }. Note that the mappings between {x j1 , x j2 , ? ? ? , x jp } and {y j1 , y j2 , ? ? ? , y jq } are unknown. During testing, there is no difference between fully and weakly supervised Re-ID problems (see <ref type="figure" target="#fig_0">Fig. 1 (c)</ref>).</p><p>Solving the weakly supervised Re-ID problem is challenging. Because without the help of strongly labeled data, it is rather difficult to model the dramatic variances across camera views, such as the variances in illumination and occlusion conditions, which makes it very challenging to learn a discriminative representation. Existing Re-ID approaches cannot solve the weakly supervised Re-ID problem. <ref type="bibr">Regardless</ref>   they are designed for computing either cross-view-invariant features or distance metrics, the existing models all assume that a strong annotation of each person image is available. This is also reflected in the existing benchmarking Re-ID datasets, most of which consist of a precise person ID for each image. None of them are designed to train a weakly supervised model. Although the weak annotations lack detailed clues for directly recognizing each person image, they usually contain global dependencies among images, which are very useful to model the variances of images across camera views. By using the weak annotations, we introduce a differentiable graphical model to address the weakly supervised Re-ID problem, which includes several steps. First, the person images are fed into the DNNs in terms of bags ( <ref type="figure" target="#fig_1">Fig. 2 (a)</ref>) to obtain the rough categorization probabilities. These categorization probabilities are modeled as the unary terms in a probabilistic graphical model; see <ref type="figure" target="#fig_1">Fig. 2 (b)</ref>. Second, we further model the relations between person images as the pairwise terms in a graph by considering their similarity in the features and appearance; see <ref type="figure" target="#fig_1">Fig. 2</ref> (b). The unary term and the pairwise term are summed to form the refined categorization probability. Third, we maximize the refined categorization probabilities and obtain the pseudo-image-level label for each image. Fourth, we use the generated pseudo labels to supervise the learning of the deep Re-ID model. Note that different from traditional nondifferentiable graphical models (e.g., CRFs <ref type="bibr" target="#b3">[4]</ref>), our model is differentiable and thus can be integrated into DNNs, which is optimized by using stochastic gradient descent (SGD). All of the above steps are trained in an end-to-end fashion. We summarize the contributions of this work as follows.</p><p>1) We define a weakly supervised Re-ID problem by replacing the image-level annotations in traditional Re-ID with bag-level annotations. This new problem is worth exploring because it significantly reduces the labor of annotation and offers the potential to obtain large-scale training data.</p><p>2) Since existing benchmarks largely ignore this weakly supervised Re-ID problem, we contribute a newly dedicated dataset called the SYSU-30k for facilitating further research in Re-ID problems. SYSU-30k contains 30k individuals, which is about 20 times larger than CUHK03 (1.3k individuals) and Market-1501 (1.5k individuals), and 30 times larger than ImageNet (1k categories). SYSU-30k contains 29,606,918 images. Moreover, SYSU-30k provides not only a large platform for the weakly supervised Re-ID problem but also a more  challenging test set that is consistent with the realistic setting for standard evaluation. <ref type="figure" target="#fig_2">Fig. 3</ref> shows some samples from the SYSU-30k dataset.</p><p>3) We introduce a differentiable graphical model to tackle the unreliable annotation dilemma in the weakly supervised Re-ID problem. When compared with the fully supervised Re-ID models, our method achieves state-of-the-art performance on SYSU-30k and other datasets.</p><p>The remainder of this work is organized as follows. Section II provides a brief review of the related work. Section III introduces the annotation of SYSU-30k, followed by the weakly supervised Re-ID model in Section IV. We also discuss the relationship of our work with previous work in Section V. The experimental results are presented in Section VI. Section VII concludes the work and presents outlooks for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Person Re-ID. Re-ID has been widely investigated in the literature. Most recent works can be categorized into three groups: (1) extracting invariant and discriminant features <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, (2) learning a robust metric or subspace for matching <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, and (3) joint learning of the above two methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Recently, there are many works on the generalization of Re-ID, such as video-based and image-to-video Re-ID <ref type="bibr" target="#b8">[9]</ref>, spatio-temporal Re-ID <ref type="bibr" target="#b9">[10]</ref>, occluded Re-ID <ref type="bibr" target="#b10">[11]</ref>, and natural language Re-ID <ref type="bibr" target="#b11">[12]</ref>. However, all these methods assume that the training labels are strong. They are thus ineffective for solving the weakly supervised Re-ID. Recently, the robustness of Re-ID are also examined by <ref type="bibr" target="#b12">[13]</ref>.</p><p>Unsupervised Re-ID. Another approach that is free from the prohibitively high cost of manual labeling is unsupervised learning Re-ID <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. These methods either use local saliency matching or resort to clustering models <ref type="bibr" target="#b13">[14]</ref>. However, without the help of labeled data, it is difficult to model the dramatic variances across camera views in feature/metric learning. Therefore, it is difficult for these pipelines to achieve high accuracies <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b18">[19]</ref>. In contrast, the proposed weakly supervised Re-ID problem has a better solution. Note that compared to unsupervised Re-ID, the annotation effort of weakly supervised Re-ID is also very inexpensive.</p><p>Semi-supervised Re-ID. Apart from our model, there have been some uncertain label learning models, among which the one-shot/one-example Re-ID <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> is the most related to ours. The main differences between their methods and ours are two-fold. First, in one-shot Re-ID, at least one accurate label for each person ID is still in desire. While in our weakly supervised Re-ID, no accurate label is needed. Second, there are bag-level labels as constraints to guide the estimation of the pseudo labels in our method, ensuring that our generated pseudo labels to be more reliable than those generated by one-shot Re-ID. Besides, <ref type="bibr" target="#b21">[22]</ref> also proposes to cope with the uncertain-label Re-ID problem using multiple-instance multiple-label learning. However, similar to <ref type="bibr" target="#b20">[21]</ref>, at least one accurate label for each person ID is still in a desire to form the probe set in <ref type="bibr" target="#b21">[22]</ref>. Therefore, mathematically, <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b21">[22]</ref> are all semi-supervised Re-ID but NOT weakly supervised Re-ID.</p><p>Weakly-supervised learning. Beyond Re-ID, although training deep models with weak annotations is challenging, it has been partially investigated in the literature, such as in tasks of image classification <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, semantic segmentation <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b27">[28]</ref>, object detection <ref type="bibr" target="#b28">[29]</ref>. Our method is related to them in that our model is also based on the generation of a pseudo label. However, the weakly supervised Re-ID problem has two unique characteristics that distinguish it from other weakly supervised learning tasks. <ref type="bibr" target="#b0">(1)</ref> We cannot find a representative image for a permanent ID because people will change their clothes at short intervals. The same person wearing different clothes may be regarded as two different persons. This results in thousands of millions of person IDs. Therefore, the label for a weakly supervised Re-ID sample is fuzzier than other tasks. (2) The entropy of the weakly supervised Re-ID problem is larger than other tasks. In weakly supervised segmentation tasks, pixels in an image share certain motion of rigidity and stability that benefits the prediction. Whereas in the weakly supervised Re-ID, persons in video bags are more unordered and irregular. Therefore, the weakly supervised Re-ID problem is considerably more challenging than other problems.</p><p>Graphical learning. To address the weakly supervised Re-ID problem, we propose to generate a pseudo label for each image by introducing differentiable graphical learning, which is inspired by the advances in image segmentation <ref type="bibr" target="#b29">[30]</ref> and videos <ref type="bibr" target="#b30">[31]</ref>. Recently, one classical graphical model, i.e., CRF, has been introduced to Re-ID for similarity learning <ref type="bibr" target="#b31">[32]</ref>. However, our method differs from <ref type="bibr" target="#b31">[32]</ref> in two aspects. First, like existing methods, <ref type="bibr" target="#b31">[32]</ref> uses CRF as a post-processing tool to refine the prediction provided by fully supervised learning, while our method exploits the supervision-independent property of graphical learning <ref type="bibr" target="#b29">[30]</ref> to generate pseudo labels for our weakly-supervised Re-ID learning. Second, different from traditional non-differentiable graphical models and <ref type="bibr" target="#b31">[32]</ref>, our proposed model directly formulates the graphical learning as an additional loss, which is differentiable to the neural network parameters and thus can be optimized by using stochastic gradient descent (SGD).</p><p>Person search. Another problem that is very related to our problem is person search <ref type="bibr" target="#b11">[12]</ref>, which aims to fuse the processes of person detection and Re-ID. There are two significant differences between weakly supervised Re-ID and person search. First, the weakly supervised Re-ID only focuses on visual matching, which is reasonable because current human detectors are competent enough to detect persons. Second, the weakly supervised Re-ID problem enjoys the inexpensive efforts of weak annotation, while the person search still needs a strong annotation for each person image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SYSU-30k DATASET</head><p>Data collection. No weakly supervised Re-ID dataset is publicly available. To fill this gap, we contribute a new Re-ID dataset named SYSU-30k in the wild to facilitate studies. We download many short program videos from the Internet. TV programs are considered as our video source for two reasons. First, the pedestrians in a TV program video are often cross-view and cross-camera because there are many movable cameras to capture the shots for post-processing. Reidentifying pedestrians in a TV program video is exactly a Re-ID problem in the wild. Second, the number of pedestrians in a program is suitable for annotation. On average, each video contains 30.5 pedestrians walking around.</p><p>Our final raw video set contains 1,000 videos. The annotators are then asked to annotate the videos in a weak fashion. In particular, each video is divided into 84,924 bags of arbitrary length. Then, the annotators record the pedestrian's identity for each bag. YOLO-v2 <ref type="bibr" target="#b37">[38]</ref> is utilized for pedestrian bounding box detection. Three annotators review the detected bounding boxes and annotate person category labels for 20 days. Finally, 29,606,918 (? 30M ) bounding boxes of 30,508 (? 30k) person categories are annotated. We then select 2,198 identities I: A comparison of different Re-ID benchmarks. Categories: We treat each person identity as a category. Scene: whether the video is taken indoors or outdoors. Annotation: whether image-level labels are provided. Images: the person images which are obtained by using a human detector to detect the video frames. Actually, the person images in this work refer to the bounding boxes.</p><p>(a) Comparision with existing Re-ID datasets. as the test set, leaving the rest as the training set. There is no overlap between the training set and the test set. Notably, we treat each person identity as a category in this paper. We provide some samples of the SYSU-30k dataset in <ref type="figure" target="#fig_2">Fig.  3</ref>. As shown, our SYSU-30k dataset exhibits challenges of illumination variance (Row 2, 7, and 9), occlusion (Row 6 and 8), low resolution (Row 2 and 9), looking-downward cameras (Row 2, 5, 6, 8, and 9), and complicated backgrounds of the real scenes (Row 2-10).</p><p>Dataset statistics. SYSU-30k contains 29,606,918 person images with 30,508 categories in total, which is further divided into 84,930 bags (only for training set). <ref type="figure" target="#fig_3">Fig. 4</ref> (a) summarizes the number of bags with respect to the number of images per bag, showing that each bag has 2,885 images on average. This histogram reveals the person image distribution of these bags in the real world without any manual cleaning and refinement. Each bag is provided with an annotation of bag-level labels.</p><p>Comparison with existing Re-ID benchmarks. We compare SYSU-30k with existing Re-ID datasets, including CUHK03 <ref type="bibr" target="#b0">[1]</ref>, Market-1501 <ref type="bibr" target="#b1">[2]</ref>, Duke <ref type="bibr" target="#b32">[33]</ref>, MSMT17 <ref type="bibr" target="#b33">[34]</ref>, CUHK01 <ref type="bibr" target="#b34">[35]</ref>, PRID <ref type="bibr" target="#b35">[36]</ref>, VIPeR <ref type="bibr" target="#b2">[3]</ref>, and CAVIAR <ref type="bibr" target="#b36">[37]</ref>. <ref type="figure" target="#fig_3">Fig.  4</ref> (c) and (d) plots the person IDs and the number of images, respectively, indicating that SYSU-30k is much larger than existing datasets. To evaluate the performance of the weakly supervised Re-ID approach, we randomly choose 2,198 person categories from SYSU-30k as the test set. These person categories are not utilized in training. We annotate an accurate person ID for each person image. We also compare the test set of SYSU-30k with existing Re-ID datasets. From <ref type="figure" target="#fig_3">Fig. 4</ref> (b) and (c), we can observe that the test set of SYSU-30k is more challenging than those of the competitors in terms of both the image number and person IDs. Thanks to the above annotation fashion, the SYSU-30k test set can adequately reflect the real-world setting and is consequently more challenging than existing Re-ID datasets. Therefore, SYSU-30k is not only a large benchmark for the weakly supervised Re-ID problem but is also a significant standard platform for evaluating existing fully-supervised Re-ID methods in the wild.</p><p>A further comparison of SYSU-30k with existing Re-ID benchmarks is shown in Table I (a), including categories, scene, annotation, cameras, and image numbers (bounding boxes). After the comparison, we summarize the new features in SYSU-30k in the following aspects. First, SYSU-30k is the first weakly annotated dataset for Re-ID. Second, SYSU-30k is the largest Re-ID dataset in terms of both person categories and image number. Third, SYSU-30k is more challenging due to many cameras, realistic indoor and outdoor scenes, and occasionally incorrect annotations. Fourth, the test set of SYSU-30k is not only suitable for the weakly supervised Re-ID problem but is also a significant standard platform to evaluate existing fully supervised Re-ID methods in the wild.</p><p>Comparison with ImageNet-1k. Beyond the Re-ID family, we also compare SYSU-30k with the well-known ImageNet-1k benchmark for general image recognition. As shown in <ref type="table">Table I</ref> (b), SYSU-30k has several appealing advantages over ImageNet-1k. First, SYSU-30k has more object categories than ImageNet-1k, i.e., 30k vs 1k. Second, SYSU-30k saves annotation due to the effective weak annotation.</p><p>Evaluation protocol. The evaluation protocol of SYSU-30k is similar to that of the previous datasets <ref type="bibr" target="#b1">[2]</ref>. Following <ref type="bibr" target="#b0">[1]</ref>, we fix the train/test partitioning. In the test set, we choose 1,000 images belonging to 1,000 different person IDs to form the query set. As the scalability is important for the practicability of Re-ID systems, we propose to challenge the scalability of a Re-ID model by providing a gallery set containing a vast volume of distractors for validation. Specifically, for each probe, there is only one matching person image as the correct answer in the gallery, while there are 478,730 mismatching person images as the wrong answer in the gallery. Thus, the evaluation protocol is to search for a needle in the ocean, just like the police search a massive amount of videos for a criminal. Following <ref type="bibr" target="#b1">[2]</ref>, we use the rank-1 accuracy as the evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. WEAKLY SUPERVISED RE-ID MODEL</head><p>A. From Supervised Re-ID to Weakly Supervised Re-ID Let b = {x 1 , ? ? ? , x j , ? ? ? , x p } denote a bag containing p images. y = {y 1 , ? ? ? , y j , ? ? ? , y p } are the image-level labels and l is the bag-level label.</p><p>In the fully supervised Re-ID, the image-level labels y are known. The goal is to learn a model by minimizing the loss between the image-level labels and the predictions.</p><p>In contrast, in the weakly supervised Re-ID, the bag-level label l is provided, but the image-level labels y are unknown. Suppose a bag contains n person IDs and there are m IDs in the entire dataset. A preliminary image-level label Y j for each image x j can be inferred from its bag-level label l:</p><formula xml:id="formula_0">Yj = ? ? ? ? ? ? ? ? ? ? Y 1 j . . . Y k j . . . Y m j ? ? ? ? ? ? ? ? ? ? , where Y k j = 1 n , if k ? l 0, otherwise ,<label>(1)</label></formula><p>Then, Y can be used as a bag constraint to deduce a pseudoimage-level labels?, which can be further used to supervise the model learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Weakly supervised Re-ID: differentiable graphical learning</head><p>In this section, we propose differentiable graphical learning to generate pseudo-image-level labels for the person images.</p><p>Graphically modeling Re-ID. In our graph, each node represents a person image x i in a bag and each edge represents the relation between person images, as illustrated in <ref type="figure" target="#fig_4">Fig. 5</ref>.</p><p>Here i is the image index in a bag. Assigning a label y i to a node x i has an energy cost. The energy cost E(y|x) of our graph is defined as:</p><formula xml:id="formula_1">E(y|x) = ?i?U ?(yi|xi) unary term + ?i,j?V ?(yi, yj|xi; xj) pairwise term ,<label>(2)</label></formula><p>where U and V denote a set of nodes and edges, respectively. ?(y i |x i ) is the unary term measuring the cost of assigning label y i to a person image x i . ?(y i , y j |x i ; x j ) is the pairwise term that measures the penalty of assigning labels to a pair of images (x i , x i ). Mathematically, graphical modeling is to smooth the uncertain prediction of person IDs. The unary term performs the prediction based on sole nodes. While the pairwise term smoothes the prediction of multiple nodes by considering their appearance and features. In summary, Eq. <ref type="formula" target="#formula_1">(2)</ref> is to clean up the spurious predictions of classifiers learned in a weakly supervised manner. Unary term. The unary terms is typically defined as:</p><formula xml:id="formula_2">?(yi|xi) = ?P y i i log(Y y i i P y i i ),<label>(3)</label></formula><p>where P i is the categorization probability of a person image x i outputted by a DNN. denotes element-wise product.</p><p>As the unary term alone is generally noisy and inconsistent. Interactions between pairwise terms are required.</p><p>Pairwise term. The pairwise term is defined as:</p><formula xml:id="formula_3">?(yi, yj|xi; xj) = ?(yi, yj) label compatibility Y y i i Y y j j bag constraint exp ? Ii ? Ij 2 2? 2 appearance similarity ,<label>(4)</label></formula><p>where I i and I j are the low-level features and based on them, a Gaussian kernel is employed to measure their appearance similarity. The hyper-parameter ? controls the scale of the Gaussian kernel. This kernel forces the images with similar appearance to have the same labels. Similar to the unary term, the pairwise terms are also bounded by the bag-level annotations Y i and Y j . The pairwise terms are widely known to provide nontrivial knowledge (e.g., structural context dependencies) that is not captured by the unary term. A simple label compatibility function ?(y i , y j ) ? {0, 1} in Eq. (4) is given by the Potts model, namely,</p><formula xml:id="formula_4">?(yi, yj) = 0, if yi = yj 1, otherwise ,<label>(5)</label></formula><p>It introduces a penalty for similar images that are assigned different labels. Considering that Eq. <ref type="formula" target="#formula_1">(2)</ref> is non-differentiable, it is incompatible with DNNs. Thus, we will instead learn a differential version of Eq. (2) in a deep learning model. Bag constraint. As mentioned above, both the unary and pairwise terms are constrained by the bag-level annotations Y i and Y j . In fact, the bag-level annotation contains extra knowledge that helps to improve the estimation. For example, if the estimator mismatches a person image to an ID that is not in the bag-level annotation, the estimation is undoubtedly considered as incorrect. Then, the estimation will be corrected by matching the image to the ID in the bag-level annotation with the most significant prediction score. Furthermore, if some IDs in the weak annotation are absent in the prediction, the proposed method will encourage a portion of the person images to be assigned to such IDs to improve the performance. In this way, knowledge of the weakly labeled data can be fully exploited. Given a bag of images and their bag-level label, we refine the DNN predictions by element-wisely multiplying P with the bag-level weak annotation Y. This is shown in the unary term in Eq. <ref type="formula" target="#formula_2">(3)</ref>. Similarly, we also impose Y i and Y j on the pairwise term in Eq. (4). Moreover, there is a natural smoothness in a video that could not be ignored. The person IDs in the adjacent bags change slowly within a short time; for instance, an image-level label y i in bag b T could also be in the bag b T +1 . A large amount of bags with overlapping IDs naturally exist in a video, which sheds light on the ability of the weakly supervised Re-ID. As a special example, if b T contains {y i , y j } and b T +1 contains {y j , y k }, then the two bags share {y j }. Meanwhile, one image in the first bag is similar to another image in the second bag. Hence, our model predicts these two images as y i . Finally, y i and y k are assigned to the remaining images.</p><p>Deducing pseudo image-level labels. By minimizing the energy cost of Eq. (2), we can obtain the pseudo image-level label? i for the person imagex i :</p><formula xml:id="formula_5">yi = arg max y i ?{1,??? ,m} E(yi|xi),<label>(6)</label></formula><p>where {1, ? ? ? , m} denote all the person IDs in the training set. Once such labels are generated, they are used to update the network parameters. Differentiablizing graphical learning. The above weakly supervised Re-ID model is not end-to-end. Because we must first use an external graphical learning solver to obtain the pseudo labels and then use another solver to train the DNNs under the supervision of the pseudo labels (see <ref type="figure" target="#fig_5">Fig. 6</ref> (a)). To enable an end-to-end optimization, we propose to make our graphical learning differentiable and compatible with DNNs (see <ref type="figure" target="#fig_5">Fig. 6 (b)</ref>).</p><p>We first investigate the mechanism of a non-differentiable graphical model. As illustrated in <ref type="figure" target="#fig_5">Fig. 6 (a)</ref>, a nondifferentiable graphical model consists of three steps. First, a preliminary categorization score P is obtained through a DNN. Second, the energy cost in Eq. <ref type="formula" target="#formula_1">(2)</ref> is minimized by re-assigning labels to the images appropriately, subject to the appearance similarity, the preliminary categorization scores, and the bag constraint. Third, the re-assigned labels are considered as the pseudo labels and used to supervise the learning of the Re-ID model.</p><p>The label reassignment in the second step claimed above is non-differentiable, which makes the graphical model incompatible with DNNs. To fill this gap, a relaxation form of Eq. (2) is desirable. With a continuous version of? and? to approximate the discrete ? and ?, we rewrite Eq. (2) as follows</p><formula xml:id="formula_6">L graph (x) = ?i?U? (xi) unary term + ?i,j?V? (xi, xj) pairwise term ,<label>(7)</label></formula><p>where? and? are defined as:</p><formula xml:id="formula_7">? ? ? ? ? ? ? ? ?? (xi) = ? m j=1 [h( arg max k?{1,??? ,m} Y k i P k i )] j log(P j i ), ?(xi, xj) = ? exp ? Ii ? Ij 2 2? 2 (YiPi) T log(YjPj).<label>(8)</label></formula><p>arg max returns the index of the largest element in a vector, h is a function that maps a scalar to one-hot vector, and the superscripts j and k mean indexing the j and k element of a vector, respectively. The differences between Eq. (3)-(4) and Eq. (8) are summarized as follows: 1) the replacement of ?(y i ) with?(x i ) facilitates an end-to-end learning. Because, in a non-differentiable model, y i is the input variable, while x i is regarded as the input of DNNs in differentiable models. 2) We use arg max to obtain the prediction, which is consistent with the nature of DNNs. Namely, during the testing phase, we directly obtain the prediction from the output of the DNN without the graphical losses. 3) We use a differentiable term ?(Y i P i ) T log(Y j P j ) to approximate the non-differential term ?(y i , y j )Y i Y j in Eq. (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network Architecture and Loss functions</head><p>The network architectures for training and testing are illustrated in <ref type="figure" target="#fig_6">Fig. 7</ref>, where the black dotted lines denote training flow, and the solid black lines denote inference flow.</p><p>Our weakly-supervised Re-ID model consists of three main modules, including (a) a feature embedding module built upon a ResNet-50 network followed by two fully connected layers, (b) a rough Re-ID module using a fully connected layer as the classifier, and (c) a refined Re-ID module that considers both the rough results and bag-level weak annotation to perform graphical modeling. It is noteworthy that we perform graphical modeling only in the training stage for two reasons. First, the graphical module is introduced to generate pseudo labels to supervise the model training, which requires a bag-level label as a constraint. However, there is no bag-level label in the testing stage. Second, due to the specificity of the Re-ID problem, the images in the inference stage are not organized in the form of a bag. For example, only a query image and a set of gallery images are provided in inference. As a result, there is no bag-level dependency among the testing images to exploit. Thus, performing graphical modeling may be infeasible in the inference stage. In the following, we will elaborate the three main modules. fc fc  ... ... ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Differentiable Graphical Model</head><p>Alice, Bob, Carol, Dave, Eve, ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bag-level Labels</head><p>Bag of Images n*2048 n*512 n*class num ... Feature embedding module. Many current best-performing Re-ID models use multi-scale features as feature embeddings <ref type="bibr" target="#b38">[39]</ref>, which guarantees a robust feature representation and thus boosts the performance. However, in this work, our focus is the mechanism of the weakly supervised Re-ID model alone, rather than other tricks. Therefore, we simply take the ResNet-50 <ref type="bibr" target="#b39">[40]</ref> as the backbone without any feature pyramid <ref type="bibr" target="#b38">[39]</ref>. Our feature embedding is similar to <ref type="bibr" target="#b40">[41]</ref>. Specifically, the last layer of the original ResNet-50 is discarded, and two new, fully connected layers are added. The first has 512 units, followed by a batch normalization <ref type="bibr" target="#b41">[42]</ref>, a Leaky ReLU <ref type="bibr" target="#b42">[43]</ref> and a dropout <ref type="bibr" target="#b43">[44]</ref>. This module is shown in <ref type="figure" target="#fig_6">Fig. 7 (a)</ref>.</p><p>Rough Re-ID module. We utilize a softmax classifier for rough Re-ID. Specifically, our model has a fully connected layer at the top of the feature embedding module, which has the same number of units to that of person ID (i.e., 'class num' in <ref type="figure" target="#fig_6">Fig. 7)</ref>. A softmax cross-entropy loss is employed for training. The derived person categorization score (e.g., P in <ref type="figure" target="#fig_6">Fig. 7)</ref> is considered as the rough Re-ID estimation, indicating the possibility of a person ID being present in a bag. This module is shown in <ref type="figure" target="#fig_6">Fig. 7 (b)</ref>.</p><p>Refined Re-ID module. Here, we aim to generate a pseudo image-level label? for each image by refining the previous estimation results. The refinement has the following inputs: 1) Rough Re-ID score. As mentioned above, the rough Re-ID module provides a preliminary categorization. 2) Appearance. Considering that rough Re-ID score is just a high-level abstraction of images, as compensation, we propose to integrate person appearance as low-level information for our refinement. 3) Bag constraint. Intuitively, our bag constraint eliminates any possibility of assigning a person image with a person ID that is absent in the bag-level annotation; on the contrast, it encourages a person image to be assigned with a person ID that is present in the bag-level annotation.</p><p>Accordingly, once the refined pseudo labels are generated, they are used to update the network weights as authentic ground truth:</p><formula xml:id="formula_8">L cls = ? n i=1 (h(?i)) T log(Pi).<label>(9)</label></formula><p>By combining Eq. (9) and Eq. <ref type="formula" target="#formula_6">(7)</ref>, we have the final loss function:</p><formula xml:id="formula_9">L = w cls L cls + w graph L graph ,<label>(10)</label></formula><p>where w cls and w graph weights two loss components, respectively. In our experiments, we search the loss weight in a grid of {1:1, 1:0.5, 1:0.1} and find that 1:0.5 has good results on the CUHK03 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Weakly-Supervised Triplet Loss</head><p>To further improve the performance of the Re-ID model, inspired by multiple granulariry network (MGN) <ref type="bibr" target="#b44">[45]</ref>, we propose a weakly-supervised triplet loss and derive our weakly supervised MGN (W-MGN) for weakly supervised Re-ID. MGN are learned with a triplet loss given strong annotations, while our weakly-supervised triplet loss in W-MGN is developed to address the strict dependency on annotations.</p><p>Recall that a fully supervised triplet loss is defined as: </p><p>where t denotes the number of bags in a training batch, || ? || 2 2 denotes a L 2 -norm, and ? is a margin. The image-level labels y k and y j are given for the fully supervised triplet loss, but they are unavailable in the weakly supervised scenario.</p><p>To address this problem, we use Y T k Y j &gt; 0 to approximate the constraint that y k = y j in Eq. <ref type="bibr" target="#b10">(11)</ref>, which means that the k-th and j-th samples belong to the same class. As Y T k Y j &gt; 0 is a necessary but not sufficient condition of the constraint that y k = y j , we relax the max operation to a median operation. Similarly, we use Y T k Y j = 0 to approximate y k = y j . Accordingly, our weakly-supervised triplet loss is formulated as:</p><formula xml:id="formula_11">L weak triplet = tp k=1 ? + median j =k;j=1,... ,tp;Y T k Y j &gt;0 ||z k ? zj || 2 2 ? min j =k;j=1,... ,tp;Y T k Y j =0 ||z k ? zj || 2 2 .<label>(12)</label></formula><p>Finally, our loss function is</p><formula xml:id="formula_12">L = w cls L cls + w graph L graph + w triplet L weak triplet ,<label>(13)</label></formula><p>where w triplet is a weight of our triplet loss. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Computational Complexity</head><p>We discuss the computational cost of our weakly supervised Re-ID model. In the training phase, the extra time cost only relates to the generation of pseudo labels, which is a graphical learning module. Generally, graphical learning needs many iterations to search the optimal solution, and thus, the process is time-consuming. However, our approach formulates the differentiable graphical learning as a simple loss. This makes our graphical module very effective. In the experiment section, we will show that our training brings an additional time cost of only 0.002?. Particularly, in the testing phase, there is no extra time cost because the pseudo label generation component is disabled. In brief, this extra time cost of our method is negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RELATIONSHIP TO PREVIOUS WORKS</head><p>In the following, we compare our weakly supervised Re-ID with previous works on Re-ID with uncertain labels, including the unsupervised/semi-supervised Re-ID. In general, our weakly supervised Re-ID possesses not only cheap annotation but also high accuracy. The details are presented below.</p><p>Unsupervised Re-ID. To get rid of the prohibitively high cost of manual labeling, unsupervised learning Re-ID proposes to use either local saliency matching models or clustering models <ref type="bibr" target="#b13">[14]</ref>. However, without the help of labeled data, it is difficult to model the dramatic variances across camera views in representation/metric learning. Therefore, it is difficult for these pipelines to obtain high accuracies <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b18">[19]</ref>. In contrast, our weakly supervised Re-ID problem has a better solution. Note that compared to unsupervised Re-ID, the annotation effort of our weakly supervised Re-ID is also very inexpensive.</p><p>Semi-supervised Re-ID. One-shot/one-example <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> propose to reduce the annotation effort by annotating only one example for each person ID. The main differences between their methods and ours are two-fold. First, in one-shot Re-ID, at least one accurate label for each person category is in desire. While in our weakly supervised Re-ID, no accurate label is needed. Second, there is a bag-level label as a constraint to estimate the pseudo labels in our method, ensuring that our generated pseudo labels to be more reliable than those generated by one-shot Re-ID.</p><p>We would also like to acknowledge the contribution of previous work <ref type="bibr" target="#b21">[22]</ref> that matches a target person image with a bag-level gallery video using multiple-instance multiple-label learning. However, similar to <ref type="bibr" target="#b20">[21]</ref>, at least one accurate label (of the target person) for each person category is still in a desire to form the probe set in <ref type="bibr" target="#b21">[22]</ref>. Hence, mathematically, <ref type="bibr" target="#b21">[22]</ref> still belongs to semi-supervised Re-ID but NOT weakly supervised Re-ID.</p><p>Section VI-C3 and VI-C1 will compare the accuracy of our weakly supervised Re-ID with previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTS</head><p>In this section, we conducts extensive experiments to evaluate our weakly supervised Re-ID approach. Section VI-A presents the experimental settings. Section VI-B provides a comprehensive ablation study. Section VI-C presents comparisons of our approach with state-of-the-art methods and also provides the discussion on the computational cost.</p><p>A. Experimental settings 1) Datasets: In addition to the proposed SYSU-30k dataset, another four simulated datasets are introduced to evaluate the effectiveness of our method by adjusting the existing datasets. Specifically, we replace the strong annotations on the training set of the PRID2011 <ref type="bibr" target="#b35">[36]</ref>, CUHK03 <ref type="bibr" target="#b0">[1]</ref>, Market-1501 <ref type="bibr" target="#b1">[2]</ref>, and MSMT17 <ref type="bibr" target="#b33">[34]</ref> with weak annotations while their test sets are kept unchanged, as the definition states that during testing, there is no difference between the fully and weakly supervised Re-ID (see <ref type="figure" target="#fig_0">Fig. 1 (c)</ref>). For a fair comparison (e.g., using the same images for both the fully and weakly supervised Re-ID), we generate the weak annotations from the strong annotations. This includes two steps. First, each bag is simulated by randomly selecting several images and packaging them. Second, the weak labels are obtained by summarizing the strong annotations, e.g., four image-level labels {Alice, Bob, Alice, Carol} are summarized as a bag-level label {Alice, Bob, Carol}. We denote n ID/bag when a bag contains n person IDs. Note that unless otherwise stated, our weakly supervised learning setting is two IDs/bag. PRID2011. Originally, PRID2011 dataset contains 200 person IDs appearing in at least two camera views and is further randomly divided into training/test sets following the general settings <ref type="bibr" target="#b8">[9]</ref>, i.e., both having 100 IDs.</p><p>CUHK03. CUHK03 is a large-scale Re-ID, which contains 14,096 images of 1,467 IDs collected from 5 different pairs II: Ablation studies of the proposed weakly supervised Re-ID method. random: Each bag contains random person IDs, which reflects the real-world state. RK: re-ranking, see <ref type="bibr" target="#b45">[46]</ref>, one of the effective tricks frequently used in fully supervised Re-ID problems. fully supervised: when each bag contains only one person ID, the weakly supervised Re-ID problem degrades into a fully supervised Re-ID problem. * full training set: the overall training set of CUHK03 contains 767 person IDs. MSMT17. MSMT17 is the current largest publicly available Re-ID dataset and there are 126,441 images in MSMT17 in total captured by 15 cameras. It has 4k person IDs, namely, 7.5 times smaller than our SYSU-30k. We follow the standard protocol to split the training and testing set <ref type="bibr" target="#b33">[34]</ref>.</p><p>2) Implementation details: The parameters of the ResNet-50 backbone are initialized using ImageNet pre-training. Other parameters are initialized by sampling from a normal distribution. For SGD, we use a minibatch of 90 images and an initial learning rate of 0.01 (0.1 for the fully connected layer), multiplying the learning rate by 0.1 after a fixed number of iterations. We use the momentum of 0.9 and a weight decay of 0.0005. Training on SYSU-30k takes approximately ten days on a single GPU (i.e., NVIDIA TITAN X).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation Study</head><p>We first present ablation studies to reveal the benefits of each main component of our method.</p><p>1) Effectiveness of the graphical learning module: As aforementioned, the graphical learning module plays the role of refining the ID prediction by correcting the errors between the rough Re-ID predictions and the weak annotations, which forms the basis of generating pseudo-image-level labels. We visualize the errors between the rough predictions and the weak annotations in <ref type="figure">Fig. 8</ref> during training. This experiment is conducted on CUHK03 using the setting of 10 IDs/bag. <ref type="figure">Fig. 8</ref> shows the errors between the rough predictions and the weak annotations in the form of a confusion matrix containing 76 ? 76 grids. Each grid indicates a bag of 10 IDs, totally summing up to 760 IDs, which approximates the number of person IDs in the full training set. We have two appealing observations from <ref type="figure">Fig. 8</ref>. First, there is a significant gap between the rough predictions and the weak annotations (see 8 (a) or (b)), indicating that the rough Re-ID results are still not competent for generating pseudo labels. Therefore, refining the ID prediction is necessary with our graphical learning module. Second, the gap between the rough predictions and the weak annotations becomes smaller as the training iteration increases (from 10 epochs in 8(a) to 70 epochs in 8 (d)). When the training model converges, the gap between the ground truth becomes significantly small, which indicates that the problem is well addressed by our graphical learning module.</p><p>To further demonstrate the effectiveness of the differential graphical models, we conduct two more empirical studies on CUHK03 and Market-1501 with and without the graphical model, respectively. The graphical module generates pseudo labels to further supervise the learning. Once it is removed, there will be no pseudo labels provided. To cope with this problem, we use the preliminary image-level labels in Eq. (1) as substitutes for pseudo labels, which is also a widely used strategy in existing weakly supervised learning. Table II (f) shows that without the graphical model, there is a significant performance drop on CUHK03, i.e., from 61.0% to 56.4%. Similarly, the performance of W-MGN drops significantly from 95.5% to 88.4% when the graphical model is removed, as shown in <ref type="table">Table III</ref> (a). These comparisons clearly demonstrate the effectiveness of our graphical model.</p><p>2) Effectiveness of the pairwise term: As claimed above, the pairwise term is necessary for label smoothness in order to train better models. To validate the necessity of the pairwise term, we conduct two more experiments on CUHK03 and Market-1501 with and without the pairwise term, respectively. Without pairwise terms, a graphical model reduces to isolated   <ref type="figure">Fig. 9</ref>: Analysis on different bag diversities. Cat/bag: the number of person IDs in each bag. Random Cat/bag: each bag contains random number of person IDs, which reflects the real-world state. Fully supervised: each bag contains only one person ID. In this case, the weakly supervised problem degrades into a fully supervised one.</p><p>nodes. Experimental results are provided in <ref type="table">Table II (f) and  Table III</ref> (a). For example, on Market-1501 test set, the accuracy drops significantly from 95.5% to 94.0% without the pairwise term.</p><p>3) Effectiveness of the weakly supervised triplet loss: To validate the effectiveness, we conduct ablation studies on our weakly-supervised triplet loss on five benchmarks (i.e., Market-1501, CUHK03, PRID2011, MSMT17, and SYSU-30k), respectively. Table III (a)-(e) show that W-MGN with the weakly-supervised triplet loss outperforms that without the weakly-supervised triplet loss ((i.e., "W-MGN w/o W-Tr")) by a large margin on all the five datasets, which verifies its effectiveness. For example, without the weakly-supervised triplet loss, W-MGN suffers a performance degradation from 95.5% to 92.9% on Market-1501. 4) Effectiveness of stronger baselines: To see how stronger baselines affect the performance of the weakly supervised Re-ID, we compare more strong baselines like Local CNN <ref type="bibr" target="#b46">[47]</ref> and MGN <ref type="bibr" target="#b44">[45]</ref> with our ResNet-50 baseline on the five datasets, i.e., Market-1501, CUHK03, PRID2011, MSMT17, and SYSU-30k. Table III (a)-(e) show that strong baselines for fully supervised Re-ID contribute to the overall performance of the weakly supervised method on all the five datasets. For example, the combination of our weakly supervised Re-ID and MGN (i.e., W-MGN) outperforms the combination of the weakly supervised Re-ID and ResNet-50 (i.e., W-Baseline) by 6.9% (95.5% vs. 88.6%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Scalability of our approach:</head><p>We have shown that a Re-ID model can be learned with weakly labeled data. Next, we investigate whether increasing the amount of weakly labeled data will improve the performance of weakly supervised learning. The entire CUHK03 training set is randomly partitioned into three subsets containing 67, 300, and 300 person IDs, respectively. We evaluate the scalability of our approach by gradually adding one subset in training. The rank-1 accuracy is reported in Table II (e). For example, the first model is trained with the first 67 person IDs, and the number of person IDs is increased to 367 IDs in the second model. The third model is trained with the full CUHK03 training set (i.e., 767 IDs). Table II (e) shows that the accuracies increase when we increase the scale of training data in CUHK03. For instance, our approach trained with full training data achieves the best performance and outperforms the other two models by 44.7% and 17.4%, respectively.   the proposed weakly supervised Re-ID approach. * Fully Sup.: each bag contains only one person ID. In this case, the weakly supervised problem degrades into a fully supervised one. 6) Impact of bag diversity: Intuitively, if a bag contains more person IDs, it is more challenging to learn a weakly supervised Re-ID model because of the increase in uncertainty. Next, we investigate the performance with respect to such bag internal diversity. We conduct experiments on PRID2011 and CUHK03. In Table II (a)-(b) and <ref type="figure">Fig. 9</ref> (a)-(b), we compare five options, i.e., each bag containing 1, 2, 3, 10, or a random number of person IDs, respectively. In particular, when each bag has only one person ID, the weakly supervised Re-ID problem degrades into a fully supervised one.</p><p>We have three major observations from Table II (a)-(b) and <ref type="figure">Fig. 9 (a)-(b)</ref>. First, the models trained with weakly labeled samples achieves comparable accuracies to the models trained with strongly labeled data (e.g., 68.0% vs. 71.8% in <ref type="table">Table II</ref> (a)). This result is quite important because a weak annotation costs much less money and time than a strong annotation.</p><p>Second, the accuracy of the weakly supervised methods gradually decreases as the number of IDs in each bag increases. In particular, the rank-1 accuracy of our approach drops by 18.5% when increasing the number of IDs per bag from 2 to 10 in <ref type="table">Table II</ref> (a). We argue that the increase in uncertainty causes this optimization difficulty. When the IDs per bag increases, the uncertainty in the label assignment also increases, making the problem more challenging.</p><p>Third, it is noteworthy that the random version has appealing performance (69.3% vs 71.8% compared with the baseline), as shown in the last line of Table II (a). Specifically, the random version refers to each bag containing a random number of person IDs (5 IDs on average), which reflects the real-world states. The high performance suggests that solving a weakly supervised Re-ID problem is feasible and appealing in reality. 7) Compatibility with fully supervised learning tricks: Intuitively, a weakly supervised Re-ID problem is likely to be upper bounded by fully supervised learning with all annotations. Next, we investigate the accuracy of our approach with respect to models with different fully-supervised learning capacities. Experiments are conducted on PRID2011 and CUHK03.</p><p>We first evaluate two different fully supervised learning baseline models with and without re-ranking post-process. Then, we evaluate them in the weakly supervised learning scenario. The setting is similar to the aforementioned fully supervised learning, except that all of the image-level annotations are replaced with bag-level annotations in the training TABLE III: Comparison with state-of-the-art methods. W-Baseline: our weakly supervised Re-ID method. Baseline: each bag contains only one person ID. In this case, the weakly supervised problem degrades into a fully supervised one. We thus consider the latter as the baseline of our weakly supervised Re-ID. w/o Tri: without triplet loss. x(y): y is the number reported by the original paper; x is the result of our reproduction. RK: re-ranking. ?: pretrained on CUHK03. w/: with. w/o: without. W-: weakly supervised version of a method. set. It is observed that our rank-1 accuracy with weak annotations is close to that of using strong annotations, as shown in <ref type="table">Table II</ref> (c)-(d). Moveover, weakly supervised learning with a stronger baseline ('weakly supervised + RK') yields better performance. For example, in the weak annotation setting, "weakly supervised + RK" yields 68.0% on PRID2011, compared to 39.9% obtained by "weakly supervised", a relative improvement of 70.4%. This comparison verifies the compatibility of our method with existing frameworks; namely, existing tricks (e.g., re-ranking) for fully supervised learning could also be applied to the weakly supervised Re-ID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with the State-of-the-Arts</head><p>In this section, we compare our weakly supervised approach with the best-performing fully-supervised / semi-supervised / unsupervised methods.</p><p>1) Accuracy on Market-1501: Our weakly supervised Re-ID is compared with state-of-the-art fully-supervised /unsupervised / semi-supervised methods.</p><p>Fully supervised Re-ID. We compare our method with the fully supervised Re-ID models. Fifteen representative stateof-the-art methods are used as comparison methods, including MSCAN <ref type="bibr" target="#b47">[48]</ref>, DF <ref type="bibr" target="#b49">[50]</ref>, SSM <ref type="bibr" target="#b51">[52]</ref>, SVDNet <ref type="bibr" target="#b54">[55]</ref>, GAN <ref type="bibr" target="#b32">[33]</ref>, PDF <ref type="bibr" target="#b48">[49]</ref>, TriNet <ref type="bibr" target="#b40">[41]</ref>, TriNet + Era. + reranking <ref type="bibr" target="#b61">[62]</ref>, PCB <ref type="bibr" target="#b38">[39]</ref>, VPM <ref type="bibr" target="#b66">[67]</ref>, JDGL <ref type="bibr" target="#b67">[68]</ref>, AANet <ref type="bibr" target="#b69">[70]</ref>, Local CNN <ref type="bibr" target="#b46">[47]</ref>, and MGN <ref type="bibr" target="#b44">[45]</ref>. Comparison results are provided in <ref type="table">Table III</ref> (a). Our approach achieves very competitive accuracy. For example, our W-MGN and W-Local CNN achieve respectively a rank-1 accuracy of 95.5% and 95.7%, which surpass many of the compared fully-supervised methods. These results verify the effectiveness of our method.</p><p>To validate the superiority of our weakly supervised Re-ID over previous annotation-saving Re-ID works, we further compare our method with state-of-the-art unsupervised and semi-supervised Re-ID methods.</p><p>Unsupervised Re-ID. In <ref type="table">Table III</ref> (a), we compare our method with 11 current best-performing models for unsupervised Re-ID, including CAMEL <ref type="bibr" target="#b13">[14]</ref>, TAUDL <ref type="bibr" target="#b74">[75]</ref>, UTAL <ref type="bibr" target="#b18">[19]</ref>, UDA <ref type="bibr" target="#b15">[16]</ref>, MAR <ref type="bibr" target="#b16">[17]</ref>, DECAMEL <ref type="bibr" target="#b77">[78]</ref>, ECN <ref type="bibr" target="#b78">[79]</ref>, PAUL <ref type="bibr" target="#b72">[73]</ref>, HHL <ref type="bibr" target="#b80">[81]</ref>, Distilled <ref type="bibr" target="#b81">[82]</ref>, and Smooting <ref type="bibr" target="#b82">[83]</ref>. The results in Table III (a) show that our weakly supervised Re-ID has obtained significant gain over unsupervised Re-ID methods. For instance, our W-MGN outperforms the bestperforming model UDA <ref type="bibr" target="#b15">[16]</ref> by a large margin (i.e., 19.7%). Note that compared to unsupervised Re-ID, the annotation effort of our weakly supervised Re-ID is also very inexpensive. These results verify the effectiveness of our method again.</p><p>Semi-supervised Re-ID. In <ref type="table">Table III</ref> (a), we compare our method with the semi-supervised Re-ID models. Five representative state-of-the-art methods are used as competing methods, including SPACO <ref type="bibr" target="#b83">[84]</ref>, HHL <ref type="bibr" target="#b80">[81]</ref>, Distilled <ref type="bibr" target="#b81">[82]</ref>, One Example <ref type="bibr" target="#b19">[20]</ref>, and Many Examples <ref type="bibr" target="#b19">[20]</ref>. The results</p><p>show that our weakly supervised Re-ID problem has obtained significant gain over semi-supervised Re-ID methods. For instance, our method outperforms the best-performing model "ManyExamples" <ref type="bibr" target="#b19">[20]</ref> by a large margin (i.e., 13%), indicating that as an annotation-saving method, our weakly Re-ID obtains higher accuracy than semi-supervised Re-ID.</p><p>2) Accuracy on CUHK03: Our weakly supervised Re-ID is compared with state-of-the-art methods in two groups on CUHK03, including the traditional fully-supervised Re-ID and the unsupervised Re-ID. The semi-supervised Re-ID is not compared here because existing works on semi-supervised Re-ID do not provide results on this dataset.</p><p>Fully supervised Re-ID. In <ref type="table">Table III</ref> (b), we compare our method with the eleven current best models, including BOW+XQDA <ref type="bibr" target="#b1">[2]</ref>, PUL <ref type="bibr" target="#b17">[18]</ref>, LOMO+XQDA <ref type="bibr" target="#b52">[53]</ref>, IDE(R) <ref type="bibr" target="#b55">[56]</ref>, IDE+DaF <ref type="bibr" target="#b57">[58]</ref>, IDE+XQ+reranking <ref type="bibr" target="#b45">[46]</ref>, PAN, DPFL <ref type="bibr" target="#b62">[63]</ref>, and newly proposed methods such as SVDNet <ref type="bibr" target="#b54">[55]</ref>, TriNets <ref type="bibr" target="#b61">[62]</ref>, ACNet <ref type="bibr" target="#b68">[69]</ref>, Local CNN <ref type="bibr" target="#b46">[47]</ref>, MGN <ref type="bibr" target="#b44">[45]</ref>, and TreeConv <ref type="bibr" target="#b64">[65]</ref>. Our W-MGN and W-Local CNN achieve a rank-1 accuracy of 69.8% and 67.6%, which surpass many of the compared fully-supervised methods. These results verify the effectiveness of our method.</p><p>Unsupervised Re-ID. In <ref type="table">Table III</ref> (b), we compare our method with the unsupervised Re-ID models. Three representative state-of-the-art methods are used as competing methods, including CAMEL <ref type="bibr" target="#b13">[14]</ref>, PatchNet <ref type="bibr" target="#b72">[73]</ref>, and PAUL <ref type="bibr" target="#b72">[73]</ref>. The results in Table III (b) show that our weakly supervised Re-ID problem has obtained significant gain over unsupervised Re-ID methods. For instance, our W-MGN outperforms the bestperforming model PAUL <ref type="bibr" target="#b72">[73]</ref> by a large margin (i.e., 17.5%). Given that our method also saves the annotation effort, we believe our weakly-supervised Re-ID balances well between annotation and accuracy.</p><p>3) Accuracy on MSMT17: Our weakly supervised Re-ID is compared with state-of-the-art methods fully-supervised / unsupervised Re-ID on MSMT17. Semi-supervised methods are not compared here because previous works on semisupervised Re-ID do not provide results on this dataset.</p><p>Fully supervised Re-ID. We compare our method with the eleven current best models, including PDC <ref type="bibr" target="#b48">[49]</ref>, GLAD <ref type="bibr" target="#b50">[51]</ref>, ABD-Net <ref type="bibr" target="#b53">[54]</ref>, GoogleNet <ref type="bibr" target="#b56">[57]</ref>, IANet <ref type="bibr" target="#b58">[59]</ref>, SFT <ref type="bibr" target="#b59">[60]</ref>, Auto-ReID <ref type="bibr" target="#b60">[61]</ref>, MVP <ref type="bibr" target="#b63">[64]</ref>, Verif-Identif <ref type="bibr" target="#b65">[66]</ref>, PCB <ref type="bibr" target="#b38">[39]</ref>, JDGL <ref type="bibr" target="#b67">[68]</ref>, ShuffleNet <ref type="bibr" target="#b70">[71]</ref>, MobileNetV2 <ref type="bibr" target="#b71">[72]</ref>, OSNet <ref type="bibr" target="#b73">[74]</ref>, Local CNN <ref type="bibr" target="#b46">[47]</ref>, and MGN <ref type="bibr" target="#b44">[45]</ref>. Comparison results are provided in <ref type="table">Table III (d)</ref>. Our W-MGN and W-Local CNN achieve a rank-1 accuracy of 81.1% and 80.6%, which surpass many of the compared fully-supervised methods. These results verify the effectiveness of our method.</p><p>Unsupervised Re-ID. We compare our method with the unsupervised Re-ID models, as shown in Table III (b). Five representative state-of-the-art methods are used as competing methods, including PTGAN <ref type="bibr" target="#b33">[34]</ref>, SSG <ref type="bibr" target="#b75">[76]</ref>, TAUDL <ref type="bibr" target="#b76">[77]</ref>, UTAL <ref type="bibr" target="#b18">[19]</ref>, ECN <ref type="bibr" target="#b78">[79]</ref>, and UGA <ref type="bibr" target="#b79">[80]</ref>. The results in Table III (d) show that our weakly supervised Re-ID problem has obtained significant gain over unsupervised Re-ID methods. For instance, our W-MGN outperforms the best-performing model UGA <ref type="bibr" target="#b79">[80]</ref> by a large margin (i.e., 81.1% vs. 49.5%). Given that our method also saves the annotation effort, we believe our weakly supervised Re-ID balances well between annotation and accuracy. 4) Accuracy on PRID2011: In Table III (c) and <ref type="figure" target="#fig_0">Fig. 10 (a)</ref>, we compare the results of our model with five current best models: the KISSME distance learning method <ref type="bibr" target="#b4">[5]</ref>, MAHAL, L2, and XQDA <ref type="bibr" target="#b52">[53]</ref>, P2SNet <ref type="bibr" target="#b8">[9]</ref>, Local CNN <ref type="bibr" target="#b46">[47]</ref>, and MGN <ref type="bibr" target="#b44">[45]</ref>. For KISSME, MAHAL, L2, and XQDA, deep features <ref type="bibr" target="#b84">[85]</ref> are utilized to represent an image of a person. For P2SNet, we train the model based on the image-to-video setting but sample one frame from each video to formulate the imageto-image setting. Our W-MGN and W-Local CNN achieve a rank-1 accuracy of 72.7% and 71.6%, which surpass many of the compared fully-supervised methods. These results verify the effectiveness of our method. 5) Accuracy on SYSU-30k: As SYSU-30k is the only weakly supervised Re-ID dataset and our method is the only weakly supervised Re-ID method, we propose to compare the traditional fully supervised Re-ID models with our weakly supervised method by using transfer learning. Specifically, six representative fully supervised Re-ID models including DARI <ref type="bibr" target="#b6">[7]</ref>, DF <ref type="bibr" target="#b5">[6]</ref>, TriNet <ref type="bibr" target="#b61">[62]</ref>, Local CNN <ref type="bibr" target="#b46">[47]</ref>, MGN <ref type="bibr" target="#b44">[45]</ref>, and MGN without triplet loss are first trained on CUHK03. Then, they are used to performed cross-dataset evaluation on the test set of SYSU-30k. In contrast, our weakly-supervised Re-ID is trained on the training set of the SYSU-30k with weak annotations and then is tested on the test set of SYSU-30k.</p><p>Table III (e) and <ref type="figure" target="#fig_0">Fig. 10 (b)</ref> are the results of the comparisons. It is observed that our W-MGN achieves state-ofthe-art performance (29.5%), even though it is trained in a weakly supervised manner while the comparison methods are trained with full supervision. The success may be attributed to two reasons. First, our model is quite effective due to the graphical modeling that generates reliable pseudo labels as compensation for the absence of strong labels. Second, the large-scale SYSU-30k dataset provides rich knowledge that improves the capacity of our model, even though SYSU-30k is annotated weakly. We also qualitatively present some query examples of W-MGN for the SYSU-30k dataset in <ref type="figure" target="#fig_0">Fig. 11</ref>. Each row represents a ranking result with the first image being the query image and the rest being the result list. The matched one in the returned list is highlighted by a red bounding box. This figure exhibits the difficulty of this dataset. Actually, in the failed examples, the images ranked higher than the matched one often look more closer to the query image as in Row 46.</p><p>6) Computational Complexity: <ref type="table" target="#tab_9">Table IV</ref> compares the computational time of Re-ID in the context of weak supervision to that in the context of full supervision in terms of time cost per 100 images. For a fair comparison, both methods are individually trained on the same desktop with 1 Titan-x GPU. As shown in the table, the weakly and fully supervised Re-ID methods have similar computational costs. Specifically, in the testing phase, both methods share the same computational costs. Even in the training phase, our method only performs 0.002? slower than the fully supervised Re-ID (0.2453 vs. 0.2448 seconds per 100 images using TITAN X.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>We have considered a more realistic Re-ID problem challenge: the weakly supervised Re-ID problem. To address this new problem, we proposed a graphical model to capture the dependencies among images in each weakly annotated bag. We further propose a weakly annotated Re-ID dataset (i.e., SYSU-30k) to facilitate future research, which is currently the largest Re-ID benchmark. Extensive experiments have conducted on our SYSU-30k dataset and other four public Re-ID datasets and a superior performance is achieved with our proposed model, providing a promising and appealing conclusion that learning a Re-ID model with less annotation efforts is possible and feasible. Future work will include building automated models <ref type="bibr" target="#b85">[86]</ref> for the weakly supervised Re-ID .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Problem definition for the weakly supervised Re-ID. (a) is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>An illustration of the proposed method for weakly supervised Re-ID. (a) shows a bag of images and their bag-level label. (b) presents the process of differentiable graphical learning. Using graphical modeling, we can obtain the pseudo image-level label for each image, as shown in (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Examples in our SYSU-30k dataset. (a) training images in terms of bag; (b) their bag-level annotations; (c) test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>The statistics of the SYSU-30k. (a) summarizes the number of the bags with respect to the number of the images per bag. (b) and (c) compare SYSU-30k with the existing datasets in terms of image number and person IDs for both the entire dataset and the test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Graphical model to generate pseudo image-level labels for person images. The unary terms are estimated by the deep networks, while the pairwise terms involve the similarity of features, the image appearance, and the bag-level label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Differentiable graphical model in DNNs, where x, Y, P,? denote the input images, bag-level label, preliminary categorization, and refined categorization, respectively. (a) is the stepwise graphical model, while (b) is our proposed end-to-end differentiable graphical model. Our model consists of two losses, i.e., an unsupervised loss for pseudo label generation and a loss supervised by the pseudo labels. Here, the black lines denote forward passing, while the blue lines denote back-propagation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Diagram of our approach. It mainly consists of three stages, i.e., feature extraction, rough Re-ID, and refined Re-ID. The solid black flow denotes the testing stage, while the black dotted flow denotes the training stage. For simplification, the back-propagation flow is omitted. The loss function is marked with a red arrow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>2 ?</head><label>2</label><figDesc>j =k;j=1,... ,tp;y k =y j ||z k ? zj || 2 min j =k;j=1,... ,tp;y k =y j ||z k ? zj || 2 2 ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>70 Fig. 8 :</head><label>708</label><figDesc>The effectiveness of our differentiable graphical learning module. Here we show the errors between the rough predictions and the weak annotations in the form of a confusion matrix containing 76 ? 76 grids. Each grid indicates a bag of 10 categories, with a total sum of 760 categories, which is approximately equivalent to the person categories in the full training set (i.e., 767 categories).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>2</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>2</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 :</head><label>10</label><figDesc>Comparison with state-of-the-art methods. Weakly sup.:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 11 :</head><label>11</label><figDesc>Search examples of W-MGN on SYSU-30k dataset. Each row represents a ranking result with the first image being the query and the rest images being the returned list. The image with the red bounding box is the matched one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>of whether arXiv:1904.03845v3 [cs.CV] 15 Jul 2020</figDesc><table><row><cell cols="2">Person A</cell><cell>Person B</cell><cell>Person C</cell></row><row><cell>Pairwise Term</cell><cell></cell><cell cols="2">Unary Term</cell></row><row><cell>Weak Label</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Pseudo Strong Label</cell></row><row><cell>(a)</cell><cell>(b)</cell><cell></cell><cell>(c)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>w/: with. w/o: without. Market-1501 is another widely-used largescale Re-ID benchmark, which contains 32,668 images of 1,501 IDs captured from 6 different cameras. The dataset is split into two parts: 12,936 images with 751 IDs for training and 19,732 images with 750 IDs for testing. In testing, 3,368 hand-drawn images with 750 IDs are used as probe set to identify the true IDs on the testing set.</figDesc><table><row><cell cols="3">(a) Impact of bag diversity on PRID2011</cell><cell></cell><cell cols="3">(b) Impact of bag diversity on CUHK03</cell><cell></cell></row><row><cell>categories / bag</cell><cell>Rank-1</cell><cell cols="2">Rank-5 Rank-10</cell><cell>categories / bag</cell><cell cols="2">Rank-1 Rank-5</cell><cell>Rank-10</cell></row><row><cell>1 ( fully supervised)</cell><cell>71.8</cell><cell>91.2</cell><cell>95.9</cell><cell>1 ( fully supervised)</cell><cell>67.5</cell><cell>88.2</cell><cell>91.8</cell></row><row><cell>2</cell><cell>68.0</cell><cell>87.5</cell><cell>94.8</cell><cell>2</cell><cell>61.0</cell><cell>82.0</cell><cell>87.0</cell></row><row><cell>3</cell><cell>66.1</cell><cell>86.4</cell><cell>92.3</cell><cell>3</cell><cell>59.4</cell><cell>80.7</cell><cell>86.7</cell></row><row><cell>10</cell><cell>49.5</cell><cell>73.9</cell><cell>82.2</cell><cell>10</cell><cell>55.2</cell><cell>79.3</cell><cell>84.5</cell></row><row><cell>random (5 on average)</cell><cell>69.3</cell><cell>89.0</cell><cell>94.0</cell><cell>random (5 on average)</cell><cell>60.6</cell><cell>81.6</cell><cell>87.0</cell></row><row><cell cols="4">(c) Fully supervised learning tricks on PRID2011</cell><cell cols="4">(d) Fully supervised learning tricks on CUHK03</cell></row><row><cell>method</cell><cell>Rank-1</cell><cell cols="2">Rank-5 Rank-10</cell><cell>method</cell><cell cols="2">Rank-1 Rank-5</cell><cell>Rank-10</cell></row><row><cell>fully supervised alone</cell><cell>48.9</cell><cell>79.6</cell><cell>88.8</cell><cell>fully supervised alone</cell><cell>52.1</cell><cell>77.9</cell><cell>85.6</cell></row><row><cell>weaky supervised alone</cell><cell>39.9</cell><cell>71.2</cell><cell>83.3</cell><cell>weaky supervised alone</cell><cell>44.0</cell><cell>70.6</cell><cell>79.7</cell></row><row><cell>fully supervised + RK</cell><cell>71.8</cell><cell>91.2</cell><cell>95.9</cell><cell>fully supervised + RK</cell><cell>67.5</cell><cell>88.2</cell><cell>91.8</cell></row><row><cell>weakly supervised + RK</cell><cell>68.0</cell><cell>87.5</cell><cell>94.8</cell><cell>weakly supervised + RK</cell><cell>61.0</cell><cell>82.0</cell><cell>87.0</cell></row><row><cell cols="3">(e) Scalability of our method on CUHK03</cell><cell></cell><cell cols="4">(f) Effectiveness of the graphical learning module on CUHK03</cell></row><row><cell>categories</cell><cell>Rank-1</cell><cell cols="2">Rank-5 Rank-10</cell><cell>method</cell><cell cols="2">Rank-1 Rank-5</cell><cell>Rank-10</cell></row><row><cell>67</cell><cell>16.3</cell><cell>34.7</cell><cell>44.9</cell><cell>w/o graphical model</cell><cell>56.4</cell><cell>80.0</cell><cell>85.1</cell></row><row><cell>367</cell><cell>43.6</cell><cell>67.0</cell><cell>75.5</cell><cell>w/o pairwise term</cell><cell>59.2</cell><cell>80.9</cell><cell>86.7</cell></row><row><cell>767 ( * full training set)</cell><cell>61.0</cell><cell>82.0</cell><cell>87.0</cell><cell>w/ graphical model</cell><cell>61.0</cell><cell>82.0</cell><cell>87.0</cell></row><row><cell cols="4">of camera views [1]. Each ID is observed by two disjointed</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">camera views. We follow the new standard protocol [46] of</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">CUHK03, i.e., a training set including 767 IDs is obtained</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>without overlap.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Market-1501.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Cat/bag 66.1% 3 Cat/bag 49.5% 10 Cat/bag 69.3% Random Cat/bag Cat/bag 59.4% 3 Cat/bag 55.2% 10 Cat/bag 60.6% Random Cat/bag</figDesc><table><row><cell></cell><cell>100</cell><cell></cell><cell></cell><cell></cell><cell>100</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy (%)</cell><cell>40 50 60 70 80</cell><cell>4 68.0% 2 (a) PRID2011 6 Rank 71.8% Fully supervised 8</cell><cell>10</cell><cell>Accuracy (%)</cell><cell>40 50 60 70 80</cell><cell>2</cell><cell>4 61.0% 2 (b) CUHK03 6 Rank 67.5% Fully supervised 8</cell><cell>10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE IV :</head><label>IV</label><figDesc>Computational complexity of weakly and fully supervised Re-ID. secs / 100 images: the time of forward-passing 100 images in the testing stage or the cycle of a forward-backward passing in the training stage when the batch size is 100. weakly (secs / 100 images) fully (secs / 100 image)</figDesc><table><row><cell>Testing</cell><cell>0.0559</cell><cell>0.0559</cell></row><row><cell>Training</cell><cell>0.2453</cell><cell>0.2448</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Viewpoint invariant pedestrian recognition with an ensemble of localized features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="262" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2288" to="2295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep feature learning with relative distance comparison for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2993" to="3003" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dari: Distance metric and representation integration for person verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The AAAI Conference on Artificial Intelligence</title>
		<meeting>The AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3611" to="3617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cross-domain visual matching via generalized similarity measure and feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1089" to="1102" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">P2snet: Can an image match a video for person re-identification in an end-to-end way</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2777" to="2787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spatial-temporal person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The AAAI Conference on Artificial Intelligence</title>
		<meeting>The AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Occluded person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Multimedia and Expo</title>
		<meeting>the IEEE International Conference on Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Person search with natural language description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1970" to="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transferable, controllable, and inconspicuous adversarial attacks on person re-identification with deep mis-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cross-view asymmetric metric learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="994" to="1002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">M2M-GAN: many-to-many generative adversarial transfer learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno>abs/1811.03768</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptive re-identification: Theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1807.11334</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification by soft multilabel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2148" to="2157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised person reidentification: Clustering and fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">83</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Communications, and Applications</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised tracklet person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Progressive learning for person re-identification with one example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2872" to="2881" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exploit the unknown gradually: One-shot video-based person re-identification by stepwise learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5177" to="5186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Weakly supervised person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="760" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="185" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep growing learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2812" to="2820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning object interactions and descriptions for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5859" to="5867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep dual learning for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2718" to="2726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep structured scene parsing by learning with image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2276" to="2284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hierarchical scene parsing by weakly supervised learning with image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="596" to="610" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with multi-fold multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="189" to="203" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Constrained clustering and its application to face clustering in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3507" to="3514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Person re-identification with deep similarity-guided graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="508" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by GAN improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3774" to="3782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="79" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Human reidentification with transferred metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="31" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Person reidentification by descriptive and discriminative classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beleznai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scandinavian conference on Image analysis</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="91" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Custom pictorial structures for re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stoppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The British Machine Vision Conference</title>
		<meeting>The British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">YOLO9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6517" to="6525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="480" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">In defense of the triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno>abs/1703.07737</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia Conference on Multimedia Conference</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="274" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Re-ranking person reidentification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3652" to="3661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Local convolutional neural networks for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia Conference on Multimedia Conference</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1074" to="1082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning deep context-aware features over body and latent parts for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="384" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pose-driven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3980" to="3989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deeply-learned part-aligned representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3239" to="3248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">GLAD: globallocal-alignment descriptor for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia Conference on Multimedia Conference</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="420" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Scalable person re-identification on supervised smoothed manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2530" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2197" to="2206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Abd-net: Attentive but diverse person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Svdnet for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3820" to="3828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Person re-identification: Past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno>abs/1610.02984</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Divide and fuse: A re-ranking approach for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The British Machine Vision Conference</title>
		<meeting>The British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Interactionand-aggregation network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9317" to="9326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Spectral feature transformation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Auto-reid: Searching for a part-aware convnet for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno>abs/1708.04896</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Person re-identification by deep learning multi-scale representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2590" to="2600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Mvp matching: A maximum-value perfect matching for mining hard samples, with application to person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Grammatically recognizing images with tree convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A discriminatively learned cnn embedding for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno>13:1- 13:20</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Communications, and Applications</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Perceive where to focus: Learning visibility-aware part-level features for partial person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="393" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Joint discriminative and generative learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2138" to="2147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Adaptively connected neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1781" to="1790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Aanet: Attribute attention network for person re-identifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7134" to="7143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Mo-bilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Patch-based discriminative feature learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3633" to="3642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Omni-scale feature learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification by deep learning tracklet association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="772" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Selfsimilarity grouping: A simple unsupervised cross domain adaptation approach for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification by deep learning tracklet association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="772" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification by deep asymmetric metric embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Invariance matters: Exemplar memory for domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="598" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Unsupervised graph association for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Generalizing a person retrieval model hetero-and homogeneously</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="176" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Distilled person re-identification: Towards a more scalable system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1187" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Smoothing adversarial domain attack and p-memory reconsolidation for cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Self-paced co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2275" to="2284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Mars: A video benchmark for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="868" to="884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Block-wisely supervised neural architecture search with knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
