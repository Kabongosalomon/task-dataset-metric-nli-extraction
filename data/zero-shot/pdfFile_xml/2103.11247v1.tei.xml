<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Paying Attention to Multiscale Feature Maps in Multimodal Image Matching CNN CNN Residual Multiscale Transformer Encoder Multiscale Transformer Encoder Attention map Attention map</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviad</forename><surname>Moreshet</surname></persName>
							<email>aviad10m@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosi</forename><surname>Keller</surname></persName>
							<email>yosi.keller@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Paying Attention to Multiscale Feature Maps in Multimodal Image Matching CNN CNN Residual Multiscale Transformer Encoder Multiscale Transformer Encoder Attention map Attention map</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: The proposed attention-based approach. A Siamese backbone CNN computes the feature maps of the input multimodal image patches, that are passed through a Spatial Pyramid Pooling (SPP) layer <ref type="bibr" target="#b16">[17]</ref>. A Transformer encoder aggregates the multiscale image embeddings. A residual connection bypasses the Encoder to facilitate end-to-end training. The attention heatmaps visualize the modality-invariant visual cues emphasized by the attention scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We propose an attention-based approach for multimodal image patch matching using a Transformer encoder attending to the feature maps of a multiscale Siamese CNN. Our encoder is shown to efficiently aggregate multiscale image embeddings while emphasizing task-specific appearanceinvariant image cues. We also introduce an attentionresidual architecture, using a residual connection bypassing the encoder. This additional learning signal facilitates end-to-end training from scratch. Our approach is experimentally shown to achieve new state-of-the-art accuracy on both multimodal and single modality benchmarks, illustrating its general applicability. To the best of our knowledge, this is the first successful implementation of the Transformer encoder architecture to the multimodal image patch matching task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image patch matching is a fundamental task in computer vision aiming to determine the correspondence between two image patches. Multimodal patch matching focuses on matching patches originating from different sources, such as visible RGB and near-infrared (NIR). Such patches are inherently more difficult to match due to the non-linear and unknown variations in pixel intensities, varying lighting conditions and colors. In particular, multimodal images are the manifestations of different physical attributes of the depicted objects, and the acquisition devices. There are a gamut of multimodal patch matching applications, such as fusing information in medical imaging devices <ref type="bibr" target="#b34">[35]</ref> and multi-sensor images alignment <ref type="bibr" target="#b18">[19]</ref>.</p><p>Pioneering work <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b0">1]</ref> extended classical local image descriptors such as SIFT <ref type="bibr" target="#b23">[24]</ref> and SURF <ref type="bibr" target="#b6">[7]</ref> to multimodal matching, attempting to derive appearance-invariant feature descriptors. The resulting descriptors were matched using the Euclidean distance, but showed limited results, being unable to capture high-level structural information. Recently, deep learning models were shown to be the most effective <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b37">38]</ref>, allowing improved performance across multiple modalities. Contemporary state-of-the-art (SOTA) approaches are based on training a Siamese CNN <ref type="bibr" target="#b20">[21]</ref>, either by directly outputting pairwise patch matching probabilities <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b13">14]</ref>, or by outputting the patches feature descriptors and encoding their similarity in a latent space <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">36]</ref>, an approach denoted as descriptor learning.</p><p>Different losses were used to optimize the networks. Cross-Entropy Loss can be used <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14]</ref> to classify the image patches as in a binary same/not-same classification task. Contrastive <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b5">6]</ref> and Triplet Losses <ref type="bibr" target="#b25">[26]</ref> were used along with Euclidean distance to learn descriptors encoding latent space similarity. Due to the high ratio of negative pairs, most of which being easy negatives barely contributing to the loss, hard negative mining schemes have become prevalent in recent studies <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20]</ref>, sometimes combined with hard positive mining <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>We propose a novel approach for attention-based multimodal patch matching. The gist of our approach, shown in <ref type="figure">Fig. 1</ref>, is to apply attention using a Transformer encoder to aggregate multiscale image embeddings into a unified image descriptor. The descriptors are learnt for both modalities by a weight-sharing Siamese network, that simultaneously learns the joint informative cues in both patch modalities. In particular, as depicted in the attention heatmaps visualized in <ref type="figure">Figs. 1 and 3</ref>, the modality-invariant informative visual cues correspond to the largest attention activations, computed by the encoder. Other image locations that are less task-informative show weak attention activations. To apply the Transformer Encoder, we formulate the matching task as a sequence-to-one problem, where the inputs are multimodal feature maps flattened into sequences and the outputs are adaptively aggregated embeddings. The spatial layout information of the image embeddings is induced by positional encoding. Due to the lack of pre-trained CNN backbones for multimodal patch matching, the backbone CNN has to be trained from scratch, starting from randomly initialized weights. This, in turn, prevents the encoder from learning meaningful sequence summarizations at the beginning of the training process, hindering the overall learning. To mitigate this issue, we propose to add a residual connection to the network bypassing the Transformer encoder, ultimately allowing end-to-end training of the proposed architecture from scratch. Our approach is experimentally shown to achieve new SOTA multimodality matching performance on multiple contemporary datasets.</p><p>In conclusion, our contribution is threefold:</p><p>1. A novel attention-based approach for multimodal image patch matching by aggregating multiscale Siamese CNN feature maps, using the Transformer encoder. To the best of our knowledge, this is the first application of the Transformer encoder architecture to the multimodal image patch matching task.</p><p>2. We propose a residual connection bypassing the encoder to enable end-to-end training from scratch, a crucial requirement in the absence of pre-trained weights for the task.</p><p>3. Our approach is experimentally shown to achieve new SOTA results on contemporary multimodal image patch matching benchmarks VIS-NIR <ref type="bibr" target="#b2">[3]</ref> and En et al. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14]</ref>. We also achieve new SOTA performance on the single modality UBC Benchmark <ref type="bibr" target="#b8">[9]</ref>, showing the generalizability of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work 2.1. CNN-based approaches for image matching</head><p>Deep learning techniques became the staple of computer vision, and were applied to multi-modality matching, showing SOTA performance. Jahrer et al. <ref type="bibr" target="#b20">[21]</ref> introduced a Siamese CNN network for metric learning of feature descriptors, outperforming classic handcrafted descriptors such as SIFT <ref type="bibr" target="#b23">[24]</ref>. Han et al. <ref type="bibr" target="#b39">[40]</ref> added a metric learning network on top of a Siamese CNN, achieving SOTA results while also decreasing the size of the learned descriptors. Zagoruyko and Komodakis <ref type="bibr" target="#b40">[41]</ref> explored multiple Siamese-based CNN architectures. The first is a 2-Channel architecture where the patches are concatenated channel-wise and are fed to a single CNN. The second is a Pseudo-Siamese CNN network with non-shared weights, and the last is a central-surround architecture, in which two Siamese CNNs are jointly trained with different input resolutions to better capture multi-resolution information. All methods presented promising results on the reported benchmarks. Aguilera et al. <ref type="bibr" target="#b2">[3]</ref> extended those architectures to the multimodal case, outperforming previous approaches. Balntas et al. suggested a weight-sharing triplet CNN termed PN-Net <ref type="bibr" target="#b4">[5]</ref>, using image triplets consisting of two matching patches alongside a non-matching one. A new curated loss was also introduced to replace hard negative mining, by penalizing small L 2 distances between nonmatching pairs and large L 2 distances between matching pair, forcing the network to always perform backpropagation using the hardest non-matching sample of each triplet. Aguilera et al. proposed a cross-spectral extension to the previous work, a weight-sharing quadruple network called Q-Net <ref type="bibr" target="#b3">[4]</ref>, utilizing two pairs of cross-spectral patches. The loss was extended to the quadruple case, mining both hard positive and hard negative samples at the same time. L2-Net <ref type="bibr" target="#b35">[36]</ref> proposed by Tian et al. addressed an inherent problem in patch matching where the number of negative samples is orders of magnitude larger than positive samples, by picking n 2 ?n hard negative samples per batch with a minimum distance to the n positive batch pairs. Mishchuk et al. proposed the HardNet <ref type="bibr" target="#b25">[26]</ref> extension to L2-Net <ref type="bibr" target="#b35">[36]</ref> using a curated hard negatives sampling scheme, ensuring that the distance of the selected negative sample is minimal. Both approaches achieved SOTA performance in unimodal benchmarks. Wang et al. introduced a novel Exponential Loss function <ref type="bibr" target="#b37">[38]</ref> forcing the network to learn more from both positive and negative hard samples than from easy ones. The scheme combines hard negative and positive mining inspired by Mishchuk et al. <ref type="bibr" target="#b25">[26]</ref> and Simo et al. <ref type="bibr" target="#b33">[34]</ref>, respectively. Irshad et al. followed this line with the Twin-Net <ref type="bibr" target="#b19">[20]</ref> approach, by proposing to mine twin negatives along with a dedicated Quad Loss, designed for single modality patches. In twin negatives mining, the first negative is mined as in Mishchuk et al. <ref type="bibr" target="#b25">[26]</ref>, and its closest negative is picked as the second negative. The Quad Loss then forces the descriptors of the chosen positive and anchor to be closer than the twin negatives descriptors, resulting in descriptors with a greater discriminatory power.</p><p>En et al. proposed an approach to utilize both joint and modality-specific information within the image patches, by combining Siamese and Pseudo Siamese CNNs in a unified architecture dubbed TS-Net <ref type="bibr" target="#b13">[14]</ref>. Ben-Baruch and Keller <ref type="bibr" target="#b5">[6]</ref> built a similar hybrid framework fusing Siamese and Pseudo Siamese CNNs features through fully connected layers. Contrary to TS-Net <ref type="bibr" target="#b13">[14]</ref>, their approach calculated L 2 -optimized patch descriptors essential for patch matching, and optimized the network using multiple auxiliary losses. These extensions have proven vital and resulted in SOTA performance on various multimodal benchmarks, introducing significant improvement. Quan et al.</p><p>proposed the AFD-Net framework <ref type="bibr" target="#b29">[30]</ref> consisting of three sub-networks, the first, aggregates multi-level feature differences, the second extracts domain-invariant features from image patch pairs, while the final one infers matching labels. This approach computes a patch similarity score, rather than image embeddings. Thus, O(n 2 ) forward passes of the network are required for matching two images containing n image patches each, in contrast to O(n) forward passes in a metric learning approach such as ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Transformer-based approaches in computer vision</head><p>The Transformer architecture was introduced by Vaswani et al. <ref type="bibr" target="#b36">[37]</ref> as a novel formulation of attention-based approaches, allowing the encoding of sequences without RNN layers such as LSTM and GRU. Transformers were first applied in Natural Language Processing tasks <ref type="bibr" target="#b11">[12]</ref>, and then proved successful in a gamut of computer vision schemes <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32]</ref>. Transformers, in contrast to convolution networks, are capable of aggregating longrange interactions between a sequence of input vectors. In computer vision, we can formulate the outputs of a backbone CNN as a sequence as in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b12">13]</ref> and encode them using a Transformer encoder that aggregates spatial interactions (attention weights) between the activation map entries. Task-informative entries are numerically emphasized, compared to the non-informative ones. The Transformer architecture is composed of stacked layers of selfattention encoders and decoders attending the encoder out-puts using a set of vectors denoted as queries. Using selfattention, the encoder produces a weighted average over the values of its inputs, such that the weights are produced dynamically using a similarity function between the inputs. Transformers-based Encoders and Decoders utilize multiple stacked Multi-Head Attention and Feed Forward layers. In contrast to the sequentially-structured RNNs, the relative position and sequential order of the sequence elements are induced by positional encodings, that are added to the Attention embeddings. In this work, we propose to combine a Siamese CNN for feature extraction with self-attention implemented by a Transformer encoder, adaptively weighing the feature maps according to their task-specific contribution, resulting in richer feature descriptors, leading to better multimodal matching capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multiscale attention-based multimodal patch matching</head><p>We propose a novel attention-based approach for multimodal image patch matching by encoding Siamese CNN multiscale feature maps using the Transformer encoder architecture. We also introduce a residual connection bypassing the encoder, that is crucial for end-to-end training. We model the task as a sequence-to-one problem, where the inputs are multiscale feature maps of patches transformed into sequences, and the output is a learned feature descriptor. An overview of the proposed scheme is shown in <ref type="figure">Fig. 1</ref>.</p><p>Let {X, Y} ? R H?W ?C be a pair of multimodal image patches. We first compute the corresponding activation maps {X, Y} ? R H?W ?C using the backbone Siamese CNN. A Spatial Pyramid Pooling (SPP) layer <ref type="bibr" target="#b16">[17]</ref> is applied to {X, Y}, to compute the K multiscale activation</p><formula xml:id="formula_0">maps { X k , Y k } ? R H k ? W k ?C , as detailed in Section 3.1. Each of { X k , Y k }</formula><p>is flattened into sequences and aggregated into a single vector by the Transformer encoder, as in Section 3.2. A fully connected layer finally maps the output representations into { X, Y} ? R 128 . We also propose a novel residual connection described in Section 3.3 that bypasses the Transformer encoder and allows end-toend training. The network is trained end-to-end using the Triplet Loss <ref type="bibr" target="#b32">[33]</ref> and a symmetric formulation of the Hard-Net approach <ref type="bibr" target="#b25">[26]</ref> detailed in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Backbone CNN</head><p>For every pair of multimodal image patches {X, Y}, we apply a Siamese CNN to extract the feature maps {X, Y}. The CNN architecture is described in <ref type="table" target="#tab_0">Table 1</ref>. We use the same backbones as previous works, to experimentally exemplify the efficiency of the proposed attention-based scheme in Section 4. Each convolutional layer is followed by Batch Normalization and ReLU activation. We use an SPP layer <ref type="bibr" target="#b16">[17]</ref> consisting of a four-level pyramid pooling, transforming the CNN feature maps into K mul- </p><formula xml:id="formula_1">tiscale feature maps { X k , Y k } ? R H k ? W k ?C such that { H k , W k } ? {8, 4, 2, 1}. Layer Output Kernel Stride Pad Dilation Conv0 64 ? 64 ? 32 3 ? 3 1 1 1 Conv1 64 ? 64 ? 32 3 ? 3 1 1 1 Conv2 31 ? 31 ? 64 3 ? 3 2 1 2 Conv3 31 ? 31 ? 64 3 ? 3 1 1 1 Conv4 29 ? 29 ? 128 3 ? 3 1 1 2 Conv5 29 ? 29 ? 128 3 ? 3 1 1 1 Conv6 29 ? 29 ? 128 3 ? 3 1 1 1 Conv7 29 ? 29 ? 128 3 ? 3 1 1 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multiscale Transformer encoder</head><p>The proposed weight-sharing Transformer encoder aggregates the K multiscale outputs of the backbone CNN and SPP layer, and is illustrated in <ref type="figure">Fig. 2</ref>. The encoder has two layers, each consisting of two multi-head attention blocks. Given K multiscale feature maps { X k , Y k }, each map is spatially flattened into a sequence of size H k * W k ? C. We prepend a learnable embedding to each sequence whose corresponding output is the aggregated result, same as in <ref type="bibr" target="#b11">[12]</ref>. In order to induce spatial information to the permutation-invariant Transformer encoder, we learn separable 2D positional encodings. The encodings are learned separately for the X and Y axes to reduce the number of learned parameters, and are denoted as</p><formula xml:id="formula_2">E X ? R H k ? C 2 and E Y ? R W k ? C 2 respectively. The final 2D positional encoding E i,j ? R C is given by: E i,j = E j X E i Y , [i, j] ? 1... H k , 1... W k<label>(1)</label></formula><p>The positional encodings are then spatially flattened into a sequence of size H k * W k ? C and fed along with the feature map sequences to the encoder. The Transformer encoder is applied to produce K aggregations of the flattened</p><formula xml:id="formula_3">embeddings { X k , Y k }, denoted O X k , O Y k . O X k and O Y k</formula><p>are separately concatenated into two vectors of dimension R 4C , alongside the residual connections embedding X 8 , Y 8 ? R 8?8?C , as discussed in Section 3.3. Each of the two concatenated embedding is then passed through an FC layer and L 2 normalized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The residual connection</head><p>It is common to apply attention layers to pre-trained backbone CNNs that are kept frozen during the training process, allowing the Transformer encoder to train based on <ref type="figure">Figure 2</ref>: The proposed multiscale Transformer encoder architecture. X is a feature map computed by the backbone CNN, and pooled into four multiscale feature maps X k by a SPP layer <ref type="bibr" target="#b16">[17]</ref>. The Transformer encoder aggregates the pooled maps X k , and the aggregations O X k ? R 128 are concatenated alongside the residual connection. meaningful feature representations computed by the backbone. In contrast, in multimodal matching the backbone has to be trained end-to-end from scratch, due to the absence of pre-trained weights for the task. Thus, at the beginning of the training, until the CNN learns to extract useful feature maps, the encoder does not manage to learn meaningful sequence aggregations either, hindering the overall learning. We propose to tackle this problem by passing the last pooled feature map straight to the output layer using a residual connection, circumventing the encoder during its initialization phases, adding a vital learning signal which ultimately allows end-to-end training from scratch. As exemplified in <ref type="table">Table 5</ref>, our architecture cannot be trained from scratch without this residual connection. Finally, the concatenated output is mapped by a fully connected layer F C producing feature descriptors for each pair of patches, denoted as { X, Y} ? R 128 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Symmetric triplet loss</head><p>The proposed network is trained by applying the Triplet Loss <ref type="bibr" target="#b32">[33]</ref> to the outputs { X, Y}:</p><formula xml:id="formula_4">L t (a i , p j , n k ) = N n,j,k max(0, m + d(a i , p j ) ? d(a i , n k )) (2)</formula><p>where a i , p j and n k are triplets of positive, anchor and negative samples, m is a margin, and d is the Euclidean distance. We used m = 1 which is a common choice for the margin that also worked well in previous works. We also experimented using other values, but the performance was not improved. We chose d as the Euclidean distance function to be able to utilize efficient large-scale K-nearest-neighbors search schemes for descriptors matching in L 2 spaces <ref type="bibr" target="#b14">[15]</ref>. As the hard negative n k is dynamically chosen within each batch following the HardNet approach <ref type="bibr" target="#b25">[26]</ref>, we utilize the symmetry between a i , p j and train the network using a symmetrized formulation of Eq. 2 as follows:</p><formula xml:id="formula_5">L s t (a i , p j , n k ) = L t (a i , p j , n k ) + L t (p j , a i , n k ) (3)</formula><p>For each positive pair of patches a i , p i originated from different modalities, we first pick the hard negative as the negative sample in the batch closest to a i and compute L t (a i , p j , n k ). The process is repeated mutandis mutatis by choosing the negative sample closest to p i and computing L t (p j , a i , n k ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>We evaluate the proposed scheme using multiple contemporary multimodal patch matching datasets. We use their publicly available setups that provide a deterministic generation of matching and non-matching train and validation patches. This allows a repeatable experimental setup that is shared and utilized by us and the SOTA schemes we compare against. We also demonstrate the generalizability of our proposal by evaluating it on the UBC Benchmark <ref type="bibr" target="#b8">[9]</ref>, a single modality, general benchmark for patch matching that was used in previous works. We measure our performance using the false positive rate at 95% recall (FPR95), where a smaller FPR95 score is better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>The VIS-NIR Benchmark <ref type="bibr" target="#b2">[3]</ref> is a cross-spectral image patch matching benchmark consisting of over 1.6 million RGB and Near-Infrared (NIR) patch pairs of size 64 ? 64. The benchmark was collected by Aguilera et al. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> from the public VIS-NIR scene dataset. Half of the patches are matching pairs and half are non-matching pairs chosen at random. A public domain code 1 by Aguilera et al. allows to deterministically recreate the training and test sets, and was used by all schemes we compare against. The En et al. Benchmark <ref type="bibr" target="#b13">[14]</ref> consists of three different multimodal datasets sampled on a uniform grid layout, with a corresponding public domain code 2 . VeDAI <ref type="bibr" target="#b30">[31]</ref>, consisting of vehicles in aerial imagery, CUHK <ref type="bibr" target="#b38">[39]</ref>, consisting of faces and face sketch pairs, and VIS-NIR <ref type="bibr" target="#b2">[3]</ref>. Following En et al. setup <ref type="bibr" target="#b13">[14]</ref>, 70% of the data is used for training, 10% for validation and 20% for testing.</p><p>UBC Benchmark <ref type="bibr" target="#b8">[9]</ref>, also known as Brown Benchmark, is a single modality dataset consisting of image pairs sampled from 3D reconstructions. The benchmark consists of three subsets: Liberty, Notredame, and Yosemite, consisting of 450K, 468k, and 634K patches respectively. Patches <ref type="bibr" target="#b0">1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training</head><p>We use PyTorch <ref type="bibr" target="#b27">[28]</ref> with three NVIDIA GeForce GTX 1080 Ti GPUs. Networks are randomly initialized from a normal distribution and optimized using Adam <ref type="bibr" target="#b22">[23]</ref> with an initial learning rate of 0.1. A warmup learning rate scheduler <ref type="bibr" target="#b15">[16]</ref> was used for eight epochs, and then replaced with a scheduler reducing the learning rate by a factor 10 every plateau of three epochs. The architecture was trained for 70 epochs using 48 size batches. Patches were mean-centered and normalized, and basic augmentations of horizontal flips and rotations were applied. The architecture is trained with random negatives until there is no loss decrease for three epochs. We then start mining hard negative samples using Mishchuk et al. technique <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">VIS-NIR Benchmark</head><p>To demonstrate the effectiveness of our approach on the multimodal image patch matching task, we compare it with 14 state-of-the-art methods, using the same setup as Aguilera et al. <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6]</ref>. The Country category is split into 80% and 20% for training and validation, respectively. The rest of the categories are used for testing. The results are shown in <ref type="table" target="#tab_3">Table 2</ref>. We compare our proposal to both handcrafted descriptors <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b1">2]</ref> and state-of-the-art deep learning methods. We quote the results reported by <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b5">6]</ref> on this benchmark.</p><p>For the handcrafted descriptors <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b1">2]</ref> we used their publicly available code. As in previous works, such approaches are outperformed by learning-based methods due to their inability to capture high-level semantic information. PN-Net <ref type="bibr" target="#b4">[5]</ref>, L2-Net <ref type="bibr" target="#b35">[36]</ref>, HardNet <ref type="bibr" target="#b25">[26]</ref> and Exp-TLoss <ref type="bibr" target="#b37">[38]</ref> are general-purpose, single modality schemes that were applied to the VIS-NIR Benchmark by Wang et al. <ref type="bibr" target="#b37">[38]</ref>. These approaches that are based on a Siamese CNN architecture, same as the proposed scheme, generalized well to the multimodal case. Aguilera et al.'s three proposals <ref type="bibr" target="#b2">[3]</ref> of vanilla Siamese CNN, Pseudo Siamese CNN, 2-Channel CNN, as well as SCFDM <ref type="bibr" target="#b28">[29]</ref> and D-Hybrid-CL <ref type="bibr" target="#b5">[6]</ref> were specifically designed for multimodal image patch matching and were tested on the VIS-NIR Benchmark. Our proposed scheme outperforms all previous approaches, achieving a new SOTA on this benchmark. In particular, it surpasses the previous SOTA Exp-TLoss <ref type="bibr" target="#b37">[38]</ref> by 43% and D-Hybrid-CL <ref type="bibr" target="#b5">[6]</ref> by 18%. Both methods are based on the same backbone CNN and SPP layer. Thus, we attribute the improved performance to the proposed attention-based inference, as opposed to previous schemes <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b5">6]</ref>    <ref type="bibr" target="#b2">[3]</ref>. Score is given in terms of FPR95.</p><p>ited to the multiscale SPP-based inference without the spatial attention-based aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">En et al. Benchmark</head><p>We compare our approach on this multimodal, multidataset benchmark following En et al. <ref type="bibr" target="#b13">[14]</ref> publicly available setup 3 with ten state-of-the-art methods, both handcrafted <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b1">2]</ref> and learning-based <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b5">6]</ref>. The results are reported at <ref type="table" target="#tab_5">Table 3</ref>. We quote the results of previous schemes reported by <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b5">6]</ref> on this benchmark.</p><p>On the VeDAI dataset, both 2-Channel <ref type="bibr" target="#b2">[3]</ref>, Hybrid <ref type="bibr" target="#b5">[6]</ref> variants and our approach achieve zero error. On CUHK, our approach achieves the same performance as Hybrid-CE <ref type="bibr" target="#b5">[6]</ref>. CUHK is a dataset composed of pairs of faces along with their face sketches. In this case, where the features are relatively limited and spatially close to each other, it seems that self-attention over the feature maps might not capture additional information. Nonetheless, we establish a new SOTA on the VIS-NIR dataset, improving it by 93%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">UBC Benchmark</head><p>To illustrate the generalizability of our approach, we compare it with 11 state-of-the-art methods using the popular single modality UBC Benchmark <ref type="bibr" target="#b8">[9]</ref>, and the publicly available setup and evaluation code 4 , that was used by all the schemes we compare against. The results are presented in <ref type="table" target="#tab_7">Table 4</ref>. We quote the previous results reported on this benchmark by <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b19">20]</ref>. <ref type="bibr" target="#b2">3</ref>   Benchmark <ref type="bibr" target="#b13">[14]</ref> consisting of three multimodal datasets sampled on a uniform grid layout. The score is given in terms of FPR95. On the VeDAI dataset, we performed a stricter evaluation using FPR99 and reported the standard deviation out of 10 tests.</p><p>Our approach achieved a new SOTA performance when trained on the Liberty and Yosemite datasets, surpassing the previous SOTA by 24%. When trained on Notredame our approach is slightly outperformed by CS L2-Net <ref type="bibr" target="#b35">[36]</ref> and Twin-Net <ref type="bibr" target="#b19">[20]</ref>. We suspect it is due to the significant geometric deformations in Notredame, degrading the feature maps extracted by our Siamese CNN. Notredame also con-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Attention maps</head><p>We further study the Transformer encoder self-attention mechanism by visualizing the attention weights as heatmaps. This allows to analyze the visual cues the encoder pays attention to. We show the attention heatmaps of four matching RGB and NIR pairs taken from the VIS-NIR Benchmark <ref type="bibr" target="#b2">[3]</ref>, originated from different scenes. The heatmaps are generated by upsampling the attention weights of the last encoder layer.</p><p>The resulting attention maps are presented in <ref type="figure">Fig. 3</ref>, showing the encoder learns to attend to modality-invariant feature locations, paying more attention to locations consisting of object blobs or their distinctive corners and edges. In the first pair 3a, the encoder summarizes the scene well by paying attention to both the bigger traffic sign in the middle and the second, smaller one in the background. In the second pair 3b, the encoder attends to the distinctively shaped tree on the left, as well as the house corner, which is a good modality-invariant feature. The third pair 3c contains two small houses and the encoder chooses to attend to both of them, also paying some attention to the grass fields in the foreground. In the fourth pair 3d, the encoder catches and attends the car in the middle of the scenes along with parts of the background house, but misses the black motorbike in the NIR image, probably due to distinction difficulties originated from low pixel intensities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Ablation study</head><p>To better understand the contribution of each component in our proposal, we experiment with modifying one compo-  <ref type="table">Table 5</ref>: Ablation results evaluated on the VIS-NIR Benchmark <ref type="bibr" target="#b2">[3]</ref>. Score is given in terms of FPR95. DIV means the model diverged and was not able to learn. nent or related parameter at a time and measuring its impact. The experiments were performed on the VIS-NIR Benchmark <ref type="bibr" target="#b2">[3]</ref>, and are reported at <ref type="table">Table 5</ref>. We first evaluate the impact of the Transformer encoder. By removing the Transformer and leaving the rest of the architecture intact, it is shown that adding the Transformer provides a 23% improvement. Removing also the SPP as well as the Transformer results in even a greater error. The performance is not improved by adding encoder layers or heads, nor adding a Transformer decoder on top of the encoder which degrades performance by 14%, probably due to overfitting. A CNN of non-shared weights, coined Pseudo-Siamese CNN and used in previous SOTA approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref>, ends up with inferior performance compared to the Siamese CNN, illustrating the importance of weight-sharing. An additional observation is the significance of the residual connection, as without it, the architecture cannot be trained end-to-end from scratch and the training diverges.   <ref type="bibr" target="#b2">[3]</ref>. Score is given in terms of FPR95.</p><p>We also measure the impact of the different positional encoding formulation in <ref type="table" target="#tab_10">Table 6</ref>, where we evaluated fixed and learned 1D and 2D positional encodings, following Parmar et al. <ref type="bibr" target="#b26">[27]</ref> and Eq. 1. It follows that learning a 2D positional encoding yields the best performance, implying that the encoder benefits from utilizing the original 2D spatial layout of the features.</p><p>The embedding dimension used by the network was studied in <ref type="table">Table 7</ref> by training the network from scratch using multiple dimensions. A larger embedding dimension can improve the network's capacity, but can also lead to overfitting. The results in <ref type="table">Table 7</ref> show that R 128 is the 'sweet spot', as the model does not benefit from a larger descriptor size, and its performance is degraded by using a smaller descriptor.</p><p>Dimension VIS-NIR 64 1.93 128 1.44 256 1.45 <ref type="table">Table 7</ref>: Descriptor size ablation results evaluated on the VIS-NIR Benchmark <ref type="bibr" target="#b2">[3]</ref>. The score is given in terms of the FPR95.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we presented a novel approach for performing multimodal image patch matching using a Transformer encoder on top of Siamese CNN multiscale feature maps, utilizing long-range feature interactions as well as modality-invariant feature aggregations. We also introduced a residual connection shown to be essential for training Transformer-based networks form scratch. The proposed scheme achieves new SOTA performance when applied to contemporary multimodal patch matching benchmarks and the popular single modality UBC Benchmark, illustrating its generalizability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The Siamese backbone CNN architecture.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Netwere extracted using Difference of Gaussian or Harris detector. Half of the patches, with the same 3D point, are matching pairs, and the rest are non-matching pairs. Following<ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b25">26]</ref> we iteratively train on one subset and test on 100k pairs of the other subsets.</figDesc><table><row><cell>VIS-NIR</cell><cell>benchmark</cell><cell>public</cell><cell>setup</cell><cell>code:</cell></row><row><cell cols="2">https://github.com/ngunsu/lcsis</cell><cell></cell><cell></cell><cell></cell></row></table><note>2 En et al. benchmark setup code: https://github.com/ensv/TS-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Patch matching results evaluated on the VIS-NIR Benchmark</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>En et al. benchmark setup code: https://github.com/ensv/TS-Net</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Method</cell><cell>VeDAI</cell><cell>CUHK VIS-NIR</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Handcrafted descriptor methods</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SIFT[24]</cell><cell>42.74</cell><cell>5.87</cell><cell>32.53</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MI-SIFT [25]</cell><cell>11.33</cell><cell>7.34</cell><cell>27.71</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>LGHD[2]</cell><cell>1.31</cell><cell>0.65</cell><cell>10.76</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Learning-based methods</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Siamese[3]</cell><cell>0.84</cell><cell>3.38</cell><cell>13.17</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Pseudo Siamese[3]</cell><cell>1.37</cell><cell>3.7</cell><cell>15.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2-Channel[3]</cell><cell>0</cell><cell>0.39</cell><cell>11.32</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Q-Net[4]</cell><cell>0.78</cell><cell>0.9</cell><cell>22.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TS-Net[14]</cell><cell>0.45</cell><cell>2.77</cell><cell>11.86</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Hybrid-CE [6]</cell><cell>0</cell><cell>0.05</cell><cell>3.66</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Hybrid-CL [6]</cell><cell>0</cell><cell>0.1</cell><cell>3.41</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ours</cell><cell>0, ? = 0</cell><cell>0.05</cell><cell>1.76</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ours FPR99</cell><cell>0, ? = 0</cell><cell>-</cell><cell>-</cell></row><row><cell>4 UBC</cell><cell>benchmark</cell><cell>setup</cell><cell>and</cell><cell>evaluation</cell><cell>code:</cell></row><row><cell cols="5">https://github.com/osdf/datasets/tree/master/patchdata</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Patch matching results evaluated on En et al.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Patch matching results evaluated on the UBC Benchmark<ref type="bibr" target="#b8">[9]</ref>. Score is given in terms of FPR95.</figDesc><table><row><cell>LIB: Liberty, NOT: Notredame, YOS: Yosemite.</cell></row><row><cell>sists of some extremely hard samples, that Twin-Net [20]</cell></row><row><cell>might be able to mine better using its improved hard sam-</cell></row><row><cell>ples mining scheme.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Positional encoding ablation results evaluated on the VIS-NIR Benchmark</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multispectral image feature points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristhian</forename><surname>Aguilera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Barrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><surname>Lumbreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">D</forename><surname>Sappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Toledo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="12661" to="12672" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Lghd: A feature descriptor for matching across non-linear intensity variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristhian</forename><surname>Aguilera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">D</forename><surname>Sappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Toledo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing (ICIP), 2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-09" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning cross-spectral similarity measures with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Aguilera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Aguilera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Sappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aguilera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Toledo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="267" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cross-spectral local descriptors via quadruplet network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cristhian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">D</forename><surname>Aguilera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristhian</forename><surname>Sappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Aguilera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toledo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Pn-net: Conjoined triple deep network for learning local image descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassileios</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lilian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<idno>abs/1601.05030</idno>
		<imprint>
			<date type="published" when="2006" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Multimodal matching using a hybrid convolutional neural network. CoRR, abs/1810.12941</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Ben Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosi</forename><surname>Keller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Speeded-up robust features (surf)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="346" to="359" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discriminative learning of local image descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Real-time multi-modal rigid registration based on a novel symmetric-sift descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progress in Natural Science</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="643" to="651" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ts-net: Combining modality specific and common features for multimodal patch matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lechervy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="3024" to="3028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Similarity search in high dimensions via hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aristides</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Very Large Data Bases, VLDB &apos;99</title>
		<meeting>the 25th International Conference on Very Large Data Bases, VLDB &apos;99<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc. 4</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="518" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training imagenet in 1 hour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno>abs/1706.02677</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improved symmetric-sift for multimodal image registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Md</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohua</forename><surname>Tanvir Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyh</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojun</forename><surname>Wei Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lackmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 International Conference on Digital Image Computing: Techniques and Applications, DICTA &apos;11</title>
		<meeting>the 2011 International Conference on Digital Image Computing: Techniques and Applications, DICTA &apos;11<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="197" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robust multi-sensor image alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth International Conference on Computer Vision (IEEE Cat. No.98CH36271)</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="959" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Twin-net descriptor: Twin negative mining with quad loss for patch-based matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aman</forename><surname>Irshad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rehan</forename><surname>Hafiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Faisal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongju</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeongil</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="136062" to="136072" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learned local descriptors for recognition and matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jahrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision Winter Workshop</title>
		<meeting>the Computer Vision Winter Workshop</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning deep descriptors with scaleaware triplet networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zetao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabiola</forename><surname>Maffra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Schmuck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margarita</forename><surname>Chli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distinctive image features from scaleinvariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2004-11-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mi-sift: Mirror and inversion invariant generalization for sift descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Image and Video Retrieval, CIVR &apos;10</title>
		<meeting>the ACM International Conference on Image and Video Retrieval, CIVR &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Working hard to know your neighbor&apos;s margins: Local descriptor learning loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishchuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Radenovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<idno>PMLR. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsm?ssan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
	<note>Jennifer Dy and Andreas Krause</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cross-spectral image patch matching by learning features of the spatially connected patches in a shared space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dou</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuefeng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ACCV 2018</title>
		<editor>C. V. Jawahar, Hongdong Li, Greg Mori, and Konrad Schindler</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Afd-net: Aggregated feature difference learning for cross-spectral image patch matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dou</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuefeng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaowei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Huyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Vehicle detection in aerial imagery : A small target detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Razakarivony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Superglue: Learning feature matching with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul-Edouard</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Discriminative learning of deep convolutional feature point descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deformable medical image registration: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aristeidis</forename><surname>Sotiras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Davatzikos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Paragios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2013-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">L2-net: Deep learning of discriminative patch descriptor in euclidean space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Better and faster: Exponential loss for image patch matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuefeng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dou</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaowei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Face photo-sketch synthesis and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1955" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Matchnet: Unifying feature and metric learning for patchbased matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xufeng</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="3279" to="3286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to compare image patches via convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="4353" to="4361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning spread-out local feature descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4605" to="4613" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
