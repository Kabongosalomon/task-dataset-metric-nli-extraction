<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Single headed attention based sequence-to-sequence model for state-of-the-art results on Switchboard</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zolt?n</forename><surname>T?ske</surname></persName>
							<email>zoltan.tuske@ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IBM Research AI</orgName>
								<orgName type="institution" key="instit2">Yorktown Heights</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Saon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IBM Research AI</orgName>
								<orgName type="institution" key="instit2">Yorktown Heights</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Audhkhasi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IBM Research AI</orgName>
								<orgName type="institution" key="instit2">Yorktown Heights</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IBM Research AI</orgName>
								<orgName type="institution" key="instit2">Yorktown Heights</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Single headed attention based sequence-to-sequence model for state-of-the-art results on Switchboard</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: encoder-decoder</term>
					<term>attention</term>
					<term>speech recognition</term>
					<term>Switchboard</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It is generally believed that direct sequence-to-sequence (seq2seq) speech recognition models are competitive with hybrid models only when a large amount of data, at least a thousand hours, is available for training. In this paper, we show that state-of-the-art recognition performance can be achieved on the Switchboard-300 database using a single headed attention, LSTM based model. Using a cross-utterance language model, our single-pass speaker independent system reaches 6.4% and 12.5% word error rate (WER) on the Switchboard and CallHome subsets of Hub5'00, without a pronunciation lexicon. While careful regularization and data augmentation are crucial in achieving this level of performance, experiments on Switchboard-2000 show that nothing is more useful than more data. Overall, the combination of various regularizations and a simple but fairly large model results in a new state of the art, 4.7% and 7.8% WER on the Switchboard and CallHome sets, using SWB-2000 without any external data resources.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Powerful neural networks have enabled the use of "end-to-end" speech recognition models that directly map a sequence of acoustic features to a sequence of words without conditional independence assumptions. Typical examples are attention based encoder-decoder <ref type="bibr" target="#b0">[1]</ref> and recurrent neural network transducer models <ref type="bibr" target="#b1">[2]</ref>. Due to training on full sequences, an utterance corresponds to a single observation from the view point of these models; thus, data sparsity is a general challenge for such approaches, and it is believed that these models are effective only when sufficient training data is available. Indeed, many end-toend speech recognition papers focus on LibriSpeech, which has 960 hours of training audio. Nevertheless, the best performing systems follow the traditional hybrid approach <ref type="bibr" target="#b2">[3]</ref>, outperforming attention based encoder-decoder models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>, and when less training data is used, the gap between "end-to-end" and hybrid models is more prominent <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref>. Several methods have been proposed to tackle data sparsity and overfitting problems; a detailed list can be found in Sec. 2. Recently, increasingly complex attention mechanisms have been proposed to improve seq2seq model performance, including stacking self and regular attention layers and using multiple attention heads in the encoder and decoder <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>We show that consistent application of various regularization techniques brings a simple, single-head LSTM attention based encoder-decoder model to state-of-the-art performance on Switchboard-300 (SWB-300), a task where data sparsity is more severe than LibriSpeech. We also note that remarkable performance has been achieved with single-head LSTM models in a recent study on language modeling <ref type="bibr" target="#b9">[10]</ref>. * currently at Google Inc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methods to improve seq2seq models</head><p>In contrast to traditional hybrid models, where even recurrent networks are trained on randomized, aligned chunks of labels and features <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, whole sequence models are more prone to memorizing the training samples. In order to improve generalization, many of the methods we investigate introduce additional noise, either directly or indirectly, to stochastic gradient descent (SGD) training to avoid narrow, local optima. The other techniques we study address the highly non-convex nature of training neural networks, ease the optimization process, and speed up convergence.</p><p>Weight decay adds the l2 norm of the trainable parameters to the loss function, which encourages the weights to stay small unless necessary, and is one of the oldest techniques to improve neural network generalization. As shown in <ref type="bibr" target="#b12">[13]</ref>, weight decay can improve generalization by suppressing some of the effects of static noise on the targets. Dropout randomly deactivates neurons with a predefined probability in every training step <ref type="bibr" target="#b13">[14]</ref> to reduce co-adaptation of neurons. DropConnect, which is similar in spirit to dropout, randomly deactivates connections between neurons by temporarily zeroing out weights <ref type="bibr" target="#b14">[15]</ref>. Zoneout, which is also inspired by dropout and was especially developed for recurrent models <ref type="bibr" target="#b15">[16]</ref>, stochastically forces some hidden units to maintain their previous values. In LSTMs, the method is applied on the cell state or on the recurrent feedback of the output. Label smoothing interpolates the hard label targets with a uniform distribution over targets, and improves generalization in many classification tasks <ref type="bibr" target="#b16">[17]</ref>. Batch normalization (BN) accelerates training by standardizing the distribution of each layer's input <ref type="bibr" target="#b17">[18]</ref>. In order to reduce the normalization mismatch between training and testing, we modify the original approach by freezing the batch normalization layers in the middle of the training when the magnitude of parameter updates is small. After freezing, the running statistics are not updated, batch statistics are ignored, and BN layers approximately operate as global normalization. Scheduled sampling stochastically uses the token produced by a sequence model instead of the true previous token during training to mitigate the effects of exposure bias <ref type="bibr" target="#b18">[19]</ref>. Residual networks address the problem of vanishing and exploding gradients by including skip connections <ref type="bibr" target="#b19">[20]</ref> in the model that force the neural network to learn a residual mapping function using a stack of layers. Optimization of this residual mapping is easier, allowing the use of much deeper structures. Curriculum learning simplifies deep neural network training by presenting training examples in a meaningful order, usually by increasing order of difficulty <ref type="bibr" target="#b20">[21]</ref>. In seq2seq models, the input acoustic sequences are frequently sorted in order of increasing length <ref type="bibr" target="#b21">[22]</ref>.</p><p>Speed and tempo perturbation changes the rate of speech, typically by ?10%, with or without altering the pitch and timbre of the speech signal <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. The goal of these methods is to increase the amount of training data for the model. Sequence noise injection adds structured sequence level noise generated from speech utterances to training examples to improve the generalization of seq2seq models <ref type="bibr" target="#b24">[25]</ref>. As previously shown, input noise during neural network training encourages convergence to a local optimum with lower curvature, which indicates better generalization <ref type="bibr" target="#b25">[26]</ref>. Weight noise adds noise directly to the network parameters to improve generalization <ref type="bibr" target="#b26">[27]</ref>. This form of noise can be interpreted as a simplified form of Bayesian inference that optimizes a minimum description length loss <ref type="bibr" target="#b27">[28]</ref>. SpecAugment masks blocks of frequency channels and blocks of time steps <ref type="bibr" target="#b3">[4]</ref> and also warps the spectrogram along the time axis to perform data augmentation. It is closely related to <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental setup</head><p>This study focuses on Switchboard-300, a standard 300-hour English conversational speech recognition task. Our acoustic and text data preparation follows the Kaldi <ref type="bibr" target="#b29">[30]</ref> s5c recipe, which is based on the transcription release of Mississippi State University [31]. Our attention based seq2seq model is similar to <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b31">33]</ref> and follows the structure of <ref type="bibr" target="#b32">[34]</ref>.</p><p>We extract 80-dimensional log-Mel filterbank features over 25ms frames every 10ms from the input speech signal. The input audio is speed and/or tempo perturbed with 5 /6 probability.</p><p>Following <ref type="bibr" target="#b24">[25]</ref>, sequence noise mixed from up to 4 utterances is injected with 40% probability and 0.3 weight. The filterbank output is mean-and-variance normalized at the speaker level, and first (?) and second (??) derivatives are also calculated. The final features presented to the network are also processed through a SpecAugment block that uses the SM policy <ref type="bibr" target="#b3">[4]</ref> with p = 0.3 and no time warping.</p><p>The encoder network comprises 8 bidirectional LSTM layers with 1536 nodes per direction per layer <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b34">36]</ref>. As shown in <ref type="figure" target="#fig_0">Fig. 1a</ref>, each LSTM block in the encoder includes a residual connection with a linear transformation that bypasses the LSTM, a 1024-dimensional linear reduction layer on the LSTM output, and batch-normalization (BN) of the block output. A pyramidal structure <ref type="bibr" target="#b31">[33]</ref> in the first two LSTM layers reduces the frame rate by a factor of 4. The final dimension of the encoder output is 256, enforced by a linear bottleneck. We apply 30% dropout to the LSTM outputs and 30% drop-connect to the hidden-to-hidden matrices <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b35">37]</ref>. As suggested by <ref type="bibr" target="#b36">[38]</ref>, the weight dropout is fixed for a batch of sequences.</p><p>The attention based decoder model is illustrated in <ref type="figure" target="#fig_0">Fig. 1b</ref>. The decoder models the sequence of 600 BPE units estimated on characters [39], where the BPE units are embedded in 256 dimensions. We use additive, location aware attention, without key/value transformations, and the attention is smoothed by 256, 5-dimensional kernels <ref type="bibr" target="#b37">[40]</ref>. The decoder block consists of 2 unidirectional LSTM layers: one is a dedicated languagemodel-like component with 512 nodes that operates only on the embedded predicted symbol sequence, and the other is a 768 unit layer processing acoustic and symbol information. The output of both LSTMs is reduced to 256 dimensions by a linear bottleneck <ref type="bibr" target="#b38">[41]</ref>. Fixed sequence-level weight dropout of 15% is applied in the decoder LSTMs, a dropout of 5% is applied to the embeddings, and a dropout of 15% is applied to the decoder LSTM outputs. The second LSTM in the decoder also uses zoneout, where the cell state update is deactivated with 15% probability and the recurrent feedback from the output maintains its previous value with 5% probability.</p><p>Overall, the model has 280M parameters, of which only 5.4M are in the decoder. Aiming at the best word error rate, this design choice is based on our observation that an external language model has significantly larger effect if the decoder is not over-parametrized <ref type="bibr" target="#b32">[34]</ref>. The model is trained for 250 epochs on 32 P100 GPUs in less than 4 days using a PyTorch <ref type="bibr" target="#b39">[42]</ref> implementation of distributed synchronous SGD with up to 32 sequences per GPU per batch. Training uses a learning rate of 0.03 and Nesterov momentum <ref type="bibr" target="#b40">[43]</ref> of 0.9. The weight decay parameter is 4e-6, the label smoothing parameter is 0.35, and teacher forcing is fixed to 0.8 throughout training. In the first 3 epochs the learning rate is warmed up and batch size is gradually increased from 8 to 32 <ref type="bibr" target="#b41">[44]</ref>. In the first 35 epochs, the neural network is trained on sequences sorted in ascending order of length of the input. Afterwards, batches are randomized within length buckets, ensuring that a batch always contains sequences with similar length. Weight noise from a normal distribution with mean 0.0 and variance 0.015 is switched on after 70 epochs. After 110 epochs, the updates of sufficient statistics in the batchnormalization layers are turned off, converting them into fixed affine transformations. The learning rate is annealed by 0.9 per epoch after 180 epochs of training, and simultaneously label smoothing is also switched off. The external language model (LM) is built on the BPE segmentation of 24M words from the Switchboard and Fisher corpora (from which the SWB-300 corpus roughly corresponds to 3M words). It is trained for 40 epochs using label smoothing of 0.15 in the first 20 epochs. The baseline LM has 57M parameters and consists of 2 unidirectional LSTM layers with 2048 nodes <ref type="bibr" target="#b42">[45]</ref> trained with drop-connect and dropout probabilities of 15%. The embedding layer has 512 nodes, and the output of the last LSTM is projected to 128 dimensions. When the LM is trained and evaluated across utterances, consecutive segments of a single-channel recording are grouped together up to 40 seconds. Perplexities (PPL) are measured at the word level on the concatenation of ground truth transcripts, while the WER is obtained by retaining the LM state of the single-best hypothesis of the preceding utterance.</p><p>Decoding uses simple beam search with a beam width of 60 hypotheses and no lexical prefix tree constraint <ref type="bibr" target="#b43">[46]</ref>. The search performs shallow fusion of the encoder-decoder score, the external language model score, a length normalization term, and a coverage term <ref type="bibr" target="#b44">[47,</ref><ref type="bibr" target="#b45">48,</ref><ref type="bibr" target="#b46">49]</ref>. For more details, please refer to <ref type="bibr" target="#b32">[34]</ref>. Hub5'00 is used as a development set to optimize decoding hyperparameters, while Hub5'01 and RT03 are used as final test sets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head><p>Our current setup is the result of incremental development. Keeping in mind that several other equally powerful setups probably exist, the focus of the following experiments is to investigate ours around the current optimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Effect of data preparation</head><p>We first investigate the importance of different data processing steps. The s5c Kaldi recipe includes a duplicate filtering step, in which the maximum number of occurrences of utterances with the same content is limited. We measure the impact of duplicate filtering and also the effect of filtering out word fragments and noise tokens from the training transcripts.</p><p>Since the LM is trained on identically filtered transcripts from Fisher+Switchboard data, word fragment and noise token filters were applied consistently. The results are summarized in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Deactivating the duplicate filter is never harmful when an external LM is used, and the gains on CallHome can be substantial. Considering performance on the complete Hub5'00 data, the best systems either explicitly handle both word fragments and noise tokens or filter them all out. When an external LM is used, the best results are obtained when word fragment and noise token filters are activated and the duplicate filter is deactivated. This setting is also appealing in cases where the external LM may be trained on text data that will not contain word fragments or noise; thus, the remaining experiments are carried out with this system setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation study</head><p>In a second set of experiments, we characterize the importance of each of the regularization methods described in Sec. 2 for our model performance by switching off one training method at a time without re-optimizing the remaining settings. In these experiments, decoding is performed without an external language model. Curriculum learning is evaluated by either switching to randomized batches after 35 epochs or leaving the sorting on throughout training. We also test the importance of ? and ?? features <ref type="bibr" target="#b47">[50]</ref>. Sorting the results by decreasing number of absolute errors on Hub5'00, <ref type="table" target="#tab_1">Table 2</ref> indicates that each regularization method contributes to the improved WER. SpecAugment is by far the most important method, while using ? and ?? features or switching off the curriculum learning in the later stage of training have marginal but positive effects. Other direct input level perturbation steps (speed/tempo perturbation and sequence noise injection) are also key techniques that can be found in the upper half of the table. If we compare the worst and baseline models, we find that the relative performance difference between them is nearly unchanged by including the external LM in decoding. Without the LM, the gap is 18% relative, while with the LM the gap is 17% relative. This clearly underlines the importance of the regularization techniques.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Optimizing the language model</head><p>The following experiments summarize our optimization of the LM. Compared to our previous LM <ref type="bibr" target="#b24">[25]</ref>, we measure better perplexity and WER if no bottleneck is used before the softmax layer (rows 1 and 3 in <ref type="table" target="#tab_2">Table 3</ref>). Increasing the model capacity to 122M parameters results in a significant gain in PPL only after the dropout rates are tuned (rows 3, 5 and 6). Similar to <ref type="bibr" target="#b48">[51,</ref><ref type="bibr" target="#b49">52]</ref>, significant PPL gain is observed if the LM was trained across utterances. However, this PPL improvement does not translate into reduced WER with a bigger model when cross utterance modeling is used (rows 4 and 7). Thus, in all other experiments we use the smaller, 57M-parameter model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Effect of beam size and number of parameters</head><p>A 280M-parameter model may be larger than is practical in many applications. Thus, we also conduct experiments to see if this model size is necessary for reasonable ASR performance. Models are trained without changing the training configuration, except that the size or number of LSTM layers is reduced. As <ref type="table" target="#tab_3">Table 4</ref> shows, although our smallest attention based model achieves reasonable results on this task, a significant loss is indeed observed with decreasing model size, especially on Call-Home. Nevertheless, an external language model reduces the performance gap. A small, 57M-parameter model together with a similar size language model is only 5% relative worse than our largest model. We note that this model already outperforms the best published attention based seq2seq model <ref type="bibr" target="#b3">[4]</ref>, with roughly 66% fewer parameters.</p><p>Additional experiments are carried out to characterize the  search and modeling errors in decoding. The results of tuning the beam size and keeping the other search hyperparameters unchanged are shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. "Small" denotes the 57M model, while "large" denotes the 280M model. When greedy search (beam 1) is used, the external language model increases WER, an effect that might be mitigated with re-optimized hyperparameters. Nevertheless, if a beam of at least 2 hypotheses is used, the positive effect of the language model is clear. We also observe that without the language model the search saturates much earlier, around beam 8, fluctuating within only a few absolute errors afterwards. On the contrary, decoding with the language model, we measure consistent but small gains with larger beams. The minimum number of word errors was measured with a relatively large beam of 240. The figure also shows that the effect of a cross-utterance language model grows with larger beams. Lastly, if the model is trained on 2000 hours of speech data (see next section), the extremely fast greedy decoding gives remarkably good performance. Although the importance of beam search decreases with an increased amount of training data, we still measure 10% relative degradation compared to a system with a cross-utterance LM and wide (240) beam search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Experiments on Switchboard-2000</head><p>As a contrast to our best results on Switchboard-300, we also train two seq2seq models on the 2000-hour Switchboard+Fisher data. The first model consists of 10 encoder layers, has 360M parameters, and is trained for only 50 epochs. The second model (lrg) has 660M parameters, 14 encoder and 4 decoder layers, and training runs for 250 epochs. Our overall results on the Hub5'00 and other evaluation sets are summarized in <ref type="table" target="#tab_4">Table 5</ref>.</p><p>The results in <ref type="figure" target="#fig_1">Fig. 2</ref> and <ref type="table" target="#tab_4">Table 5</ref> show that adding more training data and using a fairly large model greatly improves the system, over 30% relative in some cases. For comparison with others, our best 2000-hour system reaches 7.1% and 6.0% WER on rt02 and rt04. It is also worth to note that human performance is at 6.0%, 4.5%, 4.7% WER on the rt0{2,3,4} sets, measured as in <ref type="bibr" target="#b50">[53]</ref>. We observe that the regularization techniques, which are extremely important on the 300h setup, are also beneficial to train much larger models using big data. Considering the recognition speed and applicability of the 2k-lrg model without using external language model, we measure 0.73 -0.77 real-time factor and 6.5 -6.4% total WER on Hub5'00 after varying the beam between 4 and 16, using a single core of an Intel Xeon Platinum 8280 processor and 8-bit integer weight quantization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Comparison with the literature</head><p>For comparison with results in the literature we refer to the Switchboard-300 results in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b51">54,</ref><ref type="bibr" target="#b52">55]</ref> and the Switchboard-2000 results in <ref type="bibr" target="#b49">[52,</ref><ref type="bibr" target="#b51">54,</ref><ref type="bibr" target="#b50">53,</ref><ref type="bibr" target="#b53">56,</ref><ref type="bibr" target="#b54">57,</ref><ref type="bibr" target="#b55">58,</ref><ref type="bibr" target="#b56">59]</ref>. Our 300-hour model not only outperforms the previous best attention based encoder-decoder model <ref type="bibr" target="#b3">[4]</ref> by a large margin, it also surpasses the best hybrid systems with multiple LMs <ref type="bibr" target="#b7">[8]</ref>. Our single system result on Switchboard-2000 is also better than the best system combination results reported to date.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We presented an attention based encoder-decoder setup which achieves state-of-the-art performance on both Switchboard 300 and 2000. A rather simple model built from LSTM layers and a decoder with a single-headed attention mechanism outperforms the standard hybrid approach. This is particularly remarkable given that in our model neither a pronunciation lexicon nor a speech model with explicit hidden state representations is needed. We also demonstrated that excellent results are possible with smaller models and with practically search-free, greedy decoding. The best results were achieved with a speaker independent model in a single decoding pass, using a minimalistic search algorithm, and without any attention mechanism in the language model. Thus, we believe that further improvements are still possible if we apply a more complicated sequence-level training criterion and speaker adaptation. As a further possible extension of this study, the training of conventional ASR models should also be revisited for fairer comparison of different modeling approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) Building block of the encoder; (b) attention based decoder network used in the experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Effect of beam size on word error rate (WER) measured on the CallHome (chm) part of Hub5'2000. "300h" indicates models trained on SWB-300, whereas "2k" corresponds to the 2000-hour Switchboard+Fisher training setup.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Effect of data preparation steps on WER [%] measured on Hub5'00, models are trained on SWB-300. The second row corresponds to the Kaldi s5c recipe.</figDesc><table><row><cell>filter</cell><cell cols="2">w/o LM</cell><cell cols="2">w/ LM</cell></row><row><cell cols="5">frag. noise dup. swb chm swb chm</cell></row><row><cell></cell><cell>7.5</cell><cell>14.3</cell><cell>6.7</cell><cell>12.6</cell></row><row><cell></cell><cell>7.8</cell><cell>14.8</cell><cell>6.5</cell><cell>13.2</cell></row><row><cell></cell><cell>7.6</cell><cell>15.1</cell><cell>6.5</cell><cell>13.1</cell></row><row><cell></cell><cell>7.5</cell><cell>15.1</cell><cell>6.6</cell><cell>13.3</cell></row><row><cell></cell><cell>7.6</cell><cell>14.6</cell><cell>6.4</cell><cell>12.7</cell></row><row><cell></cell><cell>7.7</cell><cell>14.7</cell><cell>6.4</cell><cell>13.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on the final training recipe, models are trained on SWB-300. WER is measured without using external LM.</figDesc><table><row><cell>WER [%]</cell><cell>#err.</cell></row><row><cell cols="2">swb chm total [word]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Optimizing</figDesc><table><row><cell></cell><cell cols="5">dropout (dropo.), DropConnect (dropc.),</cell></row><row><cell cols="6">layer and bottleneck (bn) size for LSTM LM, optionally modeling</cell></row><row><cell cols="6">across utterances (x-utt.). WER is measured in shallow fusion</cell></row><row><cell cols="5">with the best SWB-300 seq2seq ASR model.</cell><cell></cell></row><row><cell cols="2">model</cell><cell></cell><cell cols="2">PPLword</cell><cell>WER</cell></row><row><cell cols="6">dropo. dropc. width bn x-utt. CV Hub5'00 swb chm</cell></row><row><cell>15% 15%</cell><cell>2048</cell><cell>128</cell><cell>56.7 46.3 52.9</cell><cell>65.7 52.7 61.2</cell><cell>6.7 13.2 6.5 13.1 6.6 13.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>44.1</cell><cell>50.1</cell><cell>6.4 12.7</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell>53.9</cell><cell>64.0</cell><cell>6.7 13.2</cell></row><row><cell>30% 30%</cell><cell>3072</cell><cell></cell><cell>50.3 41.4</cell><cell>58.3 47.0</cell><cell>6.4 13.1 6.3 12.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Effect of model size, models are trained on SWB-300.</figDesc><table><row><cell></cell><cell></cell><cell>enc.</cell><cell>dec.</cell><cell>WER</cell></row><row><cell cols="4">depth LSTM lin. LSTM</cell><cell>#par. [M] swb chm swb chm w/o LM w/ LM</cell></row><row><cell></cell><cell>6</cell><cell cols="3">0512 0384 512 028.5 9.4 17.0 7.4 14.6 057.3 8.3 15.5 6.6 13.4 0768 0512 075.1 8.4 15.1 6.8 13.4</cell></row><row><cell></cell><cell>8</cell><cell>1024 0640 1280 0896</cell><cell>768</cell><cell>125.0 7.6 15.2 6.5 13.3 201.6 7.5 15.0 6.4 12.9</cell></row><row><cell></cell><cell></cell><cell>1536 1024</cell><cell></cell><cell>280.1 7.6 14.6 6.4 12.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>chm</cell></row><row><cell></cell><cell>20</cell><cell cols="3">300h, large, w/o LM 300h, large, w/ LM</cell><cell>300h, small, w/ xutt. LM 2k, w/o LM</cell></row><row><cell></cell><cell></cell><cell cols="3">300h, large, w/ xutt. LM</cell><cell>2k, w/ LM</cell></row><row><cell></cell><cell>18</cell><cell cols="3">300h, small, w/o LM</cell><cell>2k, w/ xutt. LM</cell></row><row><cell>WER [%]</cell><cell>14 16</cell><cell></cell><cell></cell></row><row><cell></cell><cell>12</cell><cell></cell><cell></cell></row><row><cell></cell><cell>10</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>10 0</cell><cell></cell><cell>10 1</cell><cell>10 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>beam</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Detailed results with the best performing systems on both SWB-300 and SWB-2000.</figDesc><table><row><cell>SWB</cell><cell>ext. LM</cell><cell>hub5'00 xutt. swb chm swb</cell><cell>hub5'01 swb2 swb2 swb fsh rt03 p3 p4</cell></row><row><cell></cell><cell></cell><cell cols="2">7.6 14.6 8.1 11.0 15.7 17.8 10.5</cell></row><row><cell>300</cell><cell></cell><cell cols="2">6.5 13.0 7.0 09.3 13.8 15.0 08.8</cell></row><row><cell></cell><cell></cell><cell cols="2">6.4 12.5 6.8 09.1 13.4 14.8 08.4</cell></row><row><cell></cell><cell></cell><cell cols="2">5.9 10.2 6.8 08.7 11.7 10.4 07.2</cell></row><row><cell>2k</cell><cell></cell><cell cols="2">5.5 09.8 6.6 08.3 11.5 09.8 06.7</cell></row><row><cell></cell><cell></cell><cell cols="2">5.6 09.5 6.5 08.2 11.4 09.8 06.6</cell></row><row><cell>2k-lrg</cell><cell></cell><cell cols="2">4.8 08.0 5.5 06.7 10.3 08.5 07.0 4.7 07.8 5.2 06.3 09.7 08.2 06.4</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sequence transduction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Representation Learning Workshop, ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Bourlard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
		<title level="m">Connectionist Speech Recognition: A Hybrid Approach</title>
		<meeting><address><addrLine>Norwell, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">SpecAugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2613" to="2617" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A comparative study on Transformer vs RNN in speech applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ASRU</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">RWTH ASR systems for LibriSpeech: Hybrid vs attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>L?scher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="231" to="235" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Transformer-based acoustic modeling for hybrid speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1910.09799" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Training language models for long-span crosssentence evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASRU</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Single headed attention RNN: Stop thinking with your head</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1911.11423" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Unfolded recurrent neural networks for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="343" to="347" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep bi-directional recurrent networks over spectral windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-R</forename><surname>Mohamed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="78" to="83" />
		</imprint>
		<respStmt>
			<orgName>ASRU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A simple weight decay can improve generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Hertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="950" to="957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1207.0580" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using DropConnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Zoneout: regularizing RNNs by randomly preserving hidden activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-to-end speech recognition in English and Mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Elastic spectral distortion for low resource speech recognition with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Obuchi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ASRU</publisher>
			<biblScope unit="page" from="309" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Audio augmentation for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3586" to="3589" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sequence noise injected training for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="6261" to="6265" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Training with noise is equivalent to Tikhonov regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="108" to="116" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Enhanced MLP performance and fault tolerance resulting from synaptic weight noise during training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="792" to="802" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Practical variational inference for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1708.04896" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<ptr target="https://www.isip.piconepress.com/projects/switchboard" />
	</analytic>
	<monogr>
		<title level="j">ASRU</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">End-to-end attention-based large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="4945" to="4949" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: a neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="4960" to="4964" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Advancing sequence-tosequence based speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>T?ske</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3780" to="3784" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1019" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Chorowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Convolutive bottleneck network features for LVCSR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vesel?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafi?t</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gr?zl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="42" to="47" />
		</imprint>
		<respStmt>
			<orgName>ASRU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A method of solving a convex programming problem with convergence rate O(1/k 2 )</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Soviet Mathematics Doklady</title>
		<imprint>
			<date type="published" when="1983" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="372" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Accurate, large minibatch SGD: Training imagenet in 1 hour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1706.02677" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">LSTM neural networks for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schl?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="194" to="197" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Improvements in beam search for 10000-word continuous speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="9" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">On using monolingual corpora in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>G?l?ehre</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1503.03535" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Deep Speech: Scaling up end-toend speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.5567" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Modeling coverage for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="76" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Comparison of speaker recognition methods using statistical features and dynamic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="342" to="350" />
			<date type="published" when="1981-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Investigation on LSTM recurrent n-gram language models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>T?ske</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schl?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3358" to="3362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The Microsoft 2017 conversational speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICASSP</title>
		<imprint>
			<biblScope unit="page" from="5934" to="5938" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">English conversational telephone speech recognition by humans and machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="132" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">End-to-end speech recognition using lattice-free MMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hadian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in Interspeech</title>
		<imprint>
			<biblScope unit="page" from="12" to="16" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Forget a bit to learn better : Soft forgetting for CTC-based automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2618" to="2622" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Exploring neural transducers for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="206" to="213" />
		</imprint>
		<respStmt>
			<orgName>ASRU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Language modeling with highway LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kurata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="244" to="251" />
		</imprint>
		<respStmt>
			<orgName>ASRU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Improving sequence-to-sequence speech recognition training with on-the-fly data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Nguyen</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1910.13296" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">The CAPIO 2017 conversational speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Han</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1801.00059" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
