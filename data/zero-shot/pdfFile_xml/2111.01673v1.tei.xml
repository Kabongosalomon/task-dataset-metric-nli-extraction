<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Relational Self-Attention: What&apos;s Missing in Attention for Video Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjin</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeseung</forename><surname>Kwon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Relational Self-Attention: What&apos;s Missing in Attention for Video Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolution has been arguably the most important feature transform for modern neural networks, leading to the advance of deep learning. Recent emergence of Transformer networks, which replace convolution layers with self-attention blocks, has revealed the limitation of stationary convolution kernels and opened the door to the era of dynamic feature transforms. The existing dynamic transforms, including self-attention, however, are all limited for video understanding where correspondence relations in space and time, i.e., motion information, are crucial for effective representation. In this work, we introduce a relational feature transform, dubbed the relational self-attention (RSA), that leverages rich structures of spatio-temporal relations in videos by dynamically generating relational kernels and aggregating relational contexts. Our experiments and ablation studies show that the RSA network substantially outperforms convolution and self-attention counterparts, achieving the state of the art on the standard motion-centric benchmarks for video action recognition, such as Something-Something-V1&amp;V2, Diving48, and FineGym.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convolution <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26]</ref> is a feature transform that is ubiquitous in modern neural networks for visual recognition, and has driven the development of deep learning in the past decade. The stationarity of convolution kernels, however, may limit its expressivity and hinder adaptation to diverse compositional possibilities of visual concepts <ref type="bibr" target="#b17">[18]</ref>. Dynamic convolution <ref type="bibr" target="#b20">[21]</ref> and self-attention <ref type="bibr" target="#b54">[55]</ref> that construct kernels or attentions according to input contents have emerged as an alternative to static convolution in this context, being followed by further studies for dynamic feature transforms <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b59">60]</ref>. The effectiveness of self-attention has been demonstrated by the success of Transformer variants on different image understanding tasks such as image classification <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b61">62]</ref>, object detection <ref type="bibr" target="#b3">[4]</ref>, and semantic segmentation <ref type="bibr" target="#b44">[45]</ref>. Recently, it has been further extended for video understanding, replacing spatio-temporal convolution <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b52">53]</ref> with spatio-temporal selfattention <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>Despite their recent progress in the video domain, the existing dynamic feature transforms still leave much room for improvement in terms of learning relational patterns in space and time, i.e., motion information, which is known to be essential for video understanding <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b55">56]</ref>. For example, spatiotemporal self-attention <ref type="bibr" target="#b57">[58]</ref> often fails to learn motion representation without positional embeddings as demonstrated in <ref type="bibr" target="#b32">[33]</ref>, and even those with positional embeddings <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b36">37]</ref> turn out to be not effective on motion-centric action recognition benchmarks such as Something-Something <ref type="bibr" target="#b13">[14]</ref>.</p><p>In this work, we introduce a relational dynamic feature transform, dubbed relational self-attention (RSA), to address the limitation of existing methods. RSA leverages rich structures of spatio-temporal relations in videos by dynamically generating relational kernels and aggregating relational contexts. Combining these relational components with ordinary dynamic kernels and context features, our RSA learns a rich video representation that effectively captures both visual appearance and spatio-temporal motion dynamics.</p><p>The main contribution of this paper is three-fold:</p><p>? We re-interpret recent dynamic feature transforms in a unified way, and provide in-depth analysis on their capability of learning video representation.</p><p>? We introduce a new dynamic feature transform, i.e., RSA, which effectively captures both visual appearance and spatio-temporal motion dynamics for video understanding.</p><p>? Our RSA network substantially outperforms convolution and self-attention counterparts, achieving the state of the art on SS-V1&amp;V2, Diving48, and FineGym.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Convolution and its variants. Convolution <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26]</ref> has been used as a dominant neural primitive in modern neural architectures <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b49">50]</ref>. Since convolution often suffers from its limited expressivity due to its static kernel, dynamic convolution operators have been studied for enhancing composability by dynamically adjusting the convolution kernel according to input features <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b59">60]</ref>. One example is involution <ref type="bibr" target="#b27">[28]</ref>, which generates a lightweight dynamic kernel using input content and substantially outperforms convolution with less computational cost on image classification. Our RSA differs from these kernel generation methods in that it leverages relational patterns for learning a rich video representation.</p><p>Self-attention and its variants. Self-attention <ref type="bibr" target="#b54">[55]</ref> was originally introduced for neural machine translation to capture long-range interactions, and now has been widely adopted in many different domains thanks to its versatility and expandability. Following local attention methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b38">39]</ref> that employ self-attention as an alternative to convolution, ViT models have demonstrated impressive performance on a variety of image understanding tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b61">62]</ref>. On the other hand, some of previous work have attempted to design novel dynamic transforms through attention <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b62">63]</ref>. Zhao et al. <ref type="bibr" target="#b62">[63]</ref> expand the spatial attention to include the channel-wise attention. Bello <ref type="bibr" target="#b1">[2]</ref> proposes a lambda layer that focuses on interaction between visual contents and a relative position embedding without softmax, which outperforms self-attention counterparts on image classification. The proposed RSA is an extension of these techniques, yet focuses on learning rich relational features for video understanding.</p><p>Convolution and self-attention for video understanding. Action recognition is the most fundamental task for video understanding, which aims at classifying a video clip into pre-defined action classes. A key to success of this task is to capture temporal dynamics across multiple video frames, and spatio-temporal (3D) convolution <ref type="bibr" target="#b50">[51]</ref> has been a de facto for modeling the temporal dynamics <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref>. On the other hand, self-attention was used as an attachable module for 3D CNNs in the earliest method <ref type="bibr" target="#b57">[58]</ref>, yet nowadays becomes the major building block of video understanding networks such as video ViT <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b36">37]</ref>. However, both 3D CNNs and ViTs are not sufficient for modeling temporal dynamics due to the lack of inter-feature relations within the spatio-temporal context. Our RSA overcomes their limitation by explicitly utilizing relational features that imply temporal dynamics.</p><p>Learning motion for video understanding. Early approaches in video understanding rely on external motion extractors like optical flow for motion feature learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b42">43]</ref>, but recent models aim to learn motion representation internally. To this end, they estimate feature-level optical flow inside their networks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b37">38]</ref> or compute subtraction between consecutive frame-wise features <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b45">46]</ref>. Meanwhile, correlation-based methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b55">56]</ref> use inter-pixel relations as motion likelihood maps and achieve state-of-the-art on motion-centric action recognition benchmarks. Inspired by the correlation-based methods, we apply dynamic relational kernels to the correlation tensors (i.e., relational context), leading to effective motion feature learning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head><p>In this section, we revisit three instances of feature transform and analyze them in a simplified form. In our context, the role of feature transform is to update a feature map while preserving its order in space and/or time. To this end, it processes each position in space and/or time using features of its neighborhood. For example, given an input feature map of a video X ? R T ?H?W ?C , for each spatio-temporal position (t, h, w), a feature transform takes the individual feature x t,h,w as a target and transforms it using its neighborhood features {x t ,h ,w } (t ,h ,w )?N (t,h,w) as its context. For the sake of notational convenience, let us denote the target by x n ? R C , where n represents a specific position in space and/or time, and its context by X n ? R M ?C , where M is the size of neighborhood, and the corresponding output by y n ? R C . Now, without loss of generality, we can view the transform as applying to each position n a function f with learnable weights W that maps target x n to output y n using context X n : y n = f (x n , X n ; W ). Note that such a transform is typically implemented as a neural layer that processes all the targets in parallel.</p><p>In the following, we describe recent feature transform functions in computer vision as well as the conventional one, convolution. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the computational graphs of the transforms.</p><p>Convolution. Convolution <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26]</ref> is a static and translation-equivariant feature transform that updates each target x n by applying static kernels W ? R M C?C on local context X n :</p><formula xml:id="formula_0">y n = W T vec(X n ).<label>(1)</label></formula><p>While extracting different visual patterns using multiple kernels, convolution remains static in the sense that the kernel is not affected by the target x n , i.e., feature of interest ( <ref type="figure" target="#fig_0">Fig. 1a</ref>). This stationarity may not be effective in adapting to diverse compositional possibilities of visual concepts <ref type="bibr" target="#b17">[18]</ref>, and the channel-dependent weights can cause inter-channel redundancy in the kernel <ref type="bibr" target="#b19">[20]</ref>.</p><p>Self-attention. Self-attention <ref type="bibr" target="#b54">[55]</ref> is a dynamic transform that generates an attention map of context X n using target x n and then aggregates the context using the attention map as a dynamic kernel <ref type="figure" target="#fig_0">(Fig. 1b</ref>). The process starts by adapting inputs for query-key-value attention computation; using three learnable embedding matrices, E Q , E K , E V ? R C?C , it embeds the target x n into the query x Q n via x Q n = x n E Q and also projects the context X n into the key X K n and the value X V n via X K n = X n E K and X V n = X n E V , respectively. The basic attention map is computed via the softmaxed product between query x Q n and key X K n , which can be viewed as content-to-content interaction. To reinforce this content-only attention with positional information of the context, a learnable matrix P ? R M ?C , which encodes relative positions of individual features in the context, is optionally included as content-to-position interaction. The self-attention transform aggregates the value embeddings X V n of context using the attention map as kernel weights shared for all channels:</p><formula xml:id="formula_1">y n = ?(x Q n (X K n ) T + x Q n P T )X V n ,<label>(2)</label></formula><p>where ?(?) denote the softmax function. This transform is translation-equivariant if position embedding P is local and relative with respect to the target, and becomes permutation-equivariant if content-to-position interaction is removed. Unlike convolution, it is dynamic in the sense that the kernel ?(x Q n (X K n ) T + x Q n P T ), which is used to aggregate the context, depends on the target content.</p><p>It allows more flexibility in adapting to input and also consumes fewer parameters by sharing the kernel across the channels <ref type="bibr" target="#b17">[18]</ref>.</p><p>Involution &amp; lambda convolution. Involution <ref type="bibr" target="#b27">[28]</ref> is a light-weight dynamic transform that leverages content-to-position interaction only. It dynamically generates a kernel ? V n ? R M by projecting target x n using a learnable matrix P ? R M ?C via ? V n = x n P T , and then use the kernel to aggregate the context X n <ref type="figure" target="#fig_0">(Fig. 1c, left)</ref>:</p><formula xml:id="formula_2">y n = ? V n X n = x n P T X n ,<label>(3)</label></formula><p>where the matrix P plays the role of converting the target to a kernel in a position-sensitive manner. In a similar spirit, lambda convolution 2 of the lambda networks <ref type="bibr" target="#b1">[2]</ref> uses content-to-position interaction between the target and the context position. As in self-attention, using two learnable embedding matrices, E Q , E V ? R C?C , it starts by projecting the target x n and the context X n into the query x Q n and the value X Q n via x Q n = x n E Q and X V n = X n E V , respectively. The lambda convolution abstracts the context value X V n to a contextual matrix ? p n using a learnable matrix P ? R M ?C via ? p n = P T X V n ? R C?C , which in turn is used for updating the target query x Q n ( <ref type="figure" target="#fig_0">Fig. 1c</ref>, right):</p><formula xml:id="formula_3">y n = x Q n ? p n = x Q n P T X V n .<label>(4)</label></formula><p>Despite the differences in the operational procedures and the concepts, the lambda convolution has effectively the same form with involution except for additional key and value embeddings. Note that unlike self-attention, involution and lambda convolution both have no softmax nonlinearity. In fact, the absence of softmax reduces the computational cost and also increases the expressive ability by allowing negative activations <ref type="bibr" target="#b27">[28]</ref>. Both involution and lambda convolution are shown to outperform convolution and self-attention counterparts in image classification <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">28]</ref>, demonstrating the effectiveness of dynamic content-aware kernels.</p><p>Limitation of existing dynamic transforms. The aforementioned dynamic transforms are commonly based on leveraging content-to-content and/or content-to-position interactions in constructing kernels, where the target content (input feature) is used as the source of the dynamic transform. While the methods are effective for learning image representation indeed, they are all limited for learning video representation; as will be shown in the experimental section 5.3, we have found that existing dynamic transforms show only marginal or virtually no improvement over the static transform of convolution on the motion-centric action recognition benchmark. The main reason lies in the missing structure of content-to-content interactions as a relational content in representation learning. While content-to-content interactions are considered in existing dynamic transforms, they are immediately broken into individuals without being used as a whole. For example, self-attention computes query-to-key correlation x Q n (X K n ) T but uses the individual elements of the correlation only for aggregating informative contents from the context. The content-to-position interaction x Q n P T does not help in capturing the structure of interactions either since it has no access to the correlation as a whole. In videos, such structural patterns contain informative spatio-temporal contents, i.e., different levels of generic motion information, thus being crucial in video understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Our approach</head><p>In this section, we introduce a new dynamic transform, dubbed relational self-attention (RSA), which is designed to learn rich spatio-temporal interaction patterns across input contents. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the computational graph of RSA. On top of the basic kernel and context in a dynamic transform, it builds relational kernel and context and processes all the kernel-context combinations.</p><p>Here we describe the main components of RSA and their integrated form, and explain an efficient implementation of RSA.</p><p>As in self-attention, we start by adapting inputs for query-key-value interactions; using three learnable embedding matrices, E Q , E K , E V ? R C?C , target x n and context X n are embedded into query x Q n , key X K n and value X V n via x Q n = x n E Q , X K n = X n E K , and X V n = X n E V , respectively. x Q n , X K n , and X V n are then L2-normalized, and we omit the normalization term for the simplicity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Relational self-attention (RSA)</head><p>Relational kernel. The relational kernel is designed to predict the relevance of context based on the structure of content-to-content interactions. To generate the relational kernel, we compute the dotproduct correlation of the query and the key, i.e., x Q n (X K n ) T ? R M , and then project the correlation vector using a learnable matrix H ? R M ?M :</p><formula xml:id="formula_4">? R n = x Q n (X K n ) T H.<label>(5)</label></formula><p>The role of H in this relational kernel corresponds to that of P in the involution kernel (Eq. 3); while P predicts the kernel weights from the C-dimensional query vector, H predicts them from the M -dimensional query-key correlation vector. The resultant dynamic kernel aggregates the context depending on how the individual contents are related to each other in space and time, thus being particularly effective in learning motion-related patterns in videos. Note that, when we set H to an identity matrix I and add the softmax operator into Eq. 5, the relational kernel ? R n is equivalent to the self-attention kernel without content-to-position interaction, i.e., ?(x Q n (X K n ) T ). Since the dot-product correlation contracts all channel dimensions of the query and the keys, we may lose semantic information, which may help in generating an effective relational kernel. We thus take the Hadamard product <ref type="bibr" target="#b22">[23]</ref> instead so that we can leverage channel-wise query-key correlations for producing the relational kernel. Using a learnable kernel projection matrix H ? R M C?M , Eq. 5 can be reformulated as</p><formula xml:id="formula_5">? R n = vec(1(x Q n ) T X K n )H,<label>(6)</label></formula><p>where denotes the Hadamard product. H ? R M C?M predicts the kernel weights from an M Cdimensional channel-wise query-key correlation vector. Furthermore, the absence of the softmax operator allows the relational kernel to have negative activations so that it better learns relative features where the subtractive interactions are beneficial for capturing relative changes of contents over time <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>Relational context. The relational context is designed to provide the relational pattern of contentto-content (i.e., context-to-context) interactions for the kernels to aggregate. To this end, we use self-correlation <ref type="bibr" target="#b41">[42]</ref>, which effectively describes spatio-temporal intra-structure including motion information <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b55">56]</ref>. We first construct the self-correlation matrix X V n (X V n ) T ? R M ?M and then project it using a learnable matrix G ? R M ?C into the relational context X R n :</p><formula xml:id="formula_6">X R n = X V n (X V n ) T G,<label>(7)</label></formula><p>where the self-correlation X V n (X V n ) T reveals content-to-content interaction patterns within the context, and the matrix G maps it to the relational context so that the output has the same size with the basic context X V n . Combining of different types of kernels and contexts. The proposed transform, RSA, integrates the relational kernel and context into a dynamic transform. As illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, it consists of two types of kernels, ? V n and ? R n , and two types of contexts, X V n and X R n . Note that the basic kernel ? V n is computed as x Q n P T . We combine the kernels and the contexts in the RSA transform: </p><formula xml:id="formula_7">y n = (? V n + ? R n )(X V n + X R n ) = ? V n X V n + ? R n X V n + ? V n X R n + ? R n X R n ,<label>(8)</label></formula><formula xml:id="formula_8">RSA O(BN M 2 C) O(BN M (M + C) + M 2 C) + Efficient RSA O(BN M C 2 ) O(BN C 2 + M C 2 ) + Multi-query (L) O(BN M C 2 /L 2 ) O(BN C 2 /L 2 + BN C + M C 2 /L 2 ) which contains four different dynamic transforms, ? V n X V n , ? R n X V n , ? V n X R n , and ? R n X R n . The first sub-transform, ? V n X V n corresponds to the involution (Eq. 3)</formula><p>. The second and third sub-transform, ? R n X V n and ? V n X R n , both capture content-to-relation interactions, but their effects are different; while ? R n X V n aggregates the basic context with considering the content-to-content interactions, ? V n X R n aggregates the relational patterns with the content-to-position interactions. The last sub-transform, ? R n X R n , is fully dependent on the relational information. The relational kernel dynamically aggregates relational patterns, generating the deeper relational interactions. These sub-transforms altogether encode rich relational interactions, leading to comprehensive video representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Improving efficiency of RSA</head><p>Efficient relational kernel. The projection matrix H ? R M C?M of the relational kernel increases the cost of computation and memory quadratically with the context size M , causing a computational bottleneck. We reduce the complexity to the linear one by decomposing H to</p><formula xml:id="formula_9">H 1 H T 2 such that H 1 ? R M C?D and H 2 ? R M ?D ,</formula><p>where D is a latent channel dimension, which is smaller than M . Furthermore, we dramatically reduce the memory footprint by switching the computation orders.</p><formula xml:id="formula_10">When r(H 1 ) ? R M ?C?D is the reshaped H 1 ? R M C?D , the kernel equation is re-formulated as ? R n = vec(1(x Q n ) T X K n )H 1 H T 2 (9) = x Q n (X K n r(H 1 ))H T 2 , where (X K n r(H 1 )) c,d = m (X K n ) m,c (r(H 1 )) m,c,d .<label>(10)</label></formula><p>Note that X K n r(H 1 ) can be efficiently implemented by a channel-wise convolution. Our experiments show that this modification achieves a good computation-complexity trade-off <ref type="table" target="#tab_5">(Table 4b)</ref>. Please refer to the pseudo code <ref type="figure" target="#fig_5">Fig. 4</ref> in Appendix for more details.</p><p>Efficient RSA. The vanilla RSA may require a high cost of computation and memory when generating contexts and aggregating the kernels with the contexts. To reduce them, we decompose P to H 2 P 1 such that H 2 ? R M ?D , P 1 ? R D?C , where H 2 is the same one obtained by decomposing H. The RSA equation is then re-formulated as</p><formula xml:id="formula_11">y n = (? V n + ? R n )(X V n + X R n ) = (x Q n P T + x Q n (X K n r(H 1 ))H T 2 )(X V n + X V n (X V n ) T G) (11) = x Q n (P T + (X K n r(H 1 ))H T 2 )X V n (I + (X V n ) T G) = x Q n (P T 1 + X K n r(H 1 ))(H T 2 X V n )(I + (X V n ) T G) where P = H 2 P 1 .<label>(12)</label></formula><p>Note that X K n r(H 1 ), H T 2 X V n , and (X V n ) T G can be efficiently implemented with convolutions. As shown in <ref type="table" target="#tab_0">Table 1</ref>, the space and time complexities are respectively reduced to O(BN M C 2 ) and O(BN C 2 + M C 2 ), which are both linear to M . Note that the space complexity is proportional to N and M separately, so that it facilitates efficient training with sufficiently a large volume of context size M , e.g., 5 ? 7 ? 7. Please refer to the pseudo code <ref type="figure">Fig. 5</ref> in Appendix for more details.</p><p>Multi-query RSA. We adopt the multi-query setting <ref type="bibr" target="#b1">[2]</ref> for RSA, which sets L number of multiple queries and applies the same key and value context to each query, where the size of channel dimension of each query C Q becomes C/L. While the multi-head setting <ref type="bibr" target="#b54">[55]</ref> maintains the time complexity and increases the space complexity, multi-query setting significantly reduces both of them <ref type="table" target="#tab_0">(Table 1</ref>). Self-attention <ref type="bibr" target="#b38">[39]</ref> ?</p><formula xml:id="formula_12">(x Q n (X K n ) T + x Q n P T )) X V n 32.2 G 23.4 M 41.6</formula><p>70.9 Self-attention variant <ref type="bibr" target="#b38">[39]</ref> ? Architecture details. We use TSN-ResNet50 <ref type="bibr" target="#b56">[57]</ref> as our backbone and replace the standard spatial convolution layers by spatio-temporal RSA layers for every two ResNet bottlenecks <ref type="bibr" target="#b16">[17]</ref>. Unless specified otherwise, we replace 7 RSA layers, there are 7 RSA layers in total where L = 8, D = C Q , M =5?7?7. We set the input and output channel dimensions of RSA layers to be equal to those of spatial convolution layers in TSN-ResNet50.</p><formula xml:id="formula_13">(x Q n (X K n ) T ) X V<label>n</label></formula><p>Training &amp; testing details. For initialization, we randomly initialize the weights of bottlenecks including RSA layers with the MSRA method <ref type="bibr" target="#b15">[16]</ref> and use ImageNet pre-trained weights for all the other layers. We set the gamma parameter of the last batch normalization layer to zero. For training, we sample 8 or 16 frames from each video using the segment-based sampling strategy <ref type="bibr" target="#b56">[57]</ref>. For testing, we sample one or two clips consisting of 8 or 16 frames using the segment-based sampling, and average softmax scores for final prediction. Refer to Sec.1 in our Supp. for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Datasets</head><p>Something-something v1 &amp; v2 (SS-V1 &amp; V2) <ref type="bibr" target="#b13">[14]</ref> are both large-scale action recognition benchmarks, including 108k and 220k action clips, respectively. Both datasets share the same motioncentric action classes, e.g., 'pushing something from left to right,' so thus capturing fine-grained motion is crucial to achieve the better performance.</p><p>Diving-48 <ref type="bibr" target="#b30">[31]</ref> is a fine-grained action benchmark that is heavily dependent on temporal modeling <ref type="bibr" target="#b2">[3]</ref>, containing 18k videos with 48 diving classes. Due to the incorrect label issue, we only compare our result with the results on the modified version of Diving-48.</p><p>FineGym <ref type="bibr" target="#b40">[41]</ref> is a motion-centric benchmark that includes gymnastics action classes. We report results on two subsets of Gym288 and Gym99 that contain 288 and 99 action classes, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison with other transform methods.</head><p>In this experiment, we evaluate the temporal modeling capability of different transform methods: spatio-temporal convolutions <ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref><ref type="bibr" target="#b52">[53]</ref>, self-attention <ref type="bibr" target="#b38">[39]</ref> with its variants, and RSA. We replace a single 3 ? 3 spatial convolution layer in TSN-ResNet50 <ref type="bibr" target="#b56">[57]</ref> with a single spatio-temporal transform layer. We analyze the ability of modeling temporal dependency of each transform layer with an apple-to-apple comparison. We use 8 frames as the input, and the kernel size M of all spatio-temporal transforms is set to 5?7?7. <ref type="table" target="#tab_1">Table 2</ref> summarizes the results. 2D convolution baseline without modeling temporal dependency shows the lowest accuracy of 19.7%. While 3D <ref type="bibr" target="#b50">[51]</ref> and (2+1)D <ref type="bibr" target="#b52">[53]</ref> convolutions, which use static spatio-temporal transforms, show the top-1 accuracy of 43.3% and 44.1%, respectively, self-attention only achieves 41.6%. To find out the reasons behind the bad result, we ablate each component of selfattention, e.g., content-to-content interaction (x Q n (X K n ) T ), content-to-position interaction (x Q n P T ), and softmax (?), one by one. We observe that the performance of self-attention without x Q n P T significantly decreases, while that of self-attention without x Q n (X K n ) T is comparable to the original one. It indicates that the temporal modeling capability of the self-attention is actually dependent on the content-to-position interaction rather than the content-to-content interaction; the self-attention mechanism itself is permutation-invariant, so thus it is hard to learn position-specific features, e.g., motion <ref type="bibr" target="#b32">[33]</ref> without the positional embedding. After we remove the softmax non-linearity, where the kernel is equivalent to the basic kernel (? V n = x Q n P T ), outperforms both the self-attention and the standard static transforms. It demonstrates the softmax function restricts the expressive ability of the kernel <ref type="bibr" target="#b27">[28]</ref>; the softmax forces the kernel weights to be positive, so that the kernel may not compute gradients across frames, which are effective in learning motion. Please refer to the <ref type="figure" target="#fig_3">Fig. 3</ref> and While the composability of the basic kernel depends on the query content, our relational kernel depends on the local pattern of query-key correlation. As we add the relational kernel ? R n to ? V n , we improve the top-1 accuracy by 1.4%p. The result demonstrates that leveraging local query-key interactions are effective in aggregating informative context for learning motion. At last, by adding relational context X R n to basic context X V n , RSA achieves the best top-1 accuracy of 47.0%.  <ref type="table" target="#tab_3">Table 3c</ref> and <ref type="table" target="#tab_3">Table 3b</ref> present the results on Diving-48 <ref type="bibr" target="#b30">[31]</ref> and FineGym <ref type="bibr" target="#b40">[41]</ref>. For Diving-48, our model achieves 84.2%, substantially outperforming the state-of-the-art 3D CNN and transformer architectures. For FineGym, our model outperforms other methods in the averaged per-class accuracy of 50.9% and 86.4% given 288 and 99 classes, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparison to the state-of-the-art methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Ablation studies</head><p>We conduct ablation experiments to validate the effectiveness of RSA. We use 8 frames for all experiments. Other training and testing details are in Sec. 5.1.</p><p>Combinations of different kernels and contexts. In <ref type="table" target="#tab_5">Table 4a</ref>, we compare the performance of a single RSA layer with different combinations of dynamic kernels and contexts. We first vary different types of the kernels from ? V n , ? R n to ? V n + ? R n , while the context is fixed as X V n . Compared to ? V n X V n , ? R n X V n improves the accuracy by 0.6%. It indicates that predicting kernel from the composition of local query-key correlation is effective in modeling temporal dependencies. As we use both of the basic and relational transforms, (? V n + ? R n )X V n , we obtain additional improvements by 0.3%p and 0.9%p at top-1 and top-5 accuracy, respectively. We also vary different types of the context such as X V n , X R n , and X V n + X R n , while the kernel is fixed. We observe the consistent improvements across different types of contexts, which means that aggregating the relational context is beneficial for learning relational information. Finally, as we combine all dynamic transforms,</p><formula xml:id="formula_14">(? V n + ? R n )(X V n + X R n )</formula><p>, we achieve the highest accuracy of 47.0%. Latent dimension D. In <ref type="table" target="#tab_5">Table 4b</ref>, we validate the effectiveness of decomposing H with latent channel dimension D. Without decomposing H, the TSN-ResNet50 with 7 RSA layers requires 62.9 GFLOPs and 32.0 M parameters, resulting in out-of-memory error. After we decompose H with a small D, we significantly reduce FLOPs, the number of parameters, and the memory footprint. We set D = C Q as the default that performs the highest accuracy.</p><p>Kernel sizes. In <ref type="table" target="#tab_5">Table 4c</ref>, we compare the effect of the kernel size M . In most cases, larger spatiotemporal kernel improves the accuracy, except the case of M = 5 ? 9 ? 9. Considering the static convolution counterparts, the RSA effectively enlarges the spatio-temporal kernel size, requiring smaller FLOPs and parameters. We choose M = 5 ? 7 ? 7 as the default kernel size that shows the best computation-accuracy trade-off.</p><p>Correlation computation. In <ref type="table" target="#tab_5">Table 4d</ref>, we validate the effect of the group-wise correlation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b60">61]</ref>, which splits the query and the key embeddings into G groups and computes a dot product between each group. While the dot product correlation contracts all channel dimensions into a scalar correlation score as the dot-product self-attention <ref type="bibr" target="#b54">[55]</ref>, the group-wise correlation outputs a G-dimensional correlation vector, providing richer semantic information. As a result, the Hadamard product, i.e., G = C Q , that computes full element-wise correlations achieves the best performance. We thus set the Hadamard product as the default correlation function. Note that the computational complexity for generating the relational kernel remains the same while the size of G varies since we switch the orders of computation as in Eq. 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Kernel Visualization</head><p>In <ref type="figure" target="#fig_3">Fig. 3</ref>, we visualize the dynamic kernels of self-attention and RSA from the learned models in 4 th and 7 th rows in <ref type="table" target="#tab_3">Table 3</ref>, respectively. In visualization, we observe that both of the basic kernels and relational kernels resemble edge detectors, e.g., Sobel filters or Laplacian filters <ref type="bibr" target="#b48">[49]</ref>, along a temporal axis that compute a discrete approximation of spatio-temporal gradients across frames. When we reverse the temporal order of an input video clip, the relational kernel dynamically varies according to whether the object moves up or down but the basic kernel remains the same. Considering that motion information is related to the relative changes along the temporal axis, the results indicate that the RSA kernels effectively capture motion patterns within the context, whereas the self-attention kernels are limited to aggregating the local context based on the query-key similarities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented the RSA feature transform, which captures rich relational patterns for video understanding, and validated that it outperforms other dynamic feature transforms in learning motion dynamics in videos. The proposed RSANet outperforms the state-of-the art methods on standard motion-centric action benchmarks. While we have focused on video representation learning in this work, we believe the RSA will also benefit image understanding and natural language processing. We leave this for future work.</p><p>A Implementation details Architecture details. We use TSN-ResNet <ref type="bibr" target="#b56">[57]</ref> as our backbone (see <ref type="table" target="#tab_6">Table 5</ref>) and initialize it with ImageNet-pretrained weights <ref type="bibr" target="#b16">[17]</ref>. We replace its 7 spatial convolutional layers with the RSA layers; for every two ResNet blocks from the third block in res 2 to the second block in res 5 , each spatial convolutional layer is replaced with the RSA layer. Training. For the bottlenecks including RSA layers, we randomly initialize weights using MSRA initialization <ref type="bibr" target="#b15">[16]</ref> and set the gamma parameter of the last batch normalization layer to zero. We sample 8 or 16 frames from each video by using a segment-based random sampling strategy <ref type="bibr" target="#b56">[57]</ref>. We resize the resolution of each frame to 240 ? 320, and apply random cropping as 224 ? 224, scale jittering, and random horizontal flipping for data augmentation. Note that we do not flip videos of which action labels include 'left' or 'right' words, e.g., 'pulling something from left to right'. We set the initial learning rate to 0.02 and decay it with a cosine learning rate <ref type="bibr" target="#b33">[34]</ref> after the initial 5 warm-up epochs. We use SGD with the momentum of 0.9 and set the batch size as 64 across 8 V100 GPU machines. For training SS-V1&amp;V2 <ref type="bibr" target="#b13">[14]</ref>, we train models for 50 epochs in total except that the models in <ref type="table" target="#tab_1">Table 2a</ref> in our main paper are trained for 80 epochs. We use dropout of 0.3 before the final classifier, label smoothing <ref type="bibr" target="#b47">[48]</ref> of 0.1 and use stochastic depth <ref type="bibr" target="#b18">[19]</ref> with rate 0.2 for regularization. For Diving-48 <ref type="bibr" target="#b30">[31]</ref>, we sample 16 frames from each video and train for total 30 epochs. We use dropout of 0.5 before the final classifier for regularization. For FineGym <ref type="bibr" target="#b40">[41]</ref>, we sample 8 frames from each video and train for total 50 epochs. We use dropout of 0.5 before the final classifier.</p><p>Testing. For all benchmarks, we sample one or two clips, resize them to 240 ? 320, and center-crop by 224 ? 224. For Diving-48 <ref type="bibr" target="#b30">[31]</ref>, we sample two clips consist of 16 frames, and average softmax scores for inference. For FineGym <ref type="bibr" target="#b40">[41]</ref>, we sample a single clip consists of 8 frames for inference.</p><p>License. We implement our model based on TSN in Pytorch 3 under BSD 2-Clause license. All the benchmarks that we used are commonly used datasets for the academic purpose. According to https://paperswithcode.com/datasets, the license of SS-V1&amp;V2 and FineGym is the custom 4 and the CC BY-NC 4.0, respectively, and the license of Diving-48 is unknown.</p><p>Implementation of the relational kernel. Generating relational kernels spends a large amount of memory, and it thus requires CUDA implementations to be optimized. As described in Sec.4.2, we switch the computation orders, and then dramatically reduce the memory footprint. In <ref type="figure" target="#fig_5">Fig. 4</ref>, we present pseudo-codes of Eq 9 and 10 in our main paper for generating the relational kernel, and notate the size of the intermediate tensors. As illustrated in the pseudo-code, the memory footprint is reduced from O(BN M C) to O(BN C 2 ), where D = C and C is much smaller than M in our experiments. For ease description, the notation of multi-query L is omitted.</p><p>Implementation of RSA. In <ref type="figure">Fig. 5</ref>, we provide pseudo-codes of Eq. 11 and 12 in Sec. 4.2 in our main paper for computing RSA and notate the sizes of intermediate tensors. We can substantially reduce the memory by decomposing P and our RSA can be easily implemented with convolutions   <ref type="figure">Figure 5</ref>: Pseudo-code for the RSA. By switching the Eq. 11 to the Eq. 12, we can significantly reduce the memory footprint of RSA. and simple einsum operations. As illustrated in <ref type="figure">Fig. 5</ref>, the memory footprint is reduced from</p><formula xml:id="formula_15">O(BN M C + M C 2 ) to O(BN C 2 + M C 2 ),</formula><p>where D = C and C is much smaller than M in our experiments. For ease description, the notation of multi-query L is omitted.</p><p>B Additional ablation studies on RSA.</p><p>In this section, we provide additional ablation experiments on SS-V1 to validate the effect of other design components in the RSA. While specified otherwise, the training and testing details are the same as those in Sec. 5.1. Multi-query RSA. In <ref type="table" target="#tab_7">Table 6a</ref>, we investigate the effect of multi-query RSA by varying the number of queries L. The table shows that using more queries substantially reduces the computational cost of the RSA as shown in <ref type="table" target="#tab_0">Table 1</ref> in our main paper. Despite the channels of x Q n , X K n , X V n are reduced to C/L as L increased, the top-1 accuracy is slightly improved until L = 8. Since each RSA kernel generated by each query captures a distinct motion pattern, the model can learn diverse motion features (see <ref type="figure">Fig. 6</ref>). In this experiment, we choose L = 8 as the default.</p><p>Number and locations of RSA layers. In <ref type="table" target="#tab_7">Table 6b</ref>, we gradually replace the spatial convolutional layers with RSA layers. Adding a single RSA layer significantly improves the top-1 accuracy by 27.3%p. As we increase the number of RSA layers, we obtain gradual improvements of accuracy, while reducing parameters only. Note that RSA layer with the smaller kernel size M , e.g., 3 ? 5 ? 5, requires smaller amount of both FLOPs and parameters, while covering larger receptive fields than the convolution as shown in the second row of <ref type="table" target="#tab_5">Table 4c</ref> in our main paper.</p><p>Normalization. In <ref type="table" target="#tab_7">Table 6c</ref>, we investigate different normalization methods to x Q n , X K n , and X V n . In our setting, applying L2 normalization performs the best. Notably, applying no normalization perform better than applying batch normalization or normalization scheme from <ref type="bibr" target="#b1">[2]</ref> due to the ImageNet-pre-trained weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Kernel visualization</head><p>We present additional visualization results of RSA kernels. In <ref type="figure">Fig. 6</ref>, we visualize the dynamic kernels of self-attention and RSA from the learned models in 4 th and 7 th rows in <ref type="table" target="#tab_1">Table 2</ref> in the main paper, respectively. In visualization, we observe that each RSA kernel from each query captures a distinct motion pattern, and its values are dynamically modulated by the input query and the context. For example, three RSA kernels at the bottom row of each subfigure are noticeably modulated by the spatio-temporal correlations between the query and the context. It demonstrates that the RSA kernel determines its composability based on the input instance, leading to flexible video representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Potential societal impacts</head><p>Video action recognition is a fundamental task in the computer vision, and our work thus has a potential to assist numerous application scenarios, such as video retrieval, video surveillance, autonomous driving, and sports analytics, etc. However, the research on the video domain can be used for nefarious purposes, especially in the area of unauthorized surveillance. We believe the positive impacts of our work significantly outweigh the harmful impacts, but we clarify that it is dependent to the purpose of the users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Limitations</head><p>The effectiveness of the RSA transform has been demonstrated through our work, but it still leaves much room for improvement.</p><p>? First, the computational efficiency of the RSA could be further improved. For example, decomposing a spatio-temporal 3D RSA kernel into a spatial 2D and a temporal 1D kernel <ref type="bibr" target="#b52">[53]</ref> could reduce the computational complexity. ? Second, RSA captures both visual appearance and relational features with an additive manner, but we anticipate that there will be a more generalized dynamic transform that captures both features in natural. We hope that RSA could be utilized as a guideline for designing the generalized dynamic transform. ? Third, we believe that leveraging relational information can also benefit task of different domains such as image understanding or natural language processing. We leave this for future work.  <ref type="figure">Figure 6</ref>: Kernel visualization results on SS-V1. In each subfigure, we visualize the input RGB frames (top), the self-attention kernels (left), and the RSA kernels (right). The query position and the context are marked as red and yellow in RGB frames, respectively. The size of spatio-temporal kernel M is set as m t ? m h ? m w = 5 ? 7 ? 7 and 6 kernels out of L kernels (L = 8) are shown for each transform.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Computational graphs of different feature transforms. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Computational graph of RSA. RSA consists of two types of kernels (basic and relational kernel) and two contexts (basic and relational context). See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig. C in Appendix for the qualitative results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>(a) 'Moving something down.' (origin) (b) 'Moving something up.' (reversed order) Kernel visualization results on SS-V1. From the top to the bottom in each subfigure, we visualize the input RGB frames, the self-attention kernels, the basic kernels, and the relational kernels. The query position and the context are marked as red and yellow in RGB frames, respectively. The size of spatio-temporal kernel M is set as m t ? m h ? m w = 5 ? 7 ? 7 and 4 kernels out of L kernels (L = 8) are shown for each transform.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>#</head><label></label><figDesc>B: batch size, N: input size, M: kernel size, C: channel dim, D: latent dim def compute_relational_kernel(query, key, H1, H2, impl='Eq_10'): # query shape: [B,N,C], key shape: [B,N,C], H1 shape: [MC,D], H2 shape: [M,D] if impl == 'Eq_9': key = unfold(key, unfold_size=M) # key shape: [B,N,M,C] qk_Hadamard = einsum(query, key, 'BNC,BNMC?&gt;BNMC') # shape: [B,N,M,C] qk_H1 = einsum(qk_Hadamard, H1, 'BNMC,MCD?&gt;BND') # shape: [B,N,D] elif impl == 'Eq_10': H1 = reshape(H1, [M,CD]) # shape: [M,CD] key_H1 = conv1d(key, H1, group=C, kernel=M, bias=False) # shape: [B,N,CD] key_H1 = reshape(key_H1, [B,N,C,D]) # shape: [B,N,C,D] qk_H1 = einsum(query, key_H1, 'BNC,BNCD?&gt;BND') # shape: [B,N,D] relational_kernel = einsum(qk_H1,H2, 'BND,MD?&gt;BNM') # shape: [B,N,M] return relational_kernel</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Pseudo-code for the relational kernel. By switching the computation orders, we can significantly reduce the memory footprint of the relational kernel.# B: batch size, N: input size, M: context size, C: channel dim, D: latent dim def RSA(query, key, value, P, H1, H2, G, impl='Eq_12'): # query shape: [B,N,C], key shape: [B,N,C], value: [B,N,C], # P1 shape: [M,D], H1 shape: [MC,D], H2 shape: [M,D], G shape: [M,C] if impl == 'Eq_11': # kernels basic_kernel = einsum(query, P, 'BNC,CM?&gt;BNM') # shape: [B,N,M] H1 = reshape(H1, [M,CD]) # shape: [M,CD] key_H1 = conv1d(key, H1, group=C, kernel=M, bias=False) # shape: [B,N,CD] key_H1 = reshape(key_H1, [B,N,C,D]) # shape: [B,N,C,D] qk_H1 = einsum(query, key_H1, 'BNC,BNCD?&gt;BND') # shape: [B,N,D] relational_kernel = einsum(qk_H1, H2, 'BND,MD?&gt;BNM') # shape: [B,N,M] kernel = basic_kernel + relational_kernel # shape: [B,N,M] # contexts value = unfold(value, unfold_size=M) # value shape: [B,N,M,C] v_G = einsum(value, G, 'BNMC,MC?&gt;BNCC') # shape: [B,N,C,C] relational_context = einsum(value, v_G, 'BNMC,BNCC?&gt;BNMC') # shape: [B,N,M,C] context = value + relational_context # shape: [B,N,M,C] RSA = einsum(kernel, context, 'BNM,BNMC?&gt;BNC') # shape: [B,N,C] elif impl == 'Eq_12': P1_k_H1 = conv1d(key,H1, group=C, kernel=M, bias=P1) # shape: [B,N,CD] P1_k_H1 = reshape(P1_k_H1, [B,N,C,D]) # shape: [B,N,C,D] q_P1_k_H1 = einsum(query, P1_k_H1, 'BNC,BNCD?&gt;BND') # shape: [B,N,D] value = reshape(value, [B,NC,1]) # shape: [B,NC,1] H2_v = conv1d(value, H2, kernel=M, bias=False) # shape: [B,NC,D] H2_v = reshape(H2_v, [B,N,C,D]) # shape: [B,N,C,D] I_v_G = conv1d(value, G, kernel=M, bias=I) # shape: [B,NC,C] I_v_G = reshape(I_v_G, [B,N,C,C]) # shape: [B,N,C,C] H2_v_I_v_G = einsum(H2_v, I_v_G, 'BNCD,BNCC?&gt;BNCD') # shape: [B,N,C,D] RSA = einsum(q_P1_k_H1, H2_v_I_v_G, 'BND,BNCD?&gt;BNC') # shape: [B,N,C] return RSA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) 'Moving something up.' (b) 'Pretending to pick something up.'(c) 'Throwing something in the air and catching it.'</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Complexity of RSA. B, N, M, C, D, L denotes batch size, input size, context size, channel dimension, latent channel dimension, and number of queries, respectively. We simplify the complexity terms using M L, D ? C/L for ease description.</figDesc><table><row><cell>operation</cell><cell>time complexity</cell><cell>space complexity</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison with other spatio-temporal feature transform methods on SS-v1. W j ? W i (?) indicates a sequential transform of W i followed by W j . ? denotes softmax.</figDesc><table><row><cell>feature transform</cell><cell>kernel</cell><cell>context</cell><cell cols="3">FLOPs params. top-1 top-5</cell></row><row><cell>2D convolution</cell><cell>W 2D,standard</cell><cell>X n</cell><cell>32.5 G 24.3 M</cell><cell>19.7</cell><cell>46.6</cell></row><row><cell>3D convolution [51]</cell><cell>W 3D,standard</cell><cell>X n</cell><cell>57.0 G 39.3 M</cell><cell>43.3</cell><cell>72.2</cell></row><row><cell>(2+1)D convolution [53]</cell><cell>W 1D,standard ? W 2D,standard</cell><cell>X n</cell><cell>37.3 G 26.8 M</cell><cell>44.1</cell><cell>72.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison on SS-v1&amp;v2, FineGym, and Diving-48. SS-V1&amp;V2. IN and IN21K and K400 denote ImageNet-1k, ImageNet-21K, and Kinetics-400 dataset, respectively. Our method achieves a new state-of-the-art accuracy on both datasets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(b) Diving-48. Top-1 accuracy,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">FLOPs are shown. Results in the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">upper compartment are from [3].</cell></row><row><cell>model</cell><cell>pre-trained</cell><cell>#frame</cell><cell>FLOPs ?clips</cell><cell cols="4">SS-V1 top-1 top-5 top-1 top-5 SS-V2</cell><cell>model</cell><cell>FLOPs ?clips</cell><cell>top-1</cell></row><row><cell>I3D [5] from [59] TSM-R50 [32] ir-CSN-152 [52]</cell><cell>IN IN -</cell><cell>32 16 32</cell><cell>153 G?2 65 G?1 97 G?10</cell><cell>41.6 47.2 49.3</cell><cell>72.2 77.1 -</cell><cell>-63.4 -</cell><cell>-88.5 -</cell><cell cols="2">SlowFast-R101 [11] TimeSformer [3] TimeSformer-HR [3] 1703 G?3 213 G?3 196 G?3 TimeSformer-L [3] 2380 G?3</cell><cell>77.6 75.0 78.0 81.0</cell></row><row><cell>SlowFast8?8-R50 [11] CT-Net-R50 [29]</cell><cell>K400 IN</cell><cell>32 16</cell><cell>67 G?3 75 G ? 1</cell><cell>-52.5</cell><cell>-80.9</cell><cell>61.7 64.5</cell><cell>86.9 89.3</cell><cell>RSANet-R50</cell><cell>72G?2</cell><cell>84.2</cell></row><row><cell>STM-R50 [22] CorrNet-R101 [56] TEA [30]</cell><cell>IN -IN</cell><cell>16 32 16</cell><cell>67 G?30 187 G?10 70 G ?3</cell><cell>50.7 50.9 52.3</cell><cell>80.4 -81.9</cell><cell>64.2 -65.1</cell><cell>89.8 -89.9</cell><cell cols="3">(c) FineGym. The averaged per-class accuracy (%) is reported.</cell></row><row><cell>MSNet-TSM-R50 [24]</cell><cell>IN</cell><cell>16</cell><cell>67 G?1</cell><cell>52.1</cell><cell>82.3</cell><cell>64.7</cell><cell>89.4</cell><cell cols="3">All results in the upper compart-</cell></row><row><cell>NL-I3D [58] from [59]</cell><cell>IN</cell><cell>32</cell><cell>168 G?2</cell><cell>44.4</cell><cell>76.0</cell><cell>-</cell><cell>-</cell><cell cols="2">ment are from [41].</cell><cell></cell></row><row><cell>TimeSformer-HR [3]</cell><cell>IN</cell><cell>16</cell><cell>1703 G?3</cell><cell>-</cell><cell>-</cell><cell>62.2</cell><cell>-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>TimeSformer-L [3] ViViT-L [1]</cell><cell>IN IN21K &amp; K400</cell><cell>96 32</cell><cell>2380 G?3 N/A?4</cell><cell>--</cell><cell>--</cell><cell>62.4 65.4</cell><cell>-89.8</cell><cell>model TRN [64]</cell><cell cols="2">Gym288 Gym99 33.1 68.7</cell></row><row><cell>RSANet-R50 (ours) RSANet-R50 (ours) RSANet-R50EN (ours)</cell><cell>IN IN IN</cell><cell>8 16 8+16</cell><cell>36 G?1 72 G?1 108 G?1</cell><cell>51.9 54.0 55.5</cell><cell>79.6 81.1 82.6</cell><cell>64.8 66.0 67.3</cell><cell>89.1 89.8 90.8</cell><cell>I3D [5] TSM [32] TSMTwo-stream [32]</cell><cell>27.9 34.8 46.5</cell><cell>63.2 70.6 81.2</cell></row><row><cell>RSANet-R50EN (ours)</cell><cell>IN</cell><cell>8+16</cell><cell>108 G?2</cell><cell>56.1</cell><cell>82.8</cell><cell>67.7</cell><cell>91.1</cell><cell>RSANet-R50</cell><cell>50.9</cell><cell>86.4</cell></row></table><note>(a)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3a</head><label>3a</label><figDesc>compares our method to the state-of-the-art methods on SS-V1 and V2 datasets. The first compartment shows the results of the 3D CNNs. The second compartment contains the motion-modeling methods: both STM<ref type="bibr" target="#b21">[22]</ref> and TEA<ref type="bibr" target="#b29">[30]</ref> compute frame-wise differences, and both CorrNet<ref type="bibr" target="#b55">[56]</ref> and MSNet<ref type="bibr" target="#b23">[24]</ref> compute inter-frame pixel-wise correlations to learn motion features. The third compartment reports the results of global self-attention-based models. NL-I3D<ref type="bibr" target="#b57">[58]</ref> inserts non-local blocks to the 3D CNN for capturing long-range spatio-temporal dependencies. TimeSformer-L, TimeSformer-HR<ref type="bibr" target="#b2">[3]</ref> and ViViT<ref type="bibr" target="#b0">[1]</ref> are transformer architectures that learn video representations via factorized spatio-temporal self-attention. Our method, RSANet-R50, achieves 51.9% and 64.8% at top-1 accuracy on SS-V1 and V2 datasets, respectively, which are already competitive to most of existing methods while using 8 frames only. When we use 16 frames, our method outperforms other</figDesc><table /><note>existing models by achieving 54.0% and 66.0% at top-1 accuracy on SS-V1 and V2, respectively. Fi- nally, our ensemble model with 2 clips achieves 56.1% and 67.7%, which set the new state-of-the-art on SS-V1 and V2 with reasonable computational cost.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies on SS-v1 dataset. All models use TSN-ResNet50<ref type="bibr" target="#b56">[57]</ref> as the backbone. Top-1, top-5 accuracy (%), FLOPs (G) and paramaters (M) are shown. Combinations of different kernels and context. A single RSA layer is inserted into stage4. Latent dimension D. Decomposing H significantly reduces the computation cost. OOM is an abbrebiation of out-of-memory. 8 video clips per a single GPU machine are used.</figDesc><table><row><cell>kernel ? V n ? R n ? V n + ? R n</cell><cell>context X V n X V n X V n</cell><cell cols="4">FLOPs params. top-1 top-5 32.3 G 23.4 M 44.8 73.8 32.7 G 23.6 M 45.4 73.9 32.7 G 23.6 M 45.7 74.8</cell><cell>(b) D -</cell><cell cols="5">FLOPs params. memory top-1 62.9 G 32.0 M OOM OOM OOM top-5</cell></row><row><cell>? V n ? R n ? V n + ? R n ? V n ? R n ? V n + ? R n</cell><cell>X R n X R n X R n X V n +X R n X V n +X R X V n n +X R n</cell><cell>32.7 G 33.2 G 33.2 G 32.7 G 33.2 G 33.2 G</cell><cell>23.4 M 23.6 M 23.6 M 23.4 M 23.6 M 23.6 M</cell><cell>46.2 46.5 46.7 46.5 47.0 46.8</cell><cell>75.4 75.6 75.6 75.6 75.7 75.6</cell><cell>8 16 32 C Q /2 C Q</cell><cell>32.2 G 34.7 G 39.6 G 32.9 G 35.9 G</cell><cell cols="2">20.5 M 20.9 M 21.7 M 10.2 GB 8.8 GB 9.2 GB 21.1 M 8.8 GB 22.0 M 9.6 GB</cell><cell>50.1 51.3 50.9 51.1 51.5</cell><cell>78.8 78.8 79.0 79.1 79.2</cell></row><row><cell cols="6">(c) Kernel size M . In most cases, larger kernel results</cell><cell cols="6">(d) Group G. Hadamard product (G = C Q ) performs</cell></row><row><cell cols="3">in the higher accuracy.</cell><cell></cell><cell></cell><cell></cell><cell cols="6">the highest accuracy. Note that FLOPs are consistent</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">with varying G due to the switched computation order.</cell></row><row><cell cols="2">kernel size M</cell><cell cols="4">FLOPs params. top-1 top-5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">3 ? 3 ? 3</cell><cell>28.5 G</cell><cell>20.3 M</cell><cell>49.4</cell><cell>77.6</cell><cell cols="2"># Groups G</cell><cell cols="4">FLOPs params. top-1 top-5</cell></row><row><cell cols="2">3 ? 5 ? 5</cell><cell>30.2 G</cell><cell>20.7 M</cell><cell>50.5</cell><cell>78.7</cell><cell>1</cell><cell></cell><cell>35.9 G</cell><cell>20.2 M</cell><cell>50.4</cell><cell>78.9</cell></row><row><cell cols="2">3 ? 7 ? 7</cell><cell>32.6 G</cell><cell>21.2 M</cell><cell>50.7</cell><cell>78.9</cell><cell>2</cell><cell></cell><cell>35.9 G</cell><cell>20.2 M</cell><cell>50.9</cell><cell>78.9</cell></row><row><cell cols="2">3 ? 9 ? 9</cell><cell>35.8 G</cell><cell>22.0 M</cell><cell>51.1</cell><cell>79.1</cell><cell>4</cell><cell></cell><cell>35.9 G</cell><cell>20.3 M</cell><cell>51.2</cell><cell>78.9</cell></row><row><cell cols="2">5 ? 7 ? 7</cell><cell>35.9 G</cell><cell>22.0 M</cell><cell>51.5</cell><cell>79.2</cell><cell>8</cell><cell></cell><cell>35.9 G</cell><cell>20.5 M</cell><cell>51.2</cell><cell>79.0</cell></row><row><cell cols="2">5 ? 9 ? 9</cell><cell>41.3 G</cell><cell>23.3 M</cell><cell>51.2</cell><cell>78.9</cell><cell>C Q</cell><cell></cell><cell>35.9 G</cell><cell>22.0 M</cell><cell>51.5</cell><cell>79.2</cell></row></table><note>(a)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>TSN-ResNet-50 backbone.</figDesc><table><row><cell>Layers</cell><cell cols="2">TSN-ResNet-50</cell><cell>Output size</cell></row><row><cell>conv 1</cell><cell cols="2">1?7?7, 64, stride 1,2,2</cell><cell>T?112?112</cell></row><row><cell>pool 1</cell><cell cols="2">1?3?3 max pool, stride 1,2,2</cell><cell>T?56?56</cell></row><row><cell></cell><cell>1?1?1, 256</cell><cell></cell><cell></cell></row><row><cell>res 2</cell><cell>1?3?3, 64</cell><cell>?3</cell><cell>T?56?56</cell></row><row><cell></cell><cell>1?1?1, 256</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1?1?1, 512</cell><cell></cell><cell></cell></row><row><cell>res 3</cell><cell>1?3?3, 128</cell><cell>?4</cell><cell>T?28?28</cell></row><row><cell></cell><cell>1?1?1, 512</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1?1?1, 1024</cell><cell></cell><cell></cell></row><row><cell>res 4</cell><cell>1?3?3, 256</cell><cell>?6</cell><cell>T?14?14</cell></row><row><cell></cell><cell>1?1?1, 1024</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1?1?1, 2048</cell><cell></cell><cell></cell></row><row><cell>res 5</cell><cell>1?3?3, 512</cell><cell>?3</cell><cell>T?7?7</cell></row><row><cell></cell><cell>1?1?1, 2048</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">global average pool, FC</cell><cell># of classes</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Additional ablation studies on SS-v1 dataset. All models use TSN-ResNet50<ref type="bibr" target="#b56">[57]</ref> as the backbone. Top-1, top-5 accuracy (%), FLOPs (G) and paramaters (M) are shown.(a) Query L. Using multiple queries reduces the computational cost. Latent dimension D is set to 16. OOM is an abbrebiation of out-of-memory. Number and locations of RSA layers. More RSA layers lead the better performance.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">(b) number positions FLOPs params. top-1 top-5</cell></row><row><cell># query L</cell><cell>FLOPs</cell><cell>params.</cell><cell>top-1</cell><cell>top-5</cell><cell>0</cell><cell>-</cell><cell>32.5 G</cell><cell>24.3 M</cell><cell>19.7</cell><cell>46.6</cell></row><row><cell>1 2 4 8</cell><cell>252.5 G 92.8 G 48.2 G 34.7 G</cell><cell>28.1 M 23.9 M 21.9 M 20.9 M</cell><cell>OOM 50.6 50.9 51.3</cell><cell>OOM 78.9 79.0 78.8</cell><cell>1 2 4 7</cell><cell>stage4 stage4-5 stage4-5 stage2-5</cell><cell>33.2 G 33.7 G 34.6 G 35.9 G</cell><cell>23.6 M 22.6 M 22.2 M 22.0 M</cell><cell>47.0 47.2 50.4 51.5</cell><cell>75.7 76.1 78.7 79.2</cell></row><row><cell>16</cell><cell>30.2 G</cell><cell>20.4 M</cell><cell>50.0</cell><cell>78.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="6">(c) Normalization. L2-normalization performs the</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">best accuracy in our setting.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">normalization</cell><cell cols="4">FLOPs params. top-1 top-5</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>no norm.</cell><cell>35.9 G</cell><cell>22.0 M</cell><cell>50.3</cell><cell>78.6</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>batch norm.</cell><cell>35.9 G</cell><cell>22.0 M</cell><cell>49.4</cell><cell>78.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">norm. from [2]</cell><cell>35.8 G</cell><cell>22.0 M</cell><cell>49.6</cell><cell>78.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>L2-norm.</cell><cell>35.9 G</cell><cell>22.0 M</cell><cell>51.5</cell><cell>79.2</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This transform corresponds to the position lambda where the extent of the context X V n is restricted to the local neighborhood of the target x Q n<ref type="bibr" target="#b1">[2]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">URL: https://github.com/yjxiong/tsn-pytorch 4 URL: https://20bn.com/licensing/datasets/academic</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15691</idno>
		<title level="m">Vivit: A video vision transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Lambdanetworks: Modeling long-range interactions without attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08602</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05095</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamic convolution: Attention over convolution kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11030" to="11039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rubiksnet: Learnable 3d-shift for efficient video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">End-to-end learning of motion representation for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">X3d: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Miyake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Competition and cooperation in neural nets</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The&quot; something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Group-wise correlation stereo network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3273" to="3282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Local relation networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3464" to="3473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3866</idno>
		<title level="m">Speeding up convolutional neural networks with low rank expansions</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stm: Spatiotemporal and motion encoding for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.04325</idno>
		<title level="m">Hadamard product for low-rank bilinear pooling</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Motionsqueeze: Neural motion feature learning for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.09933</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning self-similarity in space and time as generalized motion for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.07092</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Motion feature network: Fixed motion filter for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.06255</idno>
		<title level="m">Involution: Inverting the inherence of convolution for visual recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">{CT}-net: Channel tensorization network for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tea: Temporal excitation and aggregation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Resound: Towards action recognition without representation bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning video representations from correspondence proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sgdr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Weightnet: Revisiting the design space of weight networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.11823</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neimark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Asselmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.00719</idno>
		<title level="m">Video transformer network</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Representation flow for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Finegym: A hierarchical video dataset for fine-grained action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Matching local self-similarities across images and videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Strudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Segmenter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05633</idno>
		<title level="m">Transformer for semantic segmentation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Optical flow guided feature: A fast and robust motion representation for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Computer vision: algorithms and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Scaling local self-attention for parameter efficient visual backbones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.12731</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Video modeling with correlation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="399" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04971</idno>
		<title level="m">Condconv: Conditionally parameterized convolutions for efficient inference</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Volumetric correspondence networks for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Exploring self-attention for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10076" to="10085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
