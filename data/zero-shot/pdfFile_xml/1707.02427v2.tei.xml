<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning for Vanishing Point Detection Using an Inverse Gnomonic Projection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Kluger</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Leibniz Universit?t Hannover</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanno</forename><surname>Ackermann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Leibniz Universit?t Hannover</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Ying</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Twente</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Leibniz Universit?t Hannover</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Learning for Vanishing Point Detection Using an Inverse Gnomonic Projection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel approach for vanishing point detection from uncalibrated monocular images. In contrast to state-of-the-art, we make no a priori assumptions about the observed scene. Our method is based on a convolutional neural network (CNN) which does not use natural images, but a Gaussian sphere representation arising from an inverse gnomonic projection of lines detected in an image. This allows us to rely on synthetic data for training, eliminating the need for labelled images. Our method achieves competitive performance on three horizon estimation benchmark datasets. We further highlight some additional use cases for which our vanishing point detection algorithm can be used.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Vanishing points (VPs) are strong cognitive cues for the human visual perception, as they provide characteristic information about the geometry of a scene, and are used as a feature for relative depth and height estimation <ref type="bibr" target="#b22">[23]</ref>. Their detection is a fundamental problem in the field of computer vision, because it underpins various higher-level tasks, including camera calibration <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">25]</ref>, 3D metrology <ref type="bibr" target="#b7">[8]</ref>, 3D scene structure analysis <ref type="bibr" target="#b12">[13]</ref>, as well as many others. A vanishing point arises from a set of parallel lines as their point of intersection, at an infinite location initially, and is uniquely defined by the lines' direction. Under a projective transformation, parallel lines in space may be transformed to converging lines on an image plane, thus leading to a finite intersection point. The detection of VPs in perspective images is therefore a search for converging lines and their intersections, which is difficult in the presence of noise, spurious line segments, near-parallel imaged lines, and intersections of non-converging lines. These reasons make vanishing point detection a hard problem. Consequently, it has not been addressed often in the past years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>Since the seminal work of Barnard <ref type="bibr" target="#b3">[4]</ref>, various methods designed to tackle this problem have been proposed. Some of them <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25]</ref> rely on the Manhattanworld assumption <ref type="bibr" target="#b6">[7]</ref>, which means that only three mutually orthogonal vanishing directions exist in a scene, as is reasonably common in urban scenes where buildings are aligned on a rectangular grid. Others <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref> rely on the less rigid Atlanta-world assumption <ref type="bibr" target="#b20">[21]</ref>, which allows multiple non-orthogonal vanishing directions that are connected by a common horizon line, and are all orthogonal to a single zenith. Few works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b23">24]</ref> -including ours -make no such assumptions. Most methods are based on oriented elements -either line segments <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref> or edges <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b20">21]</ref> -from which VPs are estimated, usually by grouping the oriented elements into clusters <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b14">15]</ref>, or by fitting a more comprehensive model <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b23">24]</ref>. It is common to refine the thereby detected vanishing points in an iterative process, such as the Expectation Maximisation (EM) algorithm <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b14">15]</ref> which we are utilising as well. Ever since the AlexNet by Krizhevsky et al. <ref type="bibr" target="#b15">[16]</ref> succeeded in the 2012 ImageNet competition, convolutional neural networks (CNNs) <ref type="bibr" target="#b16">[17]</ref> have become a popular tool for computer vision tasks, as they perform exceedingly well in a variety of settings. Borji <ref type="bibr" target="#b5">[6]</ref> recently demonstrated a CNN based approach for vanishing point detection; however, it only detects up to one horizontal vanishing point. The approach of Zhai et al. <ref type="bibr" target="#b27">[28]</ref> is much more comprehensive. It is based on a CNN which extracts prior information about the horizon line location from an image, and currently achieves the best state-of-the-art performance on horizon line detection benchmarks commonly used to evaluate vanishing point detection algorithms. Unlike other methods, their approach begins by selecting horizon line candidates first, and then jointly scores horizon candidates and horizontal VP candidates, which are eventually refined in an EM-like process. As it is based on horizon lines, their approach is inherently limited to Atlanta-world scenes.</p><p>Contributions In this work, we propose a more generalised approach using a CNN which does not operate on natural images, but on a more abstract presentation of the scene based on the Gaussian sphere representation of points and lines <ref type="bibr" target="#b3">[4]</ref>. It is identical to an inverse gnomonic projection, which -very similar to an inverse stereographic projection -is a mapping that transforms the unbounded image plane onto a bounded space, thus making vanishing points far from the image centre easier to handle. While this necessitates an additional preprocessing step, it allows us to train the CNN solely using synthetic data, which we generate in a very straightforward manner, thus eliminating the need for labelled real-world data. The use of the CNN is motivated by the fact that spurious, yet significant VP candidates can occur (cf. <ref type="figure">Fig. 4</ref>), thus approaches based on voting are prone to fail. The advantage of using a CNN over, for instance, a support vector machine is that discriminative features are automatically learned. Our CNN is able to directly estimate VP candidates on the Gaussian sphere, which are then refined with an EM-like algorithm. We furthermore devised an improved line weighting scheme for the EM process, which imposes a spatial consistency prior over the line-to-vanishing-point associations in order to become more robust in the presence of noise and clutter. This is motivated by the fact that spurious lines, for instance caused by plants or shadows, are often spatially correlated. Combined with a line segment extractor as a preprocessing step, our method allows vanishing point estimation from real-world images with competitive accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>Our approach consists of the following stages: First, line segments are extracted from the input image using the LSD line detector <ref type="bibr" target="#b10">[11]</ref>. The lines are then mapped onto the Gaussian sphere, and its image is rendered (Sec. 2.1). This image is used as the input for a CNN (Sec. 2.2) which we trained solely on synthetic data (Sec. 2.3). This CNN then provides a coarse prediction of possible VP locations, which are ultimately refined in an Expectation Maximisation based process (Sec. 2.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Parametrisation</head><p>In order to deal with infinite vanishing points, it is reasonable to transform the unbounded image plane onto a bounded space, such as the Gaussian sphere representation, which based on an inverse gnomonic projection of homogeneous points p = (p 1 , p 2 , p 3 ) T and lines in normal form l = (l 1 , l 2 , l 3 ) T , as described by <ref type="bibr">Barnard [4]</ref>:</p><formula xml:id="formula_0">p p 2 = (sin ? cos ?, sin ?, cos ? cos ?) T (1) ?(?, l) = tan ?1 ?l 1 sin ? ? l 3 cos ? l 2<label>(2)</label></formula><p>The lines are projected from the image plane at a fixed distance onto the unit sphere at origin. A square image of the sphere's front half surface is rendered, so that the lines appear as opaque curves, and the image's x,y-coordinates correspond to azimuth and elevation (?, ?) on the sphere. This sphere image (cf. <ref type="figure" target="#fig_0">Fig. 1</ref>) is later used as an input for the CNN. The vanishing points are likewise parametrised in the ?, ?-space, and are then quantised into bins on a regular N ? N grid, so that the occurrence of a vanishing point within those bins can be treated as a multi-label classification task. Normalisation: As actual images can be of various sizes, image coordinates are normalized to fit within a (?1, 1) ? (?1, 1) border by applying the following, aspect ratio preserving, transformation:</p><formula xml:id="formula_1">H norm = 1 s ? ? 2 0 ?w 0 2 ?h 0 0 s ? ?<label>(3)</label></formula><p>with h and w being the image's height and width, respectively, and s = max(w, h).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Network Architecture</head><p>We used the popular AlexNet <ref type="bibr" target="#b15">[16]</ref> as a basic architecture for our approach. This network consists of five convolutional layers -some of them extended by max pooling, local response normalisation, or ReLU layers -followed by three fully connected (FC) layers. Originally, its final layer has 1000 output nodes, to which a softmax function is applied, and a multinomial logistic loss function is used for training, as is common for one-of-many classification tasks. While a regression approach may seem well suited for a task such as vanishing point detection from line segments, training CNNs for regression tasks is notoriously difficult. We therefore decided to reformulate it as a multi-label classification task by partitioning the surface of the Gaussian sphere into N ? N patches, as described in Sec. 2.1, and assigning distinct class labels to each patch. In order to suit our task, we modified the last FC layer of the AlexNet to contain N 2 output nodes, and replaced the softmax with a sigmoid function. For training, we use a cross entropy loss function, which is well suited for multi-label classification tasks like this. The output of the network is a likelihood image of possible vanishing points in the given scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training Data</head><p>Since suitable training data for vanishing point detection tasks is scarce, and compiling annotated data on our own would have been very laborious and time consuming, yet large amounts of training data are needed to obtain good results with deep learning, we decided to solely rely on a synthetically created dataset. Since our approach does not actually need natural images as an input, but relies on line segments only, we can create such synthetic data without much effort. To create a set of line segments as a piece of data for training, we proceed as follows: First, the number of vanishing directions K d ? [1, 6] is chosen. The first three (or less) directions are then chosen randomly, but with the condition that they must be mutually orthogonal. Additional directions are set as a linear combination of two randomly chosen, previously set directions, thus vanishing directions 4 to 6 do not form an orthogonal system. For each direction, a varying number of line clusters is placed in 3D space. Each cluster consists of a varying number of parallel or collinear line segments in close proximity. Additionally, some outlier line segments which are not aligned with any of the vanishing directions are interspersed. This 3D scene is then projected into 2D using a virtual pinhole camera with randomly chosen rotation, translation and focal length. Either uniform or Gaussian noise is added to the resulting 2D line segments, with its strength varying from example to example. These line segments are then finally cropped to fit within a (?1, 1) ? (?1, 1) border. One example is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>Using this procedure, we create 96,000 examples for each number of vanishing directions, resulting in a dataset of 576,000 training examples. Each line segment is then converted into a line in normal form, and the true vanishing point for each vanishing direction is computed, so that every datum can be parametrised for CNN training as described in Sec. 2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Vanishing Point Refinement</head><p>As the response of the CNN is rather coarse, a post-processing step is needed to determine the exact vanishing point locations. We decided to utilise a variant of the Expectation Maximisation (EM) algorithm, based on the method described by Ko?eck? and Zhang <ref type="bibr" target="#b14">[15]</ref> with additional modifications: E-step: An affinity measure w ik between a line segment l i -or its corresponding homogeneous line l i -and a vanishing point candidate v k is calculated based on the posterior distribution:</p><formula xml:id="formula_2">w ik ? p(v k |l i ) = p(l i |v k ) p(v k ) p(l i )<label>(4)</label></formula><p>with p(l i ) = k p(v k )p(l i |v k ). We assume a likelihood modelled by:</p><formula xml:id="formula_3">p(l i |v k ) ? exp ?d 2 ik 2? 2 k<label>(5)</label></formula><p>with d ik being a consistency measure between l i and v k . M-Step: New vanishing point estimates are obtained by solving the following least-squares problem:</p><formula xml:id="formula_4">J(v k ) = min v k i w ik d 2 ik .<label>(6)</label></formula><p>Modifications As in <ref type="bibr" target="#b14">[15]</ref>, we measure the distance d</p><p>ik = l T i v k on the Gaussian sphere to solve (6), but use an angle-based consistency measure, similar to the suggestions of <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20]</ref>, to compute <ref type="bibr" target="#b4">(5)</ref>, as this yields better accuracy. With m i being the midpoint of l i and m i ? v k denoting a cross-product, we define:</p><formula xml:id="formula_6">d (2) ik = 1 ? cos(?(l i , m i ? v k ))<label>(7)</label></formula><p>Departing from <ref type="bibr" target="#b14">[15]</ref>, we utilise the output of our CNN to estimate the prior p(v k ) and to initialise the VP candidates, and furthermore propose a modified affinity measure w ik to consider the spatial structure of line segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vanishing point prior:</head><p>We treat the output of the CNN as an approximation of the true probability density distribution for p(v k ) in the (?, ?)-space, which we model as a mixture of Gaussians with N 2 components of standard deviation ? prior . Each component is located at the centre of the corresponding patch on the Gaussian sphere and weighted proportionally to its CNN response. This is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. Initialisation: First, the K init strongest local maxima of the CNN response are detected. Each of these corresponds to a patch on the Gaussian sphere image (cf. <ref type="figure" target="#fig_0">Fig. 1</ref>). Then, the global maximum within such a patch is detected and its position on the sphere converted back to euclidean coordinates, which yields an initial vanishing point candidate.</p><p>Affinity measure: Originally, w ik = p(v k |l i ) was used in <ref type="bibr" target="#b14">[15]</ref> as an affinity measure. As it does not take the spatial structure of line segments into account, we devised a modified affinity measure based on the following assumptions: 1. Line segments with similar orientation in close proximity likely belong to the same vanishing point. 2. Line segments that lie within a neighbourhood of similarly oriented lines less likely originate from noise. Based on this intuition, we devised a similarity measure S ij between two line segments l i , l j :</p><formula xml:id="formula_7">S ij = cos(? ij ) exp( ?d l (l i , l j ) 2 ? 2 l )<label>(8)</label></formula><p>with ? ij = min max k ? ? ?(l i , l j ), ? ? 2 , ? 2 , and d l (l i , l j ) being the shortest distance between the two line segments. Using this similarity measure, we enforce a prior on w ik in order to achieve higher spatial consistency between line segments w.r.t. their vanishing point associations. We further assign a higher relative weight to those line segments which, according to the similarity measure, appear to lie in a neighbourhood of other, similar line segments, assuming that this indicates a regular structure as opposed to noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Split and merge:</head><p>In some cases, a detection would occur at a spot within the image's borders which is not a true vanishing point, but merely a point of coincidental intersection of lines. In order to counteract this, we devised a split-andmerge technique which is applied once every f s iterations of the EM process: First, a vanishing point within the image whose associated line segments have the highest standard deviation w.r.t. their angle is selected. Then, these line segments are split into two clusters based on their angle, from which two new vanishing points are calculated, replacing the old one. If one resulting vanishing point is too close to another, they will be merged together afterwards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Horizon Line and Orthogonal Vanishing Point Estimation</head><p>As is customary, we used the horizon detection error metric to compare our approach to previous methods. We devised an algorithm that estimates three supposedly orthogonal vanishing points and a horizon line, given a set of previously determined vanishing points. First, we select the N vp most significant vanishing points -where significance is measured by the number of lines n k associated with a vanishing point v kand consider every possible triplet T . Any vanishing point with an elevation |? k | &gt; ? z on the Gaussian sphere is considered as a zenith candidate v z . We then discard unreasonable solutions, e.g. those which would result in a horizon line slope ? hor &gt; ? hor . We assume that the projection of the camera centre c coincides with the center of the image and calculate the angle ? hz,T between the tentative horizon line and the line l zc = v z ? c, which ideally should be perpendicular <ref type="bibr" target="#b4">[5]</ref>. Then we calculate a score value for each triplet:</p><formula xml:id="formula_8">s T = (1 ? cos(? hz,T )) ? i?T n i<label>(9)</label></formula><p>and select the triplet with the highest score. Finally, a horizon line h is calculated -under the condition that h and l zc be perpendicular -by minimising:  <ref type="figure">Fig. 3</ref>: Cumulative histograms of the horizon detection error. The horizon error is represented on the x-axis, while the y-axis represents the fraction of images with less than the corresponding error.</p><formula xml:id="formula_9">J(h) = min h i?(T \zenith) n i v i ? c (h T v i ) 2<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We implemented and trained our CNN with the Caffe <ref type="bibr" target="#b13">[14]</ref> framework and used an existing C++ library <ref type="bibr" target="#b10">[11]</ref> for line detection. All other pre-and post-processing steps were implemented in Python, making use of the Numpy and Scikit-learn <ref type="bibr" target="#b18">[19]</ref> packages. The parameters in Tab. 1 were used for all experiments. On an Intel Core i7-3770K CPU, our implementation takes 45 seconds on average to compute the result for a 640x480 pixel image. The majority of this time -almost 95% -is needed for the EM based refinement step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Horizon Estimation</head><p>For a quantitative evaluation of our method, we computed the horizon detection error on two benchmark datasets that were commonly used to assess the performance of vanishing point detection in previous works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>, as well as a third, more recent dataset additionally used for evaluation in <ref type="bibr" target="#b27">[28]</ref>. The horizon detection error is defined as the maximum distance between the detected and the true horizon line, relative to the image's height.</p><p>The York Urban Dataset (YUD) <ref type="bibr" target="#b8">[9]</ref> contains 102 images of indoor and urban outdoor scenes, and three vanishing points corresponding to orthogonal directions are given as ground-truth for each scene. Generally, these scenes fulfil the Manhattan-world assumption, though our method does not take advantage of that. <ref type="figure">Fig. 3a</ref> shows the cumulative horizon error histogram and the area under the curve (AUC) as a performance measure, comparing our approach to competing methods. We achieve a competitive AUC of 94.27%, compared to 94.78% of the current best state-of-the-art method <ref type="bibr" target="#b27">[28]</ref>. In contrast to <ref type="bibr" target="#b27">[28]</ref>, in which only the horizon line estimation is evaluated, we are able to identify the three orthogonal vanishing directions with an accuracy of 99.13% within a margin of error of five degrees.</p><p>The Eurasian Cities Dataset (ECD) <ref type="bibr" target="#b2">[3]</ref> contains 103 urban outdoor scenes which generally do not satisfy the Manhattan-world assumption, but often contain multiple groups of orthogonal directions, and are therefore more challenging <ref type="figure">Fig. 4</ref>: First 3 images show line segments associated with the three VPs used for horizon estimation (red, green, blue), estimated horizon (magenta) and ground truth horizon (cyan). Images 4-6 show the corresponding sphere images with most significant (green) and other detected (yellow) VPs, and ground truth (cyan). The 1st and 4th images show the best case example, the 2nd and 5th an average case example, the 3rd and 6th a failure case.</p><p>compared to the YUD. The horizon line and a varying number of vanishing points are given as ground-truth for each scene. On this dataset, we achieve an AUC of 86.26%. <ref type="figure">Fig. 3b</ref> gives a comparison to other state-of-the-art methods.</p><p>The Horizon Lines in the Wild (HLW) dataset <ref type="bibr" target="#b25">[26]</ref> is a recent benchmark dataset which is significantly more challenging than both YUD and ECD, as many of its approximately 2000 test set images do not fulfil the Atlanta-world assumption. Here, our method achieves 57.31% AUC -slightly worse than <ref type="bibr" target="#b27">[28]</ref> with 58.24%, but vastly better than <ref type="bibr" target="#b17">[18]</ref> with 52.59%, see <ref type="figure">Fig. 3c</ref>.</p><p>Generally, our method appears to perform poorly when a large number of line segments near the horizon, large curved structures, or a very large number of noisy line segments are present. A representative failure case is shown by the 3rd and 6th images in <ref type="figure">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Additional Applications</head><p>As camera systems are an essential source of data for autonomous vehicles and driver assistance systems upon which they base their actions, it is reasonable to extract as much useful information as possible from the images they capture. We want to illustrate that robustly estimated vanishing points are of great use for such applications. In order to extract metrically correct measurements within a scene from camera images, knowledge of the cameras intrinsic parameters K is required. While they can be acquired by calibration before deploying the camera in a vehicle, shock and vibration may alter the camera's internal alignment over time, resulting in a need for recalibration. Such a recalibration is possible by way of determining the image of the absolute conic ? = K ?T K ?1 from three orthogonal vanishing points if zero skew and square pixels are assumed. A method that facilitates this is outlined in <ref type="bibr" target="#b11">[12]</ref>, while a simplified version that assumes the camera's principle point to be known -but only needs two orthogonal vanishing pointsis described in <ref type="bibr" target="#b24">[25]</ref>. If the camera's intrinsic parameters are known, a homography H = KRK ?1 , which is akin to a rotation of the camera with a 3D rotational matrix R, can be computed. This can be exploited to align one or two vanishing directions with the canonical x, y or z-axes in a way that results in a rectification of any plane which is aligned with said directions. Such a rectification can be used to extract relative measurements within a plane, e.g. for computing relative widths within a traffic lane (cf. <ref type="figure" target="#fig_2">Fig. 5a-b</ref>), or to project auxiliary information -such as street names or traffic signs -into a scene (cf. <ref type="figure" target="#fig_2">Fig. 5c</ref>) and display it to the driver, thus facilitating a form of visually appealing augmented reality without the need for explicit 3D reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We introduced a novel, deep learning based vanishing point detection method, which uses a CNN that operates on artificial images arising from a Gaussian sphere representation of lines and points using an inverse gnomonic projection. It is trained using synthetic data including noise and outliers exclusively, eliminating the need for labelled data. Despite not relying on either the Manhattanworld or Atlanta-world assumptions, which most related works do, it achieves competitive results on three benchmark datasets and good results in two further applications. Obviously, the capability of the trained CNN to handle different scenes depends on the training data. Since the proposed approach relies on synthetic data, it can be easily amended to represent different cases. The results on Horizon Lines in the Wild (HLW) demonstrates that the used training data is representative for difficult real images. Even more challenging scenarios, for instance no orthogonal VPs at all, can be approached by generating suitable training data and simply re-training the CNN.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Algorithm pipeline: 1) extract line segments; 2) map lines onto Gaussian sphere with inverse gnomonic projection (Sec. 2.1); 3) render image of halfsphere surface (Sec. 2.1); 4) compute CNN (Sec. 2.2) forward pass; 5) estimate a mixture of Gaussian distribution after CNN output (Sec. 2.4); 6) compute refined vanishing points and visualise line association (Sec. 2.4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Example from the synthetic training dataset (Sec. 2.3) with four vanishing directions. Different line colours denote different directions, with outlier lines shown in black. Left: 3D line segment plot. Right: 2D projection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>(a) Image from the KITTI [10] dataset, with a detected vanishing point (red dot) arising from the central perspective. (b) Rectified version of (a) after aligning the vanishing direction with the y-axis. The relative amount of space next to the cyclist can be measured easily. (c) Image 3 of the Spalentor in Basel with a virtual sign projected onto a wall after estimating K from detected vanishing points and aligning the central vanishing point with the x-axis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Parameters of our method used for all experiments.</figDesc><table><row><cell>Name N</cell><cell cols="6">Kinit ?prior k ? Nvp ?z ? hor</cell></row><row><cell cols="2">Section 2.1, 2.2 2.4</cell><cell>2.4</cell><cell cols="4">2.4 2.5 2.5 2.5</cell></row><row><cell>Value 20</cell><cell>25</cell><cell>? 1.282N</cell><cell>9</cell><cell>20</cell><cell>? 4</cell><cell>? 6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Source: https://www.flickr.com/photos/david-perez/4493470850</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements: This research was supported by German Research</head><p>Foundation DFG within Priority Research Programme 1894 Volunteered Geographic Information: Interpretation, Visualisation and Social Computing.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vanishing point detection without any a priori information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Almansa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desolneux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vamech</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="502" to="507" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A global approach for the detection of vanishing points and mutually orthogonal vanishing directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Antunes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Barreto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1336" to="1343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Geometric image parsing in man-made environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tretiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="57" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Interpreting perspective images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Barnard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="435" to="462" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Camera calibration using vanishing points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Beardsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="416" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Vanishing point detection with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.00967</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Manhattan world: Compass direction from a single image by bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Coughlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of the Seventh IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="941" to="947" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Single view metrology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="148" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient edge-based methods for estimating manhattan frames in urban imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Estrada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="197" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2013" />
			<publisher>IJRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Lsd: A fast line segment detector with a false detection control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Von Gioi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jakubowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Randall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="722" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recovering the spatial layout of cluttered rooms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hedau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th international conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1849" to="1856" />
		</imprint>
	</monogr>
	<note>Computer vision</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Video compass</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ko?eck?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="476" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Finding vanishing points via point alignments in image primal and dual domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grompone Von Gioi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Randall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="509" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A new approach to vanishing point detection in architectural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="647" to="655" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Atlanta world: An expectation maximization framework for simultaneous low-level edge grouping and camera calibration in complex manmade environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 IEEE Computer Society Conference on</title>
		<editor>I-I. IEEE</editor>
		<meeting>the 2004 IEEE Computer Society Conference on</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Non-iterative approach for fast and accurate vanishing point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Tardif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1250" to="1257" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Eye movements converge on vanishing points during visual search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ueda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kamakura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saiki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Japanese Psychological Research</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-similar sketch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2012</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="87" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Robust camera self-calibration from monocular images of manhattan worlds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wildenauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2831" to="2838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jacobs</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.02129</idno>
		<title level="m">Horizon lines in the wild</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A minimum error vanishing point detection approach for uncalibrated monocular images of man-made environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hoogs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1376" to="1383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Detecting vanishing points using global image context in a non-manhattan world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5657" to="5665" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
