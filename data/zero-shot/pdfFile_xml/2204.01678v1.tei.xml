<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MultiMAE: Multi-modal Multi-task Masked Autoencoders Masked inputs MultiMAE predictions Target Semantic Depth RGB Masked inputs MultiMAE predictions Target Masked inputs MultiMAE predictions Target</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Bachmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Swiss Federal Institute of Technology Lausanne (EPFL)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mizrahi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Swiss Federal Institute of Technology Lausanne (EPFL)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Atanov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Swiss Federal Institute of Technology Lausanne (EPFL)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zamir</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Swiss Federal Institute of Technology Lausanne (EPFL)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MultiMAE: Multi-modal Multi-task Masked Autoencoders Masked inputs MultiMAE predictions Target Semantic Depth RGB Masked inputs MultiMAE predictions Target Masked inputs MultiMAE predictions Target</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. MultiMAE pre-training objective. We randomly select 1/6 of all 16?16 image patches from multiple modalities and learn to reconstruct the remaining 5/6 masked patches from them. The figure shows validation examples from ImageNet, where masked inputs (left), predictions (middle), and non-masked images (right) for RGB (top), depth (middle), and semantic segmentation (bottom) are provided. Since we do not compute a loss on non-masked patches, we overlay the input patches on the predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We propose a pre-training strategy called Multi-modal Multi-task Masked Autoencoders (MultiMAE). It differs from standard Masked Autoencoding in two key aspects: I) it can optionally accept additional modalities of information in the input besides the RGB image (hence "multimodal"), and II) its training objective accordingly includes predicting multiple outputs besides the RGB image (hence "multi-task").</p><p>We make use of masking (across image patches and input modalities) to make training MultiMAE tractable as well as to ensure cross-modality predictive coding is indeed learned by the network. We show this pre-training strategy leads to a flexible, simple, and efficient framework with improved transfer results to downstream tasks. In particular, the same exact pre-trained network can be flexibly used when additional information besides RGB images is available or when no information other than RGB is availablein all configurations yielding competitive to or significantly better results than the baselines. To avoid needing training datasets with multiple modalities and tasks, we train * Equal contribution.</p><p>MultiMAE entirely using pseudo labeling, which makes the framework widely applicable to any RGB dataset.</p><p>The experiments are performed on multiple transfer tasks (image classification, semantic segmentation, depth estimation) and datasets (ImageNet, ADE20K, Taskonomy, Hypersim, NYUv2). The results show an intriguingly impressive capability by the model in cross-modal/task predictive coding and transfer. Code, pre-trained models, and interactive visualizations are available at https: //multimae.epfl.ch.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Masked Autoencoders (MAEs) <ref type="bibr" target="#b37">[35]</ref> have recently been demonstrated to be a powerful, yet conceptually simple and efficient, self-supervised pre-training strategy for Vision Transformers <ref type="bibr" target="#b26">[26]</ref> (ViTs). Their training objective is to mask-out a high number of patches in an input image and to predict the missing regions. To that end, only the small number of non-masked patches are first processed using a Transformer encoder <ref type="bibr" target="#b87">[85]</ref>, and then decoded with a light-weight Transformer that reconstructs the original image. To solve this task sufficiently well, it is assumed <ref type="bibr" target="#b37">[35]</ref> that the network needs to learn representations that capture more than just low-level image statistics.</p><p>So far, however, the MAE pre-training objective has been limited to a single modality, namely RGB images, and does not make use of any other modalities that are optionally present. In practice, often more than only a single modality of information is available, either through sensing (e.g., a depth sensor) or pseudo labeling (e.g., a powerful pretrained depth estimation network). Multi-modality is also argued to be employed by biological organisms to develop resilience and better representations <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b76">74]</ref>. As we demonstrate in our experiments, making use of such optionally present modalities has the potential to greatly improve the performance of downstream tasks, compared to using only RGB images.</p><p>Besides multi-modality (i.e., different inputs), multitaskness (i.e., different outputs) is an important aspect, as it has been shown that there is usually no single pretraining objective that transfers best to all possible downstream tasks <ref type="bibr" target="#b60">[58,</ref><ref type="bibr" target="#b72">70,</ref><ref type="bibr" target="#b101">99]</ref>. Instead, pre-training with a diverse set of tasks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b85">83]</ref> has been observed to improve the performance on downstream tasks <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b82">80]</ref> and potentially learn a better representation. In general, modifying the training objectives is a powerful way to steer what representation the model will learn.</p><p>In this paper, we present Multi-modal Multi-task Masked Autoencoders (MultiMAE), a simple and effective method to make masked autoencoding include multiple modalities and tasks (see <ref type="figure">Fig. 2</ref>). In particular, in our current instantiation of this general method, we study adding dense scene depth to capture geometric information, as well as segmentation maps to include information about the semantic content of the scene. We created a multi-task dataset by pseudo labeling these tasks on ImageNet-1K <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b31">31]</ref>. This has the advantage that in order to train a MultiMAE, one only requires a large unstructured RGB dataset without annotations and only off-the-shelf neural networks to perform the pseudo labeling.</p><p>To train MultiMAE, we randomly sample a small set of patches from different input modalities, and encode them using a Transformer encoder. MultiMAE's objective is then to reconstruct the masked-out patches of all tasks using task-specific decoders. <ref type="figure">Figure 1</ref> shows example predictions for the multi-task masked reconstruction that MultiMAE performs. MultiMAE has to learn not only the original MAE objective (within-RGB in-painting), but also to reconstruct any task from any input modality (cross-modal prediction) all from a very sparse set of input patches. The first objective leads to learning spatial predictive coding while the second one leads to cross-modal predictive coding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Masked image prediction consists of learning useful representations by learning to reconstruct images corrupted by masking. This approach was pioneered with denoising autoencoders <ref type="bibr" target="#b88">[86]</ref> and context encoders <ref type="bibr" target="#b66">[64]</ref>. With the introduction of Vision Transformers (ViT) <ref type="bibr" target="#b26">[26]</ref> and motivated by the success of BERT <ref type="bibr" target="#b24">[24]</ref> in NLP, many recent works propose a variety of masked image prediction methods for pre-training vision models in a self-supervised way, using reconstruction targets such as pixels <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b37">35,</ref><ref type="bibr" target="#b95">93]</ref>, discrete tokens <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b105">103]</ref>, and (deep) features <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b90">88]</ref>. These methods scale very well and achieve strong results on various downstream tasks including motor control <ref type="bibr" target="#b93">[91]</ref>. In particular, the masked autoencoder (MAE) <ref type="bibr" target="#b37">[35]</ref> approach accelerates pre-training by using an asymmetric architecture consisting of a large encoder that operates only on unmasked patches followed by a lightweight decoder that reconstructs the masked patches from the latent representation and mask tokens. Our approach leverages the efficiency of the MAE approach and extends it to multi-modal and multitask settings.</p><p>Multi-modal learning involves building models capable of relating information from multiple sources. It can either involve training separate encoders or one unified architecture (e.g., a Transformer <ref type="bibr" target="#b87">[85]</ref>) to operate on modalities such as images and text <ref type="bibr">[4, 14, 18, 39, 42-45, 56, 57, 76, 79, 94]</ref>, video and audio <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b43">41,</ref><ref type="bibr" target="#b62">60,</ref><ref type="bibr" target="#b64">62]</ref>, video, text and audio <ref type="bibr" target="#b2">[3]</ref>, and depth, images and video <ref type="bibr" target="#b33">[32]</ref>. Our work proposes a simple approach to pre-train Transformers on multiple dense visual modalities and produce strong cross-modal interaction. Unlike most prior work which assumes that all modalities are available during inference, our approach is designed to perform well on any subset of the pre-training modalities.</p><p>Related to MultiMAE are several works that perform multi-modal autoencoding <ref type="bibr" target="#b63">[61,</ref><ref type="bibr" target="#b74">72,</ref><ref type="bibr" target="#b79">77,</ref><ref type="bibr" target="#b80">78,</ref><ref type="bibr" target="#b91">89]</ref>. Our approach differs from them in that we use a more flexible architecture and perform masked autoencoding to learn cross-modal predictive coding among optional inputs (as demonstrated in <ref type="figure">Fig. 1</ref>).</p><p>Multi-task learning consists of training models to predict multiple output domains from a single input <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b48">46]</ref>. In computer vision, the input is usually an RGB image. A common approach for multi-task learning is to use a single encoder to learn a shared representation followed by multiple task-specific decoders <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b86">84]</ref>. These methods differ from our approach as we use multiple tasks in both the input and the output along with masking.</p><p>In addition, many works study the importance of task diversity to improve transfer performance <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b60">58,</ref><ref type="bibr" target="#b72">70,</ref><ref type="bibr" target="#b84">82,</ref><ref type="bibr" target="#b101">99]</ref>. These works argue that learning from one task alone is insufficient and that a set of tasks can more effectively cover the many possible downstream tasks in vision. Our pre- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MultiMAE pre-training Single-modal fine-tuning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-modal fine-tuning</head><p>Taskspecific head(s) <ref type="figure">Figure 2</ref>. (Left) MultiMAE pre-training: A small subset of randomly sampled patches from multiple modalities (e.g., RGB, depth, and semantic segmentation) is linearly projected to tokens with a fixed dimension and encoded using a Transformer. Task-specific decoders reconstruct the masked-out patches by first performing a cross-attention step from queries to the encoded tokens, followed by a shallow Transformer. The queries consist of mask tokens (in gray), with the task-specific encoded tokens added at their respective positions. (Right) Fine-tuning: By pre-training on multiple modalities, MultiMAE lends itself to fine-tuning on single-modal and multi-modal downstream tasks. No masking is performed at transfer time.</p><p>training method operates on multiple tasks to learn more general representations capable of covering multiple downstream tasks. Self-training is a technique to incorporate unlabeled data into a supervised learning setting <ref type="bibr" target="#b50">[48,</ref><ref type="bibr" target="#b71">69,</ref><ref type="bibr" target="#b73">71,</ref><ref type="bibr" target="#b98">96]</ref>. It is one of the earliest approaches to semi-supervised learning. Self-training methods use a supervised model to generate pseudo labels on unlabeled data and then train a student model on the pseudo labeled data. These approaches have been applied to a variety of vision tasks such as image classification <ref type="bibr" target="#b67">[65,</ref><ref type="bibr" target="#b94">92,</ref><ref type="bibr" target="#b97">95]</ref>, object detection <ref type="bibr" target="#b106">[104]</ref>, and segmentation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b106">104]</ref>. Most recently, multi-task self-training (MuST) <ref type="bibr" target="#b31">[31]</ref> uses specialized teachers to create a multi-task pseudo labeled dataset and then trains a multi-task student model on this dataset to learn general feature representations. Our method also relies on pseudo labeling to produce a large-scale multi-task dataset. However, unlike prior work, pseudo labels are not only used as output targets but also as masked input modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method Description</head><p>In this Section, we describe the Multi-modal Multi-task Masked Autoencoder (MultiMAE) architecture (illustrated in <ref type="figure">Fig. 2</ref>), as well as the pre-training strategy in more detail. We first give an architectural overview of both the multimodal encoder (Sec. 3.1) and multi-task decoders (Sec. 3.2). We then describe our multi-modal token sampling strategy (Sec. 3.3) and introduce the pseudo labeled tasks we use for pre-training (Sec. <ref type="bibr">3.4)</ref>. Finally, we display the most important pre-training details (Sec. 3.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-modal encoder</head><p>Our multi-modal Transformer encoder is a ViT <ref type="bibr" target="#b26">[26]</ref>, but with patch projection layers for each additional input modality. Specifically, 16?16 patches of each modality are projected to tokens with the correct Transformer dimension using a different linear projection for each modality. Projected patches are concatenated into a sequence of tokens and given as input to the same Transformer encoder. We also add an additional global token with a learned embedding, similar to the class-token used in ViT. Due to the architectural similarities to ViT, MultiMAE pre-trained weights can directly be used in a standard single-modal ViT by loading only the desired input projection and ignoring the others.</p><p>Positional, modality and class embeddings. Since all our modalities have a 2D structure, we add 2D sine-cosine positional embeddings <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b37">35]</ref> after the linear projection. We do not explicitly add any modality-specific embeddings, since the bias term in each linear projection can act as such. In order to perform the semantic segmentation patch projection, we first replace each class index with learned 64dimensional class embeddings.</p><p>Low computational complexity. Just as in the RGB-only MAE <ref type="bibr" target="#b37">[35]</ref>, we only pass the small randomly sampled subset of all tokens to the Transformer encoder as part of the masked autoencoding objective. This is in contrast to the masked autoencoding approaches of SiT <ref type="bibr" target="#b1">[2]</ref>, BeiT <ref type="bibr" target="#b8">[9]</ref> and SimMIM <ref type="bibr" target="#b95">[93]</ref>, that encode both the masked and visible tokens. Due to the quadratic complexity of standard self-attention as a function of the number of tokens, encoding only the random subset of visible tokens becomes increasingly important as the number of input modalities grows. Indeed, the speedup and reduction in memory are significant and crucial in enabling MultiMAE's multi-modal pretraining with three dense input modalities. A comparison of the pre-training time with and without masked tokens is given in the Appendix (Sec. G).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Decoders</head><p>To reconstruct the masked-out tokens from the visible tokens, we use a separate decoder for each task. The input to each decoder is the full set of visible tokens from the respective task it is reconstructing. As in MAE <ref type="bibr" target="#b37">[35]</ref>, these visible tokens are decoded jointly with a set of mask tokens, which serve as placeholders for the decoders to write the reconstructed patches (as shown in <ref type="figure">Fig. 2</ref>). To integrate information from the encoded tokens of other modalities, we add a single cross-attention layer in each decoder using these tokens as queries and all the encoded tokens as keys / values. Sine-cosine positional embeddings and learned modality embeddings are added to the tokens before this step. This is then followed by a small MLP and Transformer blocks. Following MAE, we compute the losses only on the masked tokens.</p><p>As each task requires its own decoder, the computational cost of decoders scales linearly with the number of tasks. To keep pre-training efficient, we use shallow decoders (a single cross-attention layer and MLP, followed by two Transformer blocks) with a low dimensionality (256 dimensional). Compared to the encoder, these decoders add little to the overall computational cost, and as He et al. <ref type="bibr" target="#b37">[35]</ref> show, they perform similarly to deeper decoders on ImageNet-1K fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-modal masking strategies</head><p>For masked autoencoding to work well, a large percentage of tokens needs to be masked-out. He et al. <ref type="bibr" target="#b37">[35]</ref> showed that the choice of mask sampling strategy can have a large impact on transfer performance. More specifically for MultiMAE and generally learning multi-task representations, masking across different modalities ensures the model develops predictive coding across different modalities besides different spatial patches. For efficiency and simplicity, we choose a constant number of visible tokens for all our experiments, which we fix at 98. This corresponds to 1/6 of all tokens when using three modalities of dimensions 224?224 pixels and a patch size of 16?16. Adapting the MAE mask sampling strategy by selecting the visible tokens uniformly from all tokens would result in most modalities being represented to similar degrees. Cases where one or more modalities have very few or no samples would be very rare. We propose a multi-modal token sampling strategy that allows for a more diverse sampling approach. It can be broken down into two steps: First, selecting the number of tokens per modality, and second, randomly sampling the set of tokens for each modality. Number of tokens per modality. We select the proportion of tokens per modality ? by sampling from a symmet-</p><formula xml:id="formula_0">ric Dirichlet distribution (? RGB , ? D , ? S ) ? Dir(?), where ? RGB + ? D + ? S = 1, ? ? 0.</formula><p>The sampling is controlled by the concentration parameter ? &gt; 0. When ? = 1, the symmetric Dirichlet distribution is equivalent to a uniform distribution over the simplex (i.e., it is uniform over all points in its support). Smaller values (? &lt;&lt; 1) result in a sampling behavior where most of the tokens will be sampled from a single modality, while larger values (? &gt;&gt; 1) result in an increasingly similar number of tokens to be sampled from each modality. As a design decision, we do not bias the sampling towards certain modalities (as we use a symmetric Dirichlet), since we want to be agnostic to the choice of downstream input modalities and tasks that users might want to consider. For simplicity and better representation of any possible sampled mask, we use a concentration parameter ? = 1 for all of our experiments. Random masks sampled using ? = 1 are shown in <ref type="figure" target="#fig_0">Figure 3</ref>, and an ablation on the choice of concentration parameter is given in the Appendix (Sec. C). Sampling tokens. From each modality, we sample the number of tokens, as specified by the above Dirichlet sampling step, uniformly at random without replacement. Uniform sampling has been shown to work well for masked autoencoders, compared to less random alternatives <ref type="bibr" target="#b37">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Pseudo labeled multi-task training dataset</head><p>We pre-train MultiMAE with three tasks that we pseudo label on ImageNet-1K <ref type="bibr" target="#b23">[23]</ref>. Pseudo labeling has the advantage that we do not need a large multi-task dataset with aligned task images. Instead, having access to a good set of pre-trained neural networks for the tasks we want to train on can be effective. Pseudo labeling scales to RGB datasets of arbitrary size and is a one-time pre-processing step. Compared to the cost of training, this step is computationally cheap and fast if parallelized.</p><p>Taskonomy <ref type="bibr" target="#b101">[99]</ref> demonstrated computationally that common vision tasks cluster into three main categories, namely low-level, geometric, and semantic tasks. To have a coverage over such a space of vision tasks, we choose one representative task from each of these three clusters. We note that except for object detection and classification, these are the same pseudo labeled tasks that are used in MuST <ref type="bibr" target="#b31">[31]</ref>. In the following, we will describe them in more detail. RGB and per-patch standardized RGB. We use RGB images due to their abundance and since RGB-only masked autoencoding is shown to be a powerful pre-training task. He et al. <ref type="bibr" target="#b37">[35]</ref> study both predicting standard RGB patches, as well as per-patch standardized RGB patches. They find that predicting standardized patches slightly improves transfer performance. Since MultiMAE is naturally a multitask model, we add both versions as separate decoder heads to get the representational benefits of predicting standardized patches, and to get a version that we can visualize better. Note that we only add the per-patch standardized version as an output task, and not as an input modality. For both RGB versions, we follow MAE and compute the MSE loss between the ground truth and predicted pixels. In the rest of the paper, we will refer to the RGB and per-patch standardized RGB output tasks simply as RGB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Depth</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Depth</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Masked inputs Predictions Masked inputs Predictions Masked inputs Predictions Original Masked inputs Predictions Masked inputs Predictions Masked inputs Predictions</head><p>Scene depth. Depth is a key task informative about scene geometry. As with RGB, but unlike semantic segmentation, sensors exist to capture this modality, making it possible to use depth as an optional extra input for downstream tasks.</p><p>To pseudo label depth, we use a DPT-Hybrid <ref type="bibr" target="#b68">[66]</ref> that was trained on Omnidata <ref type="bibr" target="#b27">[27]</ref>. Since monocular depth estimation is an inherently ill-posed task due to scale and shift ambiguity, we standardize the depth values in a robust way by ignoring the top and bottom 10% of values <ref type="bibr" target="#b99">[97]</ref>. In addition, using standardized depth values as inputs allows us to use other depth images that might have different depth ranges and scales, without needing to match them to the Omnidata depth parameterization. We use the L1 loss for depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic segmentation.</head><p>Lastly, we use a Mask2Former <ref type="bibr" target="#b19">[19]</ref> with a Swin-S <ref type="bibr" target="#b54">[52]</ref> backbone trained on COCO <ref type="bibr" target="#b53">[51]</ref> to pseudo label semantic segmentation maps on ImageNet. For that, we extract 133 semantic classes by taking the argmax of the network predictions. Unlike RGB and depth, the main purpose of this task is to improve performance on downstream tasks, rather than using it as an input modality (though we show results using pseudo labeled semantic inputs in <ref type="table">Table 3</ref>). Since we use a network that was pre-trained on COCO, we do not evaluate semantic segmentation transfers on that dataset. For this task, we use the cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Pre-training details</head><p>All our MultiMAE experiments use a ViT-B <ref type="bibr" target="#b26">[26]</ref> with a patch size of 16?16 pixels. We pre-train the models for either 400 epochs (only for transfer ablation study in Sec. <ref type="bibr">4.4)</ref> or 1600 epochs (for best results and to be comparable to the MAE baseline) on 1.28M ImageNet images. We use the AdamW <ref type="bibr" target="#b57">[55]</ref> optimizer with base learning rate 1e-4 and weight decay 0.05. We warm up training for 40 epochs, starting from learning rate 1e-6, and decay it to 0 over the course of training using cosine decay <ref type="bibr" target="#b56">[54]</ref>. We set the batch size to a total of 2048 and train the models using 8 A100 GPUs with automatic mixed precision enabled. Our data augmentations are straightforward. We randomly crop the images, setting the random scale between 0.2 and 1.0 and the random aspect ratio between 0.75 and 1.33, after which we resize the crops to 224?224 pixels and apply a random horizontal flip with probability 0.5. Additional pre-training details can be found in the Appendix (Sec. A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Optimizing the pre-training objective of MultiMAE is successful as apparent in the various results shown in the main paper, the appendix, and the interactive visualizations shown on our website. In this section we provide a transfer study to measure the effectiveness of MultiMAE pretraining compared to relevant baselines. This section is organized in the following manner: After introducing the downstream tasks and datasets (Sec. 4.1), we show transfer results for the case where the only available input modality is RGB (Sec. 4.2). Then, we show that MultiMAE can significantly improve downstream performance if other modalities like depth are either available as ground truth (sensor), or can be cheaply pseudo labeled (Sec. 4.3). We follow up with an ablation on the influence of pre-training tasks on the downstream performance (Sec. 4.4), and finally we visually demonstrate that MultiMAE integrates and exchanges information across modalities (Sec. 4.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Transfer tasks and datasets</head><p>We perform downstream transfers on a variety of semantic and dense regression tasks. For all transfers, we replace the pre-trained decoders by randomly initialized taskspecific heads, and train them along with the pre-trained encoder. In the following, we give an overview over all tasks and datasets used in our transfer experiments. Exact training details are presented in the Appendix (Sec. B). Classification. We evaluate our models and baselines by fine-tuning them on the supervised ImageNet-1K <ref type="bibr" target="#b23">[23]</ref> 1000-way object classification task. We fine-tune our models for 100 epochs on the entire ImageNet-1K train split (1.28M images) and report the top-1 validation accuracy. Semantic segmentation. We further evaluate our models on semantic segmentation tasks on the ADE20K <ref type="bibr">[</ref>  <ref type="bibr" target="#b25">[25]</ref>, while on Taskonomy we report L1 losses on the tiny-split test set. In the tables, classification, semantic segmentation, and depth estimation are denoted by (C), (S), and (D), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Transfers with RGB-only</head><p>In this section, we show our transfer results when finetuning using only the RGB modality as input.</p><p>Baselines. For this setting, we compare MultiMAE with various ViT-B models, namely DeiT <ref type="bibr" target="#b83">[81]</ref> (without distillation) representing an ImageNet-supervised baseline, MoCo-v3 <ref type="bibr" target="#b16">[17]</ref>, DINO <ref type="bibr" target="#b11">[12]</ref>, and MAE <ref type="bibr" target="#b37">[35]</ref>. All these models are pre-trained on ImageNet-1K. We use the official weights for DeiT, MoCo-v3, and DINO, and reproduce MAE using the official PyTorch <ref type="bibr" target="#b65">[63]</ref> codebase following the setting specified in <ref type="bibr" target="#b37">[35]</ref> (i.e., decoder of depth 8 and width 512, per-patch standardized pixel loss, 1600 pre-training epochs, 75% mask ratio). In the Appendix (Sec. F), we compare the transfer performance of this MAE model to one with a shallower and narrower decoder (depth 2 and width 256), closer to the one used for MultiMAE.</p><p>We report the results in <ref type="table">Table 1</ref>. We find that MultiMAE performs best on all tasks, matching MAE's performance on ImageNet-1K classification and ADE20K semantic segmentation, and outperforming it on all other tasks and datasets. These results show the effectiveness of MultiMAE as a pre-training strategy: it retains the benefits of MAE when RGB is the only fine-tuning modality but can also accept other modalities, as shown next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Transfers with multiple modalities</head><p>Since MultiMAE was pre-trained on RGB, depth, and semantic segmentation, it can optionally accept any of those modalities as input during transfer learning should they be available. In this set of experiments, we study on three semantic segmentation downstream tasks how much MultiMAE can benefit from using additional modalities during transfer. Often, ground truth depth maps are not available for a given downstream dataset and for that reason, we perform additional transfers using pseudo labeled depth. As there are several datasets that do in fact contain aligned RGB and depth images (e.g., Hypersim, NYUv2, Taskonomy, etc.) and since sensors exist that can measure depth, we consider it as a more realistic input modality compared to semantic segmentation. Since our model was trained with semantic segmentation as an input modality, we perform additional experiments using pseudo labeled semantic segmentation maps as inputs.</p><p>All multi-modal transfers are performed by concatenating the projected patches of all modalities into a single sequence (i.e., no masking is performed here). Using more   <ref type="table">Table 3</ref>. Fine-tuning with RGB and pseudo labels. Semantic segmentation transfer results using pseudo labeled depth and semantic segmentation maps, measured in mIoU (?). MultiMAE benefits much more than MAE from pseudo labeled modalities as input. Text in gray indicates a modality that the model was not pre-trained on.</p><p>than two modalities during transfer quickly becomes computationally expensive, since without masking, our method now scales with the full number of modalities and tokens. For performing multi-modal transfers with the standard MAE, we train a new input projection for the additional modalities while fine-tuning. Further training details can be found in the Appendix (Sec. B). Transfers using sensory depth. First, we consider that we have access to an aligned RGB-D dataset, like NYUv2 or Hypersim. We treat depth in the exact same way as during pre-training, i.e., pre-process it by standardizing it in a robust manner <ref type="bibr" target="#b99">[97]</ref>. Because ground-truth depth maps might contain invalid measurements, we further set all these masked-out values to 0. <ref type="table">Table 2</ref> shows RGB-D transfer results on Hypersim and NYUv2. Compared to the RGB-only results in <ref type="table">Table 1</ref>, we see a substantial increase in performance when ground truth depth is available for MultiMAE. The standard MAE on the other hand is not able to sufficiently make use of the additional depth, since it was only trained on RGB images. We observe a similar story when evaluating transfers from depth-only, in that MultiMAE works well, even when no RGB information is available, while MAE does not. On Hypersim, MultiMAE depth-only transfer is even able to surpass MultiMAE RGB-only transfer, and, as expected, RGB-D works better than either RGB or depth alone. Transfers with pseudo labels. In case ground truth modalities are not available, we can pseudo label them in the same way we did for pre-training. To pseudo label depth, we use the same Omnidata DPT-Hybrid model that we used for pre-training on both ADE20K and NYUv2. On Hypersim, we use a MiDaS <ref type="bibr" target="#b69">[67]</ref> DPT-Hybrid, since the Omnidata depth model was partially trained on this dataset. For semantic segmentation pseudo labels, we use the same COCO Mask2Former model as in pre-training.</p><p>As shown in <ref type="table">Table 3</ref>, MultiMAE can use pseudo labeled depth or semantic segmentation to boost performance beyond the RGB-only setting, although the gain is smaller than using real depth. Moreover, performance can further be improved by adding both of these pseudo labeled modalities to the input. This setting performs the best out of all settings involving pseudo labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Influence of pre-training task choices and masking on transfer performance</head><p>How does the choice of MultiMAE pre-training tasks affect downstream transfer performance? In this subsection, we aim to address this question by performing transfers from MultiMAE models that were pre-trained with RGB-D, RGB-S, or RGB-D-S. We further compare MultiMAE against MAE, single-task, and multi-task baselines.</p><p>All experiments are performed on ViT-B models that were pre-trained for 400 epochs. We transfer the pre-trained models to ImageNet, NYUv2 segmentation, as well as nine dense regression tasks on Taskonomy. On Taskonomy, we report the ranking of different pre-trained models, averaged over all nine tasks. Detailed per-task results on Taskonomy can be found in the Appendix (Sec. D). Masked multi-modal pre-training. This experiment studies the influence that the choice of pre-training modalities has, when the input and output modalities are the same in MultiMAE pre-training. The transfer results are displayed in (b) Comparison to non-masked pre-training. We compare standard singletask and multi-task baselines pre-trained using non-masked RGB inputs against the RGB-D-S MultiMAE. The RGB?D-S model is conceptually similar to MuST using depth and semantic segmentation as target tasks. <ref type="table">Table 4</ref>. Ablation experiments. We study the impact of additional modalities in <ref type="table" target="#tab_3">Table 4a</ref>, and compare MultiMAE to non-masked pre-training in <ref type="table">Table 4b</ref>. All models are pre-trained for 400 epochs. We report the top-1 accuracy (?) on ImageNet-1K (IN-1K) <ref type="bibr" target="#b23">[23]</ref> classification (C), mIoU (?) on NYUv2 <ref type="bibr" target="#b75">[73]</ref> semantic segmentation (S), ?1 accuracy (?) on NYUv2 depth (D) and avg. rank ? on Taskonomy <ref type="bibr" target="#b101">[99]</ref>. While some specialized pre-trained models perform better at certain downstream tasks, they perform poorly at others. MultiMAE pre-trained with RGB, depth and semantic segmentation is a more generalist model that does well at transferring to a range of downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB input Depth prediction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic prediction</head><p>Depth input RGB prediction the RGB-D-S model has the best average rank on Taskonomy. The slight increase in performance of RGB-S on Im-ageNet and semantic segmentation compared to RGB-D-S comes at the cost of reduced flexibility, as models that were not pre-trained on depth can not as easily and effectively use it to boost performance (see Sec. 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic prediction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic input</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB prediction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Depth prediction</head><p>Comparison to non-masked pre-training. We further compare MultiMAE against standard single-task and multitask baselines, that were pre-trained with RGB as the only input modality and without applying any masking. Since we train on pseudo labels, the RGB?D-S multi-task model is conceptually similar to a MuST [31] model using depth and semantic segmentation targets. The transfer results are detailed in <ref type="table">Table 4b</ref>. On nearly all categories, MultiMAE outperforms the supervised baselines.</p><p>To summarize, the results in this section show that using all modalities to pre-train a MultiMAE results in a more generalist model that does well at transferring to a range of downstream tasks. We find that there are some specialized pre-trained models that perform better at certain downstream tasks (e.g., models pre-trained with depth perform better at transferring to geometric tasks), but they will perform poorly at others. This is supported by previous findings <ref type="bibr" target="#b60">[58,</ref><ref type="bibr" target="#b72">70,</ref><ref type="bibr" target="#b101">99]</ref> showing that there is usually no single visual pre-training task that transfers well to any arbitrary other task, and instead, a set is required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Cross-modal exchange of information</head><p>In this section, we explore visually how MultiMAE predicts the three pre-training tasks by changing the inputs it receives. <ref type="figure" target="#fig_0">Figures 1 and 3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>already showcased how</head><p>MultiMAE is able to reconstruct images from various randomly sampled input patches. Here, we will further show non-masked cross-modal predictions, and will also give examples on how MultiMAE predictions change when we change certain details about the inputs.  Single-modal predictions. <ref type="figure" target="#fig_2">Figure 4</ref> displays several examples of cross-modal prediction without any masking. We show examples where, from one single modality, the two remaining ones are predicted. We note here that even though the number of patches we input to the model is 2? higher than what was seen during training, the model still predicts very reasonable results despite the distribution shift. The robustness of the model with respect to masking ratios far from the training mask ratio is also apparent in <ref type="figure">Figure 6</ref>. Demonstration of cross-modal interaction. We demonstrate in <ref type="figure">Figure 5</ref> how MultiMAE predicts completely different but plausible RGB images when given a full depth image and three edited versions of the same two RGB input patches (no semantic segmentation maps are given as inputs). We keep one RGB patch the same, while changing the hue of another patch (part of a lizard for the first image, part of an orange for the second). We can see how MultiMAE recovers all the details in the image from the full depth input, but paints the entire lizard / oranges in the colors given in the modified patch. All the while, the background does not change. This suggests an intriguingly good representation is learned by the model as it extends the colors to the right segments without any segmentation provided in the input. More interactive examples can be seen on our website.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>We presented Multi-modal Multi-task Masked Autoencoders (MultiMAE), an effective and simple pre-training strategy for Vision Transformers. MultiMAE encodes a small random subset of visible tokens from multiple modalities and is trained to reconstruct the missing ones. By encoding only a fixed number of non-masked tokens, we can keep the bulk of the computation in the Transformer encoder constant, while only the shallow task-specific decoders scale with the number of tasks. Masking (across image patches and input modalities) ensures the network learns to perform predictive coding across different modalities, besides across different spatial patches. The experiments showed intriguing capabilities of MultiMAE at crossmodal coding and demonstrated this pre-training strategy can result in notable gains in transfer performance when additional input modalities are optionally available, either as ground truth or pseudo labels.</p><p>In the following, we briefly discuss some limitations to our approach and present exciting future directions:</p><p>Scaling pre-training modalities.</p><p>We pre-trained MultiMAE on a set of three visual modalities, chosen to cover a large fraction of common vision problems based on prior studies <ref type="bibr" target="#b101">[99]</ref>. It is, however, conceivable that our method can benefit from a rather straightforward inclusion of a more diverse set of modalities and tasks, such as videos, text, bounding boxes, sparse depth, feature maps, and more. In addition to providing more ways to use optional modalities as inputs, scaling up the number of pre-training modalities could have further transfer benefits by covering a larger space of useful vision problems and enabling more complex cross-modal predictive coding.</p><p>Scaling pre-training datasets. For pragmatic reasons and enabling comparison with prior works, we trained all of our models on pseudo labeled ImageNet-1K, but there is no reason to limit ourselves to a (classification) dataset of this size. Since we use pseudo labels, any dataset that is used for RGB-only self-supervised learning can be considered for training MultiMAE. Our method further benefits from any future improvements in model architectures, training strategy and supervised datasets that can be used to improve the quality of pseudo labels.</p><p>Probabilistic or generative modeling. Similar to standard autoencoders and MAE <ref type="bibr" target="#b37">[35]</ref>, we simply compute a pixelwise L1 or L2 loss on the reconstructed tokens. It is unsurprising then that ambiguous masked regions are often predicted in a blurry manner due to the inherent ambiguity in the problem when multiple outputs are plausible. While He et al. <ref type="bibr" target="#b37">[35]</ref> showed that improving the visual fidelity of MAE predictions might not necessarily result in better representations for downstream learning, it is conceivable that modeling the multi-modal output distribution may learn better representations.</p><p>Masking strategies. Lastly, we used a simple approach of sampling random tokens from each modality in an unbiased way. While this worked well for MultiMAE training, it does not have to be the optimal choice for learning a transferable representation. It will be an interesting direction to explore biasing the masking towards certain modalities and/or spatial locations. A. Additional pre-training implementation details</p><p>We report the default pre-training setting in <ref type="table" target="#tab_7">Table 5</ref>. The learning rate follows the linear scaling rule <ref type="bibr" target="#b34">[33]</ref>: lr = base lr ? batchsize/256. The number of non-masked tokens given to the encoder is set to 49 when using a single input modality (mask ratio of 3/4), and 98 when using 2 or 3 modalities (mask ratio of 3/4 and 5/6, respectively). Furthermore, given that the semantic segmentation map consists of 64-dimensional class embeddings, naively projecting each patch to a token is computationally expensive (when flattened, each patch would have a dimension of 16384 and the projection layer would have approx. 12M parameters). To make this projection efficient while keeping * Equal contribution. the number of segmentation patches constant, we downsample the semantic segmentation input by a factor of 4 and use patches of size 4?4. MultiMAE decoder. We illustrate the MultiMAE decoder in <ref type="figure">Fig 7.</ref> Following MAE <ref type="bibr" target="#b37">[35]</ref>, each decoder has a linear projection layer to adapt the outputs from the encoder to the decoder dimension. After this linear projection, we add both sine-cosine positional embeddings and learned modality embeddings to the decoder inputs. This is then followed by a cross-attention layer, a MLP, and two Transformer blocks. <ref type="figure">Figure 7</ref>. MultiMAE decoders: Tokens from the MultiMAE encoder (see <ref type="figure">Fig. 2</ref>) are first linearly projected to the decoder dimension, after which positional and modality-specific embeddings are added. A cross-attention step integrates information from tokens of other modalities before applying an MLP and two Transformer blocks. Finally, each token is projected and reshaped to form an image. In this illustration, each token expands into four pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Transfer implementation details B.1. ImageNet classification fine-tuning setting</head><p>For ImageNet-1K <ref type="bibr" target="#b23">[23]</ref> classification, we follow the endto-end fine-tuning procedure from MAE <ref type="bibr" target="#b37">[35]</ref> and replace the decoders by an average pooling operation over all encoded tokens, followed by LayerNorm <ref type="bibr" target="#b6">[7]</ref> and a linear projection. The default setting is shown in <ref type="table">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Semantic segmentation</head><p>The typical approach <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b37">35]</ref> to fine-tuning Vision Transformers for semantic segmentation is not suited for multi-  1.0 Drop path <ref type="bibr" target="#b42">[40]</ref> 0.1 <ref type="table">Table 6</ref>. ImageNet-1K classification setting. We follow the finetuning settings from MAE <ref type="bibr" target="#b37">[35]</ref>.</p><p>modal inputs in two aspects: 1) the segmentation head and 2) the evaluation procedure. We cover these two aspects next and propose a simplified fine-tuning setting for semantic segmentation to overcome these issues.</p><p>Segmentation head. The UPerNet <ref type="bibr" target="#b92">[90]</ref> head used in BEiT <ref type="bibr" target="#b8">[9]</ref> and MAE <ref type="bibr" target="#b37">[35]</ref> operates on a feature pyramid <ref type="bibr" target="#b52">[50]</ref>. While a Vision Transformer operating only on RGB images can be modified to return hierarchical feature maps through the use of deconvolution layers on intermediate features <ref type="bibr" target="#b103">[101]</ref>, this procedure is not so simple when the input is multi-modal. In contrast, segmentation heads that operate only on the output tokens do not have this issue. One such head is the Segmenter <ref type="bibr" target="#b77">[75]</ref>, for which tokens are passed through additional Transformer blocks, then reshaped into a feature map and upsampled to full resolution. However, the direct upsampling can result in inprecise segmentation maps and hurt performance. Instead, we propose using a simple segmentation head based on the ConvNeXt architecture <ref type="bibr" target="#b55">[53]</ref>. First, we increase the dimensionality D of the output tokens wih a linear projection, and then reshape the tokens to form a feature map of size H/4?W/4?D/8. We then apply 4 ConvNeXt blocks on this feature map before upsampling it to full resolution using bilinear interpolation. We find that this simple ConvNeXt head outperforms Segmenter, as shown in <ref type="table">Table 7</ref>. To adapt this head to multimodal inputs, we can either select only the output tokens from a single modality (as information from other modalities gets passed to these tokens through self-attention) or concatenate tokens from different modalities. We find that both approaches perform comparably and select the former  <ref type="table">Table 7</ref>. Comparison of semantic segmentation heads. We report the mIoU (?) on ADE20K <ref type="bibr" target="#b104">[102]</ref>, Hypersim <ref type="bibr" target="#b70">[68]</ref> and NYUv2 <ref type="bibr" target="#b75">[73]</ref>. The proposed segmentation head based on the Con-vNeXt <ref type="bibr" target="#b55">[53]</ref> architecture performs on average slightly better than Segmenter <ref type="bibr" target="#b77">[75]</ref>.</p><p>as it is slightly more efficient.</p><p>Evaluation procedure. Vision Transformers are commonly evaluated using the sliding window procedure from MMSegmentation <ref type="bibr" target="#b61">[59]</ref> (e.g., <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b103">101,</ref><ref type="bibr" target="#b105">103]</ref>). This procedure involves first resizing the validation images so that the smallest side matches the training resolution * , and then applying a sliding window over the resized image and averaging predictions across windows. However, this procedure is not suitable if the input modalities rely on statistics from the entire image (e.g., standardized depth) or do not have a 2D structure (e.g., object bounding boxes). Therefore, we use a simpler evaluation procedure inspired by <ref type="bibr" target="#b51">[49]</ref>, which consists of resizing the image so that the largest side matches the training resolution and padding the smallest side. As the evaluated images have a smaller resolution, this simple procedure results in slightly worse reported performance compared to sliding windows. However, it can be used regardless of the input modalities and thus allows for a more fair comparison of segmentation performance for different * In some implementations, the height is resized to the training resolution, which most often coincides with the smallest side.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ADE20K Hypersim NYUv2</head><p>Optimizer AdamW <ref type="bibr" target="#b57">[55]</ref> Learning rate 1e-4 Layer-wise lr decay <ref type="bibr" target="#b20">[20]</ref> 0  <ref type="bibr" target="#b42">[40]</ref> 0.1 <ref type="table">Table 8</ref>.</p><p>Semantic segmentation fine-tuning settings for ADE20K <ref type="bibr" target="#b104">[102]</ref>, Hypersim <ref type="bibr" target="#b70">[68]</ref> and NYUv2 <ref type="bibr" target="#b75">[73]</ref>.</p><p>modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training details.</head><p>The semantic segmentation transfer settings for all three segmentation datasets are shown in <ref type="table">Table 8</ref>.</p><p>Following <ref type="bibr" target="#b51">[49]</ref>, our main augmentation is large scale jittering (LSJ) <ref type="bibr" target="#b30">[30]</ref>.</p><p>We also apply color jittering with the following parameters: brightness=0.4, contrast=0.4, satu-ration=0.2, hue=0.1, p=0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. NYUv2 depth estimation</head><p>For depth estimation on the NYUv2 dataset. <ref type="bibr" target="#b75">[73]</ref>, we resize all images from 640 ? 480 to 341 ? 256. During training, we randomly crop the images to 256 ? 256 and during testing, we take a central crop of size 256 ? 256.</p><p>We follow <ref type="bibr" target="#b35">[34,</ref><ref type="bibr" target="#b49">47]</ref> and apply color jittering with the following parameters: brightness=0.1255, contrast=0.4, saturation=0.5, hue=0.2, p=0.5. We also randomly turn the image into gray-scale with probability p = 0.3.</p><p>We use the DPT <ref type="bibr" target="#b68">[66]</ref> head to decode layers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12]</ref> of the ViT-B encoder into the dense depth map. For training, we use the reverse Huber loss <ref type="bibr" target="#b49">[47]</ref>. Detailed transfer settings are given in <ref type="table">Table 9</ref>. For evaluation, we measure the ? 1 metric on the test set, showing the percentage of pixels p with error max{? p yp , yp yp } less than 1.25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Taskonomy dense regression tasks</head><p>We transfer to the following eight dense regression tasks from the Taskonomy <ref type="bibr" target="#b101">[99]</ref>   <ref type="bibr" target="#b42">[40]</ref> ? 0.1 <ref type="table">Table 9</ref>. Fine-tuning settings for NYUv2 <ref type="bibr" target="#b75">[73]</ref> depth estimation and eight Taskonomy <ref type="bibr" target="#b101">[99]</ref> 2D regression tasks.</p><p>split (54514 images), using the checkpoint with the lowest validation loss. For training and testing, all images are resized to 384 ? 384 and we perform no further augmentations. As for NYUv2 <ref type="bibr" target="#b75">[73]</ref> depth estimation, we use the DPT <ref type="bibr" target="#b68">[66]</ref> head, accessing layers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12]</ref> from the ViT-B encoder. All tasks are trained with an L1 loss. Detailed transfer settings are given in <ref type="table">Table 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Mask sampling strategies</head><p>We sample the number of non-masked tokens per modality using a Dirichlet distribution with concentration parameter ? = 1. <ref type="figure">Figure 8</ref> illustrates the sampling behavior under different ? values. For simplicity, we picked ? = 1 for all our experiments in the main paper, which exposes the models to a large diversity of masks. Samples using ? = 1 include cases where all tokens are sampled from a single modality (very low ?) and MultiMAE has to fully reconstruct the other two, cases where all modalities are equally represented (very high ?), and everything in between.</p><p>In <ref type="table" target="#tab_11">Table 10</ref>, we show transfer results on ImageNet-1K <ref type="bibr" target="#b23">[23]</ref> classification and ADE20K <ref type="bibr" target="#b104">[102]</ref> semantic segmentation using MultiMAE models trained with ? ? {0.2, 0.5, 1.0, ?}. By ? = ?, we denote always sampling an equal number of tokens from each modality. All models in this table were trained for 400 epochs and do not include the additional per-patch-standardized RGB head (see Sec. 3.4). Setting ? = 1 performs best on ADE20K, while being close second on ImageNet-1K behind ? = ?. Smaller values of ? do not perform better on these two RGB-only downstream tasks, even though during training they were exposed to more samples that contain tokens from only one modality. Biasing the sampling towards modalities that will be used during transfer is an interesting future direction.  <ref type="table">Table 4</ref> compared several baselines by their average rank on eight different Taskonomy <ref type="bibr" target="#b101">[99]</ref> downstream tasks. In this section, we show per-task results of all these baselines. <ref type="table" target="#tab_2">Table 12</ref> shows detailed results for the ablation on the choice of MultiMAE pre-training tasks, while <ref type="table">Table 13</ref> shows results for the comparison to single-task and multitask baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Detailed Taskonomy transfer results</head><p>Out of these eight Taskonomy tasks, the edges and 2Dkeypoints task labels were originally created from RGB images, while the other tasks were rendered from the scanned scene mesh. A pre-training scheme that includes depth should thus transfer better to the depth-related tasks, such as surface normals. Indeed, we observe this in <ref type="table" target="#tab_2">Table 12</ref>, where MultiMAE pre-trained using depth transfer better than MAE or the RGB-S MultiMAE. Importantly, additionally including semantic segmentation along RGB and depth in the pre-training does not degrade performance on these tasks.</p><p>In <ref type="table">Table 13</ref>, we see that MultiMAE performs similarly to the single-task RGB?D baseline that was trained using full RGB inputs. For the single and multi-task baselines, the right choice of pre-training task(s) is crucial, as for example the RGB?S baselines performs consistently worse than the ones including depth, as well as the MultiMAE RGB-S baseline from <ref type="table" target="#tab_2">Table 12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Robustness evaluation on ImageNet</head><p>We study the robustness of the ImageNet <ref type="bibr" target="#b23">[23]</ref> fine-tuned models by evaluating them on four different ImageNet-like validation sets <ref type="bibr" target="#b38">[36]</ref><ref type="bibr" target="#b39">[37]</ref><ref type="bibr" target="#b40">[38]</ref><ref type="bibr" target="#b89">87</ref>] that contain various domainshifts and corruptions, and we show the results in <ref type="table" target="#tab_12">Table 11</ref>. To that end, we directly use the models that were fine-tuned on ImageNet-1K classification, and evaluate them without any modifications on the respective robustness evaluation datasets. MultiMAE performs better than all baselines of the same model size (ViT-B) on ImageNet-R and ImageNet-S. It also performs better than MAE on ImageNet-C, but falls behind DINO <ref type="bibr" target="#b11">[12]</ref> and MoCo-v3 <ref type="bibr" target="#b16">[17]</ref>. On ImageNet-A, MultiMAE performs worse than DINO and MAE, but better than the supervised and MoCo-v3 baselines. </p><formula xml:id="formula_1">Method IN-1K ? IN-A ? IN-C ? IN-R ? IN-S</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Comparison of MAE variants</head><p>In Section 4.2, we compare MultiMAE to a pre-trained MAE with a decoder of depth 8, following the bestperforming setting described in <ref type="bibr" target="#b37">[35]</ref>. However, as our MultiMAE uses shallower and narrower decoders, we also pre-train MAE with a decoder of similar depth (2) and width (256). We compare these two MAE versions in <ref type="table" target="#tab_15">Table 14</ref>. We find that while these two models perform comparably on ImageNet-1K classification, as reported in <ref type="bibr" target="#b37">[35]</ref>, using a deeper decoder leads to a stark increase in performance for all other tasks. Given the benefits of a larger decoder for  <ref type="table">Table 13</ref>. Taskonomy transfer results comparing pre-trained single-task and multi-task baselines (pre-trained using non-masked RGBonly inputs) against the RGB-D-S MultiMAE. Downstream transfers are trained from RGB-only. All models were pre-trained for 400 epochs. We report L1 losses (?) and indicate with bold and underline the best and second-best results, respectively.</p><p>MAE, it stands to reason that MultiMAE could also benefit from using wider and deeper decoders, even though that would significantly increase pre-training time.</p><p>Furthermore, it has been observed that MAE models pre-trained using the official PyTorch <ref type="bibr" target="#b65">[63]</ref> implementation (such as ours) do not exactly match the results of a MAE trained using the original (and unavailable) TensorFlow <ref type="bibr" target="#b0">[1]</ref> implementation ? . Therefore, we also report results using model weights from the TensorFlow implementation to assess the impact of the codebase on transfer performance. We observe minor differences in transfer performance, with the original TensorFlow implementation slightly outperforming the PyTorch implementation on all tasks.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Comparison of pre-training time</head><p>We report the pre-training epoch time in <ref type="table" target="#tab_7">Table 15</ref>. By using shallow decoders, the training time of MultiMAE is comparable to MAE (with a decoder of depth 8) despite having twice the amount of unmasked tokens and multiple decoders. Note that removing masked tokens from the encoder, as proposed by MAE, is crucial in enabling pretraining on multiple dense modalities.   <ref type="figure">Figure 10</ref> further shows predictions where we sample three random masks for each image.  <ref type="figure">Figure 10</ref>. MultiMAE predictions on ImageNet-1K validation set samples. 98 visible patches were sampled using Dirichlet concentration parameter ? = 1. For each image, we sample three random masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Additional visualizations</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>MultiMAE predictions for several randomly sampled masks. For each ImageNet validation image, we randomly sample three masks using Dirichlet concentration parameter ? = 1. Only 1/6 of total patches are left unmasked. Even when very few tokens from one modality are visible, the resulting predictions are relatively stable and plausible due to cross-modal interaction. More examples are shown in the Appendix and on our website.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Method IN-1K (C) ADE20K (S) Hypersim (S) NYUv2 (S) NYUv2 (D)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Single-modal predictions. We visualize MultiMAE cross-modal predictions on ImageNet-1K validation images. Only a single, full modality is used as input. The predictions remain plausible despite the absence of input patches from other modalities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Demonstration of cross-modal interaction. The input is the full depth, only two RGB patches, and no semantic segmentation. By editing the hue of a single input patch, the color of the lizard (left) and oranges (right) changes, while keeping the background constant.More interactive examples are available on our website. MultiMAE predictions for a varying number of visible patches. The predictions are plausible even when given half the number of patches seen during pre-training, and the reconstruction quality improves as the number of visible patches increases. An interactive visualization is available on our website.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Method IN-1K (C) ADE20K (S) Hypersim (S) NYUv2 (S) NYUv2 (D)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9</head><label>9</label><figDesc>shows more visualizations on ImageNet-1K [23] validation set images. For all examples, 98 visible patches were sampled using Dirichlet concentration parameter ? = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>MultiMAE predictions on ImageNet-1K validation set samples. 98 visible patches were sampled using Dirichlet concentration parameter ? = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>102] (20'210 training images and 150 classes), NYUv2<ref type="bibr" target="#b75">[73]</ref> (795 training images and 40 classes), and Hypersim<ref type="bibr" target="#b70">[68]</ref> (51'674 training images and 40 classes) datasets. NYUv2 and Hypersim contain ground-truth depth maps that allow us to evaluate semantic segmentation with RGB and depth as input modalities. For all datasets, we report the mean intersection over union (mIoU) metric. On ADE20K and Hypersim, we report it on the validation split, while on NYUv2, we show the test set mIoU. Dense regression tasks. Finally, we study how our models transfer to geometric tasks, such as surface normals, depth and reshading, as well as tasks extracted from RGB images, such as keypoint or edge detection. For depth estimation, we use NYUv2 (795 training and 655 test images), while for all other tasks we train transfers on a subset of the Taskonomy dataset<ref type="bibr" target="#b101">[99]</ref> (800 training images). As performance metrics, we report ? 1 on the NYUv2 test set, showing the percentage of pixels p with error max{? p yp ,</figDesc><table><row><cell>1.25</cell></row><row><cell>yp yp } less than</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Fine-tuning with RGB-only. Fine-tuning with RGB and ground truth depth. We report semantic segmentation transfer results from combinations of RGB and depth, measured in mIoU (?). MultiMAE can effectively leverage additional modalities such as depth, while MAE cannot. Text in gray indicates a modality that the model was not pre-trained on.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Hypersim (S)</cell><cell>NYUv2 (S)</cell></row><row><cell>Supervised [81]</cell><cell>81.8</cell><cell></cell><cell>45.8</cell><cell>33.9</cell><cell>50.1</cell><cell></cell><cell>80.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DINO [12]</cell><cell>83.1</cell><cell></cell><cell>44.6</cell><cell>32.5</cell><cell>47.9</cell><cell></cell><cell>81.3</cell><cell>Method</cell><cell></cell><cell>RGB</cell><cell>D</cell><cell cols="2">RGB-D</cell><cell>RGB</cell><cell>D</cell><cell>RGB-D</cell></row><row><cell>MoCo-v3 [17] MAE [35] MultiMAE</cell><cell>82.8 83.3 83.3</cell><cell></cell><cell>43.7 46.2 46.2</cell><cell>31.7 36.5 37.0</cell><cell>46.6 50.8 52.0</cell><cell></cell><cell>80.9 85.1 86.4</cell><cell cols="2">MAE MultiMAE</cell><cell cols="2">36.5 32.5 37.0 38.5</cell><cell cols="2">36.9 47.6</cell><cell>50.8 23.4 52.0 41.4</cell><cell>49.3 56.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">We report the top-1 ac-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">curacy (?) on ImageNet-1K (IN-1K) [23] classification (C), mIoU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">(?) on ADE20K [102] , Hypersim [68] , and NYUv2 [73] seman-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">tic segmentation (S), as well as ?1 accuracy (?) on NYUv2 depth</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">(D). Text in bold and underline indicates the first and second-best</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">results, respectively. All methods are pre-trained on ImageNet-1K</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">(with pseudo labels for MultiMAE).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">ADE20K (S)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Hypersim (S)</cell><cell></cell><cell></cell><cell></cell><cell>NYUv2 (S)</cell></row><row><cell>Method</cell><cell>RGB</cell><cell>pD</cell><cell cols="3">RGB-pD RGB-pS RGB-pD-pS</cell><cell>RGB</cell><cell>pD</cell><cell cols="3">RGB-pD RGB-pS RGB-pD-pS</cell><cell></cell><cell>RGB</cell><cell>pD</cell><cell>RGB-pD RGB-pS RGB-pD-pS</cell></row><row><cell>MAE</cell><cell cols="2">46.2 20.0</cell><cell>46.3</cell><cell>46.2</cell><cell>46.3</cell><cell cols="2">36.5 21.0</cell><cell>36.9</cell><cell>37.7</cell><cell>37.3</cell><cell></cell><cell cols="2">50.1 23.8</cell><cell>49.1</cell><cell>50.1</cell><cell>49.3</cell></row><row><cell>MultiMAE</cell><cell cols="2">46.2 34.4</cell><cell>46.8</cell><cell>45.7</cell><cell>47.1</cell><cell cols="2">37.0 30.6</cell><cell>37.9</cell><cell>38.4</cell><cell>40.1</cell><cell></cell><cell cols="2">52.0 39.9</cell><cell>53.6</cell><cell>53.5</cell><cell>54.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4a .</head><label>4a</label><figDesc>The RGB-S model performs best on ImageNet classification and NYUv2 semantic segmentation, whereas</figDesc><table><row><cell>Method</cell><cell cols="4">IN-1K (C) NYUv2 (S) NYUv2 (D) Taskonomy (D)</cell><cell>Method</cell><cell cols="4">IN-1K (C) NYUv2 (S) NYUv2 (D) Taskonomy (D)</cell></row><row><cell>MAE (D2)</cell><cell>83.0</cell><cell>44.0</cell><cell>81.3</cell><cell>3.8</cell><cell>RGB?D</cell><cell>82.7</cell><cell>44.0</cell><cell>87.1</cell><cell>1.6</cell></row><row><cell>RGB-D</cell><cell>82.8</cell><cell>45.8</cell><cell>83.3</cell><cell>2.1</cell><cell>RGB?S</cell><cell>82.5</cell><cell>46.8</cell><cell>82.9</cell><cell>4.0</cell></row><row><cell>RGB-S</cell><cell>83.2</cell><cell>51.6</cell><cell>85.5</cell><cell>2.6</cell><cell>RGB?D-S</cell><cell>82.8</cell><cell>48.6</cell><cell>84.6</cell><cell>2.9</cell></row><row><cell>RGB-D-S</cell><cell>83.0</cell><cell>50.6</cell><cell>85.4</cell><cell>1.5</cell><cell>MultiMAE</cell><cell>83.0</cell><cell>50.6</cell><cell>85.4</cell><cell>1.5</cell></row><row><cell cols="5">(a) Impact of additional modalities. Transfer results of several MultiMAE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">models pre-trained on different input modalities / target tasks, compared</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">against MAE (single-modal baseline). D2 = MAE pre-trained with a decoder</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">of depth 2 and width 256, comparable in size to the decoders of MultiMAE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Original RGB image for reference Hue +0?Hue +180?Hue -60?F ull depth input Variants of masked RGB input MultiMAE predictions Original RGB image for reference Hue +0?Hue +140?Hue -50?F ull depth input Variants of masked RGB input MultiMAE predictions</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table of</head><label>of</label><figDesc>ImageNet classification fine-tuning setting . . . . . . . . . . . . . . . . 15 B.2. Semantic segmentation . . . . . . . . 15 B.3. NYUv2 depth estimation . . . . . . . 17 B.4. Taskonomy dense regression tasks . . 17</figDesc><table><row><cell>Contents</cell><cell></cell></row><row><cell>A. Additional pre-training implementation de-</cell><cell></cell></row><row><cell>tails</cell><cell>15</cell></row><row><cell>B. Transfer implementation details</cell><cell>15</cell></row><row><cell>B.1. C. Mask sampling strategies</cell><cell>17</cell></row><row><cell>D. Detailed Taskonomy transfer results</cell><cell>18</cell></row><row><cell>E. Robustness evaluation on ImageNet</cell><cell>18</cell></row><row><cell>F. Comparison of MAE variants</cell><cell>18</cell></row><row><cell>G. Comparison of pre-training time</cell><cell>19</cell></row><row><cell>H. Additional visualizations</cell><cell>19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Default pre-training setting. For ablations, the number of epochs is set to 400. For best results, the number of epochs is set to 1600.</figDesc><table><row><cell>Hyperparameters</cell><cell>Value</cell></row><row><cell>Optimizer</cell><cell>AdamW [55]</cell></row><row><cell>Base learning rate [33]</cell><cell>5e-4</cell></row><row><cell>Weight decay</cell><cell>0.05</cell></row><row><cell>Adam ?</cell><cell>(0.9, 0.999)</cell></row><row><cell cols="2">Layer-wise lr decay [20] 0.65</cell></row><row><cell>Batch size</cell><cell>1024</cell></row><row><cell>Learning rate sched.</cell><cell>Cosine decay [54]</cell></row><row><cell>Training epochs</cell><cell>100</cell></row><row><cell>Warmup learning rate</cell><cell>1e-6</cell></row><row><cell>Warmup epochs</cell><cell>5</cell></row><row><cell>Input resolution</cell><cell>224 ? 224</cell></row><row><cell>Augmentation</cell><cell>RandAugment(9, 0.5)</cell></row><row><cell>Label smoothing</cell><cell>0.1</cell></row><row><cell>Mixup [100]</cell><cell>0.8</cell></row><row><cell>Cutmix [98]</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>dataset: Principal curvature, zbuffer depth, texture edges, occlusion edges, 2D keypoints, 3D keypoints, surface normals, and reshading. We train the transfers on a random subset of the Taskonomy-tiny split, selecting 800 training and 200 validation images. The test evaluation is performed on the entire Taskonomy-tiny test</figDesc><table><row><cell>Hyperparameters</cell><cell>NYUv2 depth</cell><cell>Taskonomy tasks</cell></row><row><cell>Optimizer</cell><cell cols="2">AdamW [55]</cell></row><row><cell>Learning rate</cell><cell>1e-4</cell><cell>3e-4</cell></row><row><cell>Layer-wise lr decay [20]</cell><cell></cell><cell>0.75</cell></row><row><cell>Weight decay</cell><cell>1e-4</cell><cell>5e-2</cell></row><row><cell>Adam ?</cell><cell cols="2">(0.9, 0.999)</cell></row><row><cell>Batch size</cell><cell>128</cell><cell>32</cell></row><row><cell>Learning rate sched.</cell><cell cols="2">Cosine decay [54]</cell></row><row><cell>Training epochs</cell><cell>2000</cell><cell>100</cell></row><row><cell>Warmup learning rate</cell><cell></cell><cell>1e-6</cell></row><row><cell>Warmup epochs</cell><cell>100</cell><cell>5</cell></row><row><cell>Input resolution</cell><cell>256 ? 256</cell><cell>384 ? 384</cell></row><row><cell>RandomCrop</cell><cell>?</cell><cell>?</cell></row><row><cell>Color jitter</cell><cell>?</cell><cell>?</cell></row><row><cell>Drop path</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 .</head><label>10</label><figDesc>Comparison of mask sampling strategies. We report RGB-only transfers to ImageNet-1K<ref type="bibr" target="#b23">[23]</ref> classification and ADE20K<ref type="bibr" target="#b104">[102]</ref> semantic segmentation using MultiMAEs pretrained with different Dirichlet concentration parameter ?. All models were trained for 400 epochs and do not use the additional per-patch-standardized RGB decoder (see Sec. 3.4). By ? = ? we denote always sampling an equal number of visible tokens for each tasks.</figDesc><table><row><cell></cell><cell>RGB</cell><cell>...</cell><cell>... Depth</cell><cell>... Semantic</cell></row><row><cell>...</cell><cell></cell><cell></cell><cell>...</cell><cell>...</cell></row><row><cell cols="5">Figure 8. Multi-modal mask sampling: We sample the propor-</cell></row><row><cell cols="5">tion of tokens per modality using a symmetric Dirichlet distribu-</cell></row><row><cell cols="5">tion Dir(?) with concentration parameter ?. We illustrate here the</cell></row><row><cell cols="5">sampling behavior for different choices of ? values when selecting</cell></row><row><cell cols="5">nine tokens from three modalities. Each row represents one sam-</cell></row><row><cell cols="5">ple of tokens. With small ?, most tokens will be sampled from</cell></row><row><cell cols="5">single modalities, while large ? values result in equal representa-</cell></row><row><cell cols="5">tion of each modality. Setting ? = 1 is equivalent to sampling</cell></row><row><cell cols="5">uniformly over the support and results in a more diverse sampling</cell></row><row><cell>behavior.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>?</cell><cell cols="4">ImageNet-1K [23] ADE20K [102]</cell></row><row><cell>0.2</cell><cell></cell><cell>82.7</cell><cell></cell><cell>44.6</cell></row><row><cell>0.5</cell><cell></cell><cell>82.5</cell><cell></cell><cell>44.8</cell></row><row><cell>1.0</cell><cell></cell><cell>82.8</cell><cell></cell><cell>45.1</cell></row><row><cell>?</cell><cell></cell><cell>82.9</cell><cell></cell><cell>42.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 .</head><label>11</label><figDesc>Robustness evaluation on ImageNet variants from RGB-only.</figDesc><table><row><cell>?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 .</head><label>12</label><figDesc>Taskonomy transfer results using MultiMAE models pre-trained on a varying number of modalities, where the pre-training modalities are the same as the target tasks. Downstream transfers are trained from RGB-only. All models were pre-trained for 400 epochs. We report L1 losses (?) and indicate with bold and underline the best and second-best results, respectively.</figDesc><table><row><cell>Method</cell><cell>Curvature (?10 2 )</cell><cell>Depth (?10 2 )</cell><cell>Edges (?10 3 )</cell><cell>Occlusion (?10 4 )</cell><cell>2D-keypoints (?10 4 )</cell><cell>3D-keypoints (?10 2 )</cell><cell>Normals (?10 2 )</cell><cell>Reshading (?10)</cell><cell>Average loss (?10 2 )</cell><cell>Average rank</cell></row><row><cell>MAE (D2)</cell><cell>4.455</cell><cell cols="2">3.651 4.608</cell><cell>6.237</cell><cell>2.736</cell><cell>4.585</cell><cell>6.189</cell><cell>1.120</cell><cell>3.828</cell><cell>3.75</cell></row><row><cell>RGB-D</cell><cell>4.249</cell><cell cols="2">3.378 4.031</cell><cell>6.608</cell><cell>2.440</cell><cell>4.447</cell><cell>6.094</cell><cell>1.051</cell><cell>3.646</cell><cell>2.125</cell></row><row><cell>RGB-S</cell><cell>4.276</cell><cell cols="2">3.406 3.868</cell><cell>5.939</cell><cell>2.615</cell><cell>4.467</cell><cell>6.139</cell><cell>1.067</cell><cell>3.678</cell><cell>2.625</cell></row><row><cell>RGB-D-S</cell><cell>4.236</cell><cell cols="2">3.340 5.290</cell><cell>5.924</cell><cell>2.590</cell><cell>4.432</cell><cell>6.086</cell><cell>1.040</cell><cell>3.639</cell><cell>1.5</cell></row><row><cell>Method</cell><cell>Curvature (?10 2 )</cell><cell>Depth (?10 2 )</cell><cell>Edges (?10 3 )</cell><cell>Occlusion (?10 4 )</cell><cell>2D-keypoints (?10 4 )</cell><cell>3D-keypoints (?10 2 )</cell><cell>Normals (?10 2 )</cell><cell>Reshading (?10)</cell><cell>Average loss (?10 2 )</cell><cell>Average rank</cell></row><row><cell>RGB?D</cell><cell>4.251</cell><cell cols="3">3.222 7.038 5.914</cell><cell>2.790</cell><cell>4.458</cell><cell>5.960</cell><cell>1.013</cell><cell>3.602</cell><cell>1.625</cell></row><row><cell>RGB?S</cell><cell>4.314</cell><cell cols="3">3.666 7.206 6.051</cell><cell>3.029</cell><cell>4.595</cell><cell>6.843</cell><cell>1.155</cell><cell>3.973</cell><cell>4</cell></row><row><cell cols="2">RGB?D-S 4.266</cell><cell cols="3">3.465 6.745 5.949</cell><cell>2.899</cell><cell>4.510</cell><cell>6.264</cell><cell>1.080</cell><cell>3.759</cell><cell>2.875</cell></row><row><cell cols="2">MultiMAE 4.236</cell><cell cols="3">3.340 5.290 5.924</cell><cell>2.590</cell><cell>4.432</cell><cell>6.086</cell><cell>1.040</cell><cell>3.639</cell><cell>1.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 14 .</head><label>14</label><figDesc>Comparison of MAE variants. We report the top-1 accuracy (?) on ImageNet-1K [23] (IN-1K) classification (C), mIoU (?) on ADE20K [102], Hypersim [68], and NYUv2 [73] semantic segmentation (S), as well as ?1 accuracy (?) on NYUv2 depth (D). Text in bold and underline indicates the first and second-best results, respectively. All models are pre-trained for 1600 epochs. D2 = Decoder of depth 2 and width 256. D8 = Decoder of depth 8 and width 512.</figDesc><table /><note>? A discussion about the reproducibility issues of MAE can be found at: https://github.com/facebookresearch/mae/issues/30</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 15 .</head><label>15</label><figDesc>Pre-training time comparison. Pre-training epoch time for MAE [35] and MultiMAE on ImageNet-1K [23]. We train with 8 Nvidia A100 GPUs and use PyTorch with automatic mixed precision enabled. D2 = Decoder of depth 2 and width 256. D8 = Decoder of depth 8 and width 512. w/ [M] = Mask tokens also given to the ViT-B encoder.</figDesc><table><row><cell></cell><cell>Encoder</cell><cell>Num. unmasked</cell><cell>Epoch time (mins)</cell></row><row><cell>MAE (D2)</cell><cell>ViT-B</cell><cell>49</cell><cell>2.7</cell></row><row><cell>MAE (D8)</cell><cell>ViT-B</cell><cell>49</cell><cell>5.0</cell></row><row><cell>MultiMAE</cell><cell>ViT-B</cell><cell>98</cell><cell>6.0</cell></row><row><cell>MultiMAE, w/ [M]</cell><cell>ViT-B</cell><cell>98</cell><cell>43.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">https://multimae.epfl.ch</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Stefan Stepanovic and Alexander Sax for their help and insightful discussions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur ; Martin Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Josh Levenberg, Dandelion Man?</title>
		<editor>Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi?gas, Oriol Vinyals, Pete Warden,</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org. 19</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sit: Self-supervised vision transformer</title>
		<idno>abs/2104.03602, 2021. 3</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<editor>Sara Atito Ali Ahmed, Muhammad Awais, and Josef Kittler</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangzhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Hong</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Self-supervised multimodal versatile networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adria</forename><surname>Jean-Baptiste Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosalia</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Ramapuram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="25" to="37" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Look, listen and learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="609" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Sit: Self-supervised vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Atito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Kittler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.03602</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Data2vec: A general framework for self-supervised learning in speech, vision and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03555</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Beit: Bert pretraining of image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note>ArXiv, abs/2106.08254, 2021. 2, 3, 15</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<idno>arXiv: 2106.08254. 16</idno>
		<title level="m">BEiT: BERT Pre-Training of Image Transformers</title>
		<imprint>
			<date type="published" when="2021-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A model of inductive bias learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Baxter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="149" to="198" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Herv&amp;apos;e J&amp;apos;egou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><forename type="middle">Multitask</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Learning</surname></persName>
		</author>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997-07" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning aligned cross-modal representations from weakly aligned data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Castrejon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2940" to="2949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Naive-student: Leveraging semi-supervised learning in video sequences for urban scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><forename type="middle">Gontijo</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="695" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative Pretraining From Pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-11" />
			<biblScope unit="page" from="2640" to="3498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Uniter: Universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="104" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Masked-attention mask transformer for universal image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<idno>abs/2112.01527, 2021. 5</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Electra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<title level="m">Pre-training text encoders as discriminators rather than generators</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sensory modality segregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Virginia R De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="913" to="920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Category learning through multimodality sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Virginia R De Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1097" to="1117" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-task selfsupervised visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2051" to="2060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Omnidata: A scalable pipeline for making multi-task mid-level vision datasets from 3d scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ainaz</forename><surname>Eftekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Bachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir Roshan</forename><surname>Zamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10766" to="10776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Are large-scale datasets necessary for self-supervised pre-training?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10740</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Simple copy-paste is a strong data augmentation method for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<imprint>
			<pubPlace>Ekin Dogus Cubuk, Quoc V</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-task self-training for learning general representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="8836" to="8845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Omnivore: A single model for many visual modalities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhila</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.08377</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<publisher>Mohammad</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent-a new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno>abs/2111.06377</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The many faces of robustness: A critical analysis of out-of-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Dorundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyak</forename><surname>Tyler Lixuan Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Parajuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><forename type="middle">Xiaodong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gilmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Benchmarking neural network robustness to common corruptions and perturbations. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1903" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Jacob Steinhardt, and Dawn Xiaodong Song. Natural adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unit: Multimodal multitask learning with a unified transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1439" to="1449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skanda</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.14795</idno>
		<title level="m">Perceiver io: A general architecture for structured inputs &amp; outputs</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05137</idno>
		<title level="m">One model to learn them all</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Ishan Misra, and Nicolas Carion. Mdetrmodulated detection for end-to-end multi-modal understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1780" to="1790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Vilt: Visionand-language transformer without convolution or region supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bokyung</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
		<idno>PMLR, 2021. 2</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="5583" to="5594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6129" to="6138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Fourth international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note>Federico Tombari, and Nassir Navab</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on challenges in representation learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11429</idno>
		<idno>arXiv: 2111.11429</idno>
		<title level="m">Kaiming He, and Ross Girshick. Benchmarking Detection Transfer Learning with Vision Transformers</title>
		<imprint>
			<date type="published" when="2021-11" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.03545</idno>
	</analytic>
	<monogr>
		<title level="m">Trevor Darrell, and Saining Xie. A convnet for the 2020s</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">12-in-1: Multi-task vision and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10437" to="10446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Factors of influence for transfer learning across diverse appearance domains and task types</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Jasper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<title level="m">MMSegmentation Contributors. OpenMMLab Semantic Segmentation Toolbox and Benchmark</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Attention bottlenecks for multimodal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Audio-visual scene analysis with self-supervised multisensory features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="631" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Meta pseudo labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11557" to="11568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Vision transformers for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Towards robust monocular depth estimation: Mixing datasets for zero-shot crossdataset transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1623" to="1637" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Hypersim: A photorealistic synthetic dataset for holistic indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Paczan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Semi-supervised self-training of object detection models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuck</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshops on Applications of Computer Vision (WACV/MOTION&apos;05)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Mid-level visual representations improve generalization and sample efficiency for learning visuomotor policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Emi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Probability of error of some adaptive patternrecognition machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Scudder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="363" to="371" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Variational mixture-of-experts autoencoders for multimodal deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuge</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Siddharth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooks</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<idno>abs/1911.03393</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">The development of embodied cognition: Six lessons from babies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gasser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial life</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="13" to="29" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Segmenter: Transformer for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Strudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08530</idno>
		<title level="m">Vl-bert: Pre-training of generic visual-linguistic representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Multimodal generative learning utilizing jensen-shannondivergence. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">M</forename><surname>Sutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imant</forename><surname>Daunhawer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><forename type="middle">E</forename><surname>Vogt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Generalized multimodal ELBO. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">M</forename><surname>Sutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imant</forename><surname>Daunhawer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><forename type="middle">E</forename><surname>Vogt</surname></persName>
		</author>
		<idno>abs/2105.02470</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07490</idno>
		<title level="m">Lxmert: Learning crossmodality encoder representations from transformers</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Rethinking few-shot image classification: a good embedding is all you need? ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Herv&amp;apos;e J&amp;apos;egou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, 2021. 6</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">On the theory of transfer learning: The importance of task diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nilesh</forename><surname>Tripuraneni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7852" to="7862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">On the theory of transfer learning: The importance of task diversity. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nilesh</forename><surname>Tripuraneni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Jin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Multi-task learning for dense prediction tasks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wouter</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">intelligence</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Attention is all you need. ArXiv, abs/1706.03762, 2017. 1, 2</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">110</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Learning robust global representations by penalizing local predictive power</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songwei</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">Chase</forename><surname>Lipton</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Masked feature prediction for self-supervised visual pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.09133</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Multimodal generative models for scalable weakly-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Masked visual pre-training for motor control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.06173</idno>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Simmim: A simple framework for masked image modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno>abs/2111.09886</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">E2e-vlp: End-toend vision-language pre-training enhanced by visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenming</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.01804</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Billion-scale semi-supervised learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>I Zeki Yalniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00546</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Unsupervised word sense disambiguation rivaling supervised methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Learning to recover 3d scene shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Taskonomy: Disentangling task transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">B</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinghao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.07832</idno>
		<title level="m">ibot: Image bert pre-training with online tokenizer</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Rethinking pretraining and self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekin</forename><surname>Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3833" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
