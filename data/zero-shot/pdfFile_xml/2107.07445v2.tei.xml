<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AutoBERT-Zero: Evolving BERT Backbone from Scratch</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Shi</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhe</forename><surname>Ren</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">L H</forename><surname>Yu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">The Education University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AutoBERT-Zero: Evolving BERT Backbone from Scratch</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformer-based pre-trained language models like BERT and its variants have recently achieved promising performance in various natural language processing (NLP) tasks. However, the conventional paradigm constructs the backbone by purely stacking the manually designed global selfattention layers, introducing inductive bias and thus leads to sub-optimal. In this work, we make the first attempt to automatically discover novel pre-trained language model (PLM) backbone on a flexible search space containing the most fundamental operations from scratch. Specifically, we propose a well-designed search space which (i) contains primitive math operations in the intra-layer level to explore novel attention structures, and (ii) leverages convolution blocks to be the supplementary for attentions in the inter-layer level to better learn local dependency. To enhance the efficiency for finding promising architectures, we propose an Operation-Priority Neural Architecture Search (OP-NAS) algorithm, which optimizes both the search algorithm and evaluation of candidate models. Specifically, we propose Operation-Priority (OP) evolution strategy to facilitate model search via balancing exploration and exploitation. Furthermore, we design a Bi-branch Weight-Sharing (BIWS) training strategy for fast model evaluation. Extensive experiments show that the searched architecture (named AutoBERT-Zero) significantly outperforms BERT and its variants of different model capacities in various downstream tasks, proving the architecture's transfer and scaling abilities. Remarkably, AutoBERT-Zerobase outperforms RoBERTa-base (using much more data) and BERT-large (with much larger model size) by 2.4 and 1.4 higher score on GLUE test set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Benefiting from the powerful capacity of self-attention structures in transformers <ref type="bibr" target="#b37">(Vaswani et al. 2017)</ref>, the pretrained language models (PLM) (e.g. BERT <ref type="bibr" target="#b9">(Devlin et al. 2019)</ref>, RoBERTa <ref type="bibr" target="#b23">(Liu et al. 2019b</ref>), ALBERT <ref type="bibr" target="#b19">(Lan et al. 2020)</ref>, GPT3 <ref type="bibr" target="#b1">(Brown et al. 2020)</ref>) have achieved satisfying performance across various NLP tasks <ref type="bibr" target="#b38">(Wang et al. 2018;</ref><ref type="bibr" target="#b29">Rajpurkar et al. 2016a;</ref><ref type="bibr" target="#b28">Rajpurkar, Jia, and Liang 2018;</ref><ref type="bibr" target="#b46">Zellers et al. 2018)</ref>. All these models are based on the fixed hand-crafted self-attention structure by varying training resources, parameter numbers, layer numbers and inputs. Our searched AutoBERT-Zero is a hybrid structure with convolution layers and the novel searched attention layers, whose kernel sizes and attention structures are various across different layers.</p><p>The conventional paradigm constructs the backbone by stacking the manually-designed global self-attention layers. However, many recent works have pointed out that the design of self-attention structures is not optimal <ref type="bibr" target="#b18">(Kovaleva et al. 2019;</ref><ref type="bibr" target="#b24">Michel et al. 2019;</ref><ref type="bibr" target="#b12">Dong et al. 2021)</ref>, whose inductive bias limits its performance as well as efficiency. In particular, <ref type="bibr" target="#b12">(Dong et al. 2021)</ref> find that repeatedly stacking self-attention results to "token-uniformity" problem, meaning that different tokens are mapped to similar latent representations. Even though they claim that skip connection and multi-layer perceptions mitigate this problem, we still observe it on the BERT output (see <ref type="figure" target="#fig_5">Figure 5</ref>). Another work Reformer <ref type="bibr" target="#b17">(Kitaev et al. 2020</ref>) discovered that sharing the weights for query and key does not impact the model's performance, indicating that redundant parameters exist in self-attention structure. In addition, ConvBERT  shows that local operations such as convolution helps better learn the inherent local dependencies in natural languages. Here, we raise the following questions: Does there exist more powerful and efficient attention beyond the pure query-key-value self-attention for PLM? Can we boost the model performance and efficiency by flexibly combining global attention with local operations?</p><p>To address the above fundamental challenges in the NLP field, we resort to Neural Architecture Search (NAS), which has emerged as a powerful technique to automatically discover promising models without excessive human intervention and tedious tuning. NAS is empowered by a search algorithm and a well-designed search space. The effectiveness of NAS is validated on many computer vision tasks (e.g., image classification <ref type="bibr" target="#b49">(Zoph and Le 2016;</ref><ref type="bibr" target="#b33">Shi et al. 2020)</ref>, object detection <ref type="bibr" target="#b45">Yao et al. 2021)</ref> . Nevertheless, few works leverage NAS to design backbone structure for PLM. The only related works, AdaBERT  and DynaBERT <ref type="bibr" target="#b13">(Hou et al. 2020</ref>) use NAS to compress the fullsized BERT into small models, while Evolved Transformer <ref type="bibr" target="#b34">(So, Le, and Liang 2019)</ref> searches architecture on specific downstream tasks. Besides, as architectures in AdaBERT and Evolved Transformer are task-specific, those models are not applicable for general NLP tasks. Meanwhile, the searched models in DynaBERT and Evolved Transformer are still transformer-based, which does not explore more powerful attention structure.</p><p>To the best of our knowledge, using NAS to discover a novel general PLM backbone from scratch has not been investigated. In this work, we aim to explore powerful PLM backbone by discovering novel attention structures as well as whole backbone architecture from a flexible search space. Specifically, we design both intra-layer and interlayer search spaces that provide a wide variety of candidate architectures to prevent the inductive bias in conventional transformer. The intra-layer search space with few constraints enables finding novel self-attention mechanism, which contains various primitive mathematical operations to construct computation graph with variable path length and flexible input nodes. The inter-layer search space contains global (self-attention) and local operations (convolution) on the backbone level, which provides flexibility in learning global and local dependencies at different layers.</p><p>Since pretraining a PLM is quite time consuming, the computational burden of NAS for PLM is much more overwhelming than utilizing NAS for CV tasks, especially given that our search space is extremely huge. Thus, it is crucial to make the NAS algorithm more efficient in terms of both speed and memory. To this end, we propose a novel Operation-Priority Neural Architecture Search (OP-NAS) algorithm. During search phase, we promote Operation-Priority (OP) evolution strategy. This strategy leverages prior information of operations at each position in the computation path to flexibly balance exploration and exploitation when mutating new architectures, which escapes local optimal and speeds up the search. To facilitate model evaluation, we design Bi-branch Weight-Sharing (BIWS) training strategy, which introduces a super-net to keep track of the trained weights for both the attention structures and convolution blocks on each layer. The candidates are initialized with the weights extracted from the super-net during evaluation to prevent repeated pretraining.</p><p>Extensive experiments are conducted on the widely used Natural Language Understanding(NLU) and Question Answering(QA) benchmarks. The best searched architecture(named AutoBERT-Zero) is shown on <ref type="figure" target="#fig_0">Figure 1</ref>(c), which stacks novel searched attention structures and convolutions. Our AutoBERT-Zero achieves 87.7 GLUE score when trained on the commonly used vallina pre-train tasks, consistently outperforming current state-of-the-art (SOTA) methods by a large margin (4.1 higher than T5), while requiring fewer parameters (52.7% fewer parameters than T5). More remarkably, our AutoBERT-Zero-base surpasses RoBERTa-base (using much more data) and BERT-large (with much larger model size) by 2.4 and 1.4 higher score on GLUE test set.</p><p>Our main contributions are summarized as follows: (i) This is the first work conducting NAS to automatically discover new self-attention structures and better backbones for PLM. (ii) The well-designed search space allows flexible variations in self-attention structures/input nodes/combinations of local and global operations, which enables deriving powerful architectures. (iii) The proposed OP evolution algorithm and BIWS training significantly accelerate the model search and evaluation. (iv) Extensive downstream evaluations demonstrate the effectiveness and scaling ability of the searched model AutoBERT-Zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Pre-trained Language Model (PLM). Recently, the transformer-like paradigm <ref type="bibr" target="#b37">(Vaswani et al. 2017;</ref><ref type="bibr" target="#b27">Radford et al. 2018</ref>) has dominated the research on pre-trained language models. BERT <ref type="bibr" target="#b9">(Devlin et al. 2019)</ref> achieves SOTA performance in various NLU tasks by stacking the encoder of the transformer. Later, diverse BERT variants appear. For example, UniLM <ref type="bibr" target="#b11">(Dong et al. 2019)</ref>, XLNet , ELECTRA <ref type="bibr" target="#b6">(Clark et al. 2019)</ref> introduce new pretraining objectives; Synthesizer <ref type="bibr" target="#b36">(Tay et al. 2021</ref>) considers using random matrices to replace the dot-product selfattention mechanism; ConvBERT  replaces part of attention heads with span-based convolution. However, to the best of our knowledge, apart from ConvBERT and Synthesizer, no other work challenges the transformer-based backbone that purely uses the dot-product self-attention module. In this work, we delve into a more general formulation of attention expression by the combination of primitive math operations.</p><p>Neural Architecture Search (NAS). Early NAS methods search SOTA architectures based on reinforcement learning <ref type="bibr" target="#b49">(Zoph and Le 2016)</ref>, which is computationally expensive. Subsequently, AmoebaNet <ref type="bibr" target="#b31">(Real et al. 2019</ref>) applies the evolution algorithm for NAS. More EA-based methods were further proposed, which exploit the evaluated candidates by modifying how the population list is maintained <ref type="bibr" target="#b47">(Zhu et al. 2019;</ref><ref type="bibr" target="#b22">Liu et al. 2019a</ref>). Gradient-based methods such as DARTS <ref type="bibr" target="#b21">(Liu, Simonyan, and Yang 2018)</ref> were designed to speed up the model search at the expense of higher memory consumption. More recently, AutoML-Zero <ref type="bibr" target="#b32">(Real et al. 2020</ref>) proves that using the basic mathematical operators can successfully develop a machine learning algorithm.</p><p>NAS for Pre-trained LM. Despite the satisfying performance in CV fields, for pre-trained language model, NAS methods are only adopted to BERT compression. AdaBERT  first introduces NAS to compress BERT into small models using traditional convolution operations. However, the searched architectures are task-specific rather than general pre-trained language models. DynaBERT <ref type="bibr" target="#b13">(Hou et al. 2020</ref>) proposes a training method allowing compression in both width and depth directions w.r.t the full-sized teacher BERT model, whose searched models are still transformer backbones. Orthogonal to the above methods, in-  <ref type="figure">Figure 2</ref>: An overview of our OP-NAS framework for pre-trained language models. Our method directly searches better backbone architectures from scratch (using primitive operations). We propose a hierarchical search space for exploring new selfattention structures and an efficient combination of local and global dependencies. By introducing operation-priority(OP) evolution algorithm with BIWS strategy, our method efficiently searches over a wide range of the possible arichitecures.</p><p>spired by the view of AutoML-Zero, we design a search space containing primitive operators and propose a novel NAS method to develop novel attention structure and backbone for general PLM from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>In this section, we present an efficient PLM architecture searching pipeline that evolves the backbone from scratch, as shown in <ref type="figure">Figure 2</ref>. We first introduce our hierarchical coarse-to-fine search space, then elaborate on our operationpriority Neural Architecture Search (OP-NAS) algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Search Space Design</head><p>We design a two-level search space for discovering novel self-attention structures as well as an overall efficient PLM backbone: (i) intra-layer level search space enables exploring new self-attention structures from primitive operation level; (ii) inter-layer level search space leverages global attention layers and local convolution towards an efficient combination of local and global dependencies.</p><p>Intra-layer Search Space As shown in <ref type="figure" target="#fig_0">Figure 1</ref>(b), the original self-attention head can be expressed as follows:</p><formula xml:id="formula_0">Attn(X) = ?(XWQ(XWK ) / d h )XWV W O (1) = ?(QK / d h )V W O ,<label>(2)</label></formula><p>where X ? R n?d is the input, ? is softmax function and self-attention layer is parametered by</p><formula xml:id="formula_1">W k Q , W k K , W k V , W k O ? R d?d h (d h = d/H).</formula><p>The input nodes for a typical selfattention layer are calculated by three fully connected layers from the inputs, called query (Q = XW Q ), key (K = XW K ) and value (V = XW V ). We raise two questions: (a) Can we use fewer inputs (e.g., two inputs) to make the transformer more efficient? (b) Can we build a more powerful self-attention architecture by incorporating various mathematical operations?</p><p>(1) Flexible Input Nodes. For question (a), we allow flexible number of input nodes for our self-attention architecture. More specifically, we add another input node P to</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type</head><p>Operation Expression</p><formula xml:id="formula_2">unary neg ?x transpose x scale x/ ? d x softmax sof tmax(x) logsigmoid log(1/(1 + exp(?x))) softsign x/(1 + |x|) binary add x 1 + x 2 matmul x 1 ? x 2 cosine similarity cos(x 1 , x 2 ) euclidean distance d(x 1 , x 2 )</formula><p>Table 1: Mathematical primitive operations in our Intra-layer Search Space. We try to find a better self-attention structure by construct those operations in a DAG computation graph. construct a search space with four input nodes, where P is mapped through another linear transformation matrix from the original input (P = XW P ). Different from the original transformers with fixed three input nodes, our intra-layer search space allows a range of 2 ? 4 input nodes.</p><p>(2) Primitive Operations. The key component of transformer architecture is the self-attention layer, which first generates an attention matrix, then use it to calculate the weighted sum of values. The attention matrix measures the similarity between the queries and keys. For question (b), we enable finding a better structure of self-attention by designing a more flexible primitive operation search space. Rather than only using matmul and sof tmax as in the original transformer, our primitive operation search space includes various kinds of unary element-wise functions and binary aggregation functions as shown in <ref type="table">Table 1</ref>. The operations such as neg, add and multiplication can be performed on both scalar and matrix inputs.</p><p>(3) Computation Graph with Variable Path Length. As <ref type="figure">Figure 2</ref> illustrates, we represent the new attention structure as a directed acyclic graph (DAG), which transforms input nodes into the tensor output (i.e., the output of self-attentionI layers) with multiple primitive operators in the intermediate graph. To better promote exploration of novel attention structures, we do not fix the path length of attention computation graphs. Note that it is possible that the dimension of the input features in the computation graph are not matched during the calculation. We examine whether every operation is legit and early reject those illegal computation graphs. We also verify that the input and output dimensions of searched attention architectures are matched to ensure layers can be stacked correctly.</p><p>Inter-layer Search Space For the design of the whole backbone, we 1) incorporate local dependency via lightweight convolution and 2) adopt a macro search space to promote the flexibility of design.</p><p>(1) Incorporating Local Dependencies. As pointed out by <ref type="bibr" target="#b41">Wu et al. 2018</ref>), some of the attention heads can be replaced by local operations to better learn local dependencies as well as reduce model complexity. Thus, to enable a powerful and efficient language model, we consider searching a hybrid backbone to replace the attention-only architecture by adding local operations into our inter-layer search space. Specifically, we incorporate the lightweight convolution as our candidate operation, since its effectiveness has been proven in NLP tasks such as machine translation <ref type="bibr" target="#b41">(Wu et al. 2018)</ref>.</p><p>To explore whether different reception fields are preferred for different layers, we further allow different kernel sizes (3 ? 1, 5 ? 1, 7 ? 1, 9 ? 1, 15 ? 1, 31 ? 1, 65 ? 1) across layers. For each convolution layer, the projected input is followed by a Gated Linear Unit (GLU) <ref type="bibr" target="#b8">(Dauphin et al. 2017)</ref>, as shown in <ref type="figure">Figure 2</ref>.</p><p>(2) Macro Search Space. We adopt macro search space for the backbone architecture. Specifically, we allow each layer to have different searched self-attention structure and convolution block. Comparing with the micro (cell-based) search space adopted in previous works <ref type="bibr" target="#b21">(Liu, Simonyan, and Yang 2018;</ref><ref type="bibr" target="#b33">Shi et al. 2020)</ref>, from which a cell structure is searched and the backbone is constructed by repeatedly stacking the cell, our search space is much more flexible, which has more than 10 20 possible combinations. As a result, the searched backbone architecture is more efficient and can effectively capture both global and local contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Operation-Priority Neural Architecture</head><p>Search Algorithm (OP-NAS)</p><p>Since we search for new architectures from scratch in an extremely large macro search space, which involves both intra-layer and inter-layer level, our NAS algorithm must be efficient, scalable, and computationally feasible. Though gradient-based search algorithms such as DARTS are attractive due to their search speed, they do not fit our demand for exploring novel attention mechanism with more flexibility. The supernet in gradient-based algorithms needs to store all the intermediate variables for gradient updates, which requires huge memory cost. This drawback hinders their application on our search space, since we do not restrict the length of attention path and allow a large number of possible operation combinations. Evolution algorithms (EA) <ref type="bibr" target="#b31">(Real et al. 2019</ref>) poses less constraints over the search space as per our request. How-ever, traditional EA suffers from the risk of being trapped by local optimal in a huge search space. To this end, we propose an operation-priority (OP) acquisition method to improve the efficiency of model search by balancing exploration and exploitation. Furthermore, we propose Bi-branch Weight-Sharing (BIWS) training strategy to boost model evaluation by preventing repeated pretraining. The details are described in Algorithm 1. Algorithm 1: OP-NAS Algorithm. Evaluate c on the proxy task; 10: end for 11:</p><p>Update M with the newly evaluated children.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>Update UCB scores by Equation <ref type="formula" target="#formula_3">(3)</ref>; 13: until convergence Operation-priority Evolution Strategy Our OP-NAS is an evolution-based search algorithm. Specifically, it begins by randomly sampling candidates and evaluating them to initialize the population M. In every iteration, the top-K individuals in M are treated as the parents to generate the children via mutation. In inter-layer level, the parent follows the vanilla EA (Goldberg and Deb 1991) to perform random mutation. In intra-layer level, however, random mutation leads to severe inefficiency when searching for attention structures, as there are many possible operation combinations and the length of attention path is unconstrained.</p><p>To address the aforementioned issue, we leverage the prior information of each operation when performing intralayer mutation. The greedy assumption is that if a model performs well, then the operations in its architecture (path) are promising, which should have a higher chance to be sampled. However, the algorithm should also encourage the less frequently sampled operations to prevent getting trapped in local optimal. Thus, we adopt the upper confidence bound (UCB) <ref type="bibr" target="#b0">(Auer, Cesa-Bianchi, and Fischer 2002)</ref> acquisition function, which balances exploitation and exploration to enhance the search efficiency and reduce the number of candidates that need to be evaluated.</p><p>In contrast to previous methods which utilize acquisition functions to measure the potential of whole architectures <ref type="bibr" target="#b20">(Li et al. 2017;</ref><ref type="bibr" target="#b33">Shi et al. 2020)</ref>, while the mutation is still performed randomly, our method uses the UCB acquisition function as a metric to guide the operation selection on each position during mutation. Our method is therefore more efficient and flexible, as the prior knowledge of each operation can be harnessed to generate promising children. For operation i, the UCB score u i is calculated as:</p><formula xml:id="formula_3">ui = ?i + ? 2 log N/Ni<label>(3)</label></formula><p>where ? i is the average proxy task score of the enumerated paths where operation i is included, ? is the hyperparameter controlling the level of exploration, N i is the number of times that operation i has been sampled and N is the total number of operations sampled in history. When the operation is infrequently sampled, the right part dominates the score function. As opposed to other NAS methods such as DARTS <ref type="bibr" target="#b21">(Liu, Simonyan, and Yang 2018)</ref> and ENAS <ref type="bibr" target="#b26">(Pham et al. 2018)</ref>, whose architecture path lengths are fixed, the length of our attention path is flexible and is allowed to change during the search. Thus, assigning independent probability distributions for operations at each position is not feasible, as the position may shift due to the change of path length. To tackle this problem, we model n probability distributions, where n is the length of the longest path sampled during the search. For parent path of length k, the child path is always mutated based on the first k distributions. For convolution layers, the empirical probability distribution for different kernel sizes can be directly calculated for each layer. The probabilities for operations (or kernel sizes) are calculated as: p 1 , . . . , p n = softmax(u 1 , . . . , u n ), where u i represents the UCB score for operation i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bi-branch Weight-Sharing (BIWS) Training Strategy</head><p>To avoid the repeated pretraining of candidate models, we design BIWS training strategy to speed up the model evaluation. Note that even using a very reduced training scheme, evaluating one architecture by training from scratch requires 200 GPU hours. With our BIWS, the evaluation cost is greatly reduced by 80%. The main idea of our strategy is to reuse the trained model parameters in the previous round of searching. To achieve this, we first introduce a bi-branch super-net which contains the largest set of the possible candidate models: one branch contains max attention structure (4 input nodes), and the other branch contains the largest convolution structure (kernel size = 65 ? 1). Each candidate model is initialized by the parameters fetched from the corresponding layers and positions of the super-net. In this way, we can obtain evaluation results with high fidelity after only a few epochs of fine-tuning. To enable a reusable super-net, we design the following strategies:  (1) Convolution layer weight-sharing. Inspired by <ref type="bibr" target="#b2">(Cai et al. 2019)</ref>, we maintain the weights for the largest convolution layer (kernel size = 65 ? 1) throughout searching, then the weights at the center position are shared to initialize the small kernels for the candidate models (as shown in <ref type="figure" target="#fig_3">Figure 3</ref>). Since the shared weights play multiple roles when they are applied to sub-kernels of various sizes, the weights in those sub-kernels should have different properties of distributions and magnitudes. To this end, we introduce the kernel transformation matrices to adapt the shared weights for sub-kernels of different sizes. Specifically, different kernel transformation matrices are learnt during training for different layers, while being shared across all the channels within each layer. The weights of the sub-kernels are updated to the largest kernel in the super-net after training the candidate models in each round.</p><p>(2) Attention layer weight-sharing. The parameters in self-attention structure lie in the linear transformation matrices for key, query, value and P . Since we only mutate parts of the computation graph in each round of searching, we can directly initialize these fully-connected layers in the child individuals using the weights extracted from the corresponding layers of the super-net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Setting</head><p>Datasets and metrics. We first pre-train the backbone architectures using a large corpus of text data and then finetune the model for each specific downstream task. For pretraining, we use the BooksCorpus <ref type="bibr" target="#b48">(Zhu et al. 2015)</ref> and English Wikipedia <ref type="bibr" target="#b9">(Devlin et al. 2019)</ref>. For finetuning and evaluation, we use the General Language Understanding Evaluation (GLUE) <ref type="bibr" target="#b38">(Wang et al. 2018</ref>) and the Stanford Question Answering Dataset (SQuAD) <ref type="bibr" target="#b29">(Rajpurkar et al. 2016a</ref>). Unless stated otherwise, downstream tasks are reported using the same metrics in BERT <ref type="bibr" target="#b9">(Devlin et al. 2019)</ref>. For other settings, we follow the settings of BERT paper. Implementation Details. We use Masked Language Model (MLM) and Next Sentence Prediction (NSP) as pretraining tasks. The whole process can be divided into two phases, namely the NAS phase and the fully-train phase. For NAS phase, we train the base model, whose configuration is the same as BERT-base (L = 12, H = 768, A = 12). Initial M is set as 100, and K is set as 5. Each parent will mutate 5 child architectures. In the NAS phase, we train each candidate architecture for 40,000 steps, which is then evaluated on the proxy task (GLUE). The searching phase costs around 24K GPU hours (760+ evaluated candidates) on Nvidia V100. If we only use EA without BIWS strategy, the computation cost is estimated to be about 182K GPU hours. In fully-train phase, we first pre-train the searched basesize model. To further verify the model's scaling ability, we also fully-train the model on small model (L = 12, H = 256, A = 4) and large model (L = 24, H = 1024, A = 16). Specifically, we treat each two continuous layers as a block and expand the base model to large model by inserting the same block after the original one. More details are attached to Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Analysis</head><p>Structure Analysis of AutoBERT-Zero. We name the best searched architecture of OP-NAS AutoBERT-Zero. As shown in <ref type="figure" target="#fig_7">Figure 7</ref>, the hybrid backbone of AutoBERT-Zero is constructed with stacked conv-att blocks (searched convolution followed by searched attention layer), which effec-    <ref type="figure">Figure 4</ref>: The detailed architecture of AutoBERT-Zero. We only show the 2 nd , 6 th and 12 th discovered attention structures due to limited space. Att. and Conv. represents the searched attention layer and convolution layer respectively. The whole backbone is attached to the Appendix. tively integrates the local and global dependencies of natural language. For the searched attentions, V is shared with Q/K in shallow layers, but non-shared in the deeper layers. This is reasonable since the shallow layer only process the lowlevel features, whereas the deep layers need more parameters to capture the complex semantic features. For example, Attn(X) L2 introduces K-V and Q-V sharing mechanisms, while? ttn(X) L12 adopts separate weights for K, Q and V :</p><formula xml:id="formula_4">Attn(X)L 2 = ?(Q log(1 + exp(K ))/ d h )(K + Q)W O . Attn(X)L 12 = ?(Q(K/ d h + V ) / d h )V W O .</formula><p>Besides, the kernel sizes of convolution layers roughly follow a descending order (changing from 65 to 3), which indicates the convolution layers learn local information from wide to narrow. This is justifiable as the a larger receptive field captures more information, which helps emphasize on the informative features while suppress the unimportant ones. After the shallower layers effectively reduce the in-   After the NAS phase, the searched models are fully-trained and evaluated on downstream tasks. Our AutoBERT-Zero consistently outperforms other baselines by a large margin. To demonstrate the superiority of AutoBERT-Zero's structure, we fully-train several other searched backbones for comparison: (i) AutoBERTw/o-desc. A backbone without descending kernel sizes for convolution layers. (ii) AutoBERT-att. A backbone containing three continuous attention layers. (iii) AutoBERTconv. A backbone containing three continuous convolution layers. The details of architectures can be found in Appendix. As shown in <ref type="table" target="#tab_2">Table 2</ref>, AutoBERT-Zero achieves the highest GLUE score, with a significant performance gain over BERT-base while having less parameters and FLOPs. Specifically, AutoBERT-Zero performs much better than AutoBERT-att and AutoBERT-conv, demonstrating that the conv-att block can better integrate the local and global dependencies. Besides, AutoBERT-Zero's advantage   over AutoBERT-w/o-desc indicates that the kernel size pattern from wide to narrow in convolution layers benefits the performance. As shown in <ref type="table" target="#tab_4">Table 3</ref>, AutoBERT-Zero consistently surpasses BERT-base on both SQuAD v1.1 and v2.0, demonstrating the generalizibility of our searched model. Representation ability of AutoBERT-Zero. "Tokenuniformity" damages model's representation ability. To measure the degree of "token-uniformity", following <ref type="bibr" target="#b12">(Dong, Cordonnier, and Loukas 2021;</ref><ref type="bibr">Gong et al. 2021)</ref>, we use relative norm of residual to measure the rank of output ( the rank is equal to 1 when residual is equal to 0), and measure the average pairwise cosine-similarity between the representations of different tokens on 1,280 samples of STS-B. As shown in <ref type="figure" target="#fig_5">Figure 5</ref>, latent representations from purelystacked BERT-base have high similarity, and the rank of output is closer to 1 (relative norm of residual is closer to 0), showing no significant difference between the tokens. On the other hand, the output of AutoBERT-Zero has relatively larger residual and lower token similarity, showing that the hybrid backbone helps mitigate this problem.</p><p>Scaling ability of AutoBERT-Zero. We further extend AutoBERT-Zero structure to different capacities, showing strength in both large and small models. Specifically, <ref type="table" target="#tab_5">Table 4</ref> shows that our large model surpasses BERT-large by 3.5 in GLUE. Remarkably, our small model significantly surpasses the SOTA ConvBERT-small (4.6 higher) and BERT-small (5.4 higher) under the vanilla MLM task. Besides, our small model considerably outperforms the large GPT in terms of both performance and complexity: 1.7 higher GLUE, 88% less parameters, and 90% less FLOPs. Despite the advantage of strong pre-train task (RTD), ELECTRA-small is still outperformed by our model.</p><p>The Efficiency of OP-NAS. During the search, we observe that by adopting the proposed operation-priority strat- egy, the exploration ability of the EA is highly improved, which prevents getting trapped in local optimal (see <ref type="figure" target="#fig_6">Figure 6)</ref>. The results shows that searched model using OP-NAS outperforms other NAS algorithms by a large margin.</p><p>As the quality of model evaluation during NAS phase greatly impacts the algorithm's effectiveness, we further examine the fidelity of the evaluation results. Kendall <ref type="bibr" target="#b15">(Kendall 1938)</ref> correlation analysis is performed to evaluate the correlation between model performances in the NAS phase and fullytrain phase. As shown in Appendix B, high correlations are captured in most of the downstream tasks, which is owing to the effectiveness of our BIWS strategy. Ablation study. To investigate the superiority of searched hybrid architecture, we evaluate performance of attentiononly and convolution-only variants, which are constructed by stacking either the searched attention or the convolution layers of AutoBERT-Zero. For example, for the attentiononly variant, each convolution block is replaced with the attention layer directly behind it. From <ref type="table" target="#tab_7">Table 5</ref>, we find that the hybrid backbone architecture outperforms both attention-only and convolution-only variants. Besides, the attention-only variant surpasses BERT-base by a large margin, showing effectiveness of searched attention structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we propose a novel hierarchical search space and an efficient NAS framework to automatically find promising PLM backbones from scratch, which prevents the tedious manual tuning. The searched self-attention structure and backbone architecture can inspire new insights for model design in the NLP community.</p><p>The detailed architetcture of best searched model AutoBERT-Zero is shown in <ref type="figure" target="#fig_7">Figure 7</ref>. The hybrid backbone of AutoBERT-Zero is constructed with stacked conv-att blocks (searched convolution followed by searched attention layer), which effectively integrates the local and global dependencies of natural language.</p><p>Apart from the best searched model AutoBERT-Zero (see <ref type="figure" target="#fig_5">Figure 5</ref> in the main paper), we randomly choose several searched architectures from the final searching round as examples. Detailed architectures are shown on <ref type="figure">Figure 9</ref> ? 11. Due to space limit, these figures are shown in the last pages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Correlation between searching and fully-train phases</head><p>As the quality of model evaluation during NAS phase greatly impacts the algorithm's effectiveness, we further examine the fidelity of the evaluation results. <ref type="figure" target="#fig_8">Figure 8</ref> shows the kendall <ref type="bibr" target="#b15">(Kendall 1938)</ref> analysis between model performances in NAS phase and fully-train phase. We can find high correlation between results in the searching phase and in the fully-train phase on various downstream tasks, showing model's performance in searching phase can effectively represent the its quality in fully-train phase, which further verifies the BIWS strategy's effectiveness in pre-training a model in a short time (40,000 updates).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Datasets</head><p>GLUE is a collection of diverse natural language understanding datasets. Unless stated otherwise, follow <ref type="bibr" target="#b9">(Devlin et al. 2019)</ref>, we use the accuracy to evaluate MNLI, QNLI, RTE, SST, use F1 to evaluate MRPC and QQP, use Spearman correlation for STS-B and Matthews correlation for CoLA. The average score is denoted by GLUE. SQuAD is a question answering dataset containing 100k question/answer pairs. The target of this task is to locate the answer with the given context and questions. The Exact Match and F1 scores are reported for SQuAD. We provide further details about GLUE and SQuAD tasks below.</p><p>C.1 GLUE MNLI. A ternary classification task: Given a premise sentence and a hypothesis sentence, the target of the Multi-Genre Natural Language Inference (MNLI) <ref type="bibr" target="#b40">(Williams, Nangia, and Bowman 2018)</ref> is to predict whether the last sentence is an entailment, contradiction, or neutral relationships with respect to the first one. QQP. A binary classification task: Given two questions from Quora, the target of Quora Question Pairs (QQP) <ref type="bibr" target="#b5">(Chen et al. 2018</ref>) is to determine whether these two asked questions are semantically equivalent or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QNLI.</head><p>A binary classification task: The Question Natural Language Inference (QNLI) <ref type="bibr">(Wang, Yan, and Wu)</ref> is derived from the Stanford Question Answering Dataset <ref type="bibr" target="#b30">(Rajpurkar et al. 2016b)</ref>. Given a sentence pairs (question, sentence), the target of QNLI is to predict whether the last sentence contains the correct answer of the question.</p><p>SST-2. A binary classification task: The Stanford Sentiment Treebank (SST-2) <ref type="bibr" target="#b35">(Socher et al. 2013</ref>) aims to predict the sentiment for a single-sentence. All sentences are extracted from movie reviews with human annotations of their sentiment.</p><p>CoLA. A binary classification task: The Corpus of Linguistic Acceptability (CoLA) (Warstadt, Singh, and Bowman 2019) is consisting of English acceptability judgments extracted from books and journal articles. Given a singlesentence, the target is to determine whether the sentence is linguistically "acceptable" or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STS-B.</head><p>A quinary classification task: The Semantic Textual Similarity Benchmark (STS-B) <ref type="bibr" target="#b3">(Cer et al. 2017</ref>) aims to predict the similarity score (from 1 to 5) between a given sentence pair, whose sentence pairs are drawn from news headlines and other sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MRPC.</head><p>A binary classification task: The Microsoft Research Paraphrase Corpus (MRPC) <ref type="bibr" target="#b10">(Dolan and Brockett 2005)</ref> consists of 5, 801 sentence pairs automatically extracted from online news sources, with human annotations for whether the sentences in the pair are semantically equivalent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RTE.</head><p>A binary classification task: Recognizing Textual Entailment (RTE) is similar to MNLI aiming to predict the entailment, but with much less training data <ref type="bibr" target="#b7">(Dagan, Glickman, and Magnini 2005)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 SQuAD</head><p>SQuAD v1.1. The Stanford Question Answering Dataset (SQuAD v1.1) <ref type="bibr" target="#b30">(Rajpurkar et al. 2016b</ref>) is a large-scale question and answer task consisting of 100k question and answer pairs from more than 500 articles. Given a passage and the question from Wikipedia, the goal is to determine the start and the end token of the answer text.</p><p>SQuAD v2.0. The SQuAD v2.0 task is the extension of above SQuAD v1.1, which contains the 100k questions in SQuAD v1.1 and 50k unanswerable questions. The existence of unanswerable question makes this task more realistic and challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Implementation details</head><p>The whole process can be divided into two phases, namely the NAS phase and the fully-train phase. For both phases, we pre-train our base model, whose configuration is the same as BERT-base(L = 12, H = 768, A = 12). More specifically,   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Details in Searching Phase</head><p>Model Searching. For the model searching phase, we proposed the Operation-Priority(OP) strategy to generate new model candidates. Initial M is set as 100, and K is set as 5.</p><p>The confidence value c in the UCB function is set as 0.01. Each parent will mutate 5 child architectures. The searching phase costs around 24K GPU hours (760+ evaluated candidates) on Nvidia V100. If we only use EA without BIWS strategy, the computation cost is estimated to be about 182K GPU hours.</p><p>Model Evaluation. For the model evaluation phase, we use the super-net to initialize our child architectures, and then pre-train our model for 40,000 steps. We use the vanilla Masked Language Model (MLM) and Next Sentence Prediction (NSP) as our pre-training tasks. For fast pre-training, we set learning rate to 2e-4, batch size to 160, and warm-up proportion to 0.02. The learning rate decay strategy is set as linear. The fast pre-trained model (after 40,000 steps) will then be evaluated by the proxy task(GLUE) after finetuning 3 epochs. For fast finetuning, we set weight decay to 0.01, warm-up proportion to 0.01, and learning rate to 2e-5. The learning rate decay strategy is set as linear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Details in Fully-train Phase</head><p>For pre-training configurations, we mostly use the same hyper-parameters as BERT. See <ref type="table" target="#tab_9">Table 6</ref> for more details. We use MLM and NSP to pre-train the searched architectures for 40 epochs (same with BERT). We set input sequence length as 128 and use Adam <ref type="bibr" target="#b16">(Kingma and Ba 2015)</ref> optimizer. For small-size, base-size and large-size models, the detailed configurations are listed in      <ref type="figure" target="#fig_0">Figure 11</ref>: The detailed architecture of AutoBERT-conv. This model contains several continuous convolution layers, whose kernel sizes do not follow an obvious order (ascending/descending). However, the deeper layers also have more input nodes in the attention modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Latency and Memory</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparison between BERT and our searched model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1:</head><label></label><figDesc>Initialize population M from search space A; 2: Model evaluation in M; each parent p in P do 6: p ? M utation InterLayer (p); 7: c ? M utation IntraLayer (p ,UCB); 8:Initialize c with BIWS strategy ; 9:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of BIWS strategy. For attention layers, transformation matrices of K,Q are initialized from corresponding positions of the largest 4-node attention. For convolution layers, small kernels are initialized by the center of the largest kernel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Oversmooth exists in purely-stacked transformer structure, whereas the hybrid structure helps to tackle this problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Searching performance comparison among Random Search (RS), RS with weight sharing, EA with weight sharing and OP-NAS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>The detailed architecture of AutoBERT-Zero. This hybrid backbone is constructed with stacked conv-att blocks (searched convolution followed by searched attention layer), which effectively integrates the local and global dependencies of natural language. For the searched attentions, V is shared with Q/K in shallow layers, but non-shared in the deeper layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>High correlation can be found between the searched results and final results on different GLUE tasks. Six architectures of the candidates generated by OP-NAS are randomly selected for comparison.we set the number of transformer layers L = 12, the hidden state size H = 768, the number of heads in each layer A = 12 and the number of neurons in the intermediate layer d f f = 3072 (4 times of the hidden dimension).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>The detailed architecture of AutoBERT-w/o-desc. This model is similar to AutoBERT-Zero, but its convolution layers' kernel sizes are not following a descending order. The architectures of attention layers also become more complex as the layers go deeper. The detailed architectures of AutoBERT-att. This model has several continuous attention layers. Like other backbones, the deeper layers of this backbone also have more input nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>#Params Infer FLOPs CoLA MRPC MNLI-(m/mm) STS-B RTE QQP QNLI SST-2 AVG</figDesc><table><row><cell>Development Set</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERT-base(ours)</cell><cell>110M</cell><cell>2.9e10</cell><cell>58.1</cell><cell>89.7</cell><cell>84.8/85.2</cell><cell>88.8</cell><cell>69.0 88.2</cell><cell>91.5</cell><cell>92.9</cell><cell>83.1</cell></row><row><cell>AutoBERT-att</cell><cell>104M</cell><cell>2.3e10</cell><cell>65.4</cell><cell>92.2</cell><cell>84.6/85.0</cell><cell>90.4</cell><cell>81.6 88.5</cell><cell>91.8</cell><cell>93.8</cell><cell>85.9</cell></row><row><cell>AutoBERT-conv</cell><cell>104M</cell><cell>2.2e10</cell><cell>63.8</cell><cell>92.6</cell><cell>84.4/84.6</cell><cell>90.1</cell><cell>80.5 88.3</cell><cell>91.7</cell><cell>93.5</cell><cell>85.5</cell></row><row><cell>AutoBERT-w/o-desc</cell><cell>104M</cell><cell>2.3e10</cell><cell>65.1</cell><cell>92.8</cell><cell>84.5/85.0</cell><cell>90.5</cell><cell>78.7 88.2</cell><cell>91.6</cell><cell>93.7</cell><cell>85.6</cell></row><row><cell>AutoBERT-Zero</cell><cell>104M</cell><cell>2.3e10</cell><cell>64.5</cell><cell>93.3</cell><cell>85.5/85.3</cell><cell>90.8</cell><cell>81.9 88.9</cell><cell>92.0</cell><cell>94.2</cell><cell>86.3</cell></row><row><cell>AutoBERT-Zero  *</cell><cell>104M</cell><cell>2.3e10</cell><cell>67.3</cell><cell>93.8</cell><cell>86.4/86.3</cell><cell>90.8</cell><cell>85.2 91.7</cell><cell>92.5</cell><cell>95.2</cell><cell>87.7</cell></row><row><cell>Test Set</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GPT(Radford et al. 2018)</cell><cell>117M</cell><cell>3.0e10</cell><cell>45.4</cell><cell>82.3</cell><cell>82.1/81.4</cell><cell>82.0</cell><cell>56.0 70.3</cell><cell>88.1</cell><cell>91.3</cell><cell>75.4</cell></row><row><cell>BERT-base(Devlin et al. 2019)</cell><cell>110M</cell><cell>2.9e10</cell><cell>52.1</cell><cell>88.9</cell><cell>84.6/83.4</cell><cell>85.8</cell><cell>66.4 71.2</cell><cell>90.5</cell><cell>93.5</cell><cell>79.6</cell></row><row><cell>DynaBERT-base(Hou et al. 2020)</cell><cell>110M</cell><cell>2.9e10</cell><cell>54.9</cell><cell>87.9</cell><cell>84.5/84.1</cell><cell>84.4</cell><cell>69.9 72.1</cell><cell>91.3</cell><cell>93.0</cell><cell>80.2</cell></row><row><cell>ConvBERT-base (Jiang et al. 2020)</cell><cell>106M</cell><cell>2.7e10</cell><cell>53.7</cell><cell>89.3</cell><cell>84.6/83.6</cell><cell>86.1</cell><cell>72.1 71.3</cell><cell>90.1</cell><cell>93.5</cell><cell>80.5</cell></row><row><cell>Roberta-base (Liu et al. 2019b)</cell><cell>110M</cell><cell>2.9e10</cell><cell>50.5</cell><cell>90.0</cell><cell>86.0/85.4</cell><cell>88.1</cell><cell>73.0 70.9</cell><cell>92.5</cell><cell>94.6</cell><cell>81.1</cell></row><row><cell>BERT-Large(Devlin et al. 2019)</cell><cell>340M</cell><cell>8.7e10</cell><cell>60.5</cell><cell>89.3</cell><cell>86.7/89.5</cell><cell>86.5</cell><cell>70.1 72.1</cell><cell>92.7</cell><cell>94.9</cell><cell>82.1</cell></row><row><cell>AutoBERT-Zero</cell><cell>104M</cell><cell>2.3e10</cell><cell>55.9</cell><cell>89.5</cell><cell>85.4/84.9</cell><cell>88.3</cell><cell>77.8 71.8</cell><cell>91.2</cell><cell>94.6</cell><cell>82.2</cell></row><row><cell>AutoBERT-Zero  *</cell><cell>104M</cell><cell>2.3e10</cell><cell>59.5</cell><cell>90.5</cell><cell>86.1/86.0</cell><cell>88.9</cell><cell>80.2 72.8</cell><cell>92.1</cell><cell>95.1</cell><cell>83.5</cell></row><row><cell>AutoBERT-Zero-Large</cell><cell>318M</cell><cell>6.8e10</cell><cell>63.8</cell><cell>90.7</cell><cell>87.7/87.1</cell><cell>90.1</cell><cell>80.4 72.1</cell><cell>93.6</cell><cell>95.4</cell><cell>84.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison on the test set of GLUE. Our 12-layer base model AutoBERT-Zero significantly surpasses RoBERTa-Base and BERT-large (24 layers). Note that Roberta<ref type="bibr" target="#b23">(Liu et al. 2019b</ref>) runs on 160G corpus, whereas our model runs on 16G corpus. Infer FLOPs assumes single inputs with length 128. AutoBERT-Zero * is initialized from the surpernet.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">#Params FLOPs Pre-train Task GLUE</cell></row><row><cell>ELMO (Peters et al. 2018)</cell><cell>96M</cell><cell>2.6e10</cell><cell>LM</cell><cell>71.2</cell></row><row><cell>GPT(Radford et al. 2018)</cell><cell>117M</cell><cell>3.0e10</cell><cell>LM</cell><cell>78.8</cell></row><row><cell>BERT-small (Jiang et al. 2020)</cell><cell>14M</cell><cell>3.7e9</cell><cell>MLM</cell><cell>75.1</cell></row><row><cell>ELECTRA-small (Clark et al. 2019)</cell><cell>14M</cell><cell>3.7e9</cell><cell>RTD</cell><cell>79.9</cell></row><row><cell>ConvBERT-small (Jiang et al. 2020)</cell><cell>14M</cell><cell>4.1e9</cell><cell>MLM</cell><cell>75.9</cell></row><row><cell>AutoBERT-Zero-small</cell><cell>13M</cell><cell>2.9e9</cell><cell>MLM</cell><cell>80.5</cell></row><row><cell>BERT-large(Devlin et al. 2019)</cell><cell>340M</cell><cell>8.7e10</cell><cell>MLM</cell><cell>84.4</cell></row><row><cell>AutoBERT-Zero-large</cell><cell>318M</cell><cell>6.8e10</cell><cell>MLM</cell><cell>87.9</cell></row></table><note>Results on SQuAD(dev). "#Params of Att" counts param- eters in attention structures.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Scaling ability of the searched model. Results are reported on GLUE dev set.</figDesc><table /><note>2 formation redundancy, the deeper layers can focus on the important semantic features. Results on GLUE &amp; SQuAD.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Model comparison among AutoBERT-Zero and its Attention-only, Conv-only variants. Models are fully-trained and evaluated on GLUE dev set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>. For large model(24 layers),</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6</head><label>6</label><figDesc>For the evaluation interval, we use from {100, 10}. We use the WordPiece embedding<ref type="bibr" target="#b42">(Wu et al. 2016)</ref>, and 30, 000 tokens are contained in the dictionary. The special token [CLS] is used as the first token of each sequence. Another special token [SEP] is used to separate sentences in a sequence. The input token representation is the sum of token embedding, segmentation embedding and position embedding.</figDesc><table><row><cell></cell><cell cols="4">: Hyperparameters for pre-training.</cell><cell></cell></row><row><cell cols="6">2020) and (Devlin et al. 2019) which use extra augmenta-</cell></row><row><cell cols="6">tion data, we finetuned on the original SQuAD v1.1 data for</cell></row><row><cell cols="6">3 epochs with batch size of 32. For SQuAD v2.0, we fine-</cell></row><row><cell cols="6">tuned 2 epochs with batch size of 48. Following (Liu et al.</cell></row><row><cell cols="6">2019b), we finetune RTE, STS-B, and MRPC using a MNLI</cell></row><row><cell>checkpoint. Method</cell><cell cols="5">FLOPs Latency Memory #Params GLUE</cell></row><row><cell>BERT-base</cell><cell>2.9e10</cell><cell>29.3s</cell><cell>2.5G</cell><cell>110M</cell><cell>83.1</cell></row><row><cell cols="2">AutoBERT-Zero 2.3e10</cell><cell>27.2s</cell><cell>2.1G</cell><cell>104M</cell><cell>86.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>More comparison about model latency and memory. GLUE is reported on dev set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7</head><label>7</label><figDesc>measure models with FLOPs, parameter number, latency and memory on Nvidia V100. For FLOPs, we follow the setting of<ref type="bibr" target="#b6">(Clark et al. 2019;</ref><ref type="bibr" target="#b14">Jiang et al. 2020</ref>) and counts the inference FLOPs. For latency, we follow Dyn-aBERT<ref type="bibr" target="#b13">(Hou et al. 2020)</ref> and do experiments on QNLI training set with batch size 128. From the results, we can find that our model AutoBERT-Zero is faster and occupies less memory during inference. Backbone:AutoBERT-att    </figDesc><table><row><cell>Attention in 2nd layer</cell><cell>Attention in 4th layer</cell></row><row><cell>Attention in 8h layer</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Following ConvBERT, we count accuracy for MRPC and QQP for small model. Small model results are median results of 3 runs.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Renjie Pi and the anonymous reviewers for insightful suggestions that have significantly improved the paper. The research of Philip L.H. Yu was supported by a start-up research grant from the Education University of Hong Kong (#R4162).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Finite-time analysis of the multiarmed bandit problem. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="235" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<title level="m">Language models are few-shot learners</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Once-for-All: Train One Network and Specialize it for Efficient Deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
		<meeting>the 11th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<title level="m">AdaBERT: Task-Adaptive BERT Compression with Differentiable Neural Architecture Search. In IJCAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Quora question pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>University of Waterloo</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ELEC-TRA: Pre-training Text Encoders as Discriminators Rather Than Generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Challenges Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWP</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unified Language Model Pre-training for Natural Language Understanding and Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attention is not all you need: Pure attention loses rank doubly exponentially with depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">; C</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03404</idno>
		<idno>arXiv:2104.12753</idno>
	</analytic>
	<monogr>
		<title level="m">Improve Vision Transformers Training by Suppressing Over-smoothing</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Foundations of genetic algorithms. Gong,</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">DynaBERT: Dynamic BERT with Adaptive Width and Depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<title level="m">ConvBERT: Improving BERT with Span-based Dynamic Convolution. NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A new measure of rank correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Kendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="93" />
			<date type="published" when="1938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reformer: The Efficient Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Revealing the Dark Secrets of BERT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rumshisky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ALBERT: A Lite BERT for Selfsupervised Learning of Language Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hyperband: A novel bandit-based approach to hyperparameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desalvo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The JMLR</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6765" to="6816" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep evolutionary networks with expedited genetic algorithms for medical image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>El Basha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Sanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="306" to="315" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Are Sixteen Heads Really Better than One?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient Neural Architecture Search via Parameter Sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Improving language understanding by generative pre-training</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Know What You Don&apos;t Know: Unanswerable Questions for SQuAD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ Questions for Machine Comprehension of Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ Questions for Machine Comprehension of Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">AutoMLzero: evolving machine learning algorithms from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bridging the Gap between Sample-based and One-shot Neural Architecture Search with BONAS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The Evolved Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Synthesizer: Rethinking self-attention in transformer models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP Workshop BlackboxNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL, year=2018. Warstadt, A.; Singh, A.; and Bowman, S. 2019. Neural Network Acceptability Judgments. TACL</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pay Less Attention with Lightweight and Dynamic Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Auto-fpn: Automatic network architecture adaptation for object detection beyond classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">XLNet: Generalized Autoregressive Pretraining for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Joint-DetNAS: Upgrade Your Detector with NAS, Pruning and Dynamic Distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">EENA: efficient evolution of neural architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
