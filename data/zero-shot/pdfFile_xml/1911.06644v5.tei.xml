<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">You Only Watch Once: A Unified CNN Architecture for Real-Time Spatiotemporal Action Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="202118-10-19">October 19, 2021 18 Oct 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Okan</forename><surname>K?p?kl?</surname></persName>
							<email>okan.kopuklu@tum.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Human-Machine Communication</orgName>
								<orgName type="institution">Technical Univ. of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Human-Machine Communication</orgName>
								<orgName type="institution">Technical Univ. of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Human-Machine Communication</orgName>
								<orgName type="institution">Technical Univ. of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">You Only Watch Once: A Unified CNN Architecture for Real-Time Spatiotemporal Action Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="202118-10-19">October 19, 2021 18 Oct 2021</date>
						</imprint>
					</monogr>
					<note>* Corresponding author (Okan K?p?kl?) Preprint submitted to Journal of L A T E X Templates</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Spatiotemporal action localization requires the incorporation of two sources of information into the designed architecture: (1) temporal information from the previous frames and (2) spatial information from the key frame. Current state-of-the-art approaches usually extract these information with separate networks and use an extra mechanism for fusion to get detections. In this work, we present YOWO, a unified CNN architecture for real-time spatiotemporal action localization in video streams. YOWO is a single-stage architecture with two branches to extract temporal and spatial information concurrently and predict bounding boxes and action probabilities directly from video clips in one evaluation. Since the whole architecture is unified, it can be optimized end-to-end.</p><p>The YOWO architecture is fast providing 34 frames-per-second on 16-frames input clips and 62 frames-per-second on 8-frames input clips, which is currently the fastest state-of-the-art architecture on spatiotemporal action localization task. Remarkably, YOWO outperforms the previous state-of-the art results on J-HMDB-21 and UCF101-24 with an impressive improvement of ?3% and ?12%, respectively. Moreover, YOWO is the first and only single-stage architecture that provides competitive results on AVA dataset. We make our code and pretrained models publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The topic of spatiotemporal human action localization has been spotlighted in recent years, which aims to not only recognize the occurrence of an action but also localize it in both time and space. In such a task, comparing with object detection in static images, temporal information plays an essential role. Finding an efficient strategy to aggregate spatial as well as temporal features makes the problem even more challenging. On the other hand, real-time human action detection is becoming increasingly crucial in numerous vision applications, such as human-computer interaction (HCI) systems, unmanned aerial vehicle (UAV) monitoring, autonomous driving, and urban security systems. Therefore, it is desirable and worthwhile to explore a more efficient framework to tackle this problem.</p><p>Inspired by the remarkable object detection architecture Faster R-CNN <ref type="bibr" target="#b0">[1]</ref>, most state-of-the-art works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> extend the classic two-stage network architecture to action detection, where a number of proposals are produced in the first stage, then classification and localization refinement are performed in the second stage. However, these two-stage pipelines have three main shortcomings in the spatiotemporal action localization task. Firstly, the generation of action tubes which consist of bounding boxes across frames is much more complicated and time-consuming than 2D case. The classification performance is extremely dependent on these proposals, where the detected bounding boxes might be sub-optimal for the following classification task. Secondly, the action proposals focus only on the features of humans in the video, neglecting the relationship between humans and some attributes in the background, which yet is able to provide considerably crucial context information for action prediction. The third problem of a two-stage architecture is that training the region proposal network and the classification network separately does not guarantee to find the global optimum. Instead, only local optimum from the combination of two stages can be found. The training cost is also higher than single-stage networks, hence it takes longer time and needs more memory. In this paper, we propose a novel single-stage framework, YOWO (You Only Watch Once), for spatiotemporal action localization in videos. YOWO prevents all of the three shortcomings mentioned above with a single-stage architecture.</p><p>The intuitive idea of YOWO arises from human's visual cognitive system. For example, when we are absorbed into the story of a soap opera in front of the TV, each time our eyes capture a single frame. In order to understand which action each artist is performing, we have to relate current frame information (2D features from key frame) to the obtained knowledge from previous frames saved in our memory (3D features from clip). Afterwards, these two kinds of features are fused together to provide us with a reasonable conclusion. The example in YOWO architecture is a single-stage network with two branches. One branch extracts the spatial features of the key frame (i.e. current frame) via a 2D-CNN while the other branch models the spatiotemporal features of the clip consisting of previous frames via a 3D-CNN. To this end, YOWO is a causal architecture that can operate online on incoming video streams. In order to aggregate these 2D-CNN and 3D-CNN features smoothly, a channel fusion and attention mechanism is used, where we get the utmost out of inter-channel dependencies.</p><p>Finally, we produce frame-level detections using the fused features, and provide a linking algorithm to generate action tubes.</p><p>In order to maintain real-time capability, we have operated YOWO on RGB modality. However, it must be noted that YOWO architecture is not restricted to operate only on RGB modality. Different branches can be inserted into YOWO for different modalities such as optical flow, depth etc. Moreover, in its 2D-CNN and 3D-CNN branches, any CNN architecture can be used according to the desired run-time performance, which is critical for real-world applications.</p><p>YOWO operates with maximum 16 frames input since short clip lengths are necessary to achieve faster runtime for spatiotemporal action localization task. However, such small clip size is a limiting factor for the accumulation of temporal information. Therefore, we have made use of the long-term feature bank <ref type="bibr" target="#b3">[4]</ref> by extracting features with 3D-CNN for non-overlapping 8-frame clips for the whole videos using the trained 3D-CNN. Training of YOWO performed normally, but at inference time, we have averaged the 3D features centering the key-frame. This brought a considerable 6.9% and 1.3% frame-mAP increase on the final performance of the network.</p><p>Contributions of this paper are summarized as follows:</p><p>(i) We propose a real-time single-stage framework for spatiotemporal action localization in video streams, named YOWO, which can be trained end-to-end with high efficiency. To the best of our knowledge, this is the first work which achieves bounding box regression on features extracted by a 2D-CNN and 3D-CNN, concurrently. These two kinds of features have a complementary effect to each other for the final bounding box regression and action classification. Moreover, we use a channel attention mechanism to aggregate the features smoothly from two branches above. We experimentally prove that channel-wise attention mechanism models the inter-channel relationship within the concatenated feature maps and boosts the performance significantly by fusing features more reasonably.</p><p>(ii) We perform a detailed ablation study on the YOWO architecture. We examined the effect of 3D-CNN, 2D-CNN, their aggregation and the fusion mechanism. Moreover, we have experimented different 3D-CNN architectures and different clip lengths to explore a further trade-off between the precision and speed.</p><p>(iii) YOWO is evaluated on AVA dataset, which is the first and only singlestage architecture that achieves competitive results compared to the state-ofthe-art. Moreover, YOWO is the only causal architecture (i.e. future frames are not leveraged) that is evaluated on AVA dataset, hence can operate online.</p><p>(iv) We evaluate YOWO on J-HMDB-21 and UCF101-24 benchmarks and establish new state-of-the-art results with an impressive 3.3% and 12.2% improvements on frame-mAP, respectively. Moreover, YOWO runs with 34 fps for 16-frames input clips and 62 fps for 8-frames input clips, which is the fastest state-of-the-art architecture available for spatiotemporal action localization task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Action recognition with deep learning. Since deep learning brings significant improvements in image recognition, numerous recent research efforts have been devoted to extend it for action recognition in videos. For action recognition, however, besides spatial features extracted from each individual image, temporal context across these frames also needs to be taken into account.</p><p>Two-stream CNN is one effective strategy to extract spatial and temporal features separately and aggregate them together <ref type="bibr" target="#b4">[5]</ref> [6] <ref type="bibr" target="#b6">[7]</ref>. Most of these works are based on optical flow, which requires significant computational power to extract, resulting in a time-consuming process. An alternative option to integrate CNN features over time is the implementation of recurrent networks, whose performance, however, is not so satisfying as recent CNN-based methods <ref type="bibr" target="#b7">[8]</ref>. 3D-CNNs have been increasingly explored in video analysis tasks recently, which learns the features from both spatial and temporal dimensions simultaneously. 3D-CNNs are first exploited to extract spatiotemporal features in <ref type="bibr" target="#b8">[9]</ref>.</p><p>Afterwards, many 3D-CNN architectures have been proposed for action recog-nition task, such as C3D <ref type="bibr" target="#b9">[10]</ref>, I3D <ref type="bibr" target="#b10">[11]</ref>, P3D <ref type="bibr" target="#b11">[12]</ref>, R(2+1)D <ref type="bibr" target="#b12">[13]</ref>, SlowFast <ref type="bibr" target="#b13">[14]</ref>, etc. In <ref type="bibr" target="#b14">[15]</ref>, the effect of dataset size on performance is investigated for several 3D-CNN architectures. It must be noted that 3D-CNN architectures have much more parameters compared to 2D-CNNs, making them computationally expensive. In <ref type="bibr" target="#b15">[16]</ref>, 3D versions of some famous resource efficient CNN architectures are investigated. For resource efficiency, some other works focus on learning 2D features from single images with a 2D-CNN and then fusing them together to learn temporal features with a 3D-CNN <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatiotemporal action localization. For object detection in images,</head><p>R-CNN series extract region proposals using selective search <ref type="bibr" target="#b17">[18]</ref> or Region Proposal Network (RPN) <ref type="bibr" target="#b0">[1]</ref> in the first stage and classify the objects in these potential regions in the second stage. Although Faster R-CNN <ref type="bibr" target="#b0">[1]</ref> achieves state-of-the-art results in object detection, it is hard to implement it for realtime tasks due to its time-consuming two-stage architecture. Meanwhile, YOLO <ref type="bibr" target="#b18">[19]</ref> and SSD <ref type="bibr" target="#b19">[20]</ref> aim to simplify this process to one stage and have outstanding real-time performance. For action localization in videos, due to the success of R-CNN series most of the research approaches propose first detecting the humans in each frame and then linking these bounding boxes reasonably as action tubes <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b1">2]</ref>. Two-stream detectors introduce an additional stream on the base of the original classifier for optical flow modality <ref type="bibr" target="#b2">[3]</ref> [22] <ref type="bibr" target="#b22">[23]</ref>. Some other works produce clip tube proposals with 3D-CNNs and achieve regression as well as classification on the corresponding 3D features <ref type="bibr" target="#b1">[2]</ref>  <ref type="bibr" target="#b21">[22]</ref>, thus region proposal is necessary for them. In a recent work <ref type="bibr" target="#b23">[24]</ref>, authors propose a 3D capsule network for video action detection which can jointly perform pixel-wise action segmentation along with action classification. However, it is too expensive in terms of computational complexity and number of parameters since it is a U-Net <ref type="bibr" target="#b24">[25]</ref> based 3D-CNN architecture.</p><p>Attention modules. Attention is an effective mechanism to capture longrange dependencies and has been attempted to be used in CNNs to boost the performance in image classification <ref type="bibr" target="#b25">[26]</ref> [27] <ref type="bibr" target="#b27">[28]</ref> and scene segmentation <ref type="bibr" target="#b28">[29]</ref>.</p><p>Attention mechanism is implemented spatial-wise and channel-wise in these works, in which spatial attention addresses the inter-spatial relationship among features while channel attention enhances the most meaningful channels and weakens the others. As a channel-wise attention block, Squeeze-and-Excitation module <ref type="bibr" target="#b29">[30]</ref> is beneficial to increase CNN's performance with little computational cost. On the other hand, for video classification tasks, non-local block <ref type="bibr" target="#b30">[31]</ref> takes spatio-temporal information into account to learn the dependencies of features across frames, which can be viewed as a self-attention strategy.</p><p>Different from previous works, we have proposed a novel, unified framework called YOWO for the task of spatio-temporal action localization. We name it as YOWO as we make use of a clip only once and detect the corresponding actions in the key frame. However, to avoid the complex optical flow computation, we use 2D features of the key frame and 3D features of the clip together.</p><p>Afterwards, these two kinds of features are fused together carefully with the application of attention mechanism such that rich contextual relationships are well taken into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we first present YOWO's architecture in detail, which extracts 2D features from the key frame as well as 3D features from the input clip concurrently and aggregates them together. Afterwards the implementation of channel fusion and attention mechanism is discussed, which provides the essential performance boost. Finally we describe the details of the training process for the YOWO architecture and the improved bounding box linking strategy for generation of action tubes in untrimmed videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">YOWO architecture</head><p>The YOWO architecture is illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>, which can be divided into four major parts: 3D-CNN branch, 2D-CNN branch, CFAM and bounding box regression parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D-CNN Branch</head><p>Since contextual information is crucial for human action understanding, we utilize 3D-CNN to extract spatiotemporal features. 3D-CNNs are able to capture motion information by applying convolution operation not only in space dimension but also in time dimension. The basic 3D-CNN architecture in our framework is 3D-ResNext-101 due to its high performance in Kinetics dataset <ref type="bibr" target="#b14">[15]</ref>. In addition to 3D-ResNext-101, we have also experimented with different 3D-CNN models in our ablation study. For all 3D-CNN architectures, all of the layers after the last conv layer are discarded. The input to the 3D network is a clip of a video, which is composed of a sequence of successive frames in time order, and has a shape of [C ? D ? H ? W ], while the last conv layer of 3D ResNext-101 outputs a feature map of shape</p><formula xml:id="formula_0">[C ? D ? H ? W ]</formula><p>where C = 3, D is the number of input frames, H and W are height and width of input images, C is the number of output channels, D = 1, H = H 32 and W = W 32 . The depth dimension of the output feature map is reduced to 1 such that output volume is squeezed to [C ? H ? W ] in order to match the output feature map of 2D-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D-CNN Branch</head><p>In the meantime, to address the spatial localization problem, 2D features of the key frame are also extracted in parallel. We employ Darknet-19 <ref type="bibr" target="#b31">[32]</ref> as the basic architecture in our 2D CNN branch due to its good balance between accuracy and efficiency. The key frame with the shape</p><formula xml:id="formula_1">[C ? H ? W ]</formula><p>is the most recent frame of the input clip, thus there is no need for an additional data loader. The output feature map of Darknet-19 has a shape of makes it more flexible. YOWO is designed to be simple and effort-saving to switch models. It must be noted that although YOWO has two branches, it is a unified architecture and can be trained end-to-end.</p><formula xml:id="formula_2">[C ? H ? W ] where C = 3, C is</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature aggregation: Channel Fusion and Attention Mechanism (CFAM)</head><p>We make the outputs of both 3D and 2D networks are of the same shape in the last two dimensions such that these two feature maps can be fused easily. We fuse the two feature maps using concatenation which simply stacks the features along channels. As a result, the fused feature map encodes both motion and appearance information which we pass as input to the CFAM module, which is based on Gram matrix to map inter-channel dependencies. Although Gram matrix based attention mechanism is originally used for style transfer <ref type="bibr" target="#b32">[33]</ref> and recently in segmentation task <ref type="bibr" target="#b28">[29]</ref>, such an attention mechanism is beneficial for fusing features coming from different sources reasonably, which improves the overall performance significantly. Assume F ? R C?N is the reshaped tensor from feature map B, where N = H ? W , which means that features in every single channel is vectorized to one dimension:</p><formula xml:id="formula_3">B ? R C?H?W vectorization ? ???????? ? F ? R C?N<label>(1)</label></formula><p>Then a matrix product between F ? R C?N and its transpose F T ? R N ?C is performed to produce Gram matrix G ? R C?C , which indicates the feature correlations across channels <ref type="bibr" target="#b32">[33]</ref>:</p><formula xml:id="formula_4">G = F ? F T with G ij = N k=1 F ik ? F jk<label>(2)</label></formula><p>where each element G ij in the Gram matrix G represents the inner product between the vectorized feature maps i and j. After computing the Gram matrix, a softmax layer is applied to generate channel attention map M ? R C?C :</p><formula xml:id="formula_5">M ij = exp(G ij ) C j=1 exp(G ij )<label>(3)</label></formula><p>where M ij is a score measuring the j th channel's impact on the i th channel.</p><p>Therefore M summaries the inter-channel dependency of features given a feature map. To perform the impact of attention map to original features, a further matrix multiplication between M and F is carried out and the result is reshaped back to 3-dimensional space R C?H?W , which has the same shape as the input tensor:</p><formula xml:id="formula_6">F = M ? F (4) F ? R C?N reshape ? ???? ? F ? R C?H?W<label>(5)</label></formula><p>The output of channel attention module C ? R C?H?W combines this result with the original input feature map B with a trainable scalar parameter ? using an element-wise sum operation, and ? gradually learns a weight from 0:</p><formula xml:id="formula_7">C = ? ? F + B<label>(6)</label></formula><p>The Eq. <ref type="formula" target="#formula_7">(6)</ref>  A larger product indicates that the features in these two channels are more correlated while a smaller product suggests that they are different from each other.</p><p>For a given channel, we allocate more weights to the other channels which are much correlated and have more impact to it. By means of this mechanism, contextual relationship is emphasized and feature discriminability is enhanced. loss with beta=1 for localization as in <ref type="bibr" target="#b33">[34]</ref>, which is given as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bounding box regression</head><formula xml:id="formula_8">L 1,smooth (x, y) = ? ? ? ? ? 0.5(x ? y) 2 if |x ? y| &lt; 1 |x ? y| ? 0.5 otherwise<label>(7)</label></formula><p>where x and y refers to network prediction and ground truth, respectively.</p><p>Smooth L 1 loss is less sensitive to outliers than the MSE loss and prevents exploding gradients in some cases. We still apply the MSE loss for confidence score, which is given as follows:</p><formula xml:id="formula_9">L M SE (x, y) = (x ? y) 2<label>(8)</label></formula><p>The final detection loss becomes the summation of individual coordinate losses for x, y, width and height; and confidence score loss, which is given as follows:</p><formula xml:id="formula_10">L D = L x + L y + L w + L h + L conf<label>(9)</label></formula><p>We have applied focal loss <ref type="bibr" target="#b34">[35]</ref> for classification, which is given as follows:</p><formula xml:id="formula_11">L f ocal (x, y) = y(1 ? x) ? log(x) + (1 ? y)x ? log(1 ? x)<label>(10)</label></formula><p>where x is the softmaxed network prediction and y ? {0, 1} is the ground truth class label. ? is the modulating factor, which reduce the loss of samples with high confidence (i.e. easy samples) and increase the loss of samples with low confidence (i.e. hard samples). However, AVA dataset is a multi-label dataset</p><p>where each person performs one pose action (e.g. walking, standing, etc.) and multiple human-human or human-object interaction actions. Therefore, we have applied softmax to pose classes and sigmoid to the interaction actions. Moreover, AVA is an unbalanced dataset and modulating factor ? is not enough to tackle dataset imbalance. Therefore, we have used ?-balanced variant of focal loss <ref type="bibr" target="#b34">[35]</ref>. For ? term, we have used exponential of class sample ratios.</p><p>The final loss that is used for the optimization of YOWO architecture is the summation of detection and classification loss, which is given as follows:</p><formula xml:id="formula_12">L f inal = ?L D + L Cls<label>(11)</label></formula><p>where ? = 0.5 performs best in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Implementation details</head><p>We initialize the 3D and 2D network parameters separately: 3D part with pretrained models on Kinetics <ref type="bibr" target="#b10">[11]</ref> and 2D part with pretrained models on PAS-CAL VOC <ref type="bibr" target="#b35">[36]</ref>. Although our architecture consists of 2D-CNN and 3D-CNN branches, the parameters are able to be updated jointly. We select the minibatch stochastic gradient decent algorithm with momentum and weight decay strategy to optimize the loss function. The learning rate is initialized as 0.0001 and reduced with a factor of 0.5 after 30k, 40k, 50k and 60k iterations. For the dataset UCF101-24, the training process is completed after 5 epochs while for J-HMDB-21 after 10 epochs. The complete architecture is implemented and trained end-to-end in PyTorch using a single Nvidia Titan XP GPU.</p><p>In the trainings, because of the small number of samples in J-HMDB-21,</p><p>we freeze all the 3D conv network parameters thus the convergence is faster and over-fitting risk can be reduced. In addition, we deploy several data augmentation techniques at training time such as flipping images horizontally in the clip, random scaling and random spatial cropping. During testing, only detected bounding boxes with confidence score larger than threshold 0.25 are selected and then post-processed with non-maximum suppression with a threshold of 0.4 for UCF101-24 and J-HMDB-21 datasets; and 0.5 for AVA dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Linking Strategy</head><p>As we have already obtained frame-level action detections, next step is to link these detected bounding boxes to construct action tubes in the whole video for UCF101-24 and J-HMDB-21 datasets. We make use of the linking algorithm described in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b2">3]</ref> to find the optimal video-level action detections.</p><p>Assume R t and R t+1 are two regions from consecutive frames t and t+1, the linking score for an action class c is defined as</p><formula xml:id="formula_13">s c (R t , R t+1 ) = ?(ov) ? [s c (R t ) + s c (R t+1 ) + ? ? s c (R t ) ? s c (R t+1 ) + ? ? ov(R t , R t+1 )]<label>(12)</label></formula><p>where s c (R t ), s c (R t+1 ) are class specific scores of regions R t and R t+1 , ov is the intersection-over-union of these two regions, ? and ? are scalars. ?(ov) is a constraint which is equal to 1 if an overlap exists (ov &gt; 0), otherwise ?(ov) is equal to 0. We extend the linking score definition in <ref type="bibr" target="#b2">[3]</ref> with an extra element ? ? s c (R t ) ? s c (R t+1 ), which takes the dramatic change of scores between two successive frames into account and is able to improve the performance of video detection in experiments. After all the linking scores are computed, Viterbi algorithm is deployed to find the optimal path to generate action tubes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Long-Term Feature Bank</head><p>Although YOWO's inference is online and causal with small clip size, 16-frame input limits the temporal information required for action understanding. Therefore, we make use of a long-term feature bank (LFB) similar to <ref type="bibr" target="#b3">[4]</ref>, which con-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To evaluate YOWO's performance, three popular and challenging action detection datasets are selected: UCF101-24 <ref type="bibr" target="#b36">[37]</ref>, J-HMDB-21 <ref type="bibr" target="#b37">[38]</ref> and AVA <ref type="bibr" target="#b38">[39]</ref>.</p><p>Each of these datasets contains different characteristics, which are compared in <ref type="table" target="#tab_1">Table 1</ref>. We follow the official evaluation metrics strictly to report the results and compare the performance of our method with the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and evaluation metrics</head><p>UCF101-24 is a subset of UCF101 <ref type="bibr" target="#b36">[37]</ref>, which is originally an action recognition dataset of realistic action videos. UCF101-24 contains 24 action classes and 3207 videos, for which the corresponding spatiotemporal annotations are provided. In addition, there might be multiple action instances in each video, which have the same class label but different spatial and temporal boundaries. Such a property makes video-level action detection much more challenging. As in previous works, we perform all the experiments on the first split.</p><p>J-HMDB-21 is a subset of the HMDB-51 dataset <ref type="bibr" target="#b39">[40]</ref> and consists of 928 short videos with 21 action categories in daily life. Each video is well trimmed and has a single action instance across all the frames. We report our experimental results on the first split.</p><p>AVA is a video dataset of spatiotemporally localized Atomic Visual Actions </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation study</head><p>3D network, 2D network or both? Depending only on its own, neither 3D-CNN nor 2D-CNN can solve the spatiotemporal localization task independently. However, if they operate simultaneously, there is potential to benefit from one another. Results on comparing the performance of different architectures are reported in  It is also shown that CFAM module further boosts the performance from 73.8%</p><p>to 79.2% on UCF101-24, from 47.1% to 64.9% on J-HMDB-21 and from 16.0% to 16.4% on AVA dataset. This clearly shows the importance of the attention mechanism which strengthens the inter-dependencies among channels and helps aggregating features more reasonably.</p><p>Moreover, in order to explore the impact of each 2D-CNN, 3D-CNN and CFAM blocks, we investigate the localization and the classification performance of different architectures, which is given in <ref type="table">Table 3</ref>. For localization, we look at the recall value, which is the ratio of the number of correctly localized actions to the total number of ground truth actions. For classification, we look at the classification accuracy of the correctly localized detections. For this analysis, we have excluded AVA dataset since it contains multiple actions per person, hence classification accuracy cannot be calculated. For both UCF101-24 and J-HMDB-21 datasets, 2D network is better at localization while 3D network performs better at classification. It is also obvious that CFAM module boosts both localization and classification performance.</p><p>We have also visualized the activations maps <ref type="bibr" target="#b41">[42]</ref> for 2D and 3D backbones of the trained model, which is shown in <ref type="figure" target="#fig_10">Fig. 4</ref>. Conforming our findings in <ref type="table">Table 3</ref>,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D backbone focuses on the parts of the clip where a motion is occurring and 2D</head><p>backbone focuses on fine spatial information on complete body parts of people.      How many frames are suitable for temporal information? For 3D-CNN branch, different clip lengths with different downsampling rates can change the performance of overall YOWO architecture <ref type="bibr" target="#b42">[43]</ref>. Therefore, we conduct experiments with 8-frames and 16-frames clips with different downsampling rates, which is given in <ref type="table" target="#tab_7">Table 4</ref>. For example, 8-frames (d=3) refers to selecting 8 frames from 24 frames window with downsampling rate of 3. Specifically, we compare three downsampling rates d = 1, 2, 3 for clip length 8-frames and two downsampling rates d = 1, 2 for 16-frames clip length. As expected, we observe that the framework with input of 16 frames performs better than 8 frames since long frame sequence contains more temporal information. However, as downsampling rate is increased, the performance becomes worse. We conjecture that downsampling hinders capturing motion patterns properly and too long sequence may break the temporal contextual relationship. Especially for some quick motion classes, a long sequence may contain several unrelated frames, which can be viewed as noise.</p><p>Is it possible to save model complexity with more efficient net-  some other resource efficient 3D-CNN architectures <ref type="bibr" target="#b15">[16]</ref>. <ref type="table" target="#tab_9">Table 5</ref> reports the achieved performance on all three datasets together with the number of floating point operations (FLOPs) for each 3D backbone. We find that even with light-weight architecture in 3D backbones, our framework is still better than 2D network. However, <ref type="table" target="#tab_9">Table 5</ref> clearly shows the importance of the 3D backbone.</p><p>The stronger 3D-CNN architecture we use, better the achieved results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">State-of-the-art comparison</head><p>We have compared YOWO with other state-of-the-art architectures on J-HMDB-21, UCF101-24 and AVA datasets. For the sake of fairness, we have excluded VideoCapsuleNet <ref type="bibr" target="#b23">[24]</ref> as it uses different video-mAP calculation without constructing action tubes via some linking strategies. However, YOWO still performs around 9% and 8% better than VideoCapsuleNet in terms of frame-mAP @ 0.5 IoU on J-HMDB-21 and UCF101-24, respectively.   Performance comparison on UCF101-24 dataset.    <ref type="table">Table 9</ref>: Run time and performance comparison on dataset UCF101-24 for F-mAP and V-mAP at 0.5 IoU threshold. For YOWO, ResNeXt-101 is used in its 3D backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>not employ optical flow, which is computationally burdensome. In <ref type="table">Table 9</ref>, we compare runtime performance of YOWO with other state-of-the-art methods.</p><p>YOWO's speed is calculated in terms of frames per second (fps) on a single NVIDIA Titan Xp GPU with a batch size of 8. It must be noted that YOWO's 2D and 3D backbones can be replaced with any arbitrary CNN model according to desired runtime performance. Moreover, additional new backbones can be easily introduced for different information source such as depth or infrared modalities. The only thing to do is modification of CFAM block in order to accommodate new features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Model visualization</head><p>In general, YOWO architecture performs a decent job at localizing actions in videos, which is illustrated in <ref type="figure" target="#fig_11">Fig. 6</ref> for UCF101-24 and J-HMDB-21 dataset;</p><p>and in <ref type="figure" target="#fig_13">Fig. 7</ref> for AVA dataset. However, YOWO also has some drawbacks.</p><p>Firstly, since YOWO produces its predictions according to all the information available at the key frame and the clip, it sometimes makes some false positive detections before the actions are performed. For example, in <ref type="figure" target="#fig_11">Fig. 6</ref> first row last image, YOWO sees a person holding a ball at a basketball court and detects him very confidently although he is not shooting the ball yet. Secondly, YOWO needs enough temporal content to make correct action localization. If a person starts performing an action suddenly, localization at initial frames lacks temporal content and false actions are recognized consequently, as in <ref type="figure" target="#fig_11">Fig. 6</ref> second row last image (climbing stair instead of running). Similarly, in the right-most image in <ref type="figure" target="#fig_13">Fig. 7</ref>, processed clip and key frame does not contain pose information of the person, hence YOWO cannot confidently deduce if the person is sitting or standing. Results in <ref type="table" target="#tab_7">Table 4</ref> confirms that, increasing clip length increases the available temporal information and consequently increases YOWO's performance. LFB is also leveraged for the purpose of increasing temporal content. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we presented a novel unified architecture for spatiotemporal action localization in video streams. Our approach, YOWO, models the spatiotemporal context from successive frames for action understanding while extracting the fine spatial information from key frame to address the localization task in parallel. In addition, we make use of a channel fusion and attention mechanism for effective aggregation of these two kinds of information. Since we do not separate human detection and action classification procedures, the whole network can be optimized by a joint loss in an end-to-end framework.</p><p>We have carried out a series of comparative evaluations on three challenging datasets, UCF101-24, J-HMDB-21 and AVA, each having different characteristics. Our approach outperforms the other state-of-the-art results on UCF101-24 and J-HMDB-21 datasets while achieving competitive results on AVA dataset.</p><p>Moreover, YOWO is a causal architecture and can be operated in real-time, which makes it possible to deploy YOWO on mobile devices.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Standing or sitting? Although the person can be successfully detected, correct classification of the action cannot be made by looking only at the key frame. Temporal information from previous frames needs to be incorporated in order to understand if the person is sitting (left) or standing (right). Examples are from J-HMDB-21 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1</head><label>1</label><figDesc>illustrates our inspiration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The YOWO architecture. An input clip and corresponding key frame is fed to a 3D-CNN and 2D-CNN to produce output feature volumes of [C ? H ? W ] and [C ? H ? W ], respectively. These output volumes are fed to channel fusion and attention mechanism (CFAM) for a smooth feature aggregation. Finally, one last conv layer is used to adjust the channel number for final bounding box predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Channel fusion and attention mechanism for aggregating output feature maps coming from 2D-CNN and 3D-CNN branches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>the number of output channels, H = H 32 and W = W 32 similar to the 3D-CNN case. Another important characteristic of YOWO is that architectures in 2D CNN and 3D CNN branches can be replaced by arbitrary CNN architectures, which</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3</head><label>3</label><figDesc>illustrates the used CFAM module. The concatenated feature map A ? R (C +C )?H?W can be regarded as an abrupt combination of 2D and 3D information, which neglects interrelationship between them. Therefore, we first feed A into two convolutional layers to generate a new feature map B ? R C?H ?W . Afterwards, several operations are performed on the feature map B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>shows that the final feature of each channel is a weighted sum of the features of all channels and original features, which models the longrange semantic dependencies between feature maps. Finally, the feature map C ? R C?H ?W is fed into two more convolutional layers to generate the output feature map D ? R C * ?H ?W of the CFAM module. Two convolutional layers at the beginning and the end of CFAM modules contain utmost importance since they help to mix the features coming from different backbones and having possibly different distributions. Without these convolutional layers, CFAM marginally improves the performance. Such an architecture promotes the feature representativeness in terms of inter-dependencies among channels and thus the features from different branches can be aggregated reasonably and smoothly. Besides, Gram matrix takes the whole feature map into consideration, where the dot product of each two flattened feature vectors presents the information about the relation between them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>tains features coming from 3D backbone at different timestamps. At inference time, 3D features centering the key-frame are averaged and the resulting feature map is used as input to the CFAM block. LFB features are extracted for non-overlapping 8-frame clips using the pretrained 3D ResNeXt-101 backbone. We have used 8 features (if available) centering the key-frame. So, total number of 64 frames are utilized at inference time. Utilization of LFB increases action classification performance similar to difference between clip accuracy and video accuracy in video datasets. However, introduction of LFB makes the resulting architecture non-causal since future 3D features are used at inference time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>(</head><label></label><figDesc>AVA). The AVA dataset contains 80 atomic visual actions, which are densely annotated for 430 15-minute video clips, where actions are localized in space and time. This results in 1.58M action labels and each person is labelled with multiple actions. Following the guidelines of the ActivityNet challenge, we evaluate YOWO on most-frequent 60 action classes of AVA dataset. If not stated otherwise, we report results on version 2.2 of AVA dataset.Evaluation metrics: We employ two popular metrics used by the most researches in the region of spatio-temporal action detection to generate convincing evaluations. Following strictly the rule applied by the PASCAL VOC 2012 metric<ref type="bibr" target="#b40">[41]</ref>, frame-mAP measures the area under the precision-recall curve of the detections for each frame. On the other hand, video-mAP focuses on the action tubes<ref type="bibr" target="#b20">[21]</ref>. If the mean per frame intersection-over-union with the ground truth across the frames of the whole video is greater than a threshold and in the meanwhile the action label is correctly predicted, then this detected tube is regarded as a correct instance. Finally, the average precision for each class is computed and the average over all classes is reported. For AVA dataset, we only use frame-mAP with Intersection of Union (IoU) threshold of 0.5 since annotations are sparsely provided with 1 Hz.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>3 :</head><label>3</label><figDesc>Localization @ IoU 0.5 (recall) and classification results on UCF101-24 and J-HMDB-21 datasets. For all architectures, the input to 3D-CNNs is 8 frames clips with downsampling of 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 4 :</head><label>4</label><figDesc>Activation maps for (a) 3D-CNN backbone and (b) 2D-CNN backbone. 3D-CNN backbone focuses on areas where there is a movement/action happening, whereas 2D-CNN backbone focuses on all the people in the key-frame. Examples are volleyball spiking (top), skate boarding (middle) and rope climbing (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>6 :</head><label>6</label><figDesc>Comparison with state-of-the-art methods on the J-HMDB-21 dataset. Results are reported for frame-mAP under IoU threshold of 0.5 and video-mAP under different IoU thresholds. * version of YOWO is non-causal. Performance comparison on J-HMDB-21 dataset. YOWO is compared with the previous state-of-the-art methods on J-HMDB-21 in Table 6. Using the standard metrics, we report the frame-mAP at IoU threshold of 0.5 and the video-mAP at various IoU thresholds. YOWO (16-frame) consistently outperforms the state-of-the-art results on dataset J-HMDB-21, with a frame-mAP improvement of 3.3% and a video-mAP improvement of 3.8%, 5.2% at IoU thresholds of 0.2 and 0.5, respectively. Utilization of LFB brings further improvements on the performance. However, this improvement is marginal since the video duration of videos of J-HMDB-21 dataset is maximum 40 frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of action localizations for UCF101-24 and J-HMDB-21 datasets. Red bounding boxes are ground truth while green and orange are true and false positive localizations, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of action localizations for AVA dataset. Red dashed bounding boxes are ground truth while green bounding boxes are YOWO predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>We follow the same guidelines of YOLO [32] for bounding box regression. A final convolutional layer with 1?1 kernels is applied to generate desired number of output channels. For each grid cell in H ? W , 5 prior anchors are selected by k-means technique on corresponding datasets with NumCls class conditional action scores, 4 coordinates and confidence score making the final output size of YOWO [(5 ? (N umCls + 5)) ? H ? W ]. The regression of bounding boxes are then refined based on these anchors. We have used input resolution of 224 ? 224 for both training and testing time. Applying multi-scale training with different resolutions has not shown any performance improvement in our experiments. The loss function is defined similar to the original YOLOv2 network<ref type="bibr" target="#b31">[32]</ref> except that we apply smooth L 1</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of evaluated datasets. Background people refers that in there are people in some of the frames who are not annotated. *AVA is densely annotated with 1Hz rate.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>We first observe that a single 2D network can not provide a satisfying result since it does not take temporal information into account. A single 3D network is better at capturing motion information and the fusion of 2D and 3D networks (simple concatenation) can improve the performance by around 3%, 6% and 2% mAP compared to 3D network on UCF101-24, J-HMDB-21 and AVA datasets, respectively. This indicates that 2D-CNN learns finer spatial features and 3D-CNN concentrates more on the motion process yet the spatial drift of an action in the clip may lead to a lower localization accuracy.</figDesc><table><row><cell>Model</cell><cell cols="3">UCF101-24 J-HMDB-21 AVA</cell></row><row><cell>2D</cell><cell>61.6</cell><cell>36.0</cell><cell>13.2</cell></row><row><cell>3D</cell><cell>70.5</cell><cell>41.5</cell><cell>13.7</cell></row><row><cell>2D + 3D</cell><cell>73.8</cell><cell>47.1</cell><cell>16.0</cell></row><row><cell>2D + 3D + CFAM</cell><cell>79.2</cell><cell>64.9</cell><cell>16.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Frame-mAP @ IoU 0.5 results on UCF101-24, J-HMDB-21 and AVA datasets for</cell></row><row><cell>different models. For all architectures, the input to 3D-CNNs is 8 frames clips with downsam-</cell></row><row><cell>pling of 1.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>This validates that backbones of YOWO extract complementary features.</figDesc><table><row><cell></cell><cell>Model</cell><cell>Localization</cell><cell>Classif.</cell></row><row><cell></cell><cell></cell><cell>(recall)</cell><cell></cell></row><row><cell>UCF101-24</cell><cell>2D 3D 2D + 3D 2D + 3D + CFAM</cell><cell>91.7 90.8 93.2 93.5</cell><cell>85.9 92.9 93.7 94.5</cell></row><row><cell>J-HMDB-21</cell><cell>2D 3D 2D + 3D 2D + 3D + CFAM</cell><cell>94.3 76.3 94.5 97.3</cell><cell>50.6 69.3 63.0 76.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>Frame-mAP @ IoU 0.5 results on UCF101-24, J-HMDB-21 and AVA datasets for</cell></row><row><cell>different clip lengths and different downsampling rates d.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Performance comparison of different 3D backbones on UCF101-24, J-HMDB-21 and AVA datasets. For all architectures, Darknet-19 is used as 2D backbone. The number of floating point operation (FLOPs) are calculated for corresponding 3D backbones for 16 frames (d=1) clips with spatial resolution of 224 ? 224.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7</head><label>7</label><figDesc>presents the comparison of YOWO with the state-of-the-art methods on UCF101-24. YOWO (16-frame) achieves 80.4% with respect to frame-mAP metric, which is significantly better than the others by preceding the second best result with 5.4% improvement. As for video-mAP, our framework also produces very competi-</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Video-mAP</cell><cell></cell></row><row><cell>Method</cell><cell>Frame-mAP</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.1</cell><cell>0.2</cell><cell>0.5</cell></row><row><cell>Peng w/o MR [3]</cell><cell>64.8</cell><cell>49.5</cell><cell>41.2</cell><cell>-</cell></row><row><cell>Peng w/ MR [3]</cell><cell>65.7</cell><cell>50.4</cell><cell>42.3</cell><cell>-</cell></row><row><cell>ROAD [23]</cell><cell>-</cell><cell>-</cell><cell>73.5</cell><cell>46.3</cell></row><row><cell>T-CNN [2]</cell><cell>41.4</cell><cell>51.3</cell><cell>47.1</cell><cell>-</cell></row><row><cell>ACT [44]</cell><cell>69.5</cell><cell>-</cell><cell>77.2</cell><cell>51.4</cell></row><row><cell>MPS [47]</cell><cell>-</cell><cell>82.4</cell><cell>72.9</cell><cell>41.1</cell></row><row><cell>STEP [48]</cell><cell>75.0</cell><cell>83.1</cell><cell>76.6</cell><cell>-</cell></row><row><cell>YOWO (16-frame)</cell><cell>80.4</cell><cell>82.5</cell><cell>75.8</cell><cell>48.8</cell></row><row><cell>YOWO+LFB*</cell><cell>87.3</cell><cell>86.1</cell><cell>78.6</cell><cell>53.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Comparison with state-of-the-art methods on the UCF101-24 dataset. Results are reported for frame-mAP under IoU threshold of 0.5 and video-mAP under different IoU thresholds. * version of YOWO is non-causal.tive results even though we just utilize a simple linking strategy. Utilization of LFB brings considerable improvement this time since the duration of UCF101-24 videos is much bigger than J-HMDB-21 videos. LFB further increases frame-mAP performance by around 7%.Performance comparison on AVA dataset. We have compared the performance of YOWO on AVA dataset inTable 8. YOWO is currently the first and only single-stage architecture, which provides competitive results on AVA dataset. All the methods outperforming YOWO are non-causal (i.e. utilizing future frames) and multi-stage architectures mostly utilizing Faster-RCNN architecture. Moreover, these methods either requires high resolution input such as 600 pixels for<ref type="bibr" target="#b49">[50]</ref> and 400 pixels for<ref type="bibr" target="#b50">[51]</ref>, or strong and computationally heavy SlowFast architecture as 3D-CNN backbone such as<ref type="bibr" target="#b13">[14]</ref>. On the other hand, YOWO operates only on the current and previous frames (i.e. causal) with input resolution of 224 ? 224. Increasing clip size from 8-frames to 32-frames</figDesc><table><row><cell>Method</cell><cell>Single</cell><cell cols="4">Input AVA Pretrain mAP</cell></row><row><cell></cell><cell>Stage</cell><cell></cell><cell></cell><cell></cell></row><row><cell>I3D [39]</cell><cell></cell><cell>V+F</cell><cell></cell><cell>K400</cell><cell>15.6</cell></row><row><cell>ACRN, S3D [49]</cell><cell></cell><cell>V+F</cell><cell></cell><cell>K400</cell><cell>17.4</cell></row><row><cell>STEP, I3D [48]</cell><cell></cell><cell>V+F</cell><cell></cell><cell>K400</cell><cell>18.6</cell></row><row><cell>RTPR [50]</cell><cell></cell><cell>V+F</cell><cell></cell><cell>ImageNet</cell><cell>22.3</cell></row><row><cell>Action Transformer, I3D [51]</cell><cell></cell><cell>V</cell><cell></cell><cell>K400</cell><cell>25.0</cell></row><row><cell>LFB, R101+NL [4]</cell><cell></cell><cell>V</cell><cell>v2.1</cell><cell>K400</cell><cell>27.4</cell></row><row><cell>SlowFast, R101, 8x8 [14]</cell><cell></cell><cell>V</cell><cell></cell><cell>K400</cell><cell>26.3</cell></row><row><cell>YOWO (8-frame)</cell><cell></cell><cell>V</cell><cell></cell><cell>K400</cell><cell>15.7</cell></row><row><cell>YOWO (16-frame)</cell><cell></cell><cell>V</cell><cell></cell><cell>K400</cell><cell>17.2</cell></row><row><cell>YOWO (32-frame)</cell><cell></cell><cell>V</cell><cell></cell><cell>K400</cell><cell>18.3</cell></row><row><cell>YOWO+LFB*</cell><cell></cell><cell>V</cell><cell></cell><cell>K400</cell><cell>19.2</cell></row><row><cell>SlowFast, R101+NL, 8x8 [14]</cell><cell></cell><cell>V</cell><cell></cell><cell>K600</cell><cell>29.0</cell></row><row><cell>YOWO (8-frame)</cell><cell></cell><cell>V</cell><cell></cell><cell>K400</cell><cell>16.4</cell></row><row><cell>YOWO (16-frame)</cell><cell></cell><cell>V</cell><cell>v2.2</cell><cell>K400</cell><cell>17.9</cell></row><row><cell>YOWO (32-frame)</cell><cell></cell><cell>V</cell><cell></cell><cell>K400</cell><cell>19.1</cell></row><row><cell>YOWO+LFB*</cell><cell></cell><cell>V</cell><cell></cell><cell>K400</cell><cell>20.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Comparison with state-of-the-art methods on the AVA dataset. Results are reported for frame-mAP under IoU threshold of 0.5. * version of YOWO is non-causal. brings an improvement of almost 3% mAP. Utilization of LFB further improves the performance by around 1% maP. We also evaluate performance of YOWO (32-frames, d=1) per each class in Fig. 5. The classes are sorted by number of training samples. Although we observe some correlation with the amount of training data, there exist some classes with enough data with poor performance such as smoking.Runtime comparison Most of the state-of-the-art methods are two stage architectures, which are computationally expensive to run in real time. YOWO is a unified architecture, which can be trained end-to-end. In addition, we doFigure 5: Performance of YOWO (32-frames, d=1) on each class of AVA dataset v2.2. Classes are sorted by number of training samples in decreasing order.</figDesc><table><row><cell>Model</cell><cell>Speed (fps)</cell><cell>F-mAP</cell><cell>V-mAP</cell></row><row><cell>Saha et al. [22]</cell><cell>4</cell><cell>-</cell><cell>36.4</cell></row><row><cell>ROAD (A) [23]</cell><cell>40</cell><cell>-</cell><cell>40.9</cell></row><row><cell>ROAD (A+RTF)[23]</cell><cell>28</cell><cell>-</cell><cell>41.9</cell></row><row><cell>ROAD (A+AF)[23]</cell><cell>7</cell><cell>-</cell><cell>46.3</cell></row><row><cell>YOWO (8-frames, d=1)</cell><cell>62</cell><cell>79.2</cell><cell>47.6</cell></row><row><cell>YOWO (16-frames, d=1)</cell><cell>34</cell><cell>80.4</cell><cell>48.8</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We gratefully acknowledge the support by the Deutsche Forschungsgemeinschaft (DFG) under Grant No. RI 658/25-2. We also acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tube convolutional neural network (t-cnn) for action detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5822" to="5831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-region two-stream r-cnn for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="744" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long-term feature banks for detailed video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="284" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
	<note>Quo vadis, action recognition? a new model and the kinetics dataset</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6546" to="6555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Resource efficient 3d convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kopuklu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gunduz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV) Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolutional network for online video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="695" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
	<note>Ssd: Single shot multibox detector</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="759" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning for detecting multiple space-time action tubes in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sapienza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cuzzolin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="58" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Online real-time multiple spatiotemporal action localisation and prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sapienza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cuzzolin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3637" to="3646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Videocapsulenet: A simplified network for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7610" to="7619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scacnn: Spatial and channel-wise attention in convolutional networks for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5659" to="5667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<title level="m">Non-local neural networks, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.06576</idno>
		<title level="m">A neural algorithm of artistic style</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<title level="m">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3192" to="3199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A video dataset of spatio-temporally localized atomic visual actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Ava</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6047" to="6056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning Deep Features for Discriminative Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Analysis on temporal dimension of inputs for 3d convolutional neural networks, in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>K?p?kl?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing, Applications and Systems (IPAS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="79" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Action tubelet detector for spatio-temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kalogeiton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4405" to="4413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">P3d-ctn: Pseudo-3d convolutional tube network for spatio-temporal action detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="300" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Predicting action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cuzzolin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Cnn-based multiple path search for action tube detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H P</forename><surname>Alwando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-H</forename><surname>Fang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Step: Spatiotemporal progressive learning for video action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="264" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Actor-centric relation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="318" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Recurrent tubelet proposal and recognition networks for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="303" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Video action transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
