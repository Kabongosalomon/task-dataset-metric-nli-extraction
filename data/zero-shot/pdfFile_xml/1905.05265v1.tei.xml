<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cooper: Cooperative Perception for Connected Autonomous Vehicles based on 3D Point Clouds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of North Texas</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sihai</forename><surname>Tang</surname></persName>
							<email>sihaitang@my.unt.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of North Texas</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of North Texas</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Fu</surname></persName>
							<email>song.fu@unt.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of North Texas</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cooper: Cooperative Perception for Connected Autonomous Vehicles based on 3D Point Clouds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Autonomous vehicles may make wrong decisions due to inaccurate detection and recognition. Therefore, an intelligent vehicle can combine its own data with that of other vehicles to enhance perceptive ability, and thus improve detection accuracy and driving safety. However, multi-vehicle cooperative perception requires the integration of real world scenes and the traffic of raw sensor data exchange far exceeds the bandwidth of existing vehicular networks. To the best our knowledge, we are the first to conduct a study on raw-data level cooperative perception for enhancing the detection ability of self-driving systems. In this work, relying on LiDAR 3D point clouds, we fuse the sensor data collected from different positions and angles of connected vehicles. A point cloud based 3D object detection method is proposed to work on a diversity of aligned point clouds. Experimental results on KITTI and our collected dataset show that the proposed system outperforms perception by extending sensing area, improving detection accuracy and promoting augmented results. Most importantly, we demonstrate it is possible to transmit point clouds data for cooperative perception via existing vehicular network technologies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A significant part of the push towards autonomous driving vehicles, or self-driving vehicles, has been supported by the prospect that they will save lives by getting involved in fewer crashes with fewer injuries and deaths than humandriven cars. However, up until this point, most comparisons between human driven cars and self-driving vehicles have been unbalanced and contain various unfair elements. Self-driving cars do not experience fatigue, emotional debilitation such as anger or frustration. But, they are unable to react to uncertain and ambiguous situations with the same skill or anticipation of an attentive and seasoned human driver.</p><p>Similarly, isolated self driving vehicles may make wrong decision due to the failure of objects detection and recognition. Just as a human driver will make bad decisions while under the influence, such decisions made by the vehicle based on these failures will prove just as bad or worse than their human counterpart. Such vehicles must completely rely on itself for decision making, and thus will not have the privilege of data redundancy, i.e., no information is received from nearby vehicles. Sensor failure or any other technical error will lead to fallacious results, leading to disastrous impacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Motivations</head><p>The deficit of data due to single source will ultimately have a negative impact as well. Take the example of Tesla's crash in California, the car made a fatal decision because it's sensors picked up the concrete barrier but discarded the information due its immobile state on the radar <ref type="bibr" target="#b25">[26]</ref>. One more incident of a fatal decision is even more pronounced due to the inability to detect an vehicle from the sensors and environmental conditions. Take for example the fatal crash made by a Tesla car in Florida, where both the vehicle and the driver could not discern the white truck against a bright sky, causing the crash <ref type="bibr" target="#b7">[8]</ref>.</p><p>Of course, there are also instance of various other circumstances leading up to bad decisions, such as the Uber training incident <ref type="bibr" target="#b16">[17]</ref>. In this case, the vehicle did detect an unknown object, the pedestrian, from a distance. As the vehicle approached the unknown object, it gradually discerned the object to be a vehicle and finally a pedestrian, but by then, it was too late.</p><p>We further explore the reasons why detection failure happened. It is easy to determine that some detection failures are caused due to objects being blocked or existing in the blind zones of the sensors. Detection failures could also be caused by bad recognition because the received signal is too weak or because the signal is missing due to system malfunction.</p><p>Our motivation comes from these incidents, because in contrast to isolated autonomous driving vehicles, like the ones in the accidents, connected autonomous vehicles (CAV) can share their collected data with each other leading to more information. We propose that information sharing can improve driving performance and experiences. Constructive data redundancy will provide endless possibilities for safe driving and multiple vehicles can collaborate together to compensate for data scarcity and provide a whole new scope for the vehicle in need. Autonomous vehicles have powerful perception systems, and together, they can achieve a proper data sharing and analysis platform to gain much more reliability and accuracy <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Limitations of Prior Work</head><p>Although adding connectivity to vehicles has its benefits, it also has challenges. By adding connectivity, there can be issues with security, privacy, and data analytics and aggregation due to the large volume of information being accessed and shared.</p><p>Current state of multi-sensor fusion consists of three distinct categories: low level fusion, feature level fusion, and high level fusion <ref type="bibr" target="#b22">[23]</ref>. Each of these categories possess its own unique advantages and disadvantages. As their names imply, low level fusion consists of raw data fusion without any pre-processing done to the data. Feature-level fusion takes the features extracted from the raw data before fusion. Finally, high level fusion takes the objects detected from each individual sensors and conducts the fusion on the object detection results <ref type="bibr" target="#b22">[23]</ref>.</p><p>High level fusion is often opted over the other two levels of fusion due to being less complex, but this is not suitable for our needs. Object level relies too heavily on single vehicular sensors and will only work when both vehicles share a reference object in their detection. This does not solve the issue of previously undetected objects, which will remain undetected even after fusion. And thus, we turn our sights on the other two categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Proposed Solution</head><p>To tackle the issue, we look at one of the base categories, the low level fusion of raw data. Raw sensing data is an integral part of all sensors on autonomous driving vehicle, therefore, it is very suitable for transferring them between different cars from various manufactures. As such, the heterogeneity of different data processing algorithms would not affect the accuracy of the data being shared among vehicles. As autonomous driving is of and in itself a crucial task, being so integrated in the vehicle, even a single small error in detection can lead to a catastrophic accident. Therefore, we need the autonomous cars to perceive the environment with as much clarity as possible. To achieve this end goal, they will need a robust and reliable perception system.</p><p>Two major issues that we seek to address in doing so are as follows: (1) the type of data that we need to share among vehicles, and (2) the amount of the data that needs to be transferred versus the amount of data that is actually necessary to the recipient vehicle. The first issue arises with the shareable data within the dataset native to the car. The second problem exists in the sheer amount of data that each vehicle generates. Since each autonomous vehicle will collect more than 1000GB of data <ref type="bibr" target="#b1">[2]</ref> every day the challenge of assembling only the regional data becomes even harder. Similarly, reconstructing the shared data collected from different positions and angles by nearby perception system is another major challenge.</p><p>Of the different types of raw data, we propose to use the LiDAR (Light Detection and Ranging) point clouds as a solution for the following reasons:</p><p>? LiDAR point clouds have the advantage of spatial dimension over 2D images and video. ? Native obfuscation of entities or private data such as people's faces and license plate numbers while preserving the accurate model of the perceived object. ? Versatility in the fusion process over images and video due to the data being consisted from points rather than pixels. For image or video fusion, the requirement is a clear zone of overlap, and this is unnecessary for point cloud data, making this a much more robust choice, especially when taking the different possible point of views of cars into perspective.</p><p>With the three different highlights of using the raw LiDAR data as our fusion substrate, we propose the Cooperative Perception (Cooper) system for connected autonomous vehicles based on 3D point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Contributions</head><p>Inaccurate object detection and recognition are major impediments in achieving a powerful and effective perception system. Autonomous vehicle eventually succumb to this inability and fail to deliver the expected outcome, which is unsafe to autonomous driving. To address these issues we have proposed a solution in which an autonomous vehicle combines its own sensing data with that of other connected vehicles to help enhance perception. We also believe that data redundancy, as mentioned, is the solution to this problem and we can achieve it through data sharing and combination between autonomous vehicles. The proposed Cooper system can improve the detection performance and driving experience thus providing protection and safety. Specifically, we make the following contributions.</p><p>? We propose the Sparse Point-cloud Object Detection (SPOD) method to detect objects in low-density point clouds data. Although SPOD is designed for low-density point cloud, it also works on high-density LiDAR data. ? We show how the proposed Cooper system outperforms individual perception by extending sensing area and improving detection accuracy. ? We demonstrate that it is possible to use existing vehicular network technology to facilitate the transmission of region of interest (ROI) LiDAR data among vehicles to realize cooperative perception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. COOPERATIVE SENSING</head><p>Given the current outlook and work done in the field of data fusion in autonomous vehicles, we need to go a step further and define what we see as cooperative sensing. We envision cooperative sensing for CAVs as a series of challenges and benefits that will be an unavoidable part of progress.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Benefits of Sharing</head><p>Based on our observations, we wonder if detection accuracy can be improved using sensor data from multiple cars. As we know, the sensing devices on autonomous vehicles work together to map the local environment and monitor the motion surrounding vehicles. According to the collected data, shareable resources can be extracted from these vehicles. For example, there is a blocked area region behind obstacles on the road that could not be sensed by one car but data gathered for this same area can be sensed and provided by other nearby cars. Meanwhile, vehicles on adjacent districts or crowded zones can keep connection for a longer duration, thereby enhancing cooperative sensing, which will greatly help other vehicles by providing crucial information. Hence, we propose a cooperative perception method to improve autonomous driving performance. This framework facilitates a vehicle to combine its sensor data with that of its cooperators' to enhance perceptive ability, and thus improving detection accuracy and driving safety.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Difficulty of Sharing</head><p>Even though shareable resources offer useful information, vehicles prefer to utilize raw data rather than extracted results. The detected results from other cars are hard to authenticate and trust issues further complicate this matter. Also, since sharing all collected data is also impractical, we need to take into consideration the bandwidth and latency of vehicular networks. First, the bandwidth and latency of vehicular networks must satisfy data transmission for cooperative perception. Then, the vehicles need to reconstruct the received data because it was taken on different positions and angles. With this series of questions, we elaborate our research on building cooperative perception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Data Choice</head><p>First, we demonstrate which type of sensing data is suitable for cooperative perception. Noting that perception systems are mainly developed on image-based and LiDAR-based sensor data. As we mention before, image data holds advantage on object classification and recognition while lacking on location information. In the next section, our proposed SPOD method overcomes the shortcomings of point clouds, which were too sparse to detect objects. Based on the above reasons, we make a priority of these two sensor data for cooperative sensing. We prefer LiDAR data because it holds advantage in providing location information <ref type="bibr" target="#b21">[22]</ref>. By only extracting positional coordinates and reflection value, point clouds can be compress into 200 KB per scan. For some applications, such as small object detection, for example license plate tracking, it is difficult for point clouds to recognize plate information. However, when utilized with cooperative perception, we are still able to locate the plates in point clouds and ask for its image data from connected vehicles. Because image and LiDAR point clouds are aligned together in perception system's installation, we integrate the above demand-driven strategy mainly relying on point clouds. In some cases, it is necessary to extract a fragment of the image data in cooperative perception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Data Reconstruction</head><p>Also, vehicles need to reconstruct the received data because it was taken on different positions and angles. By exchanging LiDAR data, local environment can be reconstructed intuitively by merging point clouds into its physical positions. In order to reconstruct local environment by mapping point clouds into physical positions, additional information is encapsulated into the exchange package. Said package should be constituted from LiDAR sensor installation information and its GPS reading, which determines the center point position of every frame of point clouds. Vehicle's IMU (inertial measurement unit) reading is also required because it records the offset information of the vehicle during driving: it represents a rotation whose yaw, pitch, and roll angles are ?, ? and ?, respectively <ref type="bibr" target="#b24">[25]</ref>. A rotation matrix R will be generated in Equation <ref type="bibr" target="#b0">1</ref>.</p><formula xml:id="formula_0">R = R z (?)R y (?)R x (?)<label>(1)</label></formula><p>Here R z (?), R y (?), R x (?) are three basic rotation matrices rotate vectors by an angle on the z-, y-, x-axis in three dimensions.</p><formula xml:id="formula_1">R z (?) = ? ? cos? ?sin? 0 sin? cos? 0 0 0 1 ? ? R y (?) = ? ? cos? 0 sin? 0 1 0 ?sin? 0 cos? ? ? R x (?) = ? ? 1 0 0 0 cos? ?sin? 0 sin? cos? ? ? . ? ? X Y Z ? ? = ? ? X R Y R Z R ? ? ? ? X T Y T Z T ? ? (2) ? ? X T Y T Z T ? ? = R ? ? ? X T Y T Z T ? ? + ? ? ?d x T ?d y T ?d z T ? ?<label>(3)</label></formula><p>When connected vehicles exchange message, cooperative perception produces a new frame by combining transmitter and receiver's sensor data using Equation 2, where we have the set of all coordinates equal to the coordinates of the receiver union with the the coordinates from the transmitter. However, as the transmitting vehicle is in a different state than the receiver, we must apply a transform to the original coordinates so that they match the state of the receiving vehicle. To obtain the correct state for the transmitter's orientation, we use Equation 1.</p><p>Note, the X, Y , and Z in XY Z represents the 3-D space value of each point in the LiDAR point cloud data, and X T Y T Z T is the transmitter's point cloud after applying the transform R to the translated coordinates of the transmitting vehicle. The transform is calculated by Equation 1, using the IMU value difference between the transmitter and the receiver.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. COOPERATIVE PERCEPTION</head><p>In this section, we will show how to detect objects on cooperative sparse LiDAR point could data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Object Detection based on Point Clouds</head><p>As we know, each self-driving vehicle will extract sensor data to perceive details in the local environment, such as lane detection, traffic sign detection and objects like cars, cyclists and pedestrians. However, accurate detection of objects in point clouds is a challenge due to LiDAR point clouds being sparse and it having a highly variable point density. For example, recently, based on point clouds dataset in KITTI <ref type="bibr" target="#b8">[9]</ref>, VoxelNet <ref type="bibr" target="#b30">[31]</ref> has announced its experiments on car detection task which outperformed the state-of-the-art 3D detection methods. Its car detection average precision is 89.60%, and for smaller objects, such as pedestrians and cyclists, the average precision drops to 65.95% and 74.41% respectively in a fully visible (easy) detecting environment. While in a difficult to see (hard) detecting condition, the car, pedestrian and cyclist detection further drop to 78.57%, 56.98%, and 50.49%, respectively. Another insight here is that LiDAR provides sparse 3D point clouds with location information but is hard to classify and recognize. To analyze the results from the above works, we cannot ignore the failure detection. This allows us to approach the issue from another perspectivecooperative sensing methods to improve detection accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Sparse Point-cloud Object Detection (SPOD)</head><p>Typically autonomous vehicles use single end-to-end deep neutral network to operate on a raw point cloud. However, after cooperative sensing, the re-constructed data from different LiDAR devices may have different features like point density. For example, Velodyne <ref type="bibr" target="#b2">[3]</ref> produces 64-beam, 32beam and 16-beam LiDAR devices, which provide different density point clouds. Similar to image's resolution, 3D detector using deep neutral network may have inaccuracy recognition results when used on low density point clouds. We note that 64-beam LiDAR, which provide the highest resolution LiDAR data, is well adopted by researches and companies on 3D object detection <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b28">[29]</ref>. While some others, as in our case, use 16-beam LiDAR, which outputs sparse data but has a price advantage over its higher end counterparts. This requires our proposed detection method on its assembled 3D detection model not only to work on high density data, but also can detect objects from much sparser point clouds. Unfortunately, these convolutional neural network (CNN)based object detection methods are not suitable for low-density data because of insufficient of input features. Inspired by <ref type="bibr" target="#b28">[29]</ref> proposed SECOND, an end-to-end deep neural network that learns points-wise features from point clouds, we propose the Sparse Point-cloud Object Detection (SPOD) methods which can adapt low density point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Architecture of SPOD</head><p>The proposed detector, depicted in <ref type="figure" target="#fig_0">Fig. 1</ref>, consists of three components. Our adopted 3D LiDAR point cloud is represented as a set of cartesian coordinates, (x, y, z) with reflection values. The distribution of point clouds is much too sparse and irregular. Specifically in the preprocessing, to obtain a more compact representation, point clouds are projected onto a sphere using approach from <ref type="bibr" target="#b26">[27]</ref> to generate a dense representation. In voxel feature extractor components, our framework takes represented point clouds as input, feeding extract voxel-wise features to voxel feature encoding layer, this is well demonstrated by Voxelnet <ref type="bibr" target="#b30">[31]</ref>. Then a sparse convolutional middle layer <ref type="bibr" target="#b14">[15]</ref> is applied. Sparse CNN offers computational benefits in LiDAR-based detection because the grouping step for point clouds will generate a large number of sparse voxels. In this approach, output points are not computed if there is no related input points. Finally, Region Proposal Network (RPN) <ref type="bibr" target="#b20">[21]</ref> is constructed using single shot multibox detector (SSD) architecture <ref type="bibr" target="#b15">[16]</ref>. The feature maps as input to RPN from Sparse CNN and are concatenate into one feature map for prediction. Framework in every vehicle use this single end-to-end trainable network to produce 3D detection results not only from dense LiDAR data but also from low resolution LiDAR data from nearby vehicles. Eventually, we successfully adopt SPOD to detect objects both on our collected sparse data and on dense KITTI data. In the next section, we demonstrate a full evaluation of SPOD detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EVALUATION AND RESULT ANALYSIS</head><p>In this section, we evaluate the performance of the proposed Cooper system using two real-world LiDAR datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>In the experiment, we test Cooper on two datasets: the KITTI dataset provided by the Karlsruhe Institute of Technology and Toyota Technological Institute at Chicago, and T&amp;J dataset collected by our semi-autonomous driving golf cart. Therefore, we obtain two types (dense and sparse) of point clouds. In the dense KITTI dateset, a 64-beam LiDAR sensor is used to collect point clouds. But in our T&amp;J dataset, which supplies 16-beam point cloud, the collected point cloud is 4X more sparse than KITTI's, of course, the amount of data is 4X decreased respectively. With the two datasets, we then fully evaluate the performance of the Cooper system for a total of 19 scenarios. Based on the KITTI testset data, we choose four different sets of road driving test scenarios. And at the same time, in order to enrich the experimental content and verify our design effects, we conduct 15 experiments on Cooper using the T&amp;J dataset. Note that Cooper can also be applied to heterogeneous point clouds input. We elected not to conduct this test due to a lack of suitable LiDAR datasets.</p><p>We define single shot as point clouds collected by an individual vehicle, and cooperative sensing as merging all point clouds from nearby vehicles. We systematically analyze the test results of single shot and cooperative sensing to demonstrate the performance improvement on object detection. Qualitative results of Cooper under two experimental datasets are demonstrated in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluations on KITTI Dataset</head><p>In this section, we evaluate Cooper's performance using the KITTI dataset. As we know, KITTI provides raw consecutive 3D Velodyne point clouds in several scenarios. We choose one such segment of sensing data in folder 2011/09/26/0009 as an example, shown in <ref type="figure" target="#fig_2">Fig. 2</ref>.  To corresponding with 120 ? front view image, this LiDAR data of front-view area is evaluated. At beginning time t1, one single shot frame of 64-beam raw point cloud is collected in <ref type="figure" target="#fig_2">Fig. 2a</ref>. As the testing vehicle is moving forward after two seconds, another single shot frame of 64-beam raw point cloud is collected at time t2 shown in <ref type="figure" target="#fig_2">Fig. 2b</ref>. By merging t1 and t2's point clouds, we emulate the cooperative sensing process between two vehicles. We utilize SPOD object detector to detect cars and draw results in red boxes to bound detected cars in <ref type="figure" target="#fig_2">Fig. 2c</ref> Meanwhile, in order to compare the detection results on Cooper, we also apply SPOD on single shot point clouds collected at times t1 and t2. The detected cars are drawn in blue boxes, as shown in <ref type="figure" target="#fig_2">Fig. 2a</ref> and <ref type="figure" target="#fig_2">Fig. 2b, repectively</ref>. From the figures, we can observe two major improvements of employing cooperative perception. First, the sensing range is extended by data sharing. We can see that at t1 we observe 6 blue boxes, and at t2 we observe 6 blue boxes yet again. However, when combined, we observe a total of 9 detected cars (red boxes) in the merged data, which include all the cars detected at t1 and t2. Second, the detecting score/confidence value of some detected vehicles is increased. For example, a vehicle in <ref type="figure" target="#fig_2">Fig. 2a</ref> is detected with a detecting score of 0.76 at t1, and the same vehicle is also detected in <ref type="figure" target="#fig_2">Fig. 2c</ref>, but the detecting score of this vehicle is increased (by 13%) to 0.86. We also provide the corresponding images as the ground truth at the bottom of <ref type="figure" target="#fig_2">Fig. 2a</ref> and <ref type="figure" target="#fig_2">Fig. 2b</ref>.</p><p>The following is calculating the number of vehicles detected by single shot and cooperative sensing in four different scenarios: T-junction, stop sign, left turn and curve scenarios. The single shot data collected by two vehicles are labeled as t1 and t2, t3 and t4, t5 and t6, t7 and t8 in four scenarios, respectively. Therefore, the data marked as t1 + t2, t3 + t4, t5 + t6, and t7 + t8 are the cooperative data, combining the single shot point clouds. We then compare the vehicle detection results against the ground truth (captured in images) for each case, and depict the results in <ref type="figure" target="#fig_4">Fig. 3</ref>. The value of ?d indicates the distance between the two locations of the vehicle at two different times. Every three columns represents a cooperative process, which is similar to the example we demonstrated in <ref type="figure" target="#fig_2">Fig. 2</ref>. We draw the distribution of detection results using cells in each column. The number in each cell is the detecting score, the higher the score, the more positive the result. The symbol X represents a missing detection, i.e., the detecting score is too low. The cell without score means the object is out of detection area. Also, different colors are used to indicate the distance. The darker the color, the farther the distance. According to the actual detection distance of LiDAR, we divide it into three scales of near (&lt;10m), medium (10-25m) and far (&gt;25m), which are represented in the illustration by white, gray and black, respectively. It is clear that the amount of detected cars in cooperative data is equal to or exceeds the number in individual single shots. Then,  we use qualitative results to analyze the performance on the number and accuracy of detected vehicles shown in <ref type="figure" target="#fig_5">Fig. 4</ref>. The proposed Cooper method not only detects more cars, but also grants better detection accuracy because there is no missing detection in the cooperative point clouds. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. T&amp;J Dataset</head><p>Unfortunately, KITTI dataset does not provide enough experimental scenarios because it is a vision benchmark collected by isolated instances of single vehicles. We are committed to multi-vehicle cooperation, and thus, to improve the driving safety and experience of CAV, we build a dataset that is suitable for vehicle collaboration, naming it the T&amp;J dataset.</p><p>Our testing cars are equipped with high precision sensing systems, such as LiDAR system, radar system, vision system, and supplemental system such as GPS and IMU sensors. More specifically our sensor framework consists of the following sensors:</p><p>? 2 X Front-view cameras ? 4 X Surround-view fish eye cameras ? 1 X Inertial and GPS sensor ? 1 X Front-view 120 ? Radar ? 1 X Velodyne VLP-16 360 ? LiDAR ? 1 X Nvidia PX2 Velodyne VLP-16 360 LiDAR <ref type="bibr" target="#b2">[3]</ref> is used along with Radar, which utilizes radio waves to measure distance. LiDAR provides low resolution image information. Cameras, on the other hand, provides very high resolution image information, but, it fails to perform in extreme weather or environmental conditions. Four fish-eye lens cameras are used to perceive and navigate the surrounding environment. IMU sensors provides the system that monitors the dynamically changing movements of the vehicle. Also, GPS sensor data can be used to obtain a rough estimate of the location or the positioning of the car. Nvidia Drive PX2 <ref type="bibr" target="#b23">[24]</ref> is a scalable AI supercomputer for our autonomous driving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation on T&amp;J Dataset</head><p>We evaluate Cooper's performance on our T&amp;J dataset. We select a sequence of continuous frames of front-view LiDAR point clouds and show them in <ref type="figure" target="#fig_7">Fig. 5</ref>. It can be clearly found that our point cloud is much more sparse than that from KITTI. All the data in the T&amp;J dataset are collected in a parking lot. A frame of 16-beam raw point cloud data is shown in <ref type="figure" target="#fig_7">Fig.  5a</ref>. Another collected single shot data is shown in <ref type="figure" target="#fig_7">Fig. 5b</ref>. By merging these two frames of point clouds, we produce two vehicles' cooperative sensing. 3D detector detects cars and draws results in red boxes to bound detected cars' location in <ref type="figure" target="#fig_7">Fig. 5c</ref>. Similar to <ref type="figure" target="#fig_2">Fig. 2</ref>, SPOD detects cars in two single shots and draws them in blue boxes. SPOD also draws results in red boxes to bound detected cars in cooperative sensing. Meanwhile, ground truth images are shown at the bottom of <ref type="figure" target="#fig_7">Fig. 5a and Fig. 5b</ref>. By studying this case, we conclude that sensing area is expanded by data sharing because <ref type="figure" target="#fig_7">Fig. 5c</ref> detects all the objects in the single shots. Most importantly, we see that the presence of new cars are discovered, cars that were not presence in the previous single shots. This phenomenon is a direct proof to the shortcomings of fusion on object level. Due to neither vehicles detecting the objects by themselves, there stands no possible way for the object-level fusion to detect the objects that were missed. This, we avoid and overcome with low level fusion.</p><p>We marked the cars detected at time t1 and t2 by numbers 1 and 2, respectively. It is worth noting that there are three unmarked vehicles in <ref type="figure" target="#fig_7">Fig. 5c</ref>. This is a significant discovery as this phenomenon indicates an increase in the detection capability of cooperative perception. We can extrapolate and assume that by receiving the perceptual information from nearby vehicles, Cooper can greatly enhance a vehicle's range of perception, allowing for better detection of traffic information.</p><p>The T&amp;J dataset provided four sets of testing data, which were collected on the roads around our campus's parking lots. In the four scenarios, we conduct cooperative perception experiments. Different from the KITTI, in each experimental scenario, we sample the fusion data at different distances, so as to better display the disparity of information collected by vehicles in different regions. As <ref type="figure">Fig. 6</ref> shows, in each scenario, we list detailed detection results of Cooper at different distances. Similar to <ref type="figure" target="#fig_4">Fig. 3</ref>, every three columns corresponding the SPOD detection results on two single shots and one cooperative sensing, represents a cooperative perception case. The test car can receive both nearby sensing data and relatively longdistance sensing data. For example, in <ref type="figure">Fig. 6a</ref>, from left to right, there are three cases in which a vehicle cooperates with other three located at three distances. It can be seen that in the cooperative perception of adjacent areas, such as the left cases in Scenarios 1 and 4, the individual detection results of two single shots are similar, but both output undetected targets, because these targets are blocked by unknown means in the single shots. Through cooperative perception, point clouds of blocked area are supplemented by each other, thereby these targets are detected. Moreover, the detected targets in both cases, after cooperative perception, have a marked increased in detecting scores. We evidence this phenomenon due to the redundancy of data and the presence of more features is gathered by harvesting detailed point clouds.</p><p>In all scenarios shown in <ref type="figure">Fig. 6</ref>, we carry out the cooperative perception of two cars, both are relatively far apart from each other. As a result, the detection area is expanded even lager.     Every car can detect the target in front of itself. But for distant targets, they are powerless due to scarcity or blockage of point clouds. Cooperative perception enables global detection of objects located at far, medium, and near distance. Objects are appeared in cells of different colors. Similarly, some objects that are undetected by single shot are detected in cooperative sensing. This reinforces the fact that some objects that were not detected through traditional means can be discovered through data fusion. This shows that our design can complement some key features. This is a significant discovery on cooperative perception.</p><p>Then, we use qualitative results to analyze the performance on the number and detecting scores of detected vehicles, shown in <ref type="figure">Fig. 7</ref>. From Scenario 1, we have the single shot analysis results for three different cases. It is clear that the number of cars detected based on the fused data is much higher than either of the cars alone. Despite the high detection rate, we do see that even while fused there are still some cars not being detected.</p><p>In Scenario 2, we find that there is a high amount of cars that is hard to detect from either car alone, but shows up when fused. This change of environment hold high relevance to common place areas such as a full parking lot or congested junctions where each car is limited by the cars around it.</p><p>Should there be a speeding car that is ignoring stop signs or running the red light, the fusion will mitigate the likelihood of a missed detection for all cars involved in the immediate vicinity.</p><p>In both Scenarios 3 and 4, we find that, similar to the trend shown in Scenarios 1 and 2, we have a closely related relationship between fusion and increase in object detection. As each scenario takes place in different environments, time of the day, different levels of congestion, the fusion method is proven robust and is able to adapt to different environments while retaining its capabilities to augment the status quo. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Statistical Analysis</head><p>Our statistical analysis results show that in the experimental scenarios of KITTI and T&amp;J datasets, some of the targets in cooperative perception are detected by both, some by only one, and some are detected by neither. Detection difficulty is thereby classified as easy, moderate and hard, respectively. Specifically, easy refers to when one or more vehicles are able to detect the same object. Moderate refers to when only one vehicle is able to clearly detect this object. Finally, hard is given when no cars are able to detect this object.</p><p>In <ref type="figure" target="#fig_11">Fig. 8</ref>, we calculate the improvement of detection performance on these three types of objects. For example, from the line marked easy, we see that we have an improvement of 10% in detection score for 80% of the time. Taking the direct implication of our testing, we see that the detection scores for easy and moderate achieve a marginal yet consistent increase in detection rate; mainly distributed within 10% in detection score improvement. This is because both easy and moderate objects contain detailed and saturated sized point clouds captured from a single scene, resulting in the fusion providing only marginal improvements to the detection results.</p><p>However, note that when we test the third type of objects, the hard objects detected by neither, we find that we are consistent with our findings that we have above, our detection score improvement is a flat increase of 50% in raw detection score at worst and just this alone is enough for autonomous vehicles to note the object for avoidance prevention, because they only need to know that there is an object there where previously one was not discovered. We record time cost of detection based on single shot and cooperative data, shown in <ref type="figure" target="#fig_12">Fig. 9</ref>. As latency impacts the performance and reliability of all autonomous vehicles heavily, we tested our fusion method against both the KITTI data and our own. In the test, the SPOD model for 3D car detection is executed on a computer with a GeForce GTX 1080 Ti GPU <ref type="bibr" target="#b0">[1]</ref>. In both experiments, we compared the time needed for object detection in both single shots against the fused data. In both instances, fusing the data used 5 ms over the baseline data, a very minimal increase in detection time for a significant increase in the number and accuracy of objects detected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Fusion Robustness</head><p>From a realistic standpoint, we will inevitably have to deal with sensor drift, so to deal with this phenomenon, we must test our fusion method of robustness against sensor drift. When integrating GPS and IMU, we observe yields of less than 10 cm in positional errors <ref type="bibr" target="#b5">[6]</ref>. To test the robustness of our fusion method, we conducted procedural artificial skewing of our GPS readings. We skew the GPS data as follows:</p><p>? Skewing both x and y coordinates to the maximum bounds of known GPS drifting. ? Skewing just one axis to the limit of GPS drifting. ? Pushing past that boundary by doubling the maximum GPS drifting to simulate abnormal instances. With the GPS readings skewed, we then tested the detection score for each of the different type of drifting scenarios against the baseline GPS reading. As evinced from <ref type="figure" target="#fig_0">Fig.10</ref>, we see that with the exception of already known undetected vehicles, we have a similar clustering of the skewed detection scores versus the baseline score, with the overwhelming majority achieving successful detection. It should be noted, however, that skewing the readings surprisingly improved the detection score in several instances, possibly masking the inherent drift from the baseline GPS reading. And just as some of the skewing helped the result, it also caused the detection to fail for two instances. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Networking Requirements</head><p>Even though point clouds can be simplified to coordinate values, we still need to consider the gap between data generated by autonomous vehicles and the limited wireless networking throughput, such as the limited bandwidth provided by DSRC <ref type="bibr" target="#b11">[12]</ref>. We adopt a strategy to extract data based on the region of interest (ROI), e.g., traffic lights, blocked areas, nearby vehicles and free-space in driving path, to further reduce data size to hundreds KB per frame. Background data like buildings, trees are subtract because these information can be constructed by each vehicle after several times mapping measurement. This allows for retention of valuable information of immobile objects while keeping the size of the ROI data small. For object detection purpose, ROI data will be extracted whenever failure detection happened on this area. However, just knowing the relative ROI is not optimized enough. The ideal case is to have a multitude of real world ROI categories that provide a guideline for the bases of how much data is needed for an optimal balance of data size versus detection accuracy. To illustrate the importance of this tradeoff, we present three different types of ROI categories and their respective data consumption via <ref type="figure" target="#fig_0">Fig.11</ref> and <ref type="figure" target="#fig_0">Fig.12</ref> respectively where the sample rate in the latter is 1Hz, or 1 frame per second. We simulated and gathered the total data consumption between two cars, both utilizing a 16-beam LiDAR, every second over an eight second time frame. Note, we observe that message exchange rate for cooperative perception does not require as high a sensing rate as the standard rate for individual vehicles. Because for easy or moderate objects, detailed sized point clouds are already captured. While due to blocking or distance, we may experience an insufficiency of point clouds, making objects hard to detect. In most cases, the native data on a recipient vehicle only needs to be supplemented by a single data frame from different view perspective. Excessive exchanging of frequencies only leads to unnecessary data, hence needlessly congesting the communication channels. With efficiency and lightweight traffic as a constraint, we decided that a sample rate of 1 frame per second is enough to satisfy the needs of Cooper whilst remaining within our set of constraints. As seen in <ref type="figure" target="#fig_0">Fig.11</ref>, we have three different scenarios, each representing a general phenomenon. For the first one, we see that two cars are fairly apart from each other, laterally but fairly close horizontally. We would typically see this situation in two lane drive with opposite directions separated by a single lane divider. In this scenario, we would ideally want as much information as possible as we lack the safety of a physical buffer between the vehicles. In situations like this, we transfer the entirety of the frame of LiDAR data and this is the most costly of all scenarios as evinced by <ref type="figure" target="#fig_0">Fig.12</ref>. From the same scenario, we can calculate that for the most expensive data transaction, the total data size can be compress into around 1.8 M bit per frame for each car. Next, we have the case of cars in closer lateral proximity to each other, representing typical junctions where cars from all directions are able to see the opposing car. In situations such as this, the ROI is typically the field of view from the driver's perspective, making only a 120 degree field of view our minimal requirement. As both vehicles needs to exchange this information, the transaction cost is additive for each of the participating vehicles.</p><p>Lastly, we have the most common situation of one car needing the field of view of a leading car. The trailing car is the one needing the information and thus the transaction is one way, consuming the least amount of bandwidth out of all three scenarios.</p><p>Thus, deriving from the simulation of the three different cases, the three presented are within the capacity of DSCR bandwidth, as seen in real-world test <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Summary of Experiment Results</head><p>In summary, we prove that Copper method outperforms individual perception on extending sensing area, improving detection accuracy and complementing object detection. We find that collaboration offers more information, even some are not perceived by individuals. The most important, we conduct a feasibility study and demonstrate that the bandwidth of DSRC could satisfy point clouds transmission for cooperative perception. We would like to mention that our design succeeds in privacy preservation because only LiDAR data is involved for sharing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RELATED WORK</head><p>Rapid development of autonomous vehicles has motivated research institutions to develop representations to perceive local environment, such as lane detection, traffic sign detection and detect objects like cars, cyclists and pedestrians <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b30">[31]</ref> based on the open datasets <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. As we know, these datasets are collect by multiple sensing devices from individual vehicles. To achieving self-driving, we put heavy emphasis on accuracy cognition of the surrounding local environment. However, the detection results still has vast room for improvement even when utilizing state-of-the-art Convolutional Neural networks (CNNs) <ref type="bibr" target="#b12">[13]</ref>.</p><p>Current works make use of low level fusion of sensors to extract the features or objects for purpose of tracking <ref type="bibr" target="#b13">[14]</ref>. However, this does not incorporate the use of raw data as is for the purpose of fusion and object detection. Papers such as <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b3">[4]</ref> discuss methods of fusion that constructs theoretical architecture for low level fusion and detection. Other approaches, such as <ref type="bibr" target="#b27">[28]</ref> and <ref type="bibr" target="#b17">[18]</ref>, proposed 3D object detection methods by fusing both image and point cloud from the same vehicle. Differing from improving the detection methods for a single vehicle, we focus our method on the fusion of data between different vehicles.</p><p>To the best of our knowledge, there are no prior work done to implement the concept of multi vehicular raw sensor data for the purpose of object detection. This room for improvement is also the cause of severe consequences because self-driving cars may make wrong decisions due to failure of detection of objects. A Cooper framework for connected autonomous vehicles can solve the aforementioned issues through cooperative sensing. However, none of the public datasets and related detection methods explicitly consider low level fusion approach as a solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>We propose Cooper for connected autonomous vehicles as an entry to a broader platform for CAV. This method facilitates a CAV capable vehicle to combine its sensing data with that of its cooperators to enhance perceptive ability, thereby improving detection accuracy and driving safety. In order to reconstruct local environment, we map point clouds into their corresponding object positions. This will merge and align the shared data that is collected from nearby vehicles, which may provide data scopes coming from different positions and angles. We incorporate deep learning based SPOD with Cooper to detect 3D objects from aligned LiDAR data, marking and discovering previously undetected objects. Finally, we evaluated Cooper on KITTI and our collected dataset, showing that the Cooper is capable of enhancing detection performance by expanding the effective sensing area, capturing critical information in multiple scenarios and improving detection accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Structure of the SPOD 3D object detection method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) Single shot at t1: a vehicle utilizes SPOD on 64-beam point clouds to detect cars, and the results are shown in blue boxes.(b) Single shot at t2: as the vehicle moving forward, its detection results are drawn in blue boxes. Bottom image provides the ground truth.(c) Merging t1 and t2's point clouds to produce cooperative point clouds. The detected cars are drawn in red boxes using the same SPOD detector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Cooperative detection of vehicles based on the KITTI point clouds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :</head><label>3</label><figDesc>Vehicle detection results in four different scenarios in KITTI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>Number of cars detected and the detection scores in four KITTI scenarios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) Single shot at t1: applying SPOD on 16-beam point clouds to detect cars.(b) Single shot at t2, vehicle detection results are shown in blue boxes.(c) Cooperative perception combines two single shots. The detected cars are drawn in red boxes using the SPOD object detector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 :</head><label>5</label><figDesc>An example of cooperative perception using the T&amp;J dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>X</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>4 Fig. 6 :</head><label>46</label><figDesc>Vehicle detection results in different scenarios in the T&amp;J dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>4 Fig. 7 :</head><label>47</label><figDesc>Details on the detection results on the T&amp;J dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 8 :</head><label>8</label><figDesc>Improvement of the detection performance by cooperative perception.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 9 :</head><label>9</label><figDesc>Time needed to detect objects on single shot and cooperative sensing data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 10 :</head><label>10</label><figDesc>Cooperative perception results on GPS reading drifting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 11 :</head><label>11</label><figDesc>Illustration of ROI data exchange between two vehicles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>3 Fig. 12 :</head><label>312</label><figDesc>Volume of LiDAR data being exchanged between two cars.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>.51 Detected cars car1 car2 car1+2 car1 car3 car1+3 car1 car4 car1+4</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>? d=5.5m</cell><cell></cell><cell></cell><cell>? d=14.5m</cell><cell></cell><cell></cell><cell>? d=26.9m</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.51</cell><cell>? d=15.03m</cell><cell>? d=33.1m</cell><cell>? d=20.02m</cell><cell>? d=15.7m</cell></row><row><cell></cell><cell>13</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.60</cell><cell>0.54</cell><cell>0.52 0.62</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>X</cell><cell>0.74</cell><cell>13</cell><cell>X</cell><cell>0.65</cell></row><row><cell></cell><cell>11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>X</cell><cell>0.68</cell><cell>X</cell><cell>0.52</cell><cell>X</cell><cell>0.69</cell></row><row><cell>Detected cars</cell><cell>5 7 9 3 1</cell><cell cols="3">0.61 0.55 X car1 car2 car1+2 X 0.75 0.58 0.75 0.60 0.59</cell><cell cols="3">X X X 0.70 0.81 0.52 car1 car3 car1+3 0.60 0.56 0.70 0.79 0.79 0.64 0.61 0.54 0.55 0.59 X 0.59</cell><cell></cell><cell cols="2">X 0.54 X X 0.69 X car1 car4 car1+4 0.61 0.55 X 0.71 0.68 0.85 0.58 0.81 0.56 0.66 X 0.50</cell><cell>cars Detected</cell><cell>11 9 5 7 3 1</cell><cell>X X 0.62 car1 car2 car1+2 X X 0.71 0.77 X 0.52 X 0.55 X 0.73</cell><cell>X X 0.71 0.68 X 0.60 0.83 0.65 0.60 0.67 0.62 car1 car3 car1+3 X X 0.62</cell><cell>X 0.84 car3 car4 car3+4 0.61 X X 0.71 0.60 0.80 0.70 X 0.71 0.68</cell><cell>X 0.84 car4 car5 car4+5 0.81 0.62</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(a) Scenario 1</cell><cell></cell><cell></cell><cell></cell><cell>(b) Scenario 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.52</cell><cell>0.70</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>X</cell><cell>0.52</cell><cell></cell><cell>0.68</cell><cell>0.57</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>X</cell><cell>0.61</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.54</cell><cell>X</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>X</cell><cell>X</cell><cell>0.63</cell><cell>X</cell><cell cols="2">0.66 0.58</cell><cell>X</cell><cell>X</cell><cell>0.56</cell><cell></cell></row><row><cell></cell><cell>0.51</cell><cell cols="2">0.64 0.75</cell><cell>0.51</cell><cell></cell><cell>0.72</cell><cell>0.51</cell><cell></cell><cell>X</cell><cell></cell></row><row><cell></cell><cell>0.77</cell><cell></cell><cell>0.69</cell><cell>0.77</cell><cell></cell><cell>0.55</cell><cell>0.77</cell><cell></cell><cell>0</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geforce -Nvidia</surname></persName>
		</author>
		<ptr target="https://www.nvidia.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Intel -the coming flood of data in autonomous vehicles</title>
		<ptr target="https://www.intel.com/content/www/us/en/automotive/autonomous-vehicles.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Velodyne -Lidar</surname></persName>
		</author>
		<ptr target="https://velodynelidar.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">High-level sensor data fusion architecture for vehicle surround environment perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aeberhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kaempchen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th Int. Workshop Intell. Transp</title>
		<meeting>8th Int. Workshop Intell. Transp</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Moveset: Modular vehicle sensor technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bellows</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Wittie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Vehicular Networking Conference (VNC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The performance analysis of a real-time integrated ins/gps vehicle navigation system with abnormal gps measurement elimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-K</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="10599" to="10622" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Understanding the fatal tesla accident on autopilot and the nhtsa probe</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Lambert</surname></persName>
		</author>
		<ptr target="https://electrek.co/2016/07/01/understanding-fatal-tesla-accident-autopilot-nhtsa-probe/" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.06184</idno>
		<title level="m">The apolloscape dataset for autonomous driving</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Feature-level fusion for free-form object tracking using laserscanner and video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kaempchen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent vehicles symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="453" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dedicated short-range communications (dsrc) standards in the united states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Kenney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1162" to="1182" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cooperative fusion for multi-obstacles detection with use of stereovision and laser scanner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Labayrade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Royere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gruyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Aubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="117" to="140" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sparse convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pensky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="806" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">National Transportation Safety Board</title>
		<imprint/>
	</monogr>
	<note type="report_type">Preliminary report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<title level="m">Accurate Single Stage Detector Using Recurrent Rolling Convolution</title>
		<imprint>
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lidar: Mapping the world in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Photonics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">429</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Leveraging spatio-temporal evidence and independent vision channel to improve multi-sensor fusion for vehicle environmental perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="591" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<title level="m">Wikipedia. Nvidia -wikipedia, the free encyclopedia</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rotation matrix -Wikipedia, the free encyclopedia</title>
		<ptr target="https://en.wikipedia.org/w/index.php?title=Rotation_matrix&amp;oldid=875545324" />
	</analytic>
	<monogr>
		<title level="m">Wikipedia contributors</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Why tesla&apos;s autopilot can&apos;t see a stopped firetruck</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wired</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Squeezeseg: Convolutional neural nets with recurrent crf for real-time road-object segmentation from 3d lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1887" to="1893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pointfusion: Deep sensor fusion for 3d bounding box estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Openvdap: An open vehicular data analytics platform for cavs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Distributed Computing Systems (ICDCS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>IEEE 38th International Conference on</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-11" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
