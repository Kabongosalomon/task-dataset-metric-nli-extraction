<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">What Are Expected Queries in End-to-End Object Detection?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilong</forename><surname>Zhang</surname></persName>
							<email>zhangshilong@pjlab.org.cnwjqdev</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Xinjiang</roleName><forename type="first">Wang</forename><surname>2?</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
							<email>wangxinjiang@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
							<email>pangjiangmiao@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
							<email>chenkai@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Shanghai AI Laboratory</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">What Are Expected Queries in End-to-End Object Detection?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Object detection</term>
					<term>DETR</term>
					<term>End-to-End</term>
					<term>Query</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>End-to-end object detection is rapidly progressed after the emergence of DETR. DETRs use a set of sparse queries that replace the dense candidate boxes in most traditional detectors. In comparison, the sparse queries cannot guarantee a high recall as dense priors. However, making queries dense is not trivial in current frameworks. It not only suffers from heavy computational cost but also difficult optimization. As both sparse and dense queries are imperfect, then what are expected queries in end-to-end object detection? This paper shows that the expected queries should be Dense Distinct Queries (DDQ). Concretely, we introduce dense priors back to the framework to generate dense queries. A duplicate query removal pre-process is applied to these queries so that they are distinguishable from each other. The dense distinct queries are then iteratively processed to obtain final sparse outputs. We show that DDQ is stronger, more robust, and converges faster. It obtains 44.5 AP on the MS COCO detection dataset with only 12 epochs. DDQ is also robust as it outperforms previous methods on both object detection and instance segmentation tasks on various datasets. DDQ blends advantages from traditional dense priors and recent end-to-end detectors. We hope it can serve as a new baseline and inspires researchers to revisit the complementarity between traditional methods and end-to-end detectors. The source code is publicly available at https://github.com/jshilong/DDQ.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Object detection is one of the most fundamental challenges in computer vision which aims at localizing each object with a single bounding box. It brings a challenging problem that an accurate object detector should both detect all objects and avoid predicting duplicated boxes.</p><p>To tackle this problem, the previous state-of-the-art methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">18]</ref> mostly follow a standard paradigm that first generates dense candidate boxes and assign one ground truth (GT) to many candidates for a high object recall, as shown in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>. However, the one-to-many assignment results in redundant predictions. Since there should be only one prediction for each object in object detection, ? Equal contribution. (a) Dense priors in traditional object detectors no longer exist in (b) end-to-end object detectors. Intuitively dense queries can maintain a higher recall but are hard to implemented by (c) directly increasing the number of queries. (d) Alternatively, we use duplicate removal as a pre-process to obtain better dense distinct queries and keep the framework end-to-end. auxiliary post-processing, e.g., non-maximum suppression (NMS), is adopted to remove duplicated predictions. Although dominating object detection for years, this pipeline suffers from perfectly filtering out duplicated boxes without harming correct predictions. This paradigm is broken by DETR <ref type="bibr" target="#b1">[2]</ref>, an end-to-end object detection framework. In contrast to the conventional paradigm, it throws away dense object candidates but directly initializes a set of sparse object queries. When training, these queries are supervised by a one-to-one matching loss so that the optimization objective is consistent with the definition of object detection that only one bounding box is predicted for each object in the image. In this case, the network does not need post-processing to remove duplicated predictions anymore. However, DETR suffers from slow convergence speed which is explored in various works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b19">20]</ref>. One representative work following this paradigm is Sparse R-CNN <ref type="bibr" target="#b19">[20]</ref>. Sparse R-CNN interacts each query with a local region feature extracted by RoIAlign, leading to faster convergence speed compared to DETR.</p><p>Revisiting the framework of end-to-end object detectors that is sketched as <ref type="figure" target="#fig_0">Fig. 1(b)</ref>, there are only hundreds of sparse queries supervised by one-to-one matching loss. In this paper, we reveal that this design incurs a dilemma. On the one hand, the hundreds of sparse queries are not always sufficient to guarantee a high recall. On the other hand, when dense queries are introduced by directly increasing the number of queries to reach a higher recall, it will inevitably bring a lot of similar queries (as <ref type="figure" target="#fig_0">Fig. 1(c)</ref>). These similar queries confuse the network since different labels are assigned to similar queries. This dilemma of choosing This paper answers the question from a quantitative study and finally observes that the expected queries in end-to-end object detection should be dense distinct queries (DDQ), which means that the queries should be both densely distributed to detect all objects, as well as distinct from each other to facilitate the optimization of one-to-one matching loss.</p><p>Specifically, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, we increase the number of queries in Sparse R-CNN. The performance increases at the beginning but finally plateaus and even decreases with denser queries because the training becomes more difficult with more similar queries. By imposing a duplicate removal pre-processing to filter out similar queries and attain distinct queries before each stage of iterative refinements, the performance is improved with a clear margin. More surprisingly, the performance margin consistently increases along with more queries 1 .</p><p>Motivated by the performance of Sparse R-CNN with dense distinct queries does not achieve a plateau with around seven thousand queries, we propose to introduce densely distributed queries on each location of the image which can be then converted to dense distinct queries. These densely distributed queries guarantee a sufficient high recall to cover all potential target objects.</p><p>However, directly processing densely distributed queries by the iterative refinement of Sparse R-CNN leads to unaffordable computational and GPU memory costs.</p><p>As in <ref type="figure" target="#fig_1">Fig. 2</ref>, when there are around seven thousand queries, Sparse R-CNN requires around 45G GPU memories, while there can be more than tens of thousand pixels on an image feature map.</p><p>Accordingly, based on Sparse-RCNN, we propose a novel framework, Dense Distinct Queries (DDQ), to introduce dense distinct queries for end-to-end object detection and overcome the high computational cost. Specifically, DDQ takes the feature point on each feature map as densely distributed initial queries. Instead of heavy RoI refinement heads, a lightweight fully convolutional network (ConvNet) is applied to process all queries in a sliding window manner, which shares a similar architecture as <ref type="bibr" target="#b17">[18]</ref>. Differently, our design discards the anchor design and applies the bipartite matching algorithm to adaptively determine positive and negative samples for higher recall and robustness across different datasets. As a result, dense queries are efficiently discriminated to generate dense distinct queries with a reasonable computational cost. Moreover, a query distinctness enhancement mechanism further fuses these dense distinct queries with their corresponding RoI features to enhance their distinctness. Different from Sparse R-CNN that requires 6 stages of iterative query refinement, DDQ achieves much higher performance with only 2 refinement stages and a fast convergence speed.</p><p>Experimental results evaluate the effectiveness and efficiency of the proposed method. DDQ pushes the frontier of state-of-the-art results on multiple object detection datasets. For example, DDQ achieves 44.5 AP using a ResNet-50 backbone with a normal 1x training setting on MS-COCO <ref type="bibr" target="#b13">[14]</ref>, which largely surpasses the current state-of-the-art detectors (including both CNN-based and transformer-based) by over 2 AP with minimal inference time increase. It also has leading performance on CrowdHuman <ref type="bibr" target="#b18">[19]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>End-to-end Object Detection Designing an object detection framework free from post-processing operators such as NMS has been the target of many studies. Both DETR <ref type="bibr" target="#b1">[2]</ref> and Sparse R-CNN <ref type="bibr" target="#b19">[20]</ref> achieved the target by assigning a one-hot label for each ground truth in bipartite matching, and the cross attention module would then reason about the mutual relation to avoid duplicated predictions. OneNet <ref type="bibr" target="#b20">[21]</ref> and DeFCN <ref type="bibr" target="#b21">[22]</ref> are two representatives that developed NMS-free fully convolutional networks. Both of these works highlighted the importance of the one-to-one assignment during training for an object detector without duplicated predictions. However, all these networks need extra training time to learn a one-hot representation for each instance and are inferior in performance compared to traditional object detectors <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12]</ref>. These prior works justified the efficacy of bipartite matching to learn to remove duplicated predictions in an end-to-end manner but ignored the efficiency in achieving the goal. This study points out that similar queries are strong obstacles for bipartite matching to converge. Duplicated queries can be removed by class-agnostic NMS pre-processing at each stage to relieve the burden of the bipartite matching. This is also a different motivation from traditional detectors <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">18</ref>] that adopt class-aware NMS as a post-processing after the final prediction. Query Design in DETR-like Structures Object queries are a set of learned positional embeddings that guide the decoder to interact with the feature maps and positions to help infer the object and location at a specific region. It was first designed in <ref type="bibr" target="#b1">[2]</ref> to be a sparse and randomly initialized embedding set and learned through training, which was also adopted in the first stage in Sparse R-CNN <ref type="bibr" target="#b19">[20]</ref>. Anchor DETR <ref type="bibr" target="#b23">[24]</ref> provided correspondence between anchor points and query position. Conditional DETR <ref type="bibr" target="#b16">[17]</ref> introduced conditional spatial queries to help each cross-attention attend to the interesting regions inside a bounding box. DAB-DETR <ref type="bibr" target="#b14">[15]</ref> explicitly learned a set of 4-D anchor boxes as queries. However, these studies are still based on sparse queries and would thus suffer from low recall problem, which is in contrast with this study. Noticeably, the two-stage version of Deformable DETR <ref type="bibr" target="#b26">[27]</ref> and Efficient DETR <ref type="bibr" target="#b25">[26]</ref> also tried to introduce dense queries using the dense features from the last stage, which is similar to this study. However, both of these studies simply choose top-K high-scoring queries out of the dense predictions and ignore the similarity among the kept queries. Therefore, the optimization difficulty problem due to similar queries is still not addressed. For example, the negligence of distinctness of queries made the performance quickly plateau as queries become denser and multiple refinement stages even hurt the performance as reported in Efficient DETR <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dense Distinct Queries (DDQ)</head><p>Dense distinct queries (DDQ) is the principle of designing an object detector with a fast convergence based on recent end-to-end detectors. Therefore, it is able to generalize to different architectures. The pipeline is sketched in <ref type="figure" target="#fig_2">Fig. 3</ref>. In the following sections, we describe the steps of applying dense distinct queries (DDQ) to Sparse R-CNN, one of the recent end-to-end object detectors with state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Revisiting Sparse R-CNN</head><p>Sparse R-CNN mainly follows the paradigm of DETR and achieves better performance even without encoding layers thanks to its outstanding improvement in the decoding process. Sparse R-CNN utilizes dynamic instance interaction to replace the original cross-attention decoding part. Moreover, each object query in Sparse R-CNN only attends to features of a local region extracted by a RoIAlign operator instead of attending to all encoded features as in DETR.</p><p>Sparse R-CNN maintains N (N ?300) independent queries with each corresponding to a bounding box. It then uses the bounding boxes to extract candidate region features through the RoIAlign operator from the feature pyramid. Each query embedding is then used to generate convolutional parameters that interact with the RoI feature to output the predicted label and bounding box for each stage.</p><p>Sparse R-CNN also applies set prediction loss that utilizes bipartite matching according to the predefined matching cost to assign only one positive query for each ground truth. As discussed above, the sparse set of queries and duplicated queries are two bottlenecks for the performance and convergence of Sparse R-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dense Queries</head><p>It is described in Sec. 1 that dense queries largely increase the recall rate while also bringing unacceptable computation cost. In this study, a lightweight fully convolutional network (RPN) is applied to process all queries in a sliding window manner, The recall rate is largely increased with much smaller memory consumption thanks to the parameter sharing property of CNN structures. As traditional RPN used in e.g. Faster R-CNN still lags in recall rate and also suffers from generalization issues due to its cumbersome anchor box design and assignment strategy, we present a new RPN structure to make it more efficient and robust.</p><p>The RPN structure is shown as <ref type="figure" target="#fig_2">Fig. 3(b)</ref>. Resembling single-stage detectors such as RetinaNet, the RPN structure in this study adopts P 3 to P 7 features, where P l represents the feature map level that is downsampled by 2 l from the input image size. It avoids the use of P 2 features as in the RPN structure in Faster R-CNN to save computation costs. It has 3 consecutive 3x3 Conv-GN-ReLU layers as a shared head structure, followed by two separate branches of one 3x3 Conv-GN-ReLU layer for classification and regression subtasks. The features from the two subnetworks are then extracted and concatenated to form dense queries such that each feature point is treated as a query. In this way, the number of queries becomes much larger. For example, the number of queries reaches 13343 given an image of size 800 ? 800, which is two orders larger than that in Sparse R-CNN with only minor increase in memory consumption.</p><p>We also discard the multiple anchor design and IoU-based assignment in the original RPN and apply the bipartite matching algorithm to adaptively discriminate positive and negative samples in order to increase the robustness ability across different datasets. Noticeably, the bipartite matching is slightly modified to only select positive samples out of the center feature points in a ground truth in order to stabilize training. Specifically, top-K (K = 9 in this study) nearest feature points to the center of ground truth on each level are regarded as potential positive samples, as shown in <ref type="figure" target="#fig_2">Fig. 3(b)</ref>. (More details about this RPN can be found in supplementary material.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Distinct Queries</head><p>We would first like to point out that the non-duplicate queries are of great importance to the convergence of the bipartite matching in end-to-end training methods such as Sparse R-CNN. As queries become similar, it is more difficult for the training to converge. This is understandable in the extreme case when there exist two identical queries. In this case, the bipartite matching assigns foreground label to one of them but background label to another. Without loss of generality, we adopt binary cross-entropy loss for classification. Therefore, the loss from these two queries becomes L 1 = ? log(p 1 ) ? log(1 ? p 2 ), where p 1 and p 2 are the probability scores of the positive and negative query respectively, and satisfy p 1 = p 2 = p as they are identical queries. In contrast, the loss value when only one of the duplicated queries exists is L 0 = ? log(p). The ratio of positive score gradient between the duplicate and non-duplicate case is ?.</p><formula xml:id="formula_0">? = ?L 1 ?p / ?L 0 ?p = 1 ? p 1 ? p<label>(1)</label></formula><p>It is obvious that gradient is scaled down (? &lt; 1) at 0 &lt; p &lt; 0.5 and may even causes negative training (? &lt; 0) at p &gt; 0.5. Duplicate Query Removal As shown in the toy example, the reduced gradient or even negative training caused by duplicated queries greatly suppresses the convergence. Therefore, we propose to remove duplicated queries as a preprocessing for each stage in Sparse R-CNN, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. Since each query represents a potential instance in an image, and an instance can be uniquely represented by its location in an image <ref type="bibr" target="#b22">[23]</ref>, it comes naturally to detect similar queries using the class-agnostic overlapping ratio of the corresponding bounding boxes. Therefore, the duplicate removal operator is realized by a class-agnostic non-maximum suppression (NMS) in this study. It should be noted that the pre-processing of queries is to relieve the burden of bipartite matching, which enables us to choose an aggressive IoU threshold (defaults to 0.7 in this study, and performance Only fluctuates within 0.3 when it varies from 0.6 to 0.8) that is robust across different datasets. This pre-processing step maintains the advantages of end-to-end detectors that can align with detection definition. In contrast, traditional object detectors adopt class-aware NMS as a post-processing after the final prediction, and the IoU threshold needs to be carefully tuned. Query Distinctness Enhancement To make query features more discriminative, we enrich them with the extracted RoI features of the corresponding proposal box. Each RoI feature is first average-pooled to the size of 1 ? 1 and then concatenated with the original query, followed by a conv layer that reverts the channel number. As the RoI features contain more discriminative instance-level information compared to their corresponding queries, the combination further encourages the distinctness between different queries. The enriched queries are then applied with self-attention to infer the mutual relations, followed by dynamic head modules to interact with the RoI features. This part follows the original design in Sparse R-CNN, and details are thus omitted in this study. Light-weighted Iterative Refinement Different from Sparse R-CNN that requires 6 stages of iterative query refinement, DDQ needs as few as 2 refinement stages. Actually, the long iteration stages in Sparse R-CNN mainly compensate for the drawbacks caused by the independent sparse queries. On one hand, the corresponding region of initial sparse queries could be far from instances and thus needs long cascading stages to refine these queries, On the other hand, long refinement also helps distinguish similar queries to output one-hot prediction at each location. In contrast, the dense queries from RPN and duplicate removal pre-process before each stage addresses the above issues, and thus the number of iterative refinement can be significantly reduced without performance drop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Other Improvements</head><p>In this study, we also make extra efforts so that the network and optimization better align with dense distinct queries. Quality Focal Loss We also follow recent one-stage methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref> to adopt quality focal loss (QFL) that make IoU between bounding box predictions and gt bboxes as the target of classification. This modification is to better align the classification and regression subtasks for each query. The confidence score better reflects the regression quality and thus aids in the duplicate removal process using class-agnostic NMS. Other than the QFL classification, the regression loss function follows the design in Sparse R-CNN. RoIAlign with Flexible Receptive Field Sparse R-CNN constrains each query to attend to only the RoIAligned region, which greatly decreases the computational overhead yet brings localized receptive field. Localized receptive field make model hard to to perceive the quality of bounding boxes. Hence, we design a efficient RoIAlign with Flexible Receptive Field (FRF), which combines extra RoIAligned features from neighboring levels in the feature pyramid, as shown in <ref type="figure" target="#fig_2">Fig. 3(d)</ref>. With the help of FRF RoIAlign, each query is attending to a wider range of features without introduction of heavy computations as <ref type="bibr" target="#b8">[9]</ref>. FRF is also complementary to quality focal loss (QFL) as the alignment of classification and regression in QFL need different scales of receptive fields to perceive the quality of bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>In this section, we first show a progressive improvement in terms of both convergence speed and performance from Sparse R-CNN to DDQ. DDQ is also shown to significantly push the limit of object detection performance with various backbones, on different datasets and different tasks. 4.1 Datasets MS COCO 2017 <ref type="bibr" target="#b13">[14]</ref> detection dataset is mainly used for comparison and ablation studies. It contains 118k training, 5k validation images, and 20k test images without annotations. There are on average 7 instances per image in this dataset. We report bounding box mean average precision (AP) as the performance metric, which is the average AP over multiple thresholds. If not specified, AP on the validation set is set as default. Apart from the object detection tasks, the instance segmentation performance on MS COCO is also reported in this study.</p><p>LVIS v1.0 <ref type="bibr" target="#b9">[10]</ref> is around the same size as MS COCO in regards to training images (100k) yet has a much larger number of classes (1203) and is deemed as a harder and long-tailed dataset to benchmark. Both object detection and instance segmentation results are benchmarked on LVIS v1.0 val with 20k images.</p><p>Besides, we also report the performance on CrowdHuman <ref type="bibr" target="#b18">[19]</ref> dataset, which has 15k training images and 4.4k validation images with around 23 heavily occluded instances per image, to demonstrate the robustness of DDQ on crowded scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation details</head><p>As for model training on MS COCO, ResNet-50 <ref type="bibr" target="#b10">[11]</ref> is the default backbone structure in this study if not specified. Most models adopt the 1x training protocol in MMDetection <ref type="bibr" target="#b2">[3]</ref> with a mini-batch size 16 on 8 GPUS and a total training budget of 12 epochs. AdamW <ref type="bibr" target="#b15">[16]</ref> optimizer with weight decay 0.05 is used, and the initial learning rate is 10 ?4 and decreases by 10 at epoch 9 and 11, respectively. It is worth noting that this setting does not bring improvement for the original Sparse R-CNN, and thus the comparison with Sparse R-CNN is deemed fair. All backbones are initialized with pretrained weights on ImageNet. Images are rescaled with short side 800 pixels, and only random horizontal flip are used for data augmentation. Loss weights are the same as previous work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20]</ref>. Some models are trained using the multi-scale training protocol with short side ranging from 480 to 800 pixels and elongated training budget of 24 epochs to compare with traditional CNN-based detectors. When comparing with DETR-based models, we also adopt the same augmentation in DETR in which random crop is involved and term this augmentation as DETR Aug. We keep 300 queries from RPN and 200 queries after the first refinement stage, if not specified otherwise. <ref type="table">Table.</ref> 1 shows a progressive development from Sparse R-CNN to DDQ in this study. Sparse R-CNN using 300 queries achieves 39.4 AP using the standard 1x training protocol, which is around 5.6 AP lower than that using 3x training time and heavier augmentations. The significant drop with short training time already implies the difficult convergence of Sparse R-CNN. Applying duplicate removal for queries at the beginning of each stage boosts the performance by 2 AP to 41.4 AP with little sacrifice in inference speed. Further increasing the number of queries to 7000 increases the performance as well, yet with intimidating inference time. Replacing independent queries by the features generated by our developed RPN structure and cutting to 2 refinement stages maintains the performance of using 7000 queries but costs significantly less in both memory and inference time. Finally, DDQ is able to be comparable with Sparse R-CNN in latency but achieves 44.5 AP thanks to some other further structural improvements such as FRF RoIAlign and Query Distinctness Enhancement. This performance is ahead of the state-of-the-art object detectors by as much as 2 AP <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b3">4]</ref> that adopt the same backbone. The great improvement demonstrates the effectiveness of dense and distinct queries as a guiding principle of designing object detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">From Sparse R-CNN to DDQ</head><p>Notice that DDQ only increases marginal inference latency over Sparse R-CNN(17.7 ms vs 16.4 ms), which is much faster than other competing methods (Latency is benchmarked with Tesla A100, more detail can be found in supplementary material). For example Deformable DETR <ref type="bibr" target="#b26">[27]</ref> achieves AP 43.8 AP with the latency 21.7 ms and Cascade R-CNN <ref type="bibr" target="#b0">[1]</ref> achieves 40.3 AP with the latency 19.4 ms. DDQ both achieves better performance and inference faster than these methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with Other Detectors</head><p>Table. 2 shows the performance comparison with other latest object detectors using elongated training and heavier backbones. It is seen that DDQ with ResNet-50 backbone further increases from 44.5 AP to 47.7 AP when switching the training protocol from the standard 1x to 3x with DETR augmentations <ref type="bibr" target="#b1">[2]</ref>. This performance largely surpasses all currently available end-to-end detectors by a clear margin while costing the smallest training budget. DDQ still remains its advantage among end-to-end object detectors as the backbone becomes heavier. As for the comparison with traditional CNN-based object detectors, we opt to the more commonly used multi-scale training with 24 epochs. We report the performance on COCO test set for DDQ with ResNet-101 (R101) and ResNext-101-64d (X101-64d) <ref type="bibr" target="#b24">[25]</ref>. It is seen that the performance of DDQ also stays ahead even compared with the state-of-the-art CNN-based object detectors. It is also noted that DDQ can also benefit from additional encoder layers that are common in DETR-based detectors. For example, the performance of DDQ with R50 backbone and 3x DETR Aug reaches an amazing 49.8 AP when we add 6 dynamic blocks proposed in <ref type="bibr" target="#b3">[4]</ref> as the encoder, which is marked as DDQ-R50heavy in <ref type="table">Table.</ref>2.(For more details about DDQ-heavy see the supplementary material). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results on The Other Datasets</head><p>To verify the robustness of DDQ on the other datasets, we report the performance comparison on CrowdHuman <ref type="bibr" target="#b18">[19]</ref> and LVIS v1.0 <ref type="bibr" target="#b9">[10]</ref>. Following the standard-setting on CrowdHuman, the maximum possible detection of all CNN methods is set to 500. For a fair comparison, the number of kept queries is changed to 500 in DDQ and Sparse R-CNN <ref type="bibr" target="#b19">[20]</ref>.The standard 1x setting on COCO is adopted when compared to CNN-based detectors. The setting with multi-scale input size of range 480-800 and total training epochs of 36, which is denoted as DDQ ? , is also adopted for comparison with Sparse R-CNN ? . we use AP50, mean miss rate (mMR) (the smaller the better), and recall rate as the evaluation metrics. It is noted that DDQ leads in both settings and on every metric. A similar trend is also observed on LVIS v1.0. And It is noteworthy that Sparse R-CNN gets poor performance on such a long-tailed dataset. It lags significantly behind conventional CNN-detector. However, the performance of DDQ is still very stable and surpasses all methods by a large margin.  We adopt the same mask head as QueryInst <ref type="bibr" target="#b4">[5]</ref> to compare with other instance segmentation methods on MS-COCO test dataset. As shown in <ref type="table" target="#tab_5">Table 5</ref>, when trained using ResNet-50 backbone and same 680-800 multi-scale augmentations, DDQ outperforms QueryInst by 0.9 AP mask and 1.6 AP bbox with only 1/3 training steps.</p><p>QueryInst also cannot adapt to the long-tailed dataset LVIS v1.0, both detection and instance segmentation results lag significantly behind those of Cascade Mask R-CNN. DDQ achieves 29.6 AP bbox and 26.6 AP mask , which are respectively 6.2 AP bbox and 5.2 AP mask higher than QueryInst and 4.1 AP bbox and 3.7 AP mask higher than Cascade Mask R-CNN [1] on LVIS v1.0.  We analyze the recall with 0.5 IoU threshold, shown in <ref type="table" target="#tab_7">Table 7</ref>. We regard the first 4 stages of Sparse R-CNN as RPN to make a fair comparison with our RPN. It can be found that the recall of vanilla RPN and using 300 queries in Sparse R-CNN both have significant lower recall than that of our RPN design. Although having 7000 query can greatly improve the recall, the inference speed becomes extremely slow, which is 6.2 times slower than our method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Hyper-parameters in DDQ</head><p>We find that the Center Prior Hungarian Assigner is not sensitive to the choice of K. As shown in <ref type="table" target="#tab_8">Table 8</ref>, the performance fluctuations are all less than 0.3, only when we set k as a extremely large value such as 200, the training becomes very unstable. Therefore, we arbitrarily chose 9 and used this value in experiments on other datasets, and still achieved good results.  The RPN is quite robust for the number of share convolutions. As shown in <ref type="table">Table 9</ref>, it can be more lightweight when you decrease the number to 2 and get higher performance with 4 convolutions.</p><p>We analyze the combination of different number of stages and different number of queries. <ref type="table" target="#tab_1">Table 10</ref> shows that the best number of stages is proportional to the number of queries. This is easy to understand. When the number of queries increases, although the recall increases, the quality of the newly added queries is low, and more stages are needed to improve the quality of queries. It worth emphasizing that we use 2 stages and 300 queries to balance the performance and latency and when using 3 stages with the same number queries, our method achieves even higher performance 45.0 AP on MS COCO.</p><p>We try four ways to construct the query in our RPN. As shown in <ref type="table" target="#tab_1">Table 11</ref>, None means all queries are set to a zero tensor and the refinement stage only get meaningful query bounding boxes. This attempt drops the performance to 44.1 AP. Simply constructing queries from the FPN results in 0.3 AP degrade. Constructing queries from the last share convolution can be an alternative as it can get a comparable performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Influence of Other Components in DDQ</head><p>As shown in <ref type="table" target="#tab_1">Table 12</ref>, the Query Distinctness Enhancement(QDE) increases the performance by 0.7 AP as a standalone plugin. However, QFcoal loss and FRF Align are observed to only function well when combined, and either one alone only brings slight improvement. This is consistent with the analysis in Sec. 3.4 that FRF RoIAlign is able to help QFocal loss distinguish the quality of queries through the fusion of multi-layer RoI features. Therefore, FRF RoIAlign has a synergy effect with QFocal loss. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This paper answer the question "what are the expected queries for end-to-end object detection". Based on quantitative analysis on Sparse R-CNN, we conclude that the expected queries should be simultaneously dense and distinct. The entire framework, dubbed as DDQ, is stronger, converges faster, and is more robust across different datasets. DDQ blends advantages from the traditional dense priors and the recent end-to-end detectors. We hope it can serve as a new baseline and inspires researchers to think about the complementarity between traditional methods and end-to-end detectors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Dense priors in traditional object detectors no longer exist in (b) end-to-end object detectors. Intuitively dense queries can maintain a higher recall but are hard to implemented by (c) directly increasing the number of queries. (d) Alternatively, we use duplicate removal as a pre-process to obtain better dense distinct queries and keep the framework end-to-end.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The performance comparison of Sparse R-CNN network with and without duplicate queries removal. All models are trained using the standard 1x setting. The green dotted line represents the default number(300) of queries adopted in Sparse R-CNN. The subplot denotes the memory consumption per GPU as the number of queries increases in Sparse R-CNN. sparse or dense queries inspires us to think about what are expected queries in end-to-end object detection?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The illustration of building blocks of our method. (a) The overall framework of DDQ: The input query first passes through a duplicate query removal process and is concatenated with the corresponding RoI feature to improve the distinctness of query. (b) The RPN framework: The classification and regression subnets share three sequential convolution layers and have one unique convolution layer to produce classification scores and regression offsets. (c): Assignment Strategy of RPN: The potential positive points are chosen with center prior. (d): Flexible Reception Field RoIAlign: RoIAlign features are pulled from neighboring feature levels to get flexible reception field for QFocal loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>AP mask on MS COCO and 3.7 AP mask on LVIS v1.0<ref type="bibr" target="#b9">[10]</ref>.</figDesc><table><row><cell>with 93.2 AP and 98.2 Recall.</cell></row><row><cell>For instance segmentation, DDQ also significantly outperforms Cascade Mask</cell></row><row><cell>R-CNN [1] by 3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>From Sparse R-CNN to DDQ: We show the results of adding Duplicate Query Removal(DQR), boosting the number of queries to 7000, and replacing the first 4 stages of Sparse R-CNN with our RPN, and finally, adding FRF RoI Align and QFocal to get DDQ.</figDesc><table><row><cell>Method</cell><cell cols="2">latency(ms) AP AP50 AP75 APs APm AP l</cell></row><row><cell>Sparse R-CNN</cell><cell>16.4</cell><cell>39.4 57.7 42.5 22.4 41.8 54.3</cell></row><row><cell>DQR</cell><cell>16.8</cell><cell>41.4 60.9 44.8 23.7 44.1 56.3</cell></row><row><cell>7000 Queries + DQR</cell><cell>114.9</cell><cell>43.0 62.7 46.8 27.5 46.0 57.1</cell></row><row><cell>Change to RPN</cell><cell>16.3</cell><cell>43.0 62.2 47.4 26.6 45.6 57.2</cell></row><row><cell>DDQ</cell><cell>17.7</cell><cell>44.5 63.3 48.8 27.8 47.3 58.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison with other detectors including end-to-end detectors e.g.</figDesc><table><row><cell>, Condi-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Performance on CrowdHuman</figDesc><table><row><cell>Method</cell><cell cols="2">Epochs AP50 mMR Recall</cell></row><row><cell>ATSS</cell><cell>12</cell><cell>87.0 49.4 95.1</cell></row><row><cell>TOOD</cell><cell>12</cell><cell>88.7 46.5 95.5</cell></row><row><cell>Cascade R-CNN</cell><cell>12</cell><cell>83.8 46.5 97.5</cell></row><row><cell>DeFCN</cell><cell>32</cell><cell>89.1 48.9 96.5</cell></row><row><cell>DDQ</cell><cell>12</cell><cell>91.1 46.1 97.5</cell></row><row><cell>Sparse R-CNN  ?</cell><cell>50</cell><cell>89.2 48.3 95.9</cell></row><row><cell>DDQ  ?</cell><cell>36</cell><cell>93.2 40.5 98.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Performance on LVIS v1.0 val</figDesc><table><row><cell>Method</cell><cell cols="2">Epochs AP AP50 AP75</cell></row><row><cell>ATSS</cell><cell>12</cell><cell>22.6 32.7 23.9</cell></row><row><cell>Cascade R-CNN</cell><cell>12</cell><cell>24.6 36.1 26</cell></row><row><cell>Sparse R-CNN</cell><cell>12</cell><cell>17.0 25.1 17.6</cell></row><row><cell>Sparse R-CNN</cell><cell>36</cell><cell>21.4 30.2 22.5</cell></row><row><cell>DDQ</cell><cell>12</cell><cell>28.2 39.5 30.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Instance segmentation Results on COCO test dataset</figDesc><table><row><cell>Method</cell><cell cols="3">Epochs AP bbox AP mask</cell></row><row><cell>Cascade Mask</cell><cell>36</cell><cell>44.5</cell><cell>38.6</cell></row><row><cell>QueryInst</cell><cell>36</cell><cell>45.6</cell><cell>40.6</cell></row><row><cell>DDQ</cell><cell>12</cell><cell>47.2</cell><cell>41.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Instance segmentation Results on LVIS v1.0 val dataset</figDesc><table><row><cell>Method</cell><cell cols="3">Epochs AP bbox AP mask</cell></row><row><cell>Mask R-CNN</cell><cell>12</cell><cell>22.5</cell><cell>21.7</cell></row><row><cell>Cascade Mask</cell><cell>12</cell><cell>25.5</cell><cell>22.9</cell></row><row><cell>QueryInst</cell><cell>12</cell><cell>23.4</cell><cell>21.4</cell></row><row><cell>QueryInst</cell><cell>36</cell><cell>22.5</cell><cell>20.8</cell></row><row><cell>DDQ</cell><cell>12</cell><cell>29.6</cell><cell>26.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Recall of RPN, DQR stand for Duplicate Query Removel</figDesc><table><row><cell>Method</cell><cell cols="2">AR100 AR200 AR300 latency(ms)</cell></row><row><cell>Sparse R-CNN</cell><cell>78.4 83.4 85.5</cell><cell>13.6</cell></row><row><cell cols="2">7000 Queries &amp; DQR 88.6 92.3 93.6</cell><cell>85.6</cell></row><row><cell>Naive RPN</cell><cell>76.4 83.0 86.2</cell><cell>19.2</cell></row><row><cell>Our</cell><cell>87.6 91.1 92.7</cell><cell>13.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>Different K in Center Prior Hungarian Assigner</figDesc><table><row><cell cols="2">K AP AP50 AP75</cell></row><row><cell>1</cell><cell>44.1 62.7 48.3</cell></row><row><cell>5</cell><cell>44.4 62.9 48.7</cell></row><row><cell>9</cell><cell>44.5 63.3 48.8</cell></row><row><cell cols="2">13 44.6 63.2 48.5</cell></row><row><cell cols="2">100 44.2 63.0 48.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 .Table 10 .Table 11 .</head><label>91011</label><figDesc>Different number of convolutions in RPN Performance with different number of stages(S) and queries(Q). Impact of how to construct query in RPN.</figDesc><table><row><cell>AP AP50 AP75</cell></row><row><cell>1 44.1 62.5 48.2</cell></row><row><cell>2 44.2 62.5 48.4</cell></row><row><cell>3 44.5 63.3 48.8</cell></row><row><cell>4 44.8 63.3 49.3</cell></row><row><cell>5 44.6 63.3 49.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 12 .</head><label>12</label><figDesc>Impact of Query Distinctness Enhancement(QDE), Flexible Reception Field RoIAlign(FRF) and QFocal loss(QFL). QDE FRF QFL AP AP50 AP75 APs APm AP l 43.0 62.1 47.4 26.6 45.6 56.3 ? 43.7 62.8 48.0 26.7 46.2 57.2 ? 43.2 61.9 47.2 27.3 46.0 55.6 ? 43.2 62.3 47.9 26.1 46.1 57.0 ? ? 44.0 62.6 48.5 27.3 47.0 57.8 ? ? 44.0 62.9 48.1 27.1 47.1 57.0 ? ? 43.9 63.0 48.4 26.9 46.9 57.0 ? ? ? 44.5 63.1 48.9 27.2 47.6 57.7</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Similar trend is also observed in other end-to-end detectors such as Deformable DETR (see supplementary material).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Mmdetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dynamic head: Unifying object detection heads with attentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7373" to="7382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Instances as queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="6910" to="6919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tood: Task-aligned onestage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3510" to="3519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fast convergence of detr with spatially modulated co-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.07448</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Augfpn: Improving multi-scale feature learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12595" to="12604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lvis: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5356" to="5364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Probabilistic anchor assignment with iou prediction for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="355" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generalized focal loss v2: Learning reliable localization quality estimation for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11632" to="11641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dab-detr: Dynamic anchor boxes are better queries for detr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Conditional detr for fast training convergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3651" to="3660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Crowdhuman: A benchmark for detecting human in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00123</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sparse r-cnn: End-to-end object detection with learnable proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14454" to="14463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rethinking transformer-based set prediction for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3611" to="3620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-end object detection with fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15849" to="15858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Solo: Segmenting objects by locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="649" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Anchor detr: Query design for transformerbased detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.07107</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Efficient detr: improving end-to-end object detector with dense prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01318</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
