<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Non-Autoregressive Translation by Learning Target Categorical Codes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Bao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Collaborative Innovation Center of Novel Software Technology and Industrialization</orgName>
								<orgName type="laboratory">National Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution" key="instit1">Nanjing University</orgName>
								<orgName type="institution" key="instit2">NiuTrans Co., Ltd</orgName>
								<address>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Collaborative Innovation Center of Novel Software Technology and Industrialization</orgName>
								<orgName type="laboratory">National Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution" key="instit1">Nanjing University</orgName>
								<orgName type="institution" key="instit2">NiuTrans Co., Ltd</orgName>
								<address>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
							<email>xiaotong@mail.neu.edu.cnhuangsj</email>
							<affiliation key="aff0">
								<orgName type="department">Collaborative Innovation Center of Novel Software Technology and Industrialization</orgName>
								<orgName type="laboratory">National Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution" key="instit1">Nanjing University</orgName>
								<orgName type="institution" key="instit2">NiuTrans Co., Ltd</orgName>
								<address>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqi</forename><surname>Wang</surname></persName>
							<email>wangdq@smail.nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Collaborative Innovation Center of Novel Software Technology and Industrialization</orgName>
								<orgName type="laboratory">National Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution" key="instit1">Nanjing University</orgName>
								<orgName type="institution" key="instit2">NiuTrans Co., Ltd</orgName>
								<address>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin-Yu</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Collaborative Innovation Center of Novel Software Technology and Industrialization</orgName>
								<orgName type="laboratory">National Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution" key="instit1">Nanjing University</orgName>
								<orgName type="institution" key="instit2">NiuTrans Co., Ltd</orgName>
								<address>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
							<email>chenjj@nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Collaborative Innovation Center of Novel Software Technology and Industrialization</orgName>
								<orgName type="laboratory">National Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution" key="instit1">Nanjing University</orgName>
								<orgName type="institution" key="instit2">NiuTrans Co., Ltd</orgName>
								<address>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Non-Autoregressive Translation by Learning Target Categorical Codes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Non-autoregressive Transformer is a promising text generation model. However, current non-autoregressive models still fall behind their autoregressive counterparts in translation quality. We attribute this accuracy gap to the lack of dependency modeling among decoder inputs. In this paper, we propose CNAT, which learns implicitly categorical codes as latent variables into the non-autoregressive decoding. The interaction among these categorical codes remedies the missing dependencies and improves the model capacity. Experiment results show that our model achieves comparable or better performance in machine translation tasks, compared with several strong baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Non-autoregressive Transformer (NAT, <ref type="bibr" target="#b5">Gu et al., 2018;</ref><ref type="bibr" target="#b12">Lee et al., 2018;</ref><ref type="bibr" target="#b4">Ghazvininejad et al., 2019)</ref> is a promising text generation model for machine translation. It introduces the conditional independent assumption among the target language outputs and simultaneously generates the whole sentence, bringing in a remarkable efficiency improvement (more than 10? speed-up) versus the autoregressive model. However, the NAT models still lay behind the autoregressive models in terms of BLEU <ref type="bibr" target="#b16">(Papineni et al., 2002)</ref> for machine translation. We attribute the low-quality of NAT models to the lack of dependencies modeling for the target outputs, making it harder to model the generation of the target side translation.</p><p>A promising way is to model the dependencies of the target language by the latent variables. A line of research works <ref type="bibr" target="#b8">(Kaiser et al., 2018;</ref><ref type="bibr" target="#b21">Shu et al., 2019;</ref><ref type="bibr" target="#b14">Ma et al., 2019)</ref> introduce latent variable modeling to the non-autoregressive Transformer and improves translation quality. The latent variables could be regarded as the springboard to bridge the modeling gap, introducing more informative decoder inputs than the previously copied inputs. More specifically, the latentvariable based model first predicts a latent variable sequence conditioned on the source representation, where each variable represents a chunk of words. The model then simultaneously could generate all the target tokens conditioning on the latent sequence and the source representation since the target dependencies have been modeled into the latent sequence.</p><p>However, due to the modeling complexity of the chunks, the above approaches always rely on a large number (more than 2 15 , <ref type="bibr" target="#b8">Kaiser et al., 2018;</ref> of latent codes for discrete latent spaces, which may hurt the translation efficiencythe essential goal of non-autoregressive decoding. <ref type="bibr" target="#b0">Akoury et al. (2019)</ref> introduce syntactic labels as a proxy to the learned discrete latent space and improve the NATs' performance. The syntactic label greatly reduces the search space of latent codes, leading to a better performance in both quality and speed. However, it needs an external syntactic parser to produce the reference syntactic tree, which may only be effective in limited scenarios. Thus, it is still challenging to model the dependency between latent variables for non-autoregressive decoding efficiently.</p><p>In this paper, we propose to learn a set of latent codes that can act like the syntactic label, which is learned without using the explicit syntactic trees. To learn these codes in an unsupervised way, we use each latent code to represent a fuzzy target category instead of a chunk as the previous research <ref type="bibr" target="#b0">(Akoury et al., 2019)</ref>. More specifically, we first employ vector quantization  to discretize the target language to the latent space with a smaller number (less than 128) of latent variables, which can serve as the fuzzy word-class information each target language word. We then model the latent variables with conditional random fields (CRF, <ref type="bibr" target="#b11">Lafferty et al., 2001;</ref> arXiv:2103.11405v1 [cs.CL] 21 Mar 2021 2019). To avoid the mismatch of the training and inference for latent variable modeling, we propose using a gated neural network to form the decoder inputs. Equipping it with scheduled sampling <ref type="bibr" target="#b2">(Bengio et al., 2015)</ref>, the model works more robustly.</p><p>Experiment results on WMT14 and IWSLT14 show that CNAT achieves the new state-of-theart performance without knowledge distillation. With the sequence-level knowledge distillation and reranking techniques, the CNAT is comparable to the current state-of-the-art iterative-based model while keeping a competitive decoding speedup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Neural machine translation (NMT) is formulated as a conditional probability model p(y|x), which models a sentence y = {y 1 , y 2 , ? ? ? , y m } in the target language given the input x = {x 1 , x 2 , ? ? ? , x n } from the source language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Non-Autoregressive Neural Machine</head><p>Translation <ref type="bibr" target="#b5">Gu et al. (2018)</ref> proposes Non-Autoregressive Transformer (NAT) for machine translation, breaking the dependency among target tokens, thus achieving simultaneous decoding for all tokens. For a source sentence, a non-autoregressive decoder factorizes the probability of its target sentence as:</p><formula xml:id="formula_0">p(y|x) = m t=1 p(y t |x; ?),<label>(1)</label></formula><p>where ? is the set of model parameters.</p><p>NAT has a similar architecture to the autoregressive Transformer (AT, <ref type="bibr" target="#b23">Vaswani et al., 2017)</ref>, which consists of a multi-head attention based encoder and decoder. The model first encodes the source sentence x 1:n as the contextual representation e 1:n , then employs an extra module to predict the target length and form the decoder inputs.</p><p>? Length Prediction: Specifically, the length predictor in the bridge module predicts the target sequence length m by:</p><formula xml:id="formula_1">m = n + arg max ?L p(? L | mean(e); ?),<label>(2)</label></formula><p>where ? L is the length difference between the target and source sentence, ? is the parameter of length predictor.  ? Inputs Initialization: With the target sequence length m, we can compute the decoder inputs h = h 1:m with Softcopy <ref type="bibr" target="#b26">Wei et al., 2019)</ref> as:</p><formula xml:id="formula_2">h j = n i w ij ? e i and w ij = softmax(?|j ? i|/? ),<label>(3)</label></formula><p>where ? is a hyper-parameter to control the sharpness of the softmax function.</p><p>With the computed decoder inputs h, NAT generates target sequences simultaneously by arg max yt p(y t |x; ?) for each timestep t, effectively reduce computational overhead in decoding (see <ref type="figure" target="#fig_0">Figure 1b</ref>). Though NAT achieves around 10? speedup in machine translation than autoregressive models, it still suffers from potential performance degradation <ref type="bibr" target="#b5">(Gu et al., 2018)</ref>. The results degrade since the removal of target dependencies prevents the decoder from leveraging the inherent sentence structure in prediction. Moreover, taking the copied source representation as decoder inputs implicitly assume that the source and target language share a similar order, which may not always be the case <ref type="bibr" target="#b1">(Bao et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Latent Transformer</head><p>To bridge the gap between non-autoregressive and autoregressive decoding, <ref type="bibr" target="#b8">Kaiser et al. (2018)</ref> introduce the Latent Transformer (LT). It incorporates non-autoregressive decoding with conditional dependency as the latent variable to alleviate the degradation resulted from the absence of dependency:</p><formula xml:id="formula_3">p(y|x) = p(z|x; ?) m t=1 p(y t |z, x; ?),<label>(4)</label></formula><p>where z = {z 1 , ? ? ? , z L } is the latent sequence and the L is the length of the latent sequence, ? and ? are the parameter of latent predictor and translation model, respectively.</p><p>The LT architecture stays unchanged from the origin NAT models, except for the latent predictor and decoder inputs. During inference, the Latent Transformer first autoregressively predicts the latent variables z, then non-autoregressively produces the entire target sentence y conditioned on the latent sequence z (see <ref type="figure" target="#fig_0">Figure 1c</ref>). <ref type="bibr" target="#b14">Ma et al. (2019)</ref>; <ref type="bibr" target="#b21">Shu et al. (2019)</ref> extend this idea and model z as the continuous latent variables, achieving a promising result, which replaces the autoregressive predictor with the iterative transformation layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In this section, we present our proposed CNAT, an extension to the Transformer incorporated with non-autoregressive decoding for target tokens and autoregressive decoding for latent sequences.</p><p>In brief, CNAT follows the architecture of Latent Transformer <ref type="bibr" target="#b8">(Kaiser et al., 2018)</ref>, except for the latent variable modeling (in ? 3.1 and ? 3.2) and inputs initialization (in ? 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Modeling Target Categorical Information by Vector Quantization</head><p>Categorical information has achieved great success in neural machine translation, such as partof-speech (POS) tag in autoregressive translation <ref type="bibr">(Yang et al., 2019)</ref> and syntactic label in nonautoregressive translation <ref type="bibr" target="#b0">(Akoury et al., 2019)</ref>. Inspired by the broad application of categorical information, we propose to model the implicit categorical information of target words in a nonautoregressive Transformer. Each target sequence y = y 1:m will be assigned to a discrete latent variable sequence z = z 1:m . We assume that each z i will capture the fuzzy category of its token y i . Then, the conditional probability p(y|x) is factorized with respect to the categorical latent variable:</p><formula xml:id="formula_4">p(y|x) = z p(z|x) ? p(y|z, x).<label>(5)</label></formula><p>However, it is computationally intractable to sum all configurations of latent variables. Following the spirit of the latent based model <ref type="bibr" target="#b8">(Kaiser et al., 2018;</ref>, we employ a vector quantized technique to maintain differentiability through the categorical modeling and learn the latent variables straightforward.</p><p>Vector Quantization. The vector quantization based methods have a long history of being successfully in machine learning models. In vector quantization, each target representation repr(y i ) ? R d model is passed through a discretization bottleneck using a nearest-neighbor lookup on embedding matrix Q ? R K?d model , where K is the number of categorical codes. For each y i in the target sequence, we define its categorical variable z i and latent code q i as:</p><formula xml:id="formula_5">z i = k, q i = Q k , and k = arg min j?[K] || repr(y i ) ? Q j || 2 , (6)</formula><p>where || ? || 2 is the l 2 distance, [K] denote the set {1, 2, ? ? ? , K}. Intuitively, we adopt the embedding of y as the target representation:</p><formula xml:id="formula_6">repr(y i ) = embedding(y i )</formula><p>where the embedding matrix of the target language is shared with the softmax layer of the decoder.</p><p>Exponential Moving Average. Following the common practice of vector quantization, we also employ the exponential moving average (EMA) technique to regularize the categorical codes.</p><p>Put simply, the EMA technique could be understood as basically the k-means clustering of the hidden states with a sort of momentum. We maintain an EMA over the following two quantities for each j ? [K]: 1) the count c j measuring the number of target representations that have Q j as its nearest neighbor, and 2) Q j . The counts are updated over a mini-batch of targets {y 1 , y 2 , ? ? ? , y m?B } with:</p><formula xml:id="formula_7">c j = ?c j + (1 ? ?) m?B i 1[z i = j],<label>(7)</label></formula><p>then, the latent code Q j being updated with:</p><formula xml:id="formula_8">Q j = ?Q j +(1??) m?B i 1[z i = j] repr(y i ) c j ,<label>(8)</label></formula><p>where 1[?] is the indicator function and ? is a decay parameter, B is the size of the batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Modeling Categorical Sequence with Conditional Random Fields</head><p>Our next insight is transferring the dependencies among the target outputs into the latent spaces.</p><p>Since the categorical variable captures the fuzzy target class information, it can be a proxy of the target outputs. We further employ a structural prediction module instead of the standard autoregressive Transformer to model the latent sequence. The former can explicitly model the dependencies among the latent variables and performs exact decoding during inference.</p><p>Conditional Random Fields. We employ a linear-chain conditional random fields (CRF, <ref type="bibr" target="#b11">Lafferty et al., 2001)</ref> to model the categorical latent variables, which is the most common structural prediction model. Given the source input x = (x 1 , ? ? ? , x n ) and its corresponding latent variable sequence z = (z 1 , ? ? ? , z m ), the CRF model defines the probability of z as:</p><formula xml:id="formula_9">p(z|x) = 1 Z(x) exp m i=1 s(z i , x, i) + m i=2 t(z i?1 , z i , x, i) ,<label>(9)</label></formula><p>where Z(x) is the normalize factor, s(z i , x, i) is the emit score of z i at the position i, and the</p><formula xml:id="formula_10">t(z i?1 , z i , x, i) is the transition score from z i?1 to z i .</formula><p>Before computing the emit score and transition score in Eq. 9, we first take h = h 1:m as the inputs and compute the representation f = Transfer(h), where Transfer(?) denotes a twolayer vanilla Transformer decoding function including a self-attention block, an encoder-decoder block followed by a feed-forward neural network block <ref type="bibr" target="#b23">(Vaswani et al., 2017)</ref>.</p><p>We then compute the emit score and the transition score. For each position i, we compute the emit score with a linear transformation:</p><formula xml:id="formula_11">s(z i , x, i) = (W T f i + b) z i where W ? R d model ?K and b ? R K</formula><p>are the parameters. We incorporate the positional context and compute its transition score with:</p><formula xml:id="formula_12">M i d = Biaffine([f i?1 ; f i ]), M i = E T 1 M i d E 2 , t(z i?1 , z i , x, i) = M i z i?1 ,z i ,<label>(10)</label></formula><p>where Biaffine(?) : R 2d model ? R dt?dt is a biaffine neural network <ref type="bibr" target="#b3">(Dozat and Manning, 2017)</ref>, E 1 and E 2 ? R dt?K are the transition matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Fusing Source Inputs and Latent Codes via Gated Function</head><p>One potential issue is that the mismatch of the training and inference stage for the used categorical variables. Suppose we train the decoder with the quantized categorical variables z, which is inferred from the target reference. In that case, we may fail to achieve satisfactory performance with the predicted categorical variables during inference. We intuitively apply the gated neural network (denote as GateNet) to form the decoder inputs by fusing the copied decoder inputs h = h 1:m and the latent codes q = q 1:m , since the copied decoder inputs h is still informative to nonautoregressive decoding:</p><formula xml:id="formula_13">g i = ?(FFN([h i ; q i ])), o i = h i * g i + q(z i ) * (1 ? g i ),<label>(11)</label></formula><p>where the FFN(?) : R 2d model ? R d model is a twolayer feed-forward neural networks and ?(.) is the sigmoid function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training</head><p>While training, we first compute the reference z ref by the vector quantization and employ the EMA to update the quantized codes. The loss of the CRFbased predictor is computed with:</p><formula xml:id="formula_14">L crf = ? log p(z ref |x).<label>(12)</label></formula><p>To equip with the GateNet, we randomly mix the z ref and the predicted z pred as:</p><formula xml:id="formula_15">z mix i = z pred i if p ? ? z ref i if p &lt; ? ,<label>(13)</label></formula><p>where p ? U[0, 1] and ? is the threshold we set 0.5 in our experiments. Grounding on the z mix , the non-autoregressive translation loss is computed with:</p><formula xml:id="formula_16">L NAT = ? log p(y|z mix , x; ?).<label>(14)</label></formula><p>With the hyper-parameter ?, the overall training loss is:</p><formula xml:id="formula_17">L = L NAT + ?L crf .<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Inference</head><p>CNAT selects the best sequence by choosing the highest-probability latent sequence z with Viterbi decoding <ref type="bibr" target="#b24">(Viterbi, 1967)</ref>, then generate the tokens with:</p><formula xml:id="formula_18">z * = arg max z p(z|x; ?),</formula><p>and y * = arg max</p><formula xml:id="formula_19">y p(y|z * , x; ?),</formula><p>where identifying y * only requires independently maximizing the local probability for each output position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets. We conduct the experiments on the most widely used machine translation benchmarks: WMT14 English-German (WMT14 EN-DE, 4.5M pairs) 1 and IWSLT14 German-English (IWSLT14, 160K pairs) 2 . The datasets are processed with the Moses script <ref type="bibr" target="#b10">(Koehn et al., 2007)</ref>, and the words are segmented into subword units using byte-pair encoding <ref type="bibr">(Sennrich et al., 2016, BPE)</ref>. We use the shared subword embeddings between the source language and target language for the WMT datasets and the separated subword embeddings for the IWSLT14 dataset.</p><p>Model Setting. In the case of IWSLT14 task, we use a small setting (d model = 256, d hidden = 512, p dropout = 0.1, n layer = 5 and n head = 4) for Transformer and NAT models. For the WMT tasks, we use the Transformer-base setting (d model = 512, d hidden = 512, p dropout = 0.3, n head = 8 and n layer = 6) of the <ref type="bibr" target="#b23">Vaswani et al. (2017)</ref>. We set the hyperparameter ? used in Eq. 15 and ? in Eq. 7-8 to 1.0 and 0.999, respectively. The categorical number K is set to 64 in our experiments. We implement our model based on the open-source framework of fairseq <ref type="bibr" target="#b15">(Ott et al., 2019)</ref>.</p><p>Optimization. We optimize the parameter with the Adam (Kingma and Ba, 2015) with ? = (0.9, 0.98). We use inverse square root learning rate scheduling <ref type="bibr" target="#b23">(Vaswani et al., 2017)</ref> for the WMT tasks and linear annealing schedule <ref type="bibr" target="#b12">(Lee et al., 2018)</ref> from 3 ? 10 ?4 to 1 ? 10 ?5 for the IWSLT14 task. Each mini-batch consists of 2048 tokens for IWSLT14 and 32K tokens for WMT tasks.</p><p>Distillation. Sequence-level knowledge distillation <ref type="bibr" target="#b7">(Hinton et al., 2015)</ref> is applied to alleviate the multi-modality problem <ref type="bibr" target="#b5">(Gu et al., 2018)</ref> while training. We follow previous studies on NAT <ref type="bibr" target="#b5">(Gu et al., 2018;</ref><ref type="bibr" target="#b12">Lee et al., 2018;</ref><ref type="bibr" target="#b26">Wei et al., 2019)</ref> and use translations produced by a pre-trained autoregressive Transformer <ref type="bibr" target="#b23">(Vaswani et al., 2017)</ref> as the training data.</p><p>Reranking. We also include the results that come at reranked parallel decoding <ref type="bibr" target="#b5">(Gu et al., 2018;</ref><ref type="bibr" target="#b6">Guo et al., 2019;</ref><ref type="bibr" target="#b26">Wei et al., 2019)</ref>, which generates several decoding candidates in parallel and selects the best via re-scoring , which was called length parallel decoding (LPD). Then, we use the pre-trained teacher to rank these sequences and identify the best overall output as the final output.</p><p>Baselines. We compare the CNAT with several strong NAT baselines, including:</p><p>? We compare the proposed CNAT against baselines both in terms of generating quality and inference speedup. For all our tasks, we obtain the performance of baselines by either directly using the performance figures reported in the previous works if they are available or producing them by using the open-source implementation of baseline algorithms on our datasets.</p><p>Metrics. We evaluate using the tokenized and cased BLEU scores <ref type="bibr" target="#b16">(Papineni et al., 2002)</ref>. We highlight the best NAT result with bold text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results</head><p>Translation Quality. First, we compare CNAT with the NAT models without using advanced techniques, such as knowledge distillation, reranking,   or iterative refinements. The results are listed in <ref type="table">Table 1</ref>. The CNAT achieves significant improvements (around 11.5 BLEU in EN-DE, more than 14.5 BLEU in DE-EN) over the vanilla NAT, which indicates that modeling categorical information could improve the modeling capability of the NAT model. Also, the CNAT achieves better results than Flowseq and SynST, which demonstrates the effectiveness of CNAT in modeling dependencies between the target outputs. The performance of the NAT models with advance techniques (sequence-level knowledge distillation or reranking) is listed in <ref type="table" target="#tab_3">Table 2 and Table 3</ref>. Coupling with the knowledge distillation techniques, all NAT models achieve remarkable improvements.</p><p>Our best results are obtained with length parallel decoding, which employs a pretrained Transformer to rerank the multiple parallels generated candidates of different target lengths. Specifically, on a large scale WMT14 dataset, CNAT surpasses the NAT-DCRF by 0.71 BLEU score in DE-EN but  slightly under the NAT-DCRF around 0.20 BLEU in EN-DE, which shows that the CNAT is comparable to the state-of-the-art NAT model. Also, we can see that a larger "N" leads to better results (N = 100 vs. N = 10 of NAT-FT, N = 19 vs. N = 9 of NAT-DCRF, etc.); however, it always comes at the degradation of decoding efficiency. We also compare our CNAT with the NAT models that employ an iterative decoding technique and list the results in <ref type="table" target="#tab_6">Table 4</ref>. The iterative-based non-autoregressive Transformer captures the target language's dependencies by iterative generating based on the previous iteration output, which is an important exploration for a non-autoregressive generation. With the iteration number increasing, the performance improving, the decoding speed-up dropping, whatever the IR-NAT or CMLM. We can see that the CNAT achieves a better result than the CMLM with four iterations and IR-NAT with ten iterations, even close to the CMLM with ten iterations while keeping the benefits of a one-shot generation.</p><p>Translation Efficiency. As depicted in <ref type="figure">Figure 2</ref>, we validate the efficiency of CNAT. Put simply, the decoding speed is measured sentence-by-sentence, and the speed-up is computed by comparing it with the Transformer. <ref type="figure">Figure 2a</ref> and <ref type="figure">Figure 2b</ref> show the BLEU scores and decoding speed-up of NAT models. The former compares the pure NAT models. The latter compares NAT model inference with advanced decoding techniques (parallel reranking or iterative-based decoding) 3 .</p><p>We can see from <ref type="figure">Figure 2</ref> that the point of   CNAT is located on the top-right of the baselines. The CNAT outperforms our baselines in BLEU if speed-up is held, and in speed-up if BLEU is held, indicating CNAT outperforms previous state-ofthe-art NAT methods. Although iterative models like CMLM achieves competitive BLEU scores, they only maintain minor speed advantages over Transformer. In contrast, CNAT remarkably improves the inference speed while keeping a competitive performance.</p><p>Effectiveness of Categorical Modeling. We further conduct the experiments on the test set of IWSLT14 to analyze the effectiveness of our categorical modeling and its influence on translation quality. We regard the categorical predictor as a sequence-level generation task and list its BLEU score in <ref type="table" target="#tab_8">Table 5</ref>. As see, a better latent prediction can yield a better translation. With the z ref as the latent sequence, the model achieves surprisingly good performance on this task, showing the usefulness of the learned categorical codes. We also can see that the CNAT decoding with reference length only slightly (0.44 BLEU) better than it with predicted length, indicat-  <ref type="table">Table 6</ref>: Ablation study on the dev set of IWSLT14.</p><p>Note that we train all of the configurations with knowledge distillation. "AR" denotes an autoregressive Transformer predictor. The line 8 is our NAT baseline. ing that the model is robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>We further conduct the ablation study with different CNAT variant on dev set of IWSLT14.</p><p>Influence of K. We can see the CRF with the categorical number K = 64 achieves the highest score (line 2). A smaller or larger K neither has a better result. The AR predictor may have a different tendency: with a larger K = 128, it achieves a better performance. However, a larger K may lead to a higher latency while inference, which is not the best for non-autoregressive decoding. In our experiments, the K = 64 can achieve the highperformance and be smaller enough to keep the low-latency during inference.</p><p>CRF versus AR. Experiment results show that the CRF-based predictor is better than the AR predictor. We can see that the CRF-based predictor surpasses the Transformer predictor 3.5 BLEU (line 2 vs. line 5) with the GateNet; without the  GateNet, the gap enlarges to 5.3 BLEU (line 4 vs. line 6). It is consistent with our intuition that CRF is better than Transformer to model the dependencies among latent variables on machine translation when the number of categories is small.</p><p>GateNet. Without the GateNet, the CNAT with AR predictor degenerates a standard LT model with a smaller latent space. We can see its performance is even lower than the NAT-baselines (line 6 vs. line 8). Equipping with the GateNet and the schedule sampling, it outperforms the NAT baseline with a large margin (around 4.0 BLEU), showing that the GateNet mechanism plays an essential role in our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Code Study</head><p>To analyze the learned category, we further compute its relation to two off-the-shelf categorical information: the part-of-speech (POS) tags and the frequency-based clustered classes. For the former, we intuitively assign the POS tag of a word to its sub-words and compute the POS tag frequency for the latent codes. For the latter, we roughly assign the category of a subword according to its frequency. It needs to mention that the number of frequency-based classes is the same as that of the POS tags.</p><p>Quantitative Results. We first compute the V-Measure <ref type="bibr" target="#b18">(Rosenberg and Hirschberg, 2007)</ref> score between the latent categories to POS tags and subwords frequencies. The results are listed in <ref type="table" target="#tab_11">Table 7</ref>. Overall, the "w/ POS tags" achieves a higher V-Measure score, indicating that the latent codes are more related to the POS tags than sub-words frequencies. The homogeneity score (H-score) evaluates the purity of the category. We also can see that the former has a relatively higher H-score than the latter (0.70 vs. 0.62), which is consistent with our intuition.</p><p>Case Analysis. As shown in <ref type="figure">Figure 3</ref>, we also depict the POS tags distribution for the top 10 frequent latent variables on the test set of IWSLT14 4 . <ref type="bibr">4</ref> More details can be found in Appendix B.  <ref type="figure">Figure 3</ref>: The POS tags distribution for the top 10 frequent latent variables on the test set of IWSLT14. We list the top 3 frequent POS tags for each latent variable.</p><p>We can see a sharp distribution for each latent variable, showing that our learned fuzzy classes are meaningful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Non-autoregressive Machine Translation. <ref type="bibr" target="#b5">Gu et al. (2018)</ref> first develop a non-autoregressive Transformer (NAT) for machine translation, which produces the outputs in parallel, and the inference speed is thus significantly boosted. Due to the missing of dependencies among the target outputs, the translation quality is largely sacrificed. A line of work proposes to mitigate such performance degradation by enhancing the decoder inputs. <ref type="bibr" target="#b12">Lee et al. (2018)</ref> propose a method of iterative refinement based on the previous outputs. <ref type="bibr" target="#b6">Guo et al. (2019)</ref> enhance decoder input by introducing the phrase table in statistical machine translation and embedding transformation. There are also some work focuses on improving the decoder inputs' supervision, including imitation learning from autoregressive models <ref type="bibr" target="#b26">(Wei et al., 2019)</ref> or regularizing the hidden state with backward reconstruction error .</p><p>Another work proposes modeling the dependencies among target outputs, which is explicitly missed in the vanilla NAT models. <ref type="bibr" target="#b17">Qian et al. (2020)</ref>; <ref type="bibr" target="#b4">Ghazvininejad et al. (2019)</ref> propose to model the target-side dependencies with a masked language model, modeling the directed dependencies between the observed target and the unobserved words. Different from their work, we model the target-side dependencies in the latent space, which follows the latent variable Transformer fashion.</p><p>Latent Variable Transformer. More close to our work is the latent variable Transformer, which takes the latent variable as inputs to modeling the target-side information. <ref type="bibr" target="#b21">Shu et al. (2019)</ref> combine continuous latent variables and deterministic inference procedure to find the target sequence that maximizes the lower bound to the log-probability. <ref type="bibr" target="#b14">Ma et al. (2019)</ref> propose to use generative flows to the model complex prior distribution. <ref type="bibr" target="#b8">Kaiser et al. (2018)</ref> propose to autoregressively decode a shorter latent sequence encoded from the target sentence, then simultaneously generate the sentence from the latent sequence. <ref type="bibr" target="#b1">Bao et al. (2019)</ref> model the target position of decode input as a latent variable and introduce a heuristic search algorithm to guide the position learning. <ref type="bibr" target="#b0">Akoury et al. (2019)</ref> first autoregressively predict a chunked parse tree and then simultaneously generate the target tokens from the predicted syntax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose CNAT, which implicitly models the categorical codes of the target language, narrowing the performance gap between the nonautoregressive decoding and autoregressive decoding. Specifically, CNAT builds upon the latent Transformer and models the target-side categorical information with vector quantization and conditional random fields (CRF) model. We further employ a gated neural network to form the decoder inputs. Equipped with the scheduled sampling, CNAT works more robust. As a result, the CNAT achieves a significant improvement and moves closer to the performance of the Transformer on machine translation.  Results. We can see than in <ref type="table" target="#tab_14">Table 8</ref>   <ref type="table">Table 9</ref>: The distribution of pos tags for latent variables. For each latent variable, we list the top 3 frequent pos tags and their corresponding percentages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Different inference process of different Transformer models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The NAT builds upon latent variables: NAT-FT<ref type="bibr" target="#b5">(Gu et al., 2018)</ref>, LT<ref type="bibr" target="#b8">(Kaiser et al., 2018)</ref>, Syn-ST<ref type="bibr" target="#b0">(Akoury et al., 2019)</ref>, LV-NAR<ref type="bibr" target="#b21">(Shu et al., 2019)</ref> and Flowseq<ref type="bibr" target="#b14">(Ma et al., 2019)</ref>.? The NAT with extra autoregressive decoding or iterative refinement: NAT-DCRF, IR-NAT<ref type="bibr" target="#b12">(Lee et al., 2018)</ref>, and CMLM<ref type="bibr" target="#b4">(Ghazvininejad et al., 2019)</ref>.? The NAT with auxiliary training objectives: NAT-REG, imitate-NAT<ref type="bibr" target="#b26">(Wei et al., 2019)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results of NAT models trained with knowledge distillation on test set of WMT14 and IWSLT14.</figDesc><table><row><cell>Model</cell><cell>N</cell><cell cols="2">WMT14 EN-DE DE-EN</cell></row><row><cell>NAT-FT</cell><cell>10</cell><cell>18.66</cell><cell>22.42</cell></row><row><cell>NAT-FT</cell><cell>100</cell><cell>19.17</cell><cell>23.20</cell></row><row><cell>LT</cell><cell>10</cell><cell>21.00</cell><cell>/</cell></row><row><cell>LT</cell><cell>100</cell><cell>22.50</cell><cell>/</cell></row><row><cell>NAT-REG</cell><cell>9</cell><cell>24.61</cell><cell>28.90</cell></row><row><cell>imitate-NAT</cell><cell>9</cell><cell>24.15</cell><cell>27.28</cell></row><row><cell>Flowseq</cell><cell>15</cell><cell>24.70</cell><cell>29.44</cell></row><row><cell>Flowseq</cell><cell>30</cell><cell>25.31</cell><cell>30.68</cell></row><row><cell>NAT-DCRF</cell><cell>9</cell><cell>26.07</cell><cell>29.68</cell></row><row><cell>NAT-DCRF</cell><cell>19</cell><cell>26.80</cell><cell>30.04</cell></row><row><cell>Transformer (ours)</cell><cell>-</cell><cell>27.33</cell><cell>31.69</cell></row><row><cell>CNAT (ours)</cell><cell>9</cell><cell>26.60</cell><cell>30.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Results of NAT models with parallel decod- ing on test set of WMT14. "N" means the number of candidates to be re-ranked.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Results of NAT models with iterative refinements on test set of WMT14. "Iteration" means the number of iteration refinements.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Results on the test of IWSLT14 to analyze the effectiveness of categorical modeling. "w/ z ref " denote CNAT generate the tokens condition on the latent sequence which is quantized from the reference target. "w/ m ref " denote the CNAT generate the tokens condition on the reference length.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Clustering evaluation metrics on the test set of IWSLT14 to analyze the learned codes.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Xuewen Yang, Yingru Liu, Dongliang Xie, Xin Wang, and Niranjan Balasubramanian. 2019. Latent partof-speech sequences for neural machine translation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-</figDesc><table><row><cell cols="2">guage Processing (EMNLP-IJCNLP), pages 780-</cell></row><row><cell cols="2">790, Hong Kong, China. Association for Computa-</cell></row><row><cell>tional Linguistics.</cell><cell></cell></row><row><cell cols="2">A Non-Indo-European Translation</cell></row><row><cell cols="2">Dataset. We apply the CNAT to the non-Indo-</cell></row><row><cell cols="2">European translation tasks on the LDC Chinese-</cell></row><row><cell cols="2">English 5 (denote as LDC ZH-EN, 1.30M sentence</cell></row><row><cell cols="2">pairs) and MT02 test set of NIST ZH-EN dataset.</cell></row><row><cell cols="2">We use NLPIRICTCLAS 6 and Moses tokenizer</cell></row><row><cell cols="2">for Chinese and English tokenization, respectively.</cell></row><row><cell>Model</cell><cell>BLEU</cell></row><row><cell>Transformer</cell><cell>28.05</cell></row><row><cell>NAT</cell><cell>12.31</cell></row><row><cell>CNAT</cell><cell>22.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Results on the MT02 set of different models.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>that our model can enhance the performance of NAT with a large margin (22.16 vs. 12.31).</figDesc><table><row><cell></cell><cell>ID</cell><cell>Top 1</cell><cell>Top 2</cell><cell>Top 3</cell></row><row><cell></cell><cell>0</cell><cell>NOUN(41.06%)</cell><cell cols="2">ADJ(20.74%) VERB(20.09%)</cell></row><row><cell></cell><cell>1</cell><cell>NOUN(88.73%)</cell><cell cols="2">ADJ(3.48%) VERB(2.34%)</cell></row><row><cell></cell><cell>2</cell><cell>NOUN(58.06%)</cell><cell cols="2">ADJ(15.40%) VERB(12.29%)</cell></row><row><cell></cell><cell cols="2">3 PUNCT(100.00%)</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="4">4 PUNCT(99.80%) NOUN(0.14%) PROPN(0.05%)</cell></row><row><cell></cell><cell>5</cell><cell cols="3">PRON(99.29%) NOUN(0.42%) PROPN(0.20%)</cell></row><row><cell></cell><cell>6</cell><cell cols="2">VERB(77.95%) NOUN(9.93%)</cell><cell>ADJ(8.45%)</cell></row><row><cell></cell><cell>7</cell><cell cols="2">DET(99.33%) PUNCT(0.47%)</cell><cell>PART(0.15%)</cell></row><row><cell></cell><cell>8</cell><cell cols="3">ADJ(56.24%) ADV(15.06%) NUM(11.87%)</cell></row><row><cell></cell><cell>9</cell><cell cols="2">NOUN(52.01%) VERB(16.72%)</cell><cell>ADJ(15.75%)</cell></row><row><cell></cell><cell cols="4">10 CCONJ(99.71%) VERB(0.11%) NOUN(0.11%)</cell></row><row><cell></cell><cell>11</cell><cell>PART(71.00%)</cell><cell cols="2">ADP(27.70%) SCONJ(0.99%)</cell></row><row><cell></cell><cell>12</cell><cell cols="2">PRON(60.71%) SCONJ(30.15%)</cell><cell>DET(8.88%)</cell></row><row><cell></cell><cell>13</cell><cell cols="2">ADP(90.70%) SCONJ(5.91%)</cell><cell>ADV(3.09%)</cell></row><row><cell></cell><cell>14</cell><cell cols="3">DET(96.81%) NOUN(1.81%) PROPN(1.13%)</cell></row><row><cell></cell><cell cols="3">15 NOUN(88.49%) VERB(4.28%)</cell><cell>ADV(3.10%)</cell></row><row><cell></cell><cell>16</cell><cell cols="2">VERB(90.35%) NOUN(4.82%)</cell><cell>AUX(4.40%)</cell></row><row><cell></cell><cell>17</cell><cell cols="3">ADV(67.37%) PART(15.02%) NOUN(8.31%)</cell></row><row><cell></cell><cell>18</cell><cell>PRON(68.46%)</cell><cell cols="2">DET(29.31%) NOUN(1.65%)</cell></row><row><cell></cell><cell>19</cell><cell cols="2">ADV(61.05%) SCONJ(36.01%)</cell><cell>ADP(2.31%)</cell></row><row><cell></cell><cell>20</cell><cell cols="3">VERB(93.70%) NOUN(6.03%) PROPN(0.23%)</cell></row><row><cell></cell><cell>21</cell><cell cols="2">PRON(98.59%) NOUN(0.78%)</cell><cell>ADJ(0.39%)</cell></row><row><cell></cell><cell>22</cell><cell>ADP(93.45%)</cell><cell cols="2">ADV(2.40%) SCONJ(1.95%)</cell></row><row><cell></cell><cell>23</cell><cell cols="3">AUX(85.14%) VERB(14.51%) NOUN(0.36%)</cell></row><row><cell></cell><cell>24</cell><cell cols="3">AUX(80.60%) VERB(18.55%) NOUN(0.60%)</cell></row><row><cell></cell><cell>25</cell><cell cols="3">VERB(44.78%) AUX(40.84%) PROPN(13.49%)</cell></row><row><cell></cell><cell>26</cell><cell>ADP(73.68%)</cell><cell cols="2">ADV(9.68%) SCONJ(9.55%)</cell></row><row><cell></cell><cell>27</cell><cell cols="2">AUX(99.47%) NOUN(0.53%)</cell><cell>-</cell></row><row><cell></cell><cell>28</cell><cell cols="2">AUX(76.11%) VERB(10.26%)</cell><cell>PART(9.70%)</cell></row><row><cell></cell><cell>29</cell><cell>ADP(89.48%)</cell><cell cols="2">ADV(3.93%) NOUN(3.93%)</cell></row><row><cell></cell><cell>30</cell><cell>VERB(43.34%)</cell><cell>ADP(34.41%)</cell><cell>AUX(14.47%)</cell></row><row><cell></cell><cell>31</cell><cell cols="2">DET(53.56%) PRON(46.44%)</cell><cell>-</cell></row><row><cell></cell><cell>32</cell><cell cols="3">ADV(95.89%) SCONJ(3.72%) PROPN(0.23%)</cell></row><row><cell></cell><cell>33</cell><cell cols="2">ADP(52.12%) SCONJ(27.84%)</cell><cell>ADV(8.74%)</cell></row><row><cell>B Learned Latent Codes</cell><cell>34 35</cell><cell cols="2">DET(62.63%) PRON(20.73%) VERB(76.34%) AUX(23.66%)</cell><cell>ADV(9.90%) -</cell></row><row><cell></cell><cell>36</cell><cell>AUX(100.00%)</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>37</cell><cell cols="3">ADV(47.90%) PRON(40.51%) NOUN(11.59%)</cell></row><row><cell></cell><cell cols="2">38 NOUN(99.17%)</cell><cell>ADJ(0.73%)</cell><cell>ADV(0.10%)</cell></row><row><cell></cell><cell cols="3">39 NOUN(35.74%) VERB(28.94%)</cell><cell>ADJ(28.21%)</cell></row><row><cell></cell><cell cols="2">40 NOUN(49.84%)</cell><cell>ADJ(27.23%)</cell><cell>ADV(16.19%)</cell></row><row><cell></cell><cell>41</cell><cell cols="3">PRON(92.22%) NOUN(4.75%) PROPN(1.08%)</cell></row><row><cell></cell><cell>42</cell><cell>PRON(92.73%)</cell><cell>DET(7.05%)</cell><cell>ADV(0.22%)</cell></row><row><cell></cell><cell>43</cell><cell cols="2">ADP(32.17%) NOUN(23.05%)</cell><cell>ADJ(21.61%)</cell></row><row><cell></cell><cell cols="2">44 PUNCT(100.00%)</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">45 PRON(100.00%)</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>46</cell><cell cols="3">AUX(94.36%) VERB(4.59%) NOUN(1.05%)</cell></row><row><cell></cell><cell>47</cell><cell>ADV(67.33%)</cell><cell cols="2">ADJ(31.75%) NOUN(0.79%)</cell></row><row><cell></cell><cell>48</cell><cell cols="2">ADP(91.70%) SCONJ(8.16%)</cell><cell>ADJ(0.14%)</cell></row><row><cell></cell><cell cols="2">49 PUNCT(100.00%)</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>50</cell><cell cols="2">PART(70.91%) AUX(25.04%)</cell><cell>ADP(2.85%)</cell></row><row><cell></cell><cell cols="2">51 CCONJ(99.52%)</cell><cell>ADP(0.48%)</cell><cell>-</cell></row><row><cell></cell><cell>52</cell><cell cols="2">ADP(69.34%) SCONJ(15.89%)</cell><cell>ADV(14.13%)</cell></row><row><cell></cell><cell cols="2">53 PUNCT(100.00%)</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="3">54 NOUN(58.00%) VERB(30.58%)</cell><cell>ADJ(6.15%)</cell></row><row><cell></cell><cell>55</cell><cell cols="3">VERB(71.57%) AUX(28.04%) NOUN(0.39%)</cell></row><row><cell></cell><cell>56</cell><cell cols="3">NUM(75.73%) NOUN(20.33%) PRON(2.49%)</cell></row><row><cell></cell><cell>57</cell><cell>DET(86.03%)</cell><cell cols="2">ADJ(6.77%) PROPN(3.71%)</cell></row><row><cell></cell><cell>58</cell><cell cols="3">ADP(61.07%) ADV(31.77%) NOUN(5.37%)</cell></row><row><cell></cell><cell cols="3">59 CCONJ(90.75%) NOUN(8.48%)</cell><cell>ADJ(0.51%)</cell></row><row><cell></cell><cell>60</cell><cell cols="2">ADP(78.74%) SCONJ(19.93%)</cell><cell>ADV(1.00%)</cell></row><row><cell></cell><cell>61</cell><cell cols="2">VERB(59.11%) NOUN(31.56%)</cell><cell>ADJ(9.33%)</cell></row><row><cell>5 LDC2002E18, LDC2003E14, LDC004T08, and</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LDC2005T06</cell><cell></cell><cell></cell><cell></cell></row><row><cell>6 http://ictclas.nlpir.org/</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Our results are conducted on a single GeForce GTX 1080-TI GPU. Please note that the result inFigure 2aandFigure 2bmay be evaluated under different hardware settings, and it may not be fair to compare them directly.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Syntactically supervised transformers for faster neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nader</forename><surname>Akoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalpesh</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1122</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1269" to="1281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangtao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.10677</idno>
		<title level="m">Non-autoregressive transformer by position learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep biaffine attention for neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. Open-Review.net</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mask-predict: Parallel decoding of conditional masked language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1633</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6112" to="6121" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Nonautoregressive neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. Open-Review.net</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Non-autoregressive neural machine translation with enhanced decoder input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33013723</idno>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019-01-27" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="3723" to="3730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast decoding in sequence models using discrete latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsm?ssan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-07-10" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2395" to="2404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ond?ej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume the Demo and Poster Sessions<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning</title>
		<meeting>the Eighteenth International Conference on Machine Learning<address><addrLine>Williams College, Williamstown, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2001-06-28" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deterministic non-autoregressive neural sequence modeling by iterative refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1149</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1173" to="1182" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hint-based training for non-autoregressive translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeuralIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">FlowSeq: Nonautoregressive conditional sequence generation with generative flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1437</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4282" to="4292" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-4009</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)<address><addrLine>Minneapolis</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihua</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.07905</idno>
		<title level="m">Glancing transformer for non-autoregressive neural machine translation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Vmeasure: A conditional entropy-based external cluster evaluation measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hirschberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="410" to="420" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<title level="m">Towards a better understanding of vector quantized autoencoders. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Latent-variable nonautoregressive neural machine translation with deterministic inference using a delta posterior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Nakayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07181</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast structured decoding for sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; BC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Vancouver</publisher>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="3011" to="3020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Error bounds for convolutional codes and an asymptotically optimum decoding algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Viterbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="260" to="269" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Non-autoregressive machine translation with auxiliary regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imitation learning for nonautoregressive neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingzhen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1125</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1304" to="1312" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
