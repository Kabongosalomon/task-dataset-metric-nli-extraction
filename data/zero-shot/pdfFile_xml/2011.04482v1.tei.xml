<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DynaVSR: Dynamic Adaptive Blind Video Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suyoung</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of ECE</orgName>
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myungsub</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of ECE</orgName>
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Asri</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of ECE</orgName>
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung</forename><forename type="middle">Mu</forename><surname>Lee</surname></persName>
							<email>kyoungmu@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of ECE</orgName>
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DynaVSR: Dynamic Adaptive Blind Video Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most conventional supervised super-resolution (SR) algorithms assume that low-resolution (LR) data is obtained by downscaling high-resolution (HR) data with a fixed known kernel, but such an assumption often does not hold in real scenarios. Some recent blind SR algorithms have been proposed to estimate different downscaling kernels for each input LR image. However, they suffer from heavy computational overhead, making them infeasible for direct application to videos. In this work, we present DynaVSR, a novel meta-learning-based framework for real-world video SR that enables efficient downscaling model estimation and adaptation to the current input. Specifically, we train a multi-frame downscaling module with various types of synthetic blur kernels, which is seamlessly combined with a video SR network for input-aware adaptation. Experimental results show that DynaVSR consistently improves the performance of the state-of-the-art video SR models by a large margin, with an order of magnitude faster inference time compared to the existing blind SR approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Widespread usage of high-resolution (HR) displays in our everyday life has led to increasing popularity of superresolution (SR) technology, which allows for enhancing the resolution of visual contents from low-resolution (LR) inputs. Recent advances in deep-learning-based SR approaches for images <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b41">42]</ref> and videos <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36]</ref> are driving this trend, showing excellent performance on public SR benchmarks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b38">39]</ref>. However, majority of the models are trained under the assumption that the LR images are downscaled from the ground truth HR images with a fixed known kernel, such as MATLAB bicubic. It has been shown in Shocher et al. <ref type="bibr" target="#b29">[30]</ref> that SR performance of the existing models significantly deteriorates if test images do not match such training settings.</p><p>SR problem focusing on real-world scenarios with unknown downscaling kernels is called blind SR. Numer-* indicates equal contribution. ous methods have been proposed to accurately estimate image-specific kernels for each input <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b25">26]</ref>. These methods, however, require training the estimation network from scratch at inference time and typically runs in minutes <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref>, or sometimes up to an hour <ref type="bibr" target="#b25">[26]</ref> to handle a single image. Such heavy computational overhead makes the existing approaches impractical to run on a frame-by-frame basis for video SR, as even a short video clip typically contains over hundreds of frames.</p><p>Note that real-world LR video frames contain various different types of degradations, including spatial downsampling and motion blurs. To solve this problem by learning, we need to collect enough training data for all kinds of degradations, which is computationally infeasible. However, it can be greatly alleviated if we could effectively estimate the characteristics of the current input video, and build an adaptive model that can adjust its parameters at test time.</p><p>In this work, we propose an efficient framework for blind video SR named DynaVSR that can flexibly adapt to dynamic input videos. Our proposed framework is based on novel downscaling kernel estimation and input-aware adaptation by meta-learning. It first estimates an approximate downscaling process given input LR video sequences, and generates further downscaled version of the LR frames, which we call Super LR, or SLR in short. Then, using the constructed SLR-LR pairs, the parameters of the video SR (VSR) network as well as the downscaling network are jointly updated. The final output HR video is obtained by inference through the VSR model with the parameters adapted to the input LR video. The HR output predicted by DynaVSR for a real-world example sequence is shown in <ref type="figure">Figure 1</ref>, compared to the recent blind SR methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15]</ref>. We observe that DynaVSR greatly improves the output quality upon the VSR baselines that are trained with bicubic downsampled data, and shows more visually pleasing results than the existing approaches, even with significantly faster running time (see Sec. 5.3.1).</p><p>Overall, our contributions are summarized as follows:</p><p>? We propose DynaVSR, a novel adaptive framework for real-world VSR that combines the estimation of the Input LR EDVR Baseline IKC EDVR + DynaVSR CF + CARN KG+ZSSR <ref type="figure">Figure 1</ref>: Sample results of the proposed DynaVSR on real video with unknown degradation kernels. The state-of-the-art video SR model (EDVR <ref type="bibr" target="#b35">[36]</ref>) and recent blind SR models (KG (KernelGAN) <ref type="bibr" target="#b2">[3]</ref>, CF (CorrectionFilter) <ref type="bibr" target="#b14">[15]</ref>, and IKC <ref type="bibr" target="#b10">[11]</ref>) either show blurry outputs or generate unpleasing artifacts, while DynaVSR shows a much clearer result.</p><p>unknown downscaling process with test-time adaptation via meta-learning.</p><p>? We greatly reduce the computational complexity of estimating the real downscaling process, thereby enabling real-time execution of SR in videos.</p><p>? DynaVSR is generic and can be applied to any existing VSR model for consistently improved performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>While the field of super-resolution (SR) has a long history, in this section, we concentrate on more relevant deep-learning-based approaches and review recent adaptive methods applicable in real-world scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Single-Image SR (SISR)</head><p>Since Dong et al. <ref type="bibr" target="#b7">[8]</ref> (SRCNN) have shown that a deep learning approach can substantially outperform previous optimization based approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>, great advances have been made in SISR including VDSR <ref type="bibr" target="#b17">[18]</ref>, EDSR <ref type="bibr" target="#b22">[23]</ref>, ESRGAN <ref type="bibr" target="#b21">[22]</ref>, and others <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>. However, despite huge performance boost, many works are limited to only performing well on LR images downscaled by a fixed kernel such as bicubic, and otherwise produce undesirable artifacts.</p><p>To overcome this issue, several approaches train SR networks which are applicable to multiple types of degradations, assuming that we already know the degradation kernel (a.k.a. non-blind SR). SRMD <ref type="bibr" target="#b39">[40]</ref> used the LR image and its corresponding degradation kernel as the model inputs to generate high-quality HR images. ZSSR <ref type="bibr" target="#b29">[30]</ref> instead apply the same kernel used to generate the LR image to make a smaller LR image, then train an image-specific network. Park et al. <ref type="bibr" target="#b28">[29]</ref> and Soh et al. <ref type="bibr" target="#b30">[31]</ref> greatly reduce the time required for input-aware adaptation by incorporating meta-learning. All methods, however, cannot perform well unless we know the exact downscaling kernel, which is unavailable in real-world cases. To this end, numerous blind SR methods have been proposed.</p><p>Blind SR methods first estimate the unknown kernels in a self-supervised manner, and then apply the predicted ker-nels to the non-blind SR models. Existing kernel estimation approaches either exploit self-similarity <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b42">43]</ref> (with the hypothesis that similar patterns and structures across different scales appear in natural images) or design an iterative self-correction scheme <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref>. <ref type="bibr">Michaeli and Irani [26]</ref> first propose to estimate the downscaling kernel by exploiting the patch-recurrence property of a single image, which is further improved in KernelGAN <ref type="bibr" target="#b2">[3]</ref> by utilizing the Internal-GAN <ref type="bibr" target="#b29">[30]</ref>. IKC <ref type="bibr" target="#b10">[11]</ref> introduce an iterative correction scheme and successfully generates high-quality SR images. Hussein et al. <ref type="bibr" target="#b14">[15]</ref> also correct the downscaling kernel for many iterations, and use the final kernel in the same way as their non-blind settings.</p><p>Since most of the aforementioned methods require training the model from scratch to estimate an unknown kernel, they suffer from heavy computational overhead at test time. IKC does not need training at inference, but still requires many iterations for refining its initial output. On the other hand, our proposed framework directly integrates the inputaware kernel estimation process with video SR models and achieves better results with faster running time, enabling practical application of blind SR techniques to videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Video SR (VSR)</head><p>Video SR is different from SISR in that the input frames contain temporal information. Kappeler et al. <ref type="bibr" target="#b16">[17]</ref> first propose a convolutional neural network (CNN) based VSR method by allowing the network input to be a sequence of frames. Caballero et al. <ref type="bibr" target="#b4">[5]</ref> and Tao et al. <ref type="bibr" target="#b31">[32]</ref> incorporate optical flow estimation models to explicitly account for the motion between neighboring frames. TOFlow <ref type="bibr" target="#b36">[37]</ref> introduce task-oriented flow, a computationally lighter flow estimation module that is applicable to various video processing tasks. Since the flow-based methods are highly dependent on the motion estimation accuracy, DUF <ref type="bibr" target="#b15">[16]</ref> propose the dynamic upsampling filter network, avoiding explicit calculation of the motion information. EDVR <ref type="bibr" target="#b35">[36]</ref> also handles motion implicitly with a unified framework, including the Pyramid, Cascading and Deformable convolution (PCD) alignment and the Temporal and Spatial Attention (TSA) fusion processes. In this work, we use TOFlow, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminary on MAML</head><p>Before diving into our main framework, we briefly summarize model-agnostic meta-learning (MAML) <ref type="bibr" target="#b8">[9]</ref> algorithm that we use for test-time adaptation. The goal of meta-learning is to rapidly adapt to novel tasks with only few examples. For MAML, the adaptation process is modeled with a few gradient updates to the parameters.</p><p>Specifically, MAML first samples a set of examples D Ti from the current task T i ? p(T ), where p(T ) denotes a distribution of tasks. Then, adaptation to the current task is done by fine-tuning the model parameters ?:</p><formula xml:id="formula_0">? i = ? ? ?? ? L Ti (S ? (D Ti )),<label>(1)</label></formula><p>where L Ti is a loss function, and S ? can be any parameterized model. After adapting to each task T i , new examples D Ti are sampled from the same task to test the generalization capability and update the base parameters:</p><formula xml:id="formula_1">? ? ? ? ?? ? Ti L Ti (S ? i (D Ti )).<label>(2)</label></formula><p>Note that Eq. 1, inner loop update, is performed both at training and inference, while the outer loop update (Eq. 2, a.k.a. meta-update) is only executed during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Dynamic Adaptive Blind VSR Framework</head><p>In this section, we first summarize the overall framework and the problem formulation, and describe in detail how meta-learning is integrated for efficient adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Overall Framework</head><p>For blind VSR application, we define a task as superresolving each input video sequence. Since only the input LR frames are available at test time, we further downscale the input to form super-low-resolution (SLR) frames. Then, we can update the model by making it predict the LR frames well given the SLR frames. The resulting adapted parameters perform especially well on the current inputs, generating high-quality HR frames given the LR inputs.</p><p>In the blind SR setting, each LR input may have come through a different downscaling process, therefore test-time adaptation is crucial. For video application, real-time execution of estimating the downscaling process is also critical. Thus, we introduce an efficient Multi-Frame Downscaling Network (MFDN), and combine it with the VSR network. The proposed framework is named as DynaVSR, as it can adapt well to each of the dynamically-varying input videos. <ref type="figure" target="#fig_0">Figure 2</ref> illustrates the overall training process of Dy-naVSR, which consists of three stages: 1) estimation of the unknown downscaling process with MFDN, 2) joint adaptation of MFDN and VSR network parameters w.r.t. each input video, and 3) meta-updating the base parameters for MFDN and VSR network. At test time, only 1) and 2) are processed, and the updated parameters of the VSR network is used to generate the final super-resolved images. The detailed training and test processes are described in Sec. 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Blind VSR Problem Formulation</head><p>The goal of VSR is to accurately predict the HR frames ? HR t given the input LR frames I LR t , where t denotes the time step. In practice, many recent models such as <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref> estimate the center HR frame? HR t given the surrounding (2N + 1) LR frames I LR t?T , where T = {t ? N, ? ? ? , t + N }, and generate the HR sequence in a sliding window fashion. Thus, VSR problem for a single time step t can be formulated as:</p><formula xml:id="formula_2">I HR t = S ? I LR t?T ,<label>(3)</label></formula><p>In a fixed-kernel SR setting, a large number of training pairs is available since I LR t can be easily obtained by applying the designated downscaling kernel to I HR t . When tackling blind SR, however, the downscaling process is unknown and acquiring a large training set becomes impractical. As previously studied in KernelGAN <ref type="bibr" target="#b2">[3]</ref>, correct estimation of the downscaling process is crucial to the SR performance. This can be formalized as:</p><formula xml:id="formula_3">I LR t = D ? I HR t ,<label>(4)</label></formula><p>where D ? denotes a downscaling model parameterized by ?. Existing blind SR approaches typically find a good D ? by learning ? in a self-supervised manner. Then, using D ? that is optimized to the current inputs, the final SR results are obtained in the same way as a non-blind SR setting.</p><p>When it comes to blind video SR, efficiency becomes the key issue, since videos may contain several hundreds and thousands of frames. Existing blind image SR techniques require long processing time for finding the downscaling model D ? , and are therefore computationally infeasible (see Sec. 5.3.1 for runtime comparison). To this end, we design a new light-weight model, named Multi-Frame Downscaling Network (MFDN), for effective estimation of the downscaling process in real-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Multi-Frame Downscaling Network (MFDN)</head><p>Though video clips at different time steps can be affected by various different types of degradations (e.g. motion blurs, noises), in this work, we primarily focus on the downscaling process. Consequently, we assume that each LR frame I LR t is generated from the corresponding HR frame I HR t following the same but unknown downscaling process within a single video sequence.</p><p>To model D ? in Eq. 4, we propose MFDN that receives a multi-frame LR inputs and produces the corresponding further downscaled multi-frame outputs. This process is formulated as:</p><formula xml:id="formula_4">? SLR t?T = D ? I LR t?T ,<label>(5)</label></formula><p>where I LR t?T is an input LR sequence, and? SLR t?T denotes an estimated Super LR (SLR) sequence which is a further downscaled version. To model various kernels w.r.t. different inputs while maintaining efficiency, we model MFDN with a 7-layer CNN including non-linearities. For handling multi-frame information, 3-D convolutions are used for the first and the last layers of MFDN, and 2-D convolution layers for the rest. Contrary to the existing methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15]</ref>, MFDN does not require additional training at test time. This greatly improves the efficiency in estimating the unknown downscaling process and thereby enables application to computation-heavy problems like VSR.</p><p>To accurately predict the SLR frames for diverse cases with a single discriminative model, MFDN is first pretrained with various synthetic kernels and later employed to our meta-learning framework for further adaptation to each input. The training LR-SLR pairs are generated by random sampling of the anisotropic Gaussian kernels. Note that, the LR frames I LR t?T are themselves generated from the ground truth HR frames I HR t?T by applying randomly selected kernel in the synthetic kernel set. The corresponding SLR frames I SLR t?T are then generated from LR with the same kernel. Pre-training MFDN is done by minimizing the pixel-wise loss between I SLR t?T and the estimated output? SLR t?T . After pretraining MFDN, it is further fine-tuned during the meta-training process to be readily adaptable to each input. Though we only use synthetic kernels for training MFDN, it can generalize to the real inputs reasonably well, as we show in the experiments (see Sec. 5.4). Further analysis on the effects of MFDN is shown in Sec. 5.5.1, where we compare it to the other downscaling methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Meta-Learning for Blind VSR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Meta-training</head><p>For the inner loop update, we first generate? SLR t?T with MFDN using Eq. (5). The generated SLR sequence is then fed into the VSR network as the input:</p><formula xml:id="formula_5">I LR t = S ? ? SLR t?T = S ? D ? I LR t?T .<label>(6)</label></formula><p>We introduce two loss terms to update ? and ?: LR fidelity loss (L in LR ) and SLR guidance loss (L in SLR ). The LR fidelity loss (L in LR ) indicates the difference between? LR t and I LR t , and we match the type of loss function used for each backbone VSR network (denoted as L V SR ). However, L in LR alone cannot guarantee that the updated MFDN would produce the correct SLR frames. Inaccurate downscaling estimation can generate erroneous SLR frames, and can also give wrong update signals to the VSR network. To cope with this issue, SLR guidance loss (L in SLR ) is proposed to make sure that MFDN outputs do not move far away from the actual SLR frames. In practice, L in SLR is calculated as the 1 distance between generated SLR frames? SLR t?T and the ground truth I SLR t?T . The total loss for the inner loop update is computed as a sum of the two terms:</p><formula xml:id="formula_6">L in = L in LR + L in SLR (7) = L V SR ? LR t , I LR t + ? SLR t?T ? I SLR t?T 1 .<label>(8)</label></formula><p>This process corresponds to the left part of <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>For the outer loop, the base parameter values of ? and ? (before inner loop updates) are adjusted to make the models more adaptive to new inputs. Given the input LR sequence I LR t?T , VSR network and MFDN generate the HR and SLR predictions, correspondingly, as follows: Compute adapted parameters ? and ? with:</p><formula xml:id="formula_7">I HR t = S ? I LR t?T ,? SLR t?T = D ? I LR t?T .<label>(9)</label></formula><formula xml:id="formula_8">9 ? = ? ? ?? ? L in , ? = ? ? ?? ? L in 10 Save I HR t i , I SLR t i for meta-update 11 end 12 Update ? ? ? ? ?? ? Ti L out HR using Eq. (10) 13 Update ? ? ? ? ?? ? Ti L out SLR using Eq. (11) 14 end</formula><p>From the two predictions, we can define the two loss terms:</p><formula xml:id="formula_9">L out HR = L V SR ? HR t , I HR t ,<label>(10)</label></formula><formula xml:id="formula_10">L out SLR = ? SLR t?T ? I SLR t?T 1 ,<label>(11)</label></formula><p>where each loss is used to update the parameters in corresponding networks. Note that the loss is calculated with updated parameters, D ? and S ? , but the gradient is calculated w.r.t. ? and ?, respectively. The right part of <ref type="figure" target="#fig_0">Figure 2</ref> depicts the outer update mechanism. Algorithm 1 summarizes the full procedure for training DynaVSR. Compared to the existing blind SISR approaches, the proposed algorithm has multiple advantages: 1) DynaVSR does not require a necessary number of iterations as a hyperparameter, achieving maximum performance with only a single gradient update, leading to improved computational efficiency compared to IKC <ref type="bibr" target="#b10">[11]</ref> or CorrectionFilter <ref type="bibr" target="#b14">[15]</ref>; 2) DynaVSR is generic and can be applied to any existing VSR models, while the other methods need specific SR network architectures; 3) DynaVSR can handle multiple frames as inputs for video application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Meta-test</head><p>At test time, only the inner loop update is performed to adapt the MFDN and VSR network parameters to the test input frame sequence. Since there are no ground truth (GT) SLR frames, we replace it with the SLR frames predicted by our pretrained MFDN. Although we do not use the real GT SLR frames, we show in experiments that the pseudo GT frames generated by MFDN are still valid (see Sec. 5.3).</p><p>The final output HR frame? HR t can then be generated using the updated VSR network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Dataset</head><p>We use three popular VSR datasets for our experiments: REDS <ref type="bibr" target="#b26">[27]</ref>, Vimeo-90K <ref type="bibr" target="#b36">[37]</ref>, and Vid4 <ref type="bibr" target="#b23">[24]</ref>. Also, many low-resolution videos are gathered from YouTube to demonstrate the performance of DynaVSR on real-world scenarios. REDS dataset is composed of 270 videos each consisting of 100 frames, and each frame has 1280 ? 720 spatial resolution. Out of 270 train-validation videos, we use 266 sequences for training and the other 4 sequences (REDS-val) for testing, following the experimental settings in Wang et al. <ref type="bibr" target="#b35">[36]</ref> (EDVR). Videos from REDS dataset typically contain large and irregular motion, which makes it challenging for VSR. Vimeo-90K dataset contains 91,707 short video clips, each containing 7 frames. We use Vimeo-90K only for training, using the training split of 64,612 clips. Although the resolution of each frame is low (448 ? 256), Vimeo-90K is one of the most frequently used dataset for training VSR models. Vid4 dataset is widely used for evaluation purposes only; many previous works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref> train their models with Vimeo-90K and report their performance on Vid4 dataset, and we follow the same setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>DynaVSR can be applied to any deep-learning-based VSR model, and we show its effectiveness using EDVR <ref type="bibr" target="#b35">[36]</ref>, DUF <ref type="bibr" target="#b15">[16]</ref>, and TOFlow <ref type="bibr" target="#b36">[37]</ref> as backbone VSR networks. All models are initialized with pretrained parameters for scale factor s = 2, with a known downscaling process, MATLAB bicubic downsampling with antialiasing. Separate models are trained for each training dataset, Vimeo-90K and REDS. We denote these pretrained models as (bicubic) Baseline, and report their performance to show how existing approaches using this ideal downscaling kernel fail in synthetic and real-world settings.</p><p>When pretraining MFDN and meta-training DynaVSR, diverse kinds of downscaling kernels are used to generate the HR-LR-SLR patches for each iteration. Specifically, we select ? 1 , ? 2 ? U [0.2, 2.0], and ? ? U [??, ?] independently for randomly generating many anisotropic Gaussian kernels. More details are described in supplemantary slides. The source code along with our pretrained models is made public to facilitate reproduction and further research. <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Quantitative Results</head><p>We thoroughly evaluate the performance improvements of DynaVSR w.r.t. the bicubic baselines in three different kinds of synthetic blur kernels: isotropic Gaussian, <ref type="table" target="#tab_0">Table 1</ref>: Quantitative results, running time comparison for meta-training with recent VSR models and blind SISR methods. We evaluate the benefits of DynaVSR algorithm on Vid4 <ref type="bibr" target="#b23">[24]</ref> and REDS-val <ref type="bibr" target="#b26">[27]</ref> dataset. Performance is measured in PSNR (dB). Red denotes the best performance, and blue denotes the second best. The right part indicates the running time to make a single HD frame. DynaVSR shows the shortest time among the existing algorithms on all three VSR baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Vid4 <ref type="bibr" target="#b23">[24]</ref> REDS-val <ref type="bibr">[</ref> anisotropic Gaussian, and mixed Gaussian. For all experiments in this section, we report the standard peak signalto-noise ratio (PSNR).</p><p>For isotropic Gaussians, we adapt the Gaussian8 setting from Gu et al. <ref type="bibr" target="#b10">[11]</ref>, which consists of eight isotropic Gaussian kernels of ? ? [0.8, 1.6] for scale factor 2, originally proposed for evaluating blind image SR methods. The HR image is first blurred by the Gaussian kernel and then downsampled by bicubic interpolation. Since it is unclear from Gu et al. <ref type="bibr" target="#b10">[11]</ref> how to handle the boundary ?-s, we evaluate on nine different kernel widths including both ? = 0.8 and ? = 1.6 with step size 0.1. However, isotropic Gaussian kernels are insufficient to represent various types of degradations in the real world. Thus, we also evaluate DynaVSR on anistropic Gaussian settings, where we fix the Gaussian kernel widths to the boundary values of Gaussian8 so that (? x , ? y ) = (0.8, 1.6). Evaluation is done on 4 cases with different rotations (0 ? , 45 ? , 90 ? , and 135 ? ), and the average performance is reported. Lastly, we introduce a mixed setting which consists of randomly generated kernels. Each sequence is individually downscaled by random Gaussian kernels with ? 1 , ? 2 ? U [0.2, 2.0] and ? ? U [??, ?] with direct downsampling. Note that the sampled downscaling kernel is kept same for the entire sequence. <ref type="table" target="#tab_0">Table 1</ref> compares the results of DynaVSR with its baselines and other blind SR methods. Compared to the bicubic baseline, DynaVSR consistently improves the performance over all evaluation settings by a large margin (over 2dB on average). This proves the effectiveness of adaptation via meta-training, since we use the same architecture without introducing any additional parameters. Compared to blind SISR models, DynaVSR with any baseline VSR net-work performs favorably against existing methods in general. IKC performs well in isotropic Gaussian settings, but DynaVSR with EDVR ranks the second while greatly outperforming IKC for the other evaluation settings. Note that, while IKC is specifically optimized with isotropic Gaussian kernels only, the reported performance for DynaVSR is from our final model trained also with various other kernels including anisotropic and rotated Gaussians.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Time Complexity Analysis</head><p>The right part of <ref type="table" target="#tab_0">Table 1</ref> demonstrates the running time for generating a single HD resolution (1280 ? 720) frame from a ?2 downscaled LR frame sequence w.r.t each blind SR method. Preprocessing indicates the steps required to prepare the input LR frames for putting through the SR network, which may include kernel estimation (KG <ref type="bibr" target="#b2">[3]</ref>) or iterative correction of the inputs to modify their characteristics (CF <ref type="bibr" target="#b14">[15]</ref>). For IKC <ref type="bibr" target="#b10">[11]</ref>, it is difficult to explicitly separate each step, so we include the runtime for iterative correction to Super-Resolution category, which reports the inference time for each SR network.</p><p>Since MFDN is highly efficient, DynaVSR shows much faster preprocessing time compared to existing blind SISR models. Recent approaches, KG <ref type="bibr" target="#b2">[3]</ref> and CF <ref type="bibr" target="#b14">[15]</ref>, require minutes of preprocessing time because both models need to train from scratch at test time, which is very expensive even with a small network. On the other hand, DynaVSR * Since CF require too much time for preprocessing, we report the performance for random 10% of the validation set. For fair comparison, we show the results of the other models on the same 10% validation set in the supplementary document. We can observe that the overall trend in performance is almost same as the full evaluation.  requires only a single gradient update to the model parameters and successfully reduces the preprocessing time to less than a second (more than ?60 faster than KG, and ?100 faster than CF). Note that, the preprocessing time for Dy-naVSR is highly dependent on the architecture of the base VSR network, and the efficiency can be further improved by using more light-weight VSR models. IKC reports the shortest runtime among the other previous methods, but it still needs multiple iterations for kernel correction, and EDVR+DynaVSR shows more than 40% shorter runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EDVR Baseline</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IKC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Visual Comparison</head><p>The qualitative results for REDS-val dataset using random synthetic downscaling kernels are shown in <ref type="figure" target="#fig_1">Figure 3</ref>. DynaVSR greatly improves the visual quality over all baselines by well adapting to the input frames. Notably, blurry edges from the bicubic baselines are sharpened, and texture details become much clearer. Outputs of DynaVSR also show visually more pleasing results compared to three recent blind SISR models, which are shown in the left part of <ref type="figure" target="#fig_1">Figure 3</ref>. Results for real-world low-quality videos from YouTube, where ground truth frames are unavailable, are illustrated in <ref type="figure">Figure 1, 4</ref>. Although these videos contain various types of unknown degradations, DynaVSR is robust in producing visually pleasing outputs. For results on Vid4 dataset, additional results on REDS-val, and more extensive qualitative analysis on many real-world videos, please check our supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">Varying Downscaling Models</head><p>To analyze the effects of end-to-end training of VSR model with a downscaling network, we substitute the MFDN part  <ref type="table" target="#tab_2">Table 2</ref>. For generating the SLR frames, we first pretrain MFDN and SFDN until convergence. SFDN is a single-frame variant of MFDN with the same number of parameters, which regards the temporal dimension as the same as batch dimension, using only 2-D convolutions. As shown in the leftmost column of <ref type="table" target="#tab_2">Table 2</ref>, the MFDN achieves the best performance as a stand-alone downscaler. We believe that it is due to the multi-frame nature of MFDN, since multiple input frames with similar kernels can be observed to make it easier to recognize the actual downscaling patterns.</p><p>The final SR performance after jointly training with the VSR model is also more favorable to MFDN, achieving the closest performance to the ideal case of adapting VSR model with GT SLR frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Varying the Number of Inner Loop Updates</head><p>We also modify the number of inner loop updates and compare the results. <ref type="table" target="#tab_4">Table 3</ref> shows the performance while changing the number of iterations within 1, 3, and 5 steps. The best performance is achieved when we set the number of updates to 1, and more inner loop iterations led to diminishing results on average. We believe this phenomenon is because of overfitting to the input video sequence and forgetting the general super-resolution capability. Although the adaptation to each specific input is a crucial step in blind SR problems, it is only beneficial when the model maintains its generalization performance. Our results show that adaptation with too many inner loop updates drive the model to fall into local optima. How to regularize the inner loop optimization well to circumvent this issue is beyond the scope of this paper and can be an interesting future direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we propose DynaVSR, a novel adaptive blind video SR framework which seamlessly combines the downscaling kernel estimation model into meta-learningbased test-time adaptation scheme in an end-to-end manner. Compared to existing kernel estimation models for blind SISR, our MFDN extremely improves the computational efficiency and better estimates the downscaling process. Also, the excessive computation needed for input-aware adaptation of network parameters is minimized to a single gradient update by incorporating meta-learning. We demonstrate that DynaVSR gives substantial performance gain regardless of the VSR network architecture in various experimental settings including isotropic and anisotropic Gaussian blur kernels. Furthermore, we empirically show that DynaVSR can be readily applied to real-world videos with unknown downscaling kernels even though it is only trained with synthetic kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>The number of input frames and the loss function in DynaVSR follow the same form as the original VSR method: 5 frames, Charbonnier loss <ref type="bibr" target="#b20">[21]</ref> for EDVR, 7 frames with Huber loss for DUF and 7 frames, 1 loss for TOFlow. Since MFDN is fully convolutional, the pre-trained MFDN network can be used regardless of the number of input frames. We use Adam <ref type="bibr" target="#b19">[20]</ref> optimizer for both the inner and outer loop updates, with the corresponding learning rates of ? = ? = 10 ?5 . Training requires 30,000 iterations with a mini-batch size of 4, and ? is decayed by a factor of 5 at iterations 20,000 and 25,000, while ? is kept fixed. We use full images for Vimeo-90K training, but crop the LR patches of size 128?128 for REDS dataset due to GPU memory limits. Starting from a pretrained VSR baseline, the full meta-training of DynaVSR framework takes approximately 15 hours with a single NVIDIA Geforce RTX 2080 Ti GPU, which includes the 6-hour pretraining step for initializing MFDN parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Quantitative Results</head><p>We provide the SSIM values of <ref type="table" target="#tab_4">Table 3</ref> of the main paper in <ref type="table" target="#tab_5">Table A</ref>. Similar to the results for PSNR, EDVR+DynaVSR performs the second best in isotropic Gaussians, and reaches the best in anisotropic and mixed Gaussians. For all tables in this document, red denotes the best performance, and blue denotes the second best. <ref type="table" target="#tab_5">Table B</ref>, C, D, and E provide the detailed quantitative results for all kernel settings that we used to report the average values. Specifically, <ref type="table" target="#tab_5">Table B</ref> and C show the performance for all 9 ?-s for the isotropic Gaussian kernels. Likewise, <ref type="table" target="#tab_5">Table D</ref> and E show the performance of all 4 different angles ? for the anisotropic Gaussian kernels. <ref type="table" target="#tab_5">Table F</ref> and G are the results of the 10% of the validation set, which was used to evaluate Correlation-Filter-based models due to its high computational complexity. These data provides a fair comparison between the two models using Correction Filter (CF) algorithm, and the other methods in <ref type="table" target="#tab_0">Table 1</ref> of the main paper. We can observe that the quantitative values are almost same as full evaluation results except for the slight mismatch in the mixed setting, which was originally composed by random sampling of the downscaling kernels that are different for each video sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Effect of Inner Loop Learning Rate (?)</head><p>The inner loop learning rate ? is one of the most import hyperparameters that control the stability of training DynaVSR framework. If ? is too big, the model parameters can jump to a bad local optimum during the inner loop update, resulting in diminishing performance. On the other hand, if ? is too small, the possible performance gain with DynaVSR algorithm may not be fully exploited. We vary ? to 10 ?4 , 10 ?5 , and 10 ?6 , and report the effects in <ref type="table" target="#tab_5">Table H</ref>. We found that ? = 10 ?5 gives the best overall results and used this value for all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Larger Scale Factor (?4 SR)</head><p>Finally, <ref type="table" target="#tab_5">Table I</ref> shows the ?4 SR results using EDVR. For larger scaling factor, the size of SLR image will get smaller, and the adaptation using SLR-LR pair becomes more difficult. However, DynaVSR still makes substantial performance improvements in both dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Qualitative Results</head><p>For extensive visual comparison both for synthetic and real video examples, please refer to our supplementary slides. We also demonstrate the effectiveness of the proposed DynaVSR framework with the attached video demo. Note that, due to the long running time required for KernelGAN <ref type="bibr" target="#b2">[3]</ref> and Correction Filter <ref type="bibr" target="#b14">[15]</ref> algorithms, making a full video is too inefficient. Therefore, DynaVSR is mainly compared with IKC <ref type="bibr" target="#b10">[11]</ref> in our video demo. The video demo can be downloaded by the following URL: https://bit.ly/3pisuGj.  <ref type="bibr" target="#b23">[24]</ref> and REDS-val <ref type="bibr" target="#b26">[27]</ref> dataset. Red denotes the best performance, and blue denotes the second best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Vid4 <ref type="bibr" target="#b23">[24]</ref> REDS-val <ref type="bibr" target="#b26">[27]</ref> Iso          </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Overall training procedure for the proposed DynaVSR framework. (a) Both MFDN and VSR network are jointly updated in the inner loop. (b) The base parameters, ? and ?, are separately updated in the outer loop. DUF, and EDVR as the baseline VSR models and show how DynaVSR can consistently improve the performance of those models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative results on REDS-val<ref type="bibr" target="#b26">[27]</ref> dataset. DynaVSR consistently improves the visual details upon all baseline VSR models, and also produces visually more pleasing outputs compared to recent blind SR approaches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>SSIM Method ? = 0 ? ? = 45 ? ? = 90 ? ? = 135 ? Average DynaVSR 0.9356 0.9344 0.9400 0.9351 0.9363 DUF Baseline 0.8549 0.8607 0.8666 0.8669 0.8623 DUF + DynaVSR 0.8947 0.9034 0.9026 0.9083 0.9023 TOF Baseline 0.8582 0.8622 0.8702 0.8685 0.8648 TOF + DynaVSR 0.9045 0.9070 0.9127 0.9105 0.9087</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1 :</head><label>1</label><figDesc>DynaVSR training Require: p(T ): uniform distribution over videos Require: ?, ?: inner / outer-loop learning rates 1 Initialize parameters ? and ? 2 while not converged do Sample a batch of sequences T i ? p(T ) Calculate ? ?,? L in using Eq. (8)</figDesc><table><row><cell>4</cell><cell>foreach i do</cell><cell></cell><cell></cell></row><row><cell>5</cell><cell cols="3">Generate I HR t T i using random synthetic kernels i , I LR t i , I SLR t</cell><cell>i from</cell></row><row><cell>6</cell><cell>Generate ? SLR t</cell><cell>i</cell><cell>using Eq. (5)</cell></row><row><cell>8</cell><cell></cell><cell></cell><cell></cell></row></table><note>37</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Quantitative comparison w.r.t. different downscaling models. We evaluate on REDS-val using EDVR baseline. The right 3 columns indicate the SR performance for each downscaling model in our framework, where the joint training with MFDN shows the best results.</figDesc><table><row><cell cols="5">Downscaling models SLR Isotropic Anisotropic Mixed</cell></row><row><cell>Bicubic</cell><cell cols="2">34.78 31.82</cell><cell>33.26</cell><cell>33.39</cell></row><row><cell>KernelGAN [3]</cell><cell cols="2">40.84 31.99</cell><cell>33.31</cell><cell>33.48</cell></row><row><cell>SFDN</cell><cell cols="2">45.36 32.34</cell><cell>33.41</cell><cell>33.63</cell></row><row><cell>MFDN</cell><cell cols="2">45.71 32.45</cell><cell>33.41</cell><cell>33.67</cell></row><row><cell>GT</cell><cell>?</cell><cell>32.87</cell><cell>33.84</cell><cell>34.27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Qualitative results on real-world videos from YouTube where no ground truth HR frames exist. We use EDVR as the base VSR network. DynaVSR generates cleaner results while the other blind SISR methods show severe artifacts or blurry outputs. Note that, in the second row, all blind SR methods except DynaVSR generate additional boundaries near the corner due to strong ringing artifacts, which does not exist in the original LR frames.</figDesc><table><row><cell>Input Image</cell><cell>EDVR Baseline</cell><cell>KG + ZSSR</cell><cell>CF + CARN</cell><cell>IKC</cell><cell>EDVR + DynaVSR</cell></row><row><cell>Figure 4:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>of DynaVSR with four different downscaling models: bicu- bic downscaling, KernelGAN [3], single-frame variant of MFDN (SFDN, Single Frame Downscaling Network), and the ground truth SLR frame downscaled from the LR frame with the correct kernel. We also evaluate the performance of SLR frame estimation, and report the average PSNR values of VSR network for same settings. The results are summa- rized in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Effect of the number of inner loop iterations.We evaluate on REDS-val using EDVR. Just 1 inner loop update yields the best performance in general.</figDesc><table><row><cell cols="4"># Isotropic Anisotropic Mixed</cell></row><row><cell>1</cell><cell>32.45</cell><cell>33.41</cell><cell>33.67</cell></row><row><cell>3</cell><cell>32.75</cell><cell>32.38</cell><cell>32.96</cell></row><row><cell>5</cell><cell>32.17</cell><cell>32.23</cell><cell>32.57</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table A :</head><label>A</label><figDesc>SSIM results for meta-training with recent VSR models and blind SISR methods. We evaluate the benefits of DynaVSR algorithm on Vid4</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Since CF takes a long time, we report the performance for random 10% of the validation set.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>. Aniso. Mixed</cell><cell>Iso. Aniso. Mixed</cell></row><row><cell>Blind SISR</cell><cell cols="2">KG [3] + ZSSR [30] CF [15] + DBPN [12] * CF [15] + CARN [2] * IKC [11]</cell><cell>0.8395 0.8075 0.7495 0.8673 0.8366 0.7721 0.8675 0.8353 0.7602 0.8741 0.8696 0.8202 0.8826 0.8731 0.8388 0.8855 0.8973 0.8788 0.9125 0.8308 0.8650 0.9361 0.8821 0.9030</cell></row><row><cell></cell><cell>EDVR [36]</cell><cell cols="2">Baseline DynaVSR 0.9031 0.8946 0.9042 0.9286 0.9363 0.9388 0.7846 0.8227 0.8387 0.8470 0.8729 0.8826</cell></row><row><cell>Video SR</cell><cell>DUF [16] TOFlow [37]</cell><cell cols="2">Baseline DynaVSR 0.8600 0.8756 0.8812 0.8935 0.9023 0.9041 0.7815 0.8154 0.8376 0.8426 0.8623 0.8759 Baseline 0.7815 0.8132 0.8366 0.8431 0.8648 0.8808 DynaVSR 0.8509 0.8601 0.8735 0.9031 0.9087 0.9126</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table B :</head><label>B</label><figDesc>Quantitative results of DynaVSR combined with recent VSR algorithms in isotropic Gaussian kernels on Vid4. We evaluate the benefits of DynaVSR algorithm on Vid4<ref type="bibr" target="#b23">[24]</ref> dataset.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">PSNR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="10">? = 0.8 ? = 0.9 ? = 1.0 ? = 1.1 ? = 1.2 ? = 1.3 ? = 1.4 ? = 1.5 ? = 1.6 Average</cell></row><row><cell>KG + ZSSR</cell><cell>23.70</cell><cell>24.39</cell><cell>25.56</cell><cell>26.21</cell><cell>26.67</cell><cell>27.00</cell><cell>27.03</cell><cell>26.65</cell><cell>26.06</cell><cell>25.92</cell></row><row><cell>CF + DBPN</cell><cell>25.62</cell><cell>26.92</cell><cell>27.94</cell><cell>27.81</cell><cell>28.00</cell><cell>27.86</cell><cell>27.65</cell><cell>27.20</cell><cell>26.68</cell><cell>27.30</cell></row><row><cell>CF + CARN</cell><cell>27.47</cell><cell>28.09</cell><cell>28.30</cell><cell>28.63</cell><cell>28.52</cell><cell>28.28</cell><cell>27.93</cell><cell>27.37</cell><cell>26.95</cell><cell>27.95</cell></row><row><cell>IKC</cell><cell>30.21</cell><cell>30.17</cell><cell>30.06</cell><cell>29.88</cell><cell>29.59</cell><cell>29.26</cell><cell>28.97</cell><cell>28.63</cell><cell>28.37</cell><cell>29.46</cell></row><row><cell>EDVR Baseline</cell><cell>28.04</cell><cell>27.16</cell><cell>26.38</cell><cell>25.69</cell><cell>25.09</cell><cell>24.57</cell><cell>24.11</cell><cell>23.71</cell><cell>23.35</cell><cell>25.35</cell></row><row><cell cols="2">EDVR + DynaVSR 29.37</cell><cell>29.15</cell><cell>28.87</cell><cell>28.66</cell><cell>28.59</cell><cell>28.59</cell><cell>28.56</cell><cell>28.46</cell><cell>28.20</cell><cell>28.72</cell></row><row><cell>DUF Baseline</cell><cell>27.82</cell><cell>26.99</cell><cell>26.25</cell><cell>25.60</cell><cell>25.02</cell><cell>24.52</cell><cell>24.08</cell><cell>23.68</cell><cell>23.33</cell><cell>25.26</cell></row><row><cell>DUF + DynaVSR</cell><cell>29.48</cell><cell>29.16</cell><cell>28.69</cell><cell>28.12</cell><cell>27.50</cell><cell>26.87</cell><cell>26.25</cell><cell>25.66</cell><cell>25.12</cell><cell>27.43</cell></row><row><cell>TOF Baseline</cell><cell>27.79</cell><cell>27.01</cell><cell>26.29</cell><cell>25.64</cell><cell>25.06</cell><cell>24.55</cell><cell>24.09</cell><cell>23.70</cell><cell>23.34</cell><cell>25.27</cell></row><row><cell>TOF + DynaVSR</cell><cell>28.90</cell><cell>28.59</cell><cell>28.20</cell><cell>27.75</cell><cell>27.26</cell><cell>26.73</cell><cell>26.20</cell><cell>25.65</cell><cell>25.12</cell><cell>27.16</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>SSIM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="10">? = 0.8 ? = 0.9 ? = 1.0 ? = 1.1 ? = 1.2 ? = 1.3 ? = 1.4 ? = 1.5 ? = 1.6 Average</cell></row><row><cell>KG + ZSSR</cell><cell cols="10">0.8083 0.8204 0.8454 0.8529 0.8581 0.8576 0.8528 0.8411 0.8189 0.8395</cell></row><row><cell>CF + DBPN</cell><cell cols="10">0.8195 0.8669 0.8906 0.8901 0.8900 0.8839 0.8724 0.8561 0.8394 0.8675</cell></row><row><cell>CF + CARN</cell><cell cols="10">0.8932 0.9002 0.9000 0.9005 0.8954 0.8864 0.8750 0.8552 0.8375 0.8826</cell></row><row><cell>IKC</cell><cell cols="10">0.9206 0.9203 0.9196 0.9181 0.9153 0.9116 0.9077 0.9024 0.8965 0.9125</cell></row><row><cell>EDVR Baseline</cell><cell cols="10">0.8839 0.8607 0.8357 0.8096 0.7834 0.7578 0.7332 0.7096 0.6873 0.7846</cell></row><row><cell cols="11">EDVR + DynaVSR 0.9169 0.9132 0.9089 0.9057 0.9039 0.9023 0.8993 0.8937 0.8838 0.9031</cell></row><row><cell>DUF Baseline</cell><cell cols="10">0.8772 0.8551 0.8312 0.8062 0.7809 0.7560 0.7317 0.7085 0.6865 0.7815</cell></row><row><cell>DUF + DynaVSR</cell><cell cols="10">0.9192 0.9114 0.9004 0.8860 0.8686 0.8485 0.8262 0.8024 0.7776 0.8600</cell></row><row><cell>TOF Baseline</cell><cell cols="10">0.8757 0.8546 0.8312 0.8065 0.7814 0.7564 0.7321 0.7088 0.6868 0.7815</cell></row><row><cell>TOF + DynaVSR</cell><cell cols="10">0.9053 0.8969 0.8864 0.8736 0.8585 0.8411 0.8213 0.7994 0.7760 0.8509</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table C :</head><label>C</label><figDesc>Quantitative results of DynaVSR combined with recent VSR algorithms in isotropic Gaussian kernels on REDS-val. We evaluate the benefits of DynaVSR algorithm on REDS-val<ref type="bibr" target="#b26">[27]</ref> dataset.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">PSNR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="10">? = 0.8 ? = 0.9 ? = 1.0 ? = 1.1 ? = 1.2 ? = 1.3 ? = 1.4 ? = 1.5 ? = 1.6 Average</cell></row><row><cell>KG + ZSSR</cell><cell>25.62</cell><cell>26.81</cell><cell>28.12</cell><cell>29.31</cell><cell>30.29</cell><cell>30.64</cell><cell>30.58</cell><cell>30.30</cell><cell>29.80</cell><cell>29.05</cell></row><row><cell>CF + DBPN</cell><cell>28.97</cell><cell>29.74</cell><cell>30.05</cell><cell>31.01</cell><cell>31.18</cell><cell>31.19</cell><cell>30.85</cell><cell>30.48</cell><cell>29.85</cell><cell>30.37</cell></row><row><cell>CF + CARN</cell><cell>31.17</cell><cell>31.32</cell><cell>31.53</cell><cell>31.55</cell><cell>31.44</cell><cell>31.00</cell><cell>30.81</cell><cell>30.29</cell><cell>29.72</cell><cell>30.98</cell></row><row><cell>IKC</cell><cell>34.57</cell><cell>34.54</cell><cell>34.50</cell><cell>34.38</cell><cell>34.19</cell><cell>34.02</cell><cell>33.81</cell><cell>33.58</cell><cell>33.29</cell><cell>34.10</cell></row><row><cell>EDVR Baseline</cell><cell>31.83</cell><cell>30.94</cell><cell>30.16</cell><cell>29.49</cell><cell>28.90</cell><cell>28.38</cell><cell>27.93</cell><cell>27.51</cell><cell>27.15</cell><cell>29.14</cell></row><row><cell cols="2">EDVR + DynaVSR 33.63</cell><cell>32.32</cell><cell>32.90</cell><cell>32.48</cell><cell>32.16</cell><cell>31.97</cell><cell>31.89</cell><cell>31.87</cell><cell>31.82</cell><cell>32.45</cell></row><row><cell>DUF Baseline</cell><cell>31.44</cell><cell>30.69</cell><cell>30.00</cell><cell>29.37</cell><cell>28.82</cell><cell>28.32</cell><cell>27.88</cell><cell>27.46</cell><cell>27.13</cell><cell>29.01</cell></row><row><cell>DUF + DynaVSR</cell><cell>33.03</cell><cell>32.79</cell><cell>32.40</cell><cell>31.92</cell><cell>31.36</cell><cell>30.77</cell><cell>30.17</cell><cell>29.59</cell><cell>29.04</cell><cell>31.23</cell></row><row><cell>TOF Baseline</cell><cell>31.58</cell><cell>30.79</cell><cell>30.06</cell><cell>29.40</cell><cell>28.82</cell><cell>28.31</cell><cell>27.86</cell><cell>27.46</cell><cell>27.10</cell><cell>29.04</cell></row><row><cell>TOF + DynaVSR</cell><cell>32.61</cell><cell>32.52</cell><cell>32.35</cell><cell>32.09</cell><cell>31.73</cell><cell>31.31</cell><cell>30.82</cell><cell>30.30</cell><cell>29.76</cell><cell>31.50</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>SSIM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="10">? = 0.8 ? = 0.9 ? = 1.0 ? = 1.1 ? = 1.2 ? = 1.3 ? = 1.4 ? = 1.5 ? = 1.6 Average</cell></row><row><cell>KG + ZSSR</cell><cell cols="10">0.8137 0.8398 0.8623 0.8794 0.8903 0.8922 0.8863 0.8776 0.8643 0.8673</cell></row><row><cell>CF + DBPN</cell><cell cols="10">0.8535 0.8723 0.8718 0.8960 0.9020 0.8946 0.8787 0.8611 0.8366 0.8741</cell></row><row><cell>CF + CARN</cell><cell cols="10">0.9172 0.9157 0.9140 0.9090 0.8985 0.8804 0.8678 0.8454 0.8212 0.8855</cell></row><row><cell>IKC</cell><cell cols="10">0.9446 0.9438 0.9429 0.9409 0.9379 0.9350 0.9314 0.9271 0.9217 0.9361</cell></row><row><cell>EDVR Baseline</cell><cell cols="10">0.9155 0.8989 0.8814 0.8635 0.8458 0.8285 0.8120 0.7962 0.7815 0.8470</cell></row><row><cell cols="11">EDVR + DynaVSR 0.9446 0.9411 0.9366 0.9319 0.9278 0.9244 0.9213 0.9176 0.9120 0.9286</cell></row><row><cell>DUF Baseline</cell><cell cols="10">0.9051 0.8906 0.8750 0.8587 0.8422 0.8259 0.8101 0.7950 0.7806 0.8426</cell></row><row><cell>DUF + DynaVSR</cell><cell cols="10">0.9327 0.9274 0.9201 0.9107 0.8992 0.8859 0.8711 0.8553 0.8389 0.8935</cell></row><row><cell>TOF Baseline</cell><cell cols="10">0.9079 0.8926 0.8763 0.8593 0.8424 0.8258 0.8097 0.7944 0.7799 0.8431</cell></row><row><cell>TOF + DynaVSR</cell><cell cols="10">0.9305 0.8275 0.9231 0.9171 0.9093 0.8995 0.8877 0.8743 0.8593 0.9031</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table D :</head><label>D</label><figDesc>Quantitative results of DynaVSR combined with recent VSR algorithms in anisotropic Gaussian kernels on Vid4. We evaluate the benefits of DynaVSR algorithm on Vid4 [24] dataset. DynaVSR 0.8756 0.8806 0.8699 0.8764 0.8756 DynaVSR 0.8640 0.8614 0.8579 0.8569 0.8601</figDesc><table><row><cell></cell><cell>PSNR</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="4">? = 0 ? ? = 45 ? ? = 90 ? ? = 135 ? Average</cell></row><row><cell>KG + ZSSSR</cell><cell>25.09 23.71</cell><cell>25.10</cell><cell>23.55</cell><cell>24.36</cell></row><row><cell>CF + DBPN</cell><cell>26.22 25.49</cell><cell>25.70</cell><cell>25.01</cell><cell>25.61</cell></row><row><cell>CF + CARN</cell><cell>27.54 26.64</cell><cell>26.51</cell><cell>26.57</cell><cell>26.82</cell></row><row><cell>IKC</cell><cell>26.10 26.12</cell><cell>26.50</cell><cell>25.97</cell><cell>26.17</cell></row><row><cell>EDVR Baseline</cell><cell>25.84 25.87</cell><cell>25.91</cell><cell>25.75</cell><cell>25.84</cell></row><row><cell cols="2">EDVR + DynaVSR 29.65 28.12</cell><cell>29.39</cell><cell>28.06</cell><cell>28.81</cell></row><row><cell>DUF Baseline</cell><cell>25.68 25.71</cell><cell>25.81</cell><cell>25.60</cell><cell>25.70</cell></row><row><cell>DUF + DynaVSR</cell><cell>27.29 27.71</cell><cell>27.58</cell><cell>27.59</cell><cell>27.54</cell></row><row><cell>TOF Baseline</cell><cell>25.60 25.71</cell><cell>25.72</cell><cell>25.62</cell><cell>25.66</cell></row><row><cell>TOF + DynaVSR</cell><cell>27.02 27.10</cell><cell>27.16</cell><cell>26.99</cell><cell>27.07</cell></row><row><cell></cell><cell>SSIM</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="4">? = 0 ? ? = 45 ? ? = 90 ? ? = 135 ? Average</cell></row><row><cell>KG + ZSSSR</cell><cell cols="4">0.8305 0.8257 0.8438 0.8175 0.8075</cell></row><row><cell>CF + DBPN</cell><cell cols="4">0.8496 0.8310 0.8401 0.8206 0.8353</cell></row><row><cell>CF + CARN</cell><cell cols="4">0.8848 0.8699 0.8662 0.8715 0.8731</cell></row><row><cell>IKC</cell><cell cols="4">0.8338 0.8266 0.8445 0.8181 0.8308</cell></row><row><cell>EDVR Baseline</cell><cell cols="4">0.8320 0.8252 0.8142 0.8195 0.8227</cell></row><row><cell cols="5">EDVR + DynaVSR 0.9179 0.8805 0.9045 0.8756 0.8946</cell></row><row><cell>DUF Baseline</cell><cell cols="4">0.8225 0.8174 0.8102 0.8117 0.8154</cell></row><row><cell>DUF + TOF Baseline</cell><cell cols="4">0.8186 0.8158 0.8076 0.8108 0.8132</cell></row><row><cell>TOF +</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table E :</head><label>E</label><figDesc>Quantitative results of DynaVSR combined with recent VSR algorithms in anisotropic Gaussian kernels on REDS-val. We evaluate the benefits of DynaVSR algorithm on REDS-val<ref type="bibr" target="#b26">[27]</ref> dataset.PSNR Method ? = 0 ? ? = 45 ? ? = 90 ? ? = 135 ? Average</figDesc><table><row><cell>KG + ZSSR</cell><cell>27.65 26.66</cell><cell>28.59</cell><cell>26.28</cell><cell>27.30</cell></row><row><cell>CF + DBPN</cell><cell>29.51 28.66</cell><cell>30.16</cell><cell>28.88</cell><cell>29.30</cell></row><row><cell>CF + CARN</cell><cell>30.90 29.91</cell><cell>30.96</cell><cell>30.09</cell><cell>30.47</cell></row><row><cell>IKC</cell><cell>29.90 30.03</cell><cell>30.30</cell><cell>30.21</cell><cell>30.11</cell></row><row><cell>EDVR Baseline</cell><cell>29.33 29.45</cell><cell>30.05</cell><cell>29.82</cell><cell>29.66</cell></row><row><cell cols="2">EDVR + DynaVSR 33.20 33.10</cell><cell>33.53</cell><cell>33.40</cell><cell>33.41</cell></row><row><cell>DUF Baseline</cell><cell>29.06 29.23</cell><cell>29.68</cell><cell>29.56</cell><cell>29.38</cell></row><row><cell>DUF + DynaVSR</cell><cell>30.84 31.30</cell><cell>31.33</cell><cell>31.69</cell><cell>31.29</cell></row><row><cell>TOF Baseline</cell><cell>29.15 29.28</cell><cell>29.76</cell><cell>29.63</cell><cell>29.46</cell></row><row><cell>TOF + DynaVSR</cell><cell>31.27 31.45</cell><cell>31.80</cell><cell>31.71</cell><cell>31.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table F :</head><label>F</label><figDesc>Quantitative results(PSNR) for meta-training with recent VSR models and blind SISR methods. We evaluate the benefits of DynaVSR algorithm on Vid4 [24] and REDS-val [27] dataset. Only the 10% of the full data are measured in this table. 25.83 26.31 29.13 29.65 29.58 DynaVSR 28.70 28.79 29.41 32.43 33.40 33.50 DUF [16] Baseline 25.25 25.69 26.53 29.01 29.38 29.71 DynaVSR 27.42 27.53 27.97 31.23 31.29 31.11 TOFlow [37] Baseline 25.25 25.65 26.66 29.03 29.45 29.97 DynaVSR 27.15 27.06 27.91 31.47 31.52 31.69</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Vid4 [24]</cell><cell>REDS-val [27]</cell></row><row><cell></cell><cell cols="2">Method</cell><cell>Iso. Aniso. Mixed</cell><cell>Iso. Aniso. Mixed</cell></row><row><cell>Blind SISR</cell><cell cols="2">KG [3] + ZSSR [30] CF [15] + DBPN [12] CF [15] + CARN [2] IKC [11]</cell><cell>25.94 24.38 21.33 27.30 25.61 24.03 27.95 26.82 25.62 29.49 26.17 27.57</cell><cell>28.96 27.43 25.54 30.37 29.30 28.02 30.98 30.47 29.70 34.10 30.11 31.42</cell></row><row><cell></cell><cell>EDVR [36]</cell><cell>Baseline</cell><cell>25.34</cell></row><row><cell>Video SR</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table G :</head><label>G</label><figDesc>SSIM results for meta-training with recent VSR models and blind SISR methods. We evaluate the benefits of DynaVSR algorithm on Vid4<ref type="bibr" target="#b23">[24]</ref> and REDS-val<ref type="bibr" target="#b26">[27]</ref> dataset. Only the 10% of the full data are measured in this table.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Vid4 [24]</cell><cell>REDS-val [27]</cell></row><row><cell></cell><cell cols="2">Method</cell><cell>Iso. Aniso. Mixed</cell><cell>Iso. Aniso. Mixed</cell></row><row><cell>Blind SISR</cell><cell cols="2">KG [3] + ZSSR [30] CF [15] + DBPN [12] CF [15] + CARN [2] IKC [11]</cell><cell>0.8390 0.8093 0.7131 0.8670 0.8394 0.7704 0.8675 0.8353 0.7602 0.8741 0.8696 0.8202 0.8826 0.8731 0.8388 0.8855 0.8973 0.8788 0.9125 0.8301 0.8554 0.9362 0.8822 0.9042</cell></row><row><cell></cell><cell>EDVR [36]</cell><cell cols="2">Baseline DynaVSR 0.9026 0.8943 0.9059 0.9285 0.9362 0.9374 0.7845 0.8225 0.8481 0.8468 0.8728 0.8700</cell></row><row><cell>Video SR</cell><cell>DUF [16] TOFlow [37]</cell><cell cols="2">Baseline DynaVSR 0.8595 0.8752 0.8906 0.8936 0.9026 0.8999 0.7815 0.8150 0.8531 0.8425 0.8624 0.8666 Baseline 0.7806 0.8124 0.8538 0.8429 0.8645 0.8708 DynaVSR 0.8503 0.8594 0.8842 0.9028 0.9082 0.9095</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table H :</head><label>H</label><figDesc>Effect of inner loop learning rate ? for EDVR on REDS-val.</figDesc><table><row><cell>?</cell><cell cols="3">Isotropic Anisotropic Mixed</cell></row><row><cell cols="2">10 ?4 31.47</cell><cell>30.80</cell><cell>30.97</cell></row><row><cell cols="2">10 ?5 32.45</cell><cell>33.41</cell><cell>33.67</cell></row><row><cell cols="2">10 ?6 30.96</cell><cell>32.94</cell><cell>33.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table I :</head><label>I</label><figDesc>Additional experimental results for ?4 SR in EDVR<ref type="bibr" target="#b35">[36]</ref>. The best performance is written in bold. Note that DynaVSR shows better performance also in ?4 SR task.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Vid4 [24]</cell><cell></cell><cell>REDS-val [27]</cell></row><row><cell>Method</cell><cell>Iso.</cell><cell cols="2">Aniso. Mixed</cell><cell>Iso.</cell><cell>Aniso. Mixed</cell></row><row><cell>EDVR baseline</cell><cell cols="2">21.14 21.53</cell><cell>21.11</cell><cell cols="2">24.80 25.22</cell><cell>25.29</cell></row><row><cell cols="3">EDVR+DynaVSR 21.14 24.74</cell><cell>23.84</cell><cell cols="2">25.81 26.19</cell><cell>27.11</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/esw0116/DynaVSR</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Fast, accurate, and lightweight super-resolution with cascading residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namhyuk</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungkon</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyung-Ah</forename><surname>Sohn</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Blind super-resolution kernel estimation using an internal-gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sefi</forename><surname>Bell-Kligler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assaf</forename><surname>Shocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
		<editor>NeurIPS.</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Low-complexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aline</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie Line Alberi-Morel</forename></persName>
		</author>
		<editor>BMVC.</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Realtime video super-resolution with spatio-temporal networks and motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A neural approach to blind motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Second-order attention network for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianrui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Image superresolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Superresolution from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Blind super-resolution with iterative kernel correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep back-projection networks for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recurrent back-projection network for video superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Single image super-resolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Correction filter for single image super-resolution: Robustifying off-theshelf deep super-resolvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Shady Abu Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Tirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Giryes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep video super-resolution network using dynamic upsampling filters without explicit motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younghyun</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeyeon</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Video super-resolution with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armin</forename><surname>Kappeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghwan</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aggelos K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Computational Imaging</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deeplyrecursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Photorealistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">On bayesian adaptive video super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doron</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Nonparametric blind super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ntire 2019 challenge on video deblurring and superresolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungyong</forename><surname>Baik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokil</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Single image super-resolution via a holistic attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangde</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianping</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuzhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Shen</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Fast adaptation to super-resolution networks via meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seobin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsu</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae Hyun</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.02905</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">zero-shot&quot; super-resolution using deep internal learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assaf</forename><surname>Shocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Metatransfer learning for zero-shot super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Woong</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunwoo</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Ik</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Detail-revealing deep video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Tdan: Temporally-deformable alignment network for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Anchored neighborhood regression for fast example-based super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A+: Adjusted anchored neighborhood regression for fast superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<editor>ACCV.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Edvr: Video restoration with enhanced deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
		<title level="m">Video enhancement with task-oriented flow. IJCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image super-resolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE trans. on Image Processing</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Curves and Surfaces</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning a single convolutional super-resolution network for multiple degradations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Internal statistics of a single natural image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Zontak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
