<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Privacy-Preserving Portrait Matting</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Virtual Event</publisher>
				<availability status="unknown"><p>Copyright Virtual Event</p>
				</availability>
				<date>October 20-24, 2021. October 20-24, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhizi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sihan</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
							<email>jing.zhang1@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<email>dacheng.tao@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">JD Explore Academy</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhizi</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sihan</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">JD Explore Academy</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Privacy-Preserving Portrait Matting</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 29th ACM International Conference on Multimedia (MM &apos;21)</title>
						<meeting>the 29th ACM International Conference on Multimedia (MM &apos;21) <address><addrLine>China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Virtual Event</publisher>
							<date type="published">October 20-24, 2021. October 20-24, 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3474085.3475512</idno>
					<note>Event, China. ACM, New York, NY, USA, 9 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Computing methodologies ? Computer vision tasks</term>
					<term>Im- age segmentation</term>
					<term>KEYWORDS portrait matting, deep learning, privacy-preserving, benchmark, trimap, semantic segmentation ACM Reference Format:</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, there has been an increasing concern about the privacy issue raised by using personally identifiable information in machine learning. However, previous portrait matting methods were all based on identifiable portrait images. To fill the gap, we present P3M-10k in this paper, which is the first large-scale anonymized benchmark for Privacy-Preserving Portrait Matting. P3M-10k consists of 10,000 high-resolution face-blurred portrait images along with high-quality alpha mattes. We systematically evaluate both trimap-free and trimap-based matting methods on P3M-10k and find that existing matting methods show different generalization capabilities when following the Privacy-Preserving Training (PPT) setting, . ., "training on face-blurred images and testing on arbitrary images". To devise a better trimap-free portrait matting model, we propose P3M-Net, which leverages the power of a unified framework for both semantic perception and detail matting, and specifically emphasizes the interaction between them and the encoder to facilitate the matting process. Extensive experiments on P3M-10k demonstrate that P3M-Net outperforms the state-of-theart methods in terms of both objective metrics and subjective visual quality. Besides, it shows good generalization capacity under the PPT setting, confirming the value of P3M-10k for facilitating future research and enabling potential real-world applications. The source code and dataset are available at https://github.com/JizhiziLi/P3M. <ref type="figure">Figure 1:</ref> (a) Some anonymized portrait images from our P3M-500-P test set. (b) Some non-anonymized celebrity or back portrait images from our P3M-500-NP test set. We also provide the alpha mattes predicted by our P3M-Net, following the privacy-preserving training setting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The success of deep learning in many computer vision and multimedia areas, largely relies on large-scale of training data <ref type="bibr" target="#b45">[46]</ref>. However, for some tasks such as face recognition <ref type="bibr" target="#b28">[29]</ref>, human activity analysis <ref type="bibr" target="#b37">[38]</ref>, and speech recognition, privacy concerns about the personally identifiable information in the datasets, . . face, gait, and voice, have attracted increasing attention recently. Unfortunately, how to alleviate the privacy concerns in data while not affecting the performance remains challenging and under-explored <ref type="bibr" target="#b41">[42]</ref>. For instance, portrait matting, which refers to estimating the accurate foregrounds from portrait images, also involves the privacy issue, as the images usually contain identifiable faces in previous matting datasets <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41]</ref>. This issue has received more and more concerns due to the popular of virtual video meeting during the COVID-19 pandemic, since portrait matting is a key technique in this multimedia application for changing virtual background. However, we found that all the previous portrait matting methods pay less attention to the privacy issue and adopted the intact identifiable portrait images for both training and evaluation, leaving privacy-preserving portrait matting (P3M) as an open problem.</p><p>In this paper, we make the first attempt to fill this gap by presenting a large-scale anonymized portrait matting benchmark (P3M-10k) and investigating the impact of privacy-preserving training (PPT) on portrait matting models. P3M-10k consists of 10,000 highresolution face-blurred portrait images where we carefully collect and filter from a huge number of images with diverse foregrounds, backgrounds and postures, along with the carefully labeled high quality ground truth alpha mattes. It surpasses the existing matting datasets <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b46">47]</ref> in terms of diversity, volume and quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2104.14222v2 [cs.CV] 29 Jul 2021</head><p>Besides, we choose face obfuscation as the privacy protection technique to remove the identifiable face information while retaining fine details such as hairs. We split out 500 images from P3M-10k to serve as a face-blurred validation set, named P3M-500-P. Some examples are shown in <ref type="figure">Figure 1(a)</ref>. Furthermore, to evaluate the generalization ability of matting models on normal images, which are trained following the PPT setting, . ., only using the face-blurred portrait images in P3M-10k training set, we construct a validation set with 500 images without privacy concerns, named P3M-500-NP. All the images in P3M-500-NP are either frontal images of celebrities or profile/back images without any identifiable faces. Some examples are shown in <ref type="figure">Figure 1(b)</ref>.</p><p>It is very interesting and of significant practical meaning to see whether the privacy-preserving training will have a side impact on the matting models, since face obfuscation brings noticeable artefacts to the images which are not observed in normal portrait images. We notice that a contemporary work <ref type="bibr" target="#b41">[42]</ref> has shown empirical evidences that face obfuscation only has minor side impact on object detection and recognition models. However, in the context of portrait matting, where the pixel-wise alpha matte (a soft mask) with fine details is expected to be estimated from a high-resolution portrait image, the impact remains unclear.</p><p>In this paper, we systematically evaluate both trimap-based and trimap-free matting methods on the unmodified version and faceblurred version of P3M-10k, and provide our insight and analysis about the impact. Specifically, we found that for trimap-based matting, where the trimap is used as an auxiliary input, face obfuscation shows little impact on the matting models, . ., a slight performance change of models following the PPT setting. As for trimap-free matting which involves two sub-tasks: foreground segmentation and detail matting, we found that the methods using a multi-task framework that explicitly model and jointly optimize both tasks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30]</ref> are able to mitigate the impact of face obfuscation to an acceptable level (2% to 5%). Besides, the matting methods that solve the problem through sequential segmentation and matting <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b35">36]</ref>, show a significant performance drop, since face obfuscation leads to segmentation errors that will mislead the subsequent matting model. Other methods that involve several stages of networks to progressively refine the alpha mattes from coarse to fine, shows to be less affected by face obfuscation but still observe a performance drop due to the lack of explicit semantic guidance. Meanwhile, these methods are a little awkward due to the tedious training process.</p><p>Based on the above observations, we propose a novel automatic portrait matting network named P3M-Net, which is able to serve as a strong trimap-free matting baseline for the P3M task (See the results in <ref type="figure">Figure 1</ref>). Technically, we also adopt a multi-task framework like <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30]</ref> as our basic structure, which learns common visual features through a sharing encoder and task-aware features through a segmentation decoder and a matting decoder. In contrast to previous methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30]</ref>, we specifically emphasize the interaction between two decoders and those between encoder and decoders. To this end, we devise a Tripartite-Feature Integration (TFI) module for each matting decoder block to effectively fuse the matting decoder features from the previous block, semantic features from the segmentation decoder block, and base visual features from the encoder block. Besides, we devise a Deep Bipartite-Feature Integration (dBFI) module and a Shallow Bipartite-Feature Integration (sBFI) module to leverage deep features with high-level semantics and shallow features with fine details for improving the segmentation decoder and matting decoder, respectively. Experiments on the P3M-10k benchmark demonstrate that P3M-Net achieves a neglectable performance drop under the PPT setting and outperforms all the previous trimap-free matting methods by a large margin.</p><p>To sum up, the contributions of this paper are three-fold. First, to the best of our knowledge, we are the first to study the problem of privacy-preserving portrait matting and establish the largest privacy-preserving portrait matting dataset P3M-10k, which can serve as benchmark for P3M. Second, we systematically investigate the impact of face obfuscation on both trimap-based and trimapfree matting models under the privacy-preserving training setting and provide insights about the evaluation protocol, performance analysis, and model design. Third, We propose a novel trimap-free portrait matting network named P3M-Net that follows a multi-task framework and specifically focuses on the interactions between encoders and decoders. P3M-Net demonstrates its value for privacypreserving portrait matting and can serve as a strong baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Image Matting</head><p>Image matting is a typical ill-posed problem to estimate the foreground, background, and alpha matte from a single image. Specifically, portrait matting refers to a specific image matting task where the input image is a portrait. From the perspective of input, image matting can be divided into two categories, . ., trimap-based methods and trimap-free methods. Trimap-based matting methods use a user defined trimap, . ., a 3-class segmentation map, as an auxiliary input, which provides explicit guidance on the transition area. Previous methods include affinity-based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21]</ref>, samplingbased methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b34">35]</ref>, and deep learning based methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b26">27]</ref>. Besides, there are other methods used different auxiliary inputs, . ., a background image <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b33">34]</ref>, or a coarse map <ref type="bibr" target="#b43">[44]</ref>.</p><p>To enable automatic (portrait) image matting, recent works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b46">47]</ref> tried to estimate the alpha matte directly from a single image without using any auxiliary input, also known as trimap-free methods. For example, DAPM <ref type="bibr" target="#b35">[36]</ref> and SHM <ref type="bibr" target="#b7">[8]</ref> tackled the task by separating it into two sequential stages, . ., segmentation and matting. However, the semantic error produced in the first stage will mislead the matting stage and can not be corrected. LF <ref type="bibr" target="#b46">[47]</ref> and SHMC <ref type="bibr" target="#b25">[26]</ref> solved the problem by generating coarse alpha matte first and then refining it. Besides of the tedious training process, these methods suffer from ambiguous boundaries due to the lack of explicit semantic guidance. HATT <ref type="bibr" target="#b29">[30]</ref> and GFM <ref type="bibr" target="#b21">[22]</ref> proposed to model both the segmentation and matting tasks in a unified multi-task framework, where a sharing encoder was used to learn base visual features and two individual decoders are used to learn task-relevant features. However, HATT <ref type="bibr" target="#b29">[30]</ref> lacks explicit supervision on the global guidance while GFM <ref type="bibr" target="#b21">[22]</ref> lacks modeling the interactions between both tasks. By contrast, we propose a novel model named P3M-Net, which is also based on the multi-task framework but specifically focuses on modeling the interactions between encoders and decoders. Besides, we comprehensively investigate their performance under the PPT setting on P3M-10k and provide some useful insights on their model structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Privacy Issues in Visual Tasks</head><p>There are two kinds of privacy issues in visual tasks, . ., private data protection and private content protection in public academic datasets. For the former, there are concerns of information leak caused by insecure data transferring and membership inference attacks to the trained models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b36">37]</ref>. Privacy-preserving machine learning (PPML) aims to solve these problems by homonorphic encryption <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b42">43]</ref> and differential privacy algorithms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>For public academic datasets, there is no concern for information leak, thus PPML is no longer needed. But, there still exists privacy breach incurred by exposure of personally identifiable information, . . faces, addresses. It is a common problem in the benchmark datasets for many visual tasks, . . object recognition and semantic segmentation. Recently a contemporary work <ref type="bibr" target="#b41">[42]</ref> has shown empirical evidences that face obfuscation, as an effective data anonymization technique, only has minor side impact on object detection and recognition. However, since portrait matting requires to estimate a pixel-wise soft mask (alpha matte) for a high-resolution portrait image, the impact remains unclear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Privacy-Preserving Methods</head><p>Normally, to protect the private information in the public images, a common method is to capture or process the data in special low quality conditions <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10]</ref>. For example, <ref type="bibr">Dai</ref> . captured the anonymized video data in extremely low resolution to avoid the leak of personally identifiable information such as frontal faces <ref type="bibr" target="#b9">[10]</ref>. Another way is to add empirical obfuscations <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b38">39]</ref>, such as blurring and mosaicing at certain regions. <ref type="bibr">Yang</ref> . used face blurring to obfuscate the faces in the ImageNet dataset <ref type="bibr" target="#b41">[42]</ref>. <ref type="bibr">Caesar</ref> . detected and blurred the license plates in nuScenes dataset to avoid privacy concerns <ref type="bibr" target="#b5">[6]</ref>. For the portrait matting task, all previous benchmarks or methods pay little attention to the privacy issue. By contrast, we make the first attempt to construct a large-scale anonymized dataset for privacy-preserving portrait matting named P3M-10k. Specifically, we use face obfuscation as the privacy-preserving strategy to anonymize the identities of all images. Intuitively, the anonymized images with blurred faces may degenerate the performance of matting models due to the domain gap between anonymized training images and normal test images. In this paper, we make the first attempt to investigate the impact of face obfuscation on portrait matting under the PPT setting and identify that it has negligible impact on trimap-based matting methods but has different impact on trimap-free matting methods, depending on their model structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Matting Datasets</head><p>Existing matting datasets either contain only a small number of high-quality images and annotations, or the images and annotations are in low-quality. For example, the online benchmark alphamatting <ref type="bibr" target="#b30">[31]</ref> only provides 27 high-resolution training images and 8 test images. None of them is portrait image. Composition-1k <ref type="bibr" target="#b40">[41]</ref>, the most commonly used dataset, contains 431 foregrounds for training and 20 foregrounds for testing. However, many of them are consecutive video frames, making it less diverse. GFM <ref type="bibr" target="#b21">[22]</ref> provides 2,000 high-resolution natural images with alpha mattes, but they are all animal images. With respect to portrait image matting dataset, DAPM <ref type="bibr" target="#b35">[36]</ref> provided a large dataset of 2,000 low-resolution portrait images with alpha mattes generated by KNN matting <ref type="bibr" target="#b8">[9]</ref> and closed form matting <ref type="bibr" target="#b20">[21]</ref>, whose quality is limited. Late fusion <ref type="bibr" target="#b46">[47]</ref> built a human image matting dataset by combining 228 portrait images from Internet and 211 human images in Composition-1k. Distinction-646 <ref type="bibr" target="#b29">[30]</ref> is a dataset containing 364 human images but with only foregrounds provided. There are also some large-scale portrait datasets, . ., SHM <ref type="bibr" target="#b7">[8]</ref>, SHMC <ref type="bibr" target="#b25">[26]</ref>, and background matting <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b33">34]</ref>, which are unfortunately not public. Most importantly, no privacy preserving method is used to anonymize the images in the aforementioned datasets, making all the frontal faces exposed. By contrast, we establish the first large-scale matting dataset with 10,000 high-resolution portrait images with high-quality alpha mattes and anonymize all images using face obfuscation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A BENCHMARK FOR P3M AND BEYOND</head><p>Privacy-preserving portrait matting (P3M) is an important and meaningful topic due to the increasing privacy concerns. In this section, we first define this problem, then establish a large-scale anonymized portrait matting dataset P3M-10k to serve as a benchmark for P3M. A systematic evaluation of the existing trimap-based and trimap-free matting methods on P3M-10k is conducted to investigate the impact of the privacy-preserving training (PPT) setting on different matting models and gain some useful insights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PPT Setting and P3M-10k Dataset</head><p>3.1.1 PPT Setting. Due to the privacy concern, we propose the privacy-preserving training (PPT) setting in portrait matting, . ., training on privacy-preserved images ( . ., processed by face obfuscation) and testing on arbitrary images with or without privacy content. As an initial step towards privacy-preserving portrait matting problem, we only define the identifiable faces in frontal and some profile portrait images as the private content in this work. Intuitively, PPT setting is challenging since face obfuscation brings noticeable artefacts to the images which are not observed in normal portrait images, . ., probably resulting in a domain gap between training and testing. Therefore, it is very interesting and of significant practical meaning to see whether the PPT setting will have a side impact on the matting models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">P3M-10k</head><p>Dataset. To answer the above question, we establish the first large-scale privacy-preserving portrait matting benchmark named P3M-10k. It contains 10,000 anonymized high-resolution portrait images by face obfuscation along with high-quality ground truth alpha mattes. Specifically, we carefully collect, filter, and annotate about 10,000 high-resolution images from the Internet Test set Metric Closed <ref type="bibr" target="#b20">[21]</ref> IFM <ref type="bibr" target="#b2">[3]</ref> KNN <ref type="bibr" target="#b8">[9]</ref> Compre <ref type="bibr" target="#b34">[35]</ref> Robust <ref type="bibr" target="#b39">[40]</ref> Learning <ref type="bibr" target="#b47">[48]</ref> Global <ref type="bibr" target="#b15">[16]</ref>    <ref type="bibr" target="#b21">[22]</ref> 13.20 0.0050 0.0080 13.08 0.0050 0.0080 13.54 0.0048 0.0078 10.73 0.0033 0.0063 BASIC 15.13 0.0058 0.0088 15.52 0.0060 0.0090 24.38 0.0109 0.0141 14.52 0.0054 0.0085 P3M-Net (Ours) 8.73 0.0026 0.0051 9.22 0.0028 0.0053 11.22 0.0040 0.0065 9.06 0.0028 0.0053 <ref type="table">Table 3</ref>: Results of trimap-free methods on P3M-500-P. Please refer to <ref type="table" target="#tab_1">Table 2</ref> for the meaning of different symbols.</p><p>with free use license. There are 9,421 images in the training set and 500 images in the test set, denoted as P3M-500-P. In addition, we also collect and annotate another 500 public celebrity images from the Internet without face obfuscation, to evaluate the performance of matting models under the PPT setting on normal portrait images. Some examples are shown in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>Our P3M-10k outperforms existing matting datasets in terms of dataset volume, image diversity, privacy preserving, and providing natural images instead of composited ones. The diversity is not only shown in foreground, . ., half and full body, frontal, profile, and back portrait, different genders, races, and ages, ., but also in background. Images in P3M-10k are captured in different indoor and outdoor environments with various illumination conditions. Some examples are shown in <ref type="figure" target="#fig_0">Figure 2</ref>. In addition, we argue that large volume and high diversity of P3M-10k enable models to train on the natural images without the need of image composition. Image composition using low-resolution background images is a common practice in previous works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b40">41]</ref> to increase data diversity due to the small dataset volume. However, there are obvious composition artefacts in the composition images due to the discrepancy of foreground and background images in noise, resolution, and illumination. By contrast, the background in the natural images are compatible with the foreground since they are captured from the same scene. The composition artefacts may have a side impact on the generalization ability of matting models as shown in <ref type="bibr" target="#b21">[22]</ref>. We leave it as the future work to systematically investigate this problem and only focus on the PPT setting in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Privacy-Preserving Method in P3M-10k</head><p>We propose to use blurring to obfuscate the identifiable faces. Instead of using a face detector to obtain the bounding box of face and blurring it accordingly as in <ref type="bibr" target="#b41">[42]</ref>, we adopt facial landmark detectors <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b44">45]</ref> to obtain the face mask. It is because different from the classification and detection tasks in <ref type="bibr" target="#b41">[42]</ref>, which may not be sensitive to the blurry boundaries, portrait matting requires to estimate the foreground alpha matte with clear boundaries, including the transition areas of face such as cheek and hair. As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, after obtaining the landmarks, a pixel-level face mask is automatically generated along the cheek and eyebrow landmarks in step <ref type="bibr" target="#b2">(3)</ref>. Then, we exclude the transition area shown in step (4) and generate an adjusted face mask at step <ref type="bibr" target="#b4">(5)</ref>. Finally, we use Gaussian blur to obfuscate the identifiable faces in the mask and the final result is shown in step <ref type="bibr" target="#b5">(6)</ref>. Note that for those images with failure landmark detection, we manually annotate the face mask. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Benchmark Setup</head><p>3.3.1 Methods. We evaluate both trimap-based and trimap-free matting methods. The full list of methods are shown in <ref type="table" target="#tab_0">Table 1</ref>, 2, 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Evaluation</head><p>Metrics. We use the common metrics including MSE, SAD, and MAD for evaluation. For trimap-based methods, the metrics are only calculated over the transition area, while for trimap-free methods, they are calculated over the whole image.  <ref type="table" target="#tab_0">Table 1</ref>, trimap-based traditional methods show neglectable performance variance under different training and evaluation protocols, indicating that PPT setting brings little impact on these methods. This observation is reasonable, since traditional methods mainly make prediction based on local pixels in the transition area with no blurring, although a few of sampled neighboring pixels may be blurred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Study on The Impact of PPT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Impact on Trimap-based Traditional Methods. As in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Impact on Trimap-based Deep Learning</head><p>Methods. Similar to traditional trimap-based methods, deep learning methods also show very minor changes across different settings, as shown in <ref type="table" target="#tab_1">Table 2</ref>. This is because trimap-based deep learning methods use the ground truth trimap as an auxiliary input and focus on estimating the alpha matte of the transition area, probably guiding the model to pay less attention to the blurred areas. In addition, there are also some observations opposed to intuition. When testing on normal images, models trained on the normal training images surprisingly fall behind of those trained on the blurred ones. For instance, the SAD of IndexNet on "N:N" is 0.6 higher than the score on "B:N". Similar results can also be found for AlphaGAN, GCA in <ref type="table" target="#tab_1">Table 2</ref>. We suspect that the blurred pixels near the transition area may serve as a random noise during the training process, which makes the model more robust and leads to a better generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Impact on</head><p>Trimap-free Methods. Different from trimap-based methods, trimap-free methods show significant performance changes under four protocols. The results are shown in <ref type="table">Table 3</ref>. First, we start with the test set of normal images by comparing results in the "B:N" and "N:N" tracks. Models trained on normal training images (N:N) usually outperform those using the blurred ones (B:N), . ., from 24.33 to 17.13 at SAD for SHM. This observation makes sense since there is a domain gap between blurred images and normal ones due to face obfuscation. By comparison, we found trimap-free methods show different generalization ability in dealing with this domain gap. For example, SHM is the worst with a large drop of 7 in SAD, while HATT and GFM only show a drop less than 3 in SAD. We suspect that an end-to-end multi-task framework may probably mitigate the domain gap issue via joint optimization. By contrast, two-stage methods such as SHM may produce segmentation errors, which can mislead the following matting stage and can not be corrected. To validate this hypothesis, we devise a baseline model called "BASIC" by adopting a similar multi-task framework like HATT and GFM but removing the bells and whistles, . ., only using a sharing encoder and two individual decoders. As shown in <ref type="table">Table 3</ref>, the small performance drop (less than 1 in SAD) proves its superiority in overcoming domain gap and supports our hypothesis.</p><p>Second, we focus on the test set of blurred images by comparing results in the "B:B" and "N:B" tracks. The performance drop in most methods, . ., 9.03 in SAD for HATT, proves that without seeing the blurred pattern during training, models cannot generalize well to the face-blurred images. It implies the value of the blurred training set in P3M-10k for training models that will be deployed in privacypreserving scenarios, where faces may be blurred.</p><p>Third, we fix the training set to be the blurred one. By comparing the "B:B" and "B:N" tracks, we observe similar performance of most methods under these two settings, . ., SADs of HATT are 25.99 and 26.5 on the blurred test set and normal one. These results imply that we can use the performance on the blurred test images in P3M-500-P as a bold indicator of that on the normal ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">A STRONG BASELINE FOR P3M 4.1 A Multi-task Framework</head><p>As discussed in Section 3.4.3, trimap-free matting models benefit from explicitly modeling both semantic segmentation and detail matting and jointly optimizing them in an end-to-end multi-task framework. Therefore, we follow GFM <ref type="bibr" target="#b21">[22]</ref> to adopt the multi-task framework, where base visual features are learned from a sharing encoder and task-relevant features from individual decoders, . ., <ref type="figure">Figure 4</ref>: Diagram of the proposed P3M-Net structure. It adopts a multi-task framework, which consists of a sharing encoder, a segmentation decoder, and a matting decoder. Specifically, a TFI module, a dBFI module, and a sBFI module are devised to model different interactions among the encoder and the two decoders. Red arrows denote the network's outputs.</p><p>semantic decoder and matting decoder, respectively. For the sharing encoder, we choose a modified version of ResNet-34 <ref type="bibr" target="#b16">[17]</ref> with max pooling layers to serve as our light-weight backbone as in AIM <ref type="bibr" target="#b22">[23]</ref>. We also keep the indices of the max pooling operation to reserve the details and used in the unpooling layers in the matting decoder. Both semantic decoder and matting decoder have five blocks, each of which contains three convolution layers. We then choose different upsampling operations to suit each task. We use bilinear interpolation in the semantic decoder for simplicity and use max unpooling operation with the indices from corresponding encoder blocks in the matting decoder to learn features for fine details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">TFI: Tripartite-Feature Integration</head><p>Most of the previous matting methods either model the interaction between encoder and decoder such as the U-Net <ref type="bibr" target="#b32">[33]</ref> style structure in <ref type="bibr" target="#b21">[22]</ref> or model the interaction between two decoders such as the attention module in <ref type="bibr" target="#b29">[30]</ref>. In this paper, we comprehensively model all the interactions between the sharing encoder and two decoders, . ., 1) a tripartite-feature integration (TFI) module in each matting decoder block to model the interaction between encoder, segmentation decoder, and the matting decoder; 2) a deep bipartitefeature integration (dBFI) module to model the interaction between the encoder and segmentation decoder; and 3) a shallow bipartitefeature integration (sBFI) module to model the interaction between the encoder and matting decoder.</p><p>Specifically, for each TFI, it has three inputs, . ., the feature map of the previous matting decoder block F i m ? R ? / ? / , the feature map from same level semantic decoder block F i s ? R ? / ? / , and the feature map from the symmetrical encoder block F i e ? R ? / ? / , where ? {1, 2, 3, 4} stands for the block index, stands for the downsample ratio of the feature map compared to the input size, and = 2 . For each feature map, we use an 1 ? 1 convolutional projection layer P for further embedding and channel reduction. The output of P for each feature map is F i ? R /2? / ? / . We then concatenate the three embedded feature maps and feed them into a convolutional block C containing a 3 ? 3 convolutional layer, a batch normalization layer, and a ReLU layer. As shown in Eq. 1, the output feature is F i m ? R ? / ? / :</p><formula xml:id="formula_0">F i m = C( (P (F i m )</formula><p>, P (F i s ), P (F i e ))).</p><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">sBFI: Shallow Bipartite-Feature Integration</head><p>The matting task requires to distinguish fine foreground details from the background. Therefore, the features in the shallow layers in the encoder may be useful since they contain abundant structural detail features. To leverage them to improve the matting decoder, we propose the shallow bipartite-feature integration (sBFI) module. As shown in <ref type="figure">Figure 4</ref>, we use the feature map E 0 ? R 64? ? in the first encoder block as a guidance to refine the output feature map F i m ? R ? / ? / from the previous matting decoder block since E 0 contains many details and local structural information. Here, ? {1, 2, 3} stands for the layer index, stands for the downsample ratio of the feature map compared to the input size, and = 2 . Since E 0 and F i m are with different resolution, we first adopt max pooling with a ratio on E 0 to generate a low-resolution feature map E ? 0 ? R 64? / ? / . We then feed both E ? 0 and F i m to two projection layers P implemented by 1 ? 1 convolution layers for further embedding and channel reduction, . ., from to /2. Finally, the two feature maps are concatenated and and fed into a convolutional block C containing a 3 ? 3 convolutional layer, a bacth normalization layer, and a ReLu layer. As shown in Eq. 2, we adopt the residual learning idea by adding the output feature map back to the input matting decoder feature map F i m :</p><formula xml:id="formula_1">F i m = C( (P (MP (E 0 )), P (F i m ))) + F i m .<label>(2)</label></formula><p>In this way, sBFI helps the matting decoder block to focus on the fine details guided by E 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">dBFI: Deep Bipartite-Feature Integration</head><p>Same as sBFI, features in the encoder can also provide valuable guidance to the segmentation decoder. In contrast to sBFI, we chose the feature map E 4 ? R 512? /32? /32 from the last encoder block, since it encodes abundant global semantics. Specifically, we devise the deep bipartite-feature integration (dBFI) module to fuse it with the feature map F i s ? R ? / ? / from the th segmentation decoder block to improve the feature representation ability for the high-level semantic segmentation task. Here, ? {1, 2, 3}. Note that since E 4 is in low-resolution, we use a upsampling operation with a ratio 32/ on E 4 to generate E ? 4 ? R 512? / ? / . we then feed both E ? 4 and F i s into two projection layers P, concatenated together, and fed into a convolutional block C. We adopt the identical structures for P and C as those in sBFI. Similarly, this process can be described as:</p><formula xml:id="formula_2">F i s = C( (P (UP (E 4 )), P (F i s ))) + F i s<label>(3)</label></formula><p>Note that we reuse the symbols of C and P in Eq. 1, Eq. 2, and Eq. 3 for simplicity, although each of them denotes a specific layer (block) in TFI, sBFI, and dFI, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Training Objective</head><p>For the segmentation task, we leverage the deep supervision idea and add side loss on the segmentation decoder to stable and improve the training performance. Specifically, we use a 3 3 convolution layer and an upsample operation on each output feature map F i s from dBFI blocks, to predict the segmentation map with 3 channels and in the same resolution as input, denoting as ? R 3? ? . We then calculate the cross-entropy loss L between and the ground truth trimap label ? R 3? ? ( . ., the one-shot representation) defined as follows:</p><formula xml:id="formula_3">L = ? 3 ?? =1 ?? ?=1 ?? =1 ( , ?, ) ( , ?, ) ,<label>(4)</label></formula><p>where represents the number of classes in the trimap. Following <ref type="bibr" target="#b21">[22]</ref>, for the matting decoder, we adopt alpha loss L and Laplacian loss L calculated only on the transition region.</p><p>For the segmentation decoder, we adopt a cross-entropy loss L on its final output. For the final output, we adopt alpha loss L , Laplacian loss L , and composition loss L calculated on the whole image. The final training objective is a combination of all the aforementioned losses, . .,</p><formula xml:id="formula_4">L = L + L + 3 ?? =1 + 3L + 2L + 2L + L ,<label>(5)</label></formula><p>where = 2, = 1/6, and = 1 are loss weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS 5.1 Experiment Settings</head><p>To compare the proposed P3M-Net with existing trimap-free methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b46">47]</ref>, we train them on the P3M-10k face-blurred images and evaluate them on 1) P3M-500-P face-blurred validation set; and 2) P3M-500-NP normal, following the PPT setting. Implementation Details For training P3M-Net, we crop a patch from the image with a size randomly chosen from 512?512, 768?768, 1024 ? 1024, and then resize it to 512 ? 512. We randomly flip the patches for the data augmentation. The learning rate is fixed as 1 ? 10 ?5 . We train P3M-Net on a single NVIDIA Tesla V100 GPU with a batch size of 8. P3M-Net is trained 150 epochs for about 2 days. It takes 0.132s to test on an 800 ? 800 image. For GFM <ref type="bibr" target="#b21">[22]</ref> and LF <ref type="bibr" target="#b46">[47]</ref>, we use the code provided by the authors. For SHM <ref type="bibr" target="#b7">[8]</ref>, HATT <ref type="bibr" target="#b29">[30]</ref> and DIM <ref type="bibr" target="#b40">[41]</ref> without codes, we re-implement them. Evaluation Metrics We follow previous works and adopt the evaluation metrics including the sum of absolute differences (SAD), mean squared error (MSE), mean absolute difference (MAD), gradient (Grad.) and Connectivity (Conn.) <ref type="bibr" target="#b31">[32]</ref>. We calculated them over the whole image for trimap-free methods. We also report the SAD-T, MSE-T, MAD-T metrics within the transition area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Objective and Subjective Results</head><p>The objective and subjective results of different methods are listed in <ref type="table">Table 4</ref> and <ref type="figure">Figure 5</ref>. As can be seen, P3M-Net outperforms all the trimap-free methods in all metrics and even achieves competitive results with trimap-based method DIM <ref type="bibr" target="#b40">[41]</ref>, which requires the ground truth trimap as an auxiliary input, denotes as DIM?. These results support the designed integration modules, which are able to model abundant interactions between encoder and decoders. As for SHM <ref type="bibr" target="#b7">[8]</ref>, it has worse SAD than P3M-Net on both datasets, . ., 21.56 vs. 8.73 and 20.77 vs. 11.23, due to its stage-wise structure, which produces many segmentation errors. LF <ref type="bibr" target="#b46">[47]</ref> and HATT <ref type="bibr" target="#b29">[30]</ref> have large error in transition area, . ., 12.43 and 11.03 SAD vs. 6.89 SAD of ours, since they lack explicit semantic guidance for the matting task. As in <ref type="figure">Figure 5</ref>, they have ambiguous segmentation results and inaccurate matting details. GFM <ref type="bibr" target="#b21">[22]</ref> is able to predict more accurate semantic mask owing to its multi-task framework. However, it still fails to predict correct context (the last row) and has worse performance than ours, . ., 13.20 vs. 8.73 in SAD, since it lacks of interactions between encoder and decoders. DIM <ref type="bibr" target="#b40">[41]</ref> has lower SAD compare with us since it uses ground truth trimap. Nevertheless, P3M-Net still achieves competitive performance in the transition area, . ., 6.89 vs. 4.89 SAD. It is also noteworthy that even trained only on privacy-preserving training set, most methods can generalize well on arbitrary images, clearly validating the effectiveness of the proposed P3M-10k and the practical value of the PPT setting. Meanwhile, the performance gap when testing on face-blurred images and normal images, . ., 8.73 SAD vs. 11.23 SAD of P3M-Net, also implies more efforts can be made to advance the research for privacy-preserving portrait matting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Studies</head><p>We conduct ablation studies of P3M-Net on two datasets P3M-500-P and P3M-500-NP. As seen from <ref type="table">Table 5</ref>, the basic multi-task P3M-500-P P3M-500-NP <ref type="table">Method SAD  MSE  MAD SAD-T MSE-T MAD-T GRAD CONN SAD  MSE  MAD SAD-T MSE-T MAD-T GRAD CONN  LF</ref>   <ref type="table">Table 4</ref>: Results of P3M-Net and other methods on P3M-500-P and P3M-500-NP. DIM? uses ground truth trimap.</p><p>Image GT LF <ref type="bibr" target="#b46">[47]</ref> HATT <ref type="bibr" target="#b29">[30]</ref> SHM <ref type="bibr" target="#b7">[8]</ref> GFM <ref type="bibr" target="#b21">[22]</ref> DIM <ref type="bibr" target="#b40">[41]</ref>+Trimap Our P3M-Net <ref type="figure">Figure 5</ref>: Subjective results of different methods on P3M-500-P and P3M-500-NP. Please zoom in for more details.</p><p>structure without any advanced modules can achieve a fairly good result compared with previous methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b46">47]</ref>. With TFI, SAD decreases dramatically to 11.32 and 13.7, owing to the valuable semantic features from encoder and segmentation decoder for matting. Besides, sBFI (dBFI) decreases SAD from 11.32 to 9.47 (9.76) on P3M-500-P and from 13.7 to 12. <ref type="bibr" target="#b35">36</ref>   <ref type="table">Table 5</ref>: Ablation study of P3M-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>In this paper, we make the first study on the privacy-preserving portrait matting (P3M) problem to respond to the increasing privacy concerns. Specifically, we define the privacy-preserving training (PPT) setting, and establish the first large-scale anonymized portrait dataset P3M-10k, containing 10,000 face-blurred images and ground truth alpha mattes. We empirically find that the PPT setting has little side impact on trimap-based methods while trimap-free methods perform differently, depending on their model structures.</p><p>We identify that trimap-free methods using a multi-task framework that explicitly models and optimizes both segmentation and matting tasks can effectively mitigate the side impact of PPT. Accordingly, we provide a strong baseline model named P3M-Net, which specifically focuses on modeling the interactions between encoder and decoders, showing promising performance and outperforming all previous trimap-free methods. We hope this study can open a new perspective for the research of portrait matting and attract more attention from the community to address the privacy concerns.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>(a) Samples from the P3M-10k training set and P3M-500-P test set. (b) Samples from P3M-500-NP test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the face blurring process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3. 3 . 3</head><label>33</label><figDesc>Training and Evaluation Protocols. Four kinds of training and evaluation protocols are proposed, including "trained on blurred images, test on blurred ones (B:B)", "trained on blurred images and test on normal ones (B:N)", "trained on normal images and test on blurred ones (N:B)", and "trained on normal images and test on normal ones (N:N)". The first two protocols correspond to the proposed PPT settings. All the methods are trained using the normal or blurred images in the P3M-10k training set and evaluated on P3M-500-P test set. The only difference between blurred and normal images is whether or not face obfuscation is applied.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results of trimap-based traditional methods on the blurred images ("B") and normal images ("N") in P3M-500-P.</figDesc><table><row><cell>Shared [15]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Setting</cell><cell></cell><cell>B:B</cell><cell></cell><cell>B:N</cell><cell></cell><cell>N:B</cell><cell></cell><cell>N:N</cell><cell></cell></row><row><cell>Method</cell><cell>SAD</cell><cell>MSE</cell><cell>MAD SAD</cell><cell>MSE</cell><cell>MAD SAD</cell><cell>MSE</cell><cell>MAD SAD</cell><cell>MSE</cell><cell>MAD</cell></row><row><cell>SHM [8]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Results of trimap-based deep learning methods on P3M-500-P. "B" denotes the blurred images while "N" denotes the normal images. "B:N" denotes training on blurred images while testing on normal images, vice versa.21.56 0.0100 0.0125 24.33 0.0116 0.0140 23.91 0.0115 0.0139 17.13 0.0075 0.0099 LF [47] 42.95 0.0191 0.0250 30.84 0.0129 0.0178 41.01 0.0174 0.0240 31.22 0.0123 0.0181 HATT [30] 25.99 0.0054 0.0152 26.5 0.0055 0.0155 35.02 0.0103 0.0204 22.93 0.0040 0.0133 GFM</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>(12.45) on P3M-500-NP, confirming their values in providing useful guidance from relevant visual features. With all three modules, the SAD decreases from 15.13 to 8.73, and 17.01 to 11.23, indicating that our proposed modules bring about 50% relative performance improvement.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>P3M-500-P</cell><cell></cell><cell>P3M-500-NP</cell></row><row><cell cols="4">TFI sBFI dBFI SAD</cell><cell>MSE</cell><cell>MAD</cell><cell>SAD</cell><cell>MSE</cell><cell>MAD</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">15.13 0.0058 0.0088 17.01 0.0062 0.0099</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell cols="4">11.32 0.0042 0.00066 13.7</cell><cell>0.0052</cell><cell>0.008</cell></row><row><cell>?</cell><cell>?</cell><cell></cell><cell cols="4">9.47 0.0030 0.0055 12.36 0.0043 0.0072</cell></row><row><cell>?</cell><cell></cell><cell>?</cell><cell cols="4">9.76 0.0031 0.0057 12.45 0.0043 0.0073</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell cols="4">8.73 0.0026 0.0051 11.23 0.0035 0.0065</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning with differential privacy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Mironov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM SIGSAC conference on computer and communications security</title>
		<meeting>the 2016 ACM SIGSAC conference on computer and communications security</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="308" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic soft segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya?iz</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Hyun</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Matusik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Designing effective inter-pixel information flow for natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yagiz</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Tunc Ozan Aydin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="29" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2D &amp; 3D Face Alignment problem? (and a dataset of 230,000 3D facial landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The privacy-utility tradeoff for remotely teleoperated robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maya</forename><surname>Roesner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cakmak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth annual ACM/IEEE international conference on human-robot interaction</title>
		<meeting>the tenth annual ACM/IEEE international conference on human-robot interaction</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">2020. nuscenes: A multimodal dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anush</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<biblScope unit="page" from="11621" to="11631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The secret sharer: Evaluating and testing unintended memorization in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?lfar</forename><surname>Erlingsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jernej</forename><surname>Kos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th {USENIX} Security Symposium</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>{USENIX} Security 19</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic human matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiezheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">KNN matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingzeyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2175" to="2188" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards privacy-preserving recognition of human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behrouz</forename><surname>Saghafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janusz</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakash</forename><surname>Ishwar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE international conference on image processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4238" to="4242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Privacy-preserving face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekeriya</forename><surname>Erkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Guajardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Katzenbeisser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International symposium on privacy enhancing technologies symposium</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Inald Lagendijk, and Tomas Toft</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Forte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Piti?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; F</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alpha</forename><surname>Matting</surname></persName>
		</author>
		<idno>abs/2003.07711</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Model inversion attacks that exploit confidence information and basic countermeasures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Ristenpart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security</title>
		<meeting>the 22nd ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1322" to="1333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large-scale privacy protection in google street view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Abdulkader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Zennaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartmut</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE 12th international conference on computer vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2373" to="2380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Shared sampling for realtime alpha matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Eduardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel M</forename><surname>Gastal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="575" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A global sampling method for alpha matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011. IEEE</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2049" to="2056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Membership Inference Attacks on Sequence-to-Sequence Models: Is My Data In Your Machine Translation System? Transactions of the</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sorami</forename><surname>Hisamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Context-aware image matting for simultaneous foreground and alpha estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4130" to="4139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Auditing Differentially Private Machine Learning: How Private is Private SGD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Jagielski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Oprea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22205" to="22216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A closed-form solution to natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="228" to="242" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhizi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.16188</idno>
		<title level="m">End-to-end Animal Image Matting</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep Automatic Natural Image Matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhizi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21. International Joint Conferences on Artificial Intelligence Organization</title>
		<meeting>the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21. International Joint Conferences on Artificial Intelligence Organization</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Natural image matting via guided contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanchuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Ryabtsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumyadip</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07810</idno>
		<title level="m">Real-Time High-Resolution Background Matting</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Boosting Semantic Human Matting with Coarse Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaomiao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuansong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8563" to="8572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Indices Matter: Learning to Index for Deep Image Matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songcen</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3266" to="3275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">AlphaGAN: Generative adversarial networks for natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Lutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Amplianitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Smolic</surname></persName>
		</author>
		<ptr target="http://bmvc2018.org/contents/papers/0915.pdf" />
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<meeting><address><addrLine>Newcastle, UK</addrLine></address></meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="page">259</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep face recognition: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 31st SIBGRAPI conference on graphics, patterns and images (SIBGRAPI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="471" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention-Guided Hierarchical Structure Aggregation for Image Matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Perceptually Motivated Online Benchmark for Image Matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margrit</forename><surname>Gelautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Rott</surname></persName>
		</author>
		<ptr target="http://publik.tuwien.ac.at/files/PubDat_180666.pdf" />
	</analytic>
	<monogr>
		<title level="m">Posterpr?sentation: IEEE Conference on Computer Vision and Pattern Recognition, 2009. CVPR &apos;09</title>
		<meeting><address><addrLine>Miami, Florida, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06-20" />
		</imprint>
	</monogr>
	<note>Proceddings of the IEEE Conference on Computer Vision and Pattern Recognition. 8 pages</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A perceptually motivated online benchmark for image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margrit</forename><surname>Gelautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Rott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1826" to="1833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Background Matting: The World is Your Green Screen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumyadip</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Jayaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2291" to="2300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improving image matting using comprehensive sampling sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Shahrian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepu</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="636" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep automatic portrait matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="92" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Membership inference attacks against machine learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Stronati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congzheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Privacy protection in street-view panoramas using depth and multiview imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ries</forename><surname>Uittenbogaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clint</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Vijverberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bas</forename><surname>Boom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10581" to="10590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Optimized color sampling for robust matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael F Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2970" to="2979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">A Study of Face Obfuscation in ImageNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacqueline</forename><surname>Yau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.06191</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Privacy-preserving visual learning using doubly permuted homomorphic encryption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryo</forename><surname>Yonetani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naresh</forename><surname>Vishnu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Boddeti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoichi</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2040" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Mask Guided Matting via Progressive Refinement Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1154" to="1163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Towards high performance human keypoint detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Empowering things with intelligence: a survey of the progress, challenges, and opportunities in artificial intelligence of things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="7789" to="7817" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A Late Fusion CNN for Digital Matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunke</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiran</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7469" to="7478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning based digital matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjie</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Kambhamettu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th international conference on computer vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="889" to="896" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
