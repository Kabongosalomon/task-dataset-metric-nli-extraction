<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. 1 Meta-RangeSeg: LiDAR Sequence Semantic Segmentation Using Multiple Feature Aggregation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianke</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Ruixiang</forename><surname>Zhang</surname></persName>
						</author>
						<title level="a" type="main">IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. 1 Meta-RangeSeg: LiDAR Sequence Semantic Segmentation Using Multiple Feature Aggregation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-3D semantic segmentation</term>
					<term>LiDAR perception</term>
					<term>autonomous vehicle</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>LiDAR sensor is essential to the perception system in autonomous vehicles and intelligent robots. To fulfill the realtime requirements in real-world applications, it is necessary to efficiently segment the LiDAR scans. Most of previous approaches directly project 3D point cloud onto the 2D spherical range image so that they can make use of the efficient 2D convolutional operations for image segmentation. Although having achieved the encouraging results, the neighborhood information is not wellpreserved in the spherical projection. Moreover, the temporal information is not taken into consideration in the single scan segmentation task. To tackle these problems, we propose a novel approach to semantic segmentation for LiDAR sequences named Meta-RangeSeg, where a new range residual image representation is introduced to capture the spatial-temporal information. Specifically, Meta-Kernel is employed to extract the meta features, which reduces the inconsistency between the 2D range image coordinates input and 3D Cartesian coordinates output. An efficient U-Net backbone is used to obtain the multiscale features. Furthermore, Feature Aggregation Module (FAM) strengthens the role of range channel and aggregates features at different levels. We have conducted extensive experiments for performance evaluation on SemanticKITTI and Semantic-POSS. The promising results show that our proposed Meta-RangeSeg method is more efficient and effective than the existing approaches. Our full implementation is publicly available at https://github.com/songw-zju/Meta-RangeSeg.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>L IDAR can accurately measure the range by taking advantage of its active sensor, which plays an increasingly important role in the perception system of modern autonomous vehicles and robotics. Due to the characteristics of disorder and irregularity in point cloud, it is challenging to perform scene understanding on LiDAR sequences.</p><p>LiDAR semantic segmentation aims to estimate the labels for each point, which is the key to understand the surroundings for the perception system. During past decade, extensive research efforts have been devoted to this task. Point-based methods <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref> directly extract features from the raw output of LiDAR sensor. However, point convolution is usually computational intensive. To address this issue, projectionbased methods <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref> and voxel-based approaches <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b8">[9]</ref> convert the irregular raw point cloud data into regular grid Our presented Meta-RangeSeg method obtains the promising results on both multiple scans and single scan semantic segmentation in SemanticKITTI benchmark <ref type="bibr" target="#b12">[13]</ref> and runs at real-time.</p><p>representations so that the conventional convolutional layer for image can be employed. Nevertheless, they fail to preserve the original neighborhood relationship. In practice, the hybrid methods <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b11">[12]</ref> fuse two or more of the above feature representations, which can obtain better results. Unfortunately, this incurs the extra computational load. Generally, scene analysis for autonomous driving is conducted within a sequence of LiDAR scans. Most of previous approaches only take into account of single frame, where the important temporal information is usually ignored. Moreover, some methods <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> aim to deal with multiple scans simultaneously. This may lead to information redundancy and slow inference speed, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>To tackle the above challenges, we propose a novel approach to semantic segmentation on LiDAR sequences named Meta-RangeSeg. To this end, a new range residual image representation is introduced to capture the spatial-temporal information. In contrast to the direct fusion methods, our proposed range residual image efficiently represents multi-frame point cloud information, which can improve the accuracy and the speed of training and inference under the limited computing resources. Since the range residual image obtained from spherical projection may not effectively capture the local geometric structures, we take advantage of the Meta-Kernel operator <ref type="bibr" target="#b15">[16]</ref> to extract the meta features by dynamically learning the weights from the relative Cartesian coordinates and range values. Thus, it reduces the inconsistency between the 2D range image coordinates input and Cartesian coordinates output. Moreover, an efficient U-Net backbone <ref type="bibr" target="#b16">[17]</ref> is used to obtain the multiscale features. Feature Aggregation Module (FAM) aggregates the meta features and multi-scale features with range guided information. We have conducted extensive experiments for performance evaluation on SemanticKITTI <ref type="bibr" target="#b12">[13]</ref> and Seman-ticPOSS <ref type="bibr" target="#b17">[18]</ref> datasets. The promising results show that our proposed method is more efficient and effective than the existing approaches.</p><p>In summary, the main contributions of this paper are: 1) a novel framework for semantic segmentation on LiDAR sequences by taking advantage of range residual image, which is able to capture the spatial-temporal information efficiently; 2) an effective Meta-Kernel based feature extraction method for LiDAR semantic segmentation; 3) a Feature Aggregation Module (FAM) to aggregate features at various scales and levels for range-based object segmentation; 4) experiments on SemanticKITTI and SemanticPOSS benchmark show that our proposed approach is promising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>With the prevalence of autonomous driving, a surge of research efforts have been spent on semantic scene understanding <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. In this work, we focus on the task of semantic segmentation using LiDAR scans <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b17">[18]</ref>. Generally, most of existing studies on LiDAR semantic segmentation can be categorized into four groups according to the different feature representations, including point, projectionbased image, voxel and hybird.</p><p>Point-based methods directly extract features from the raw point cloud data, which are able to preserve the 3D spatial structure information. Due to irregularity of point cloud data, it is challenging to design the efficient neural network layer for it. Qi et al. <ref type="bibr" target="#b0">[1]</ref> extract the deep features on point cloud by the shared Multi-Layer Perceptrons (MLP) for classification and segmentation. The subsequent series of works <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b19">[20]</ref> try to address the limitation in extracting local features, which obtain the encouraging results on the indoor semantic segmentation. The main showstopper for these approaches is their high computational cost and memory consumption, which hinders them from the large-scale outdoor driving scenarios. One remedy is to reduce their time complexity and information loss by randomly sampling and local feature aggregation <ref type="bibr" target="#b2">[3]</ref>. Despite of its efficiency on the large scenes, there is noticeable performance drop due to sub-sampling.</p><p>Voxel representation is able to make use of the 3D convolution neural network that can effectively solve the irregularity problem of point cloud. The regular 3D dense convolution for semantic segmentation requires the huge memory and heavy computational power for the fine resolution, which limit their capability of processing the large scale outdoor LiDAR scans. To this end, the sparse convolution <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> is employed to reduce the computational cost. Zhu et al. <ref type="bibr" target="#b8">[9]</ref> propose a cylindrical voxel division method with asymmetric convolution based on LiDAR point cloud distribution.</p><p>By projecting 3D point cloud onto 2D space, range image is a promising representation, which can take advantage of a large amount of advanced layers for image feature extraction with fast training and inference. To account for the mechanism of LiDAR scanning, most of existing LiDAR semantic segmentation approaches <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> make use of spherical projection to obtain range images. Besides range view (RV), Zhang et al. <ref type="bibr" target="#b22">[23]</ref> and Wen et al. <ref type="bibr" target="#b23">[24]</ref> employ a bird's-eye view (BEV) for semantic segmentation. Some studies <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> combine these two projection methods in order to achieve more accurate segmentation results. However, this will lead to the extra memory consumption and computational overhead. Moreover, directly fusing two different projections ignores the underlying geometric structure of LiDAR scan.</p><p>The hybrid approaches intend to fuse the different feature representations for better LiDAR semantic segmentation. Zhang et al. <ref type="bibr" target="#b9">[10]</ref> propose a point-voxel feature aggregation module that aggregates features among neighborhood voxels and produces point-wise prediction. Thereby, it is able to avoid the time-consuming neighborhood search while achieving the encouraging results on outdoor LiDAR data. Tang et al. <ref type="bibr" target="#b10">[11]</ref> present an efficient point-voxel fusion pipeline. Voxels provide the coarse-grained local features, and points preserve the finegrained geometric features through a simple MLP. Xu et al. <ref type="bibr" target="#b11">[12]</ref> fuse three different feature representations, including point, range image and voxel, which achieve the promising fusion results by interacting features at various stages. Besides, Zhuang et al. <ref type="bibr" target="#b26">[27]</ref> try to fuse the multiple modalities like image and point cloud.</p><p>Most of existing approaches perform the LiDAR semantic segmentation on single scan, where the temporal information is usually neglected. There are only few methods focusing on the multiple scans task. Shi et al. <ref type="bibr" target="#b14">[15]</ref> employ a voxel-based 3D sparse convolutional network to fuse local information from the previous and current frames through local interpolation, which only make use of the two consecutive scans. Duerr et al. <ref type="bibr" target="#b27">[28]</ref> propose a novel recurrent segmentation framework using range images, which recursively aggregate the features of previous scans in order to exploit the short term temporal dependencies. Sch?tt et al. <ref type="bibr" target="#b28">[29]</ref> extend the original Lat-ticeNet <ref type="bibr" target="#b29">[30]</ref> with a novel abstract flow module for temporal semantic segmentation. In <ref type="bibr" target="#b8">[9]</ref>, accumulating point clouds in 3D space is adopted for multiple scans segmentation, whose memory consumption and computational time increase linearly with the total number of scans per input model. In this paper, we introduce an efficient range residual image representation, where the effective features can be extracted by Meta-Kernel and U-Net backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. META-RANGESEG FOR LIDAR SEMANTIC SEGMENTATION</head><p>In this section, we present an efficient neural network Meta-RangeSeg for LiDAR semantic segmentation on multiple scans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>In this paper, we aim to predict the semantic labels from the consecutive LiDAR sequences. Unlike the conventional approaches transforming the sequential point cloud into global coordinates <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b14">[15]</ref> in 3D space, we suggest a novel approach named Meta-RangeSeg to efficiently process multiple scans in range view for the subsequent feature extraction. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, our proposed network takes advantage of the range residual image with nine channels built from the current scan and previous ones. Then, the meta features are extracted by a Meta-Kernel block, and the multi-scale features are obtained via a U-Net network. Finally, we get the semantic labels for raw data by post-processing the aggregated features. In the following, we will give the detailed description of range residual image and our network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Range Residual Image</head><p>The traditional range image is a multi-channel pseudo image obtained by spherical projection of the LiDAR point cloud. Each channel represents range (r), x, y, z and remission (e) sequentially. Range image representation has the advantage of using the effective 2D operations for fast training and inference. To this end, we map the scattered LiDAR points into their corresponding 2D spherical coordinates through a mapping function R 3 ? R 2 as below:</p><formula xml:id="formula_0">u v = 1 2 1 ? arctan(y, x) ? ?1 W 1 ? arcsin(z r ?1 ) + f up f ?1 H ,<label>(1)</label></formula><p>where (H, W ) are the height and width of the range image, and r = x 2 + y 2 + z 2 is the range value of the point in 3D space. (u, v) are the image coordinates under range view. f = f up + f down is the vertical field-of-view of the LiDAR. Motivated by the residual image feature for segmenting moving objects in video analysis <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, we introduce it into the task of semantic segmentation on LiDAR sequences to capture the temporal information. Based on the range images from current scan and previous frames using spherical projection, the input of our proposed neural network is made of range images and their residuals, as illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref></p><formula xml:id="formula_1">. Specifically, it is a range residual image with the size of 9 ? H ? W , where each pixel (u, v) contains a vector (r, x, y, z, e, d 1 , d 2 , d 3 , m).</formula><p>The mask m indicates whether the pixel position is a projected point or not. d k (k = 1, 2, 3..) is from the residual image that calculates the range differences between the previous k th scan and the current one. We use the last three LiDAR scans in our implementation.</p><p>To effectively fuse different scans, residual image is calculated in the following operations. We firstly transform the point clouds of previous frames into the coordinate of current frame. Then the transformed point clouds are projected into the range view with Eq. (1). Finally, the residual value d k for each pixel is obtained by calculating the absolute differences between the ranges of current scan and the transformed one with normalization as</p><formula xml:id="formula_2">d k = |r ? r k | r ,<label>(2)</label></formula><p>where r is the range value in current scan, and r k is the corresponding one from the transformed point cloud projected at the same image coordinate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Feature Extraction</head><p>To facilitate the effective LiDAR semantic segmentation, we design a feature extraction module that consists of a Meta-Kernel block and a U-Net backbone, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>In our empirical study, the conventional convolution operations do not perform well on the range residual image. To address this issue, we take advantage of the Meta-Kernel block to extract the meta features by dynamically learning the weights from the relative Cartesian coordinates and range values. As in <ref type="bibr" target="#b15">[16]</ref>, the Meta-Kernel is designed to effectively locate objects in LiDAR scans by exploiting the geometric information from the Cartesian coordinates. In this paper, we employ it to capture the spatial and temporal information for semantic segmentation.</p><p>In order to achieve a larger receptive field on the range residual image for Meta-Kernel, we enlarge the size of sliding window into 5 ? 5. Therefore, we can get the relative Cartesian coordinates of 25 neighbors p j (x j , y j , z j ) for </p><formula xml:id="formula_3">i (x i , y i , z i ) : (x j ? x i , y j ? y i , z j ? z i ).</formula><p>More importantly, the range difference is added to enhance the perception of spatial information. Furthermore, a shared MLP is employed to generate 25 weight vectors from the input</p><formula xml:id="formula_4">(r j ? r i , x j ? x i , y j ? y i , z j ? z i ).</formula><p>We multiply the learned weight vectors w j element-wisely with the channel information (r j , x j , y j , z j , e j , d 1,j , d 2,j , d 3,j , m j ) at its corresponding position within the sliding window. Finally, a 1?1 convolution is used to obtain the meta features, which aggregates the information from different channels and different sampling locations. From the above all, the whole process of Meta-Kernel block is summarized into <ref type="figure" target="#fig_2">Fig. 3</ref>.</p><p>We obtain the multi-scale features through the U-Net backbone that is an encoder-decoder architecture commonly used in semantic segmentation <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b16">[17]</ref>. Specifically, we firstly employ four down-sampling layers to extract the features of different scales from meta features, and then restore the original resolution through four up-sampling layers. Moreover, skip connection is adopted to assist in reconstructing highresolution semantic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Post Processing</head><p>Once the meta and multi-scale features are obtained, we perform feature aggregation to predict the labels from range perspective. To this end, the Feature Aggregation Module is designed to make use of range information for object segmentation in the different ranges by aggregating features at various scales and levels, as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>.</p><p>As the x, y and z information has been encoded in the horizontal and vertical coordinates under range view, the range channel is the most important one in range residual image. Thus, they are extracted separately. It is further fed into the context module <ref type="bibr" target="#b5">[6]</ref> to capture the global features with more detailed range context. Then, we fuse the range features with multi-scale features through an attention fusion layer. To this extent, the range-guided features are able to aggregate the range context information and semantic features. Furthermore, the meta features are reused through skip connections. We concatenate the meta features with the range-guided features. Afterwards, the final features are fused through the convolution and residual connection. Thus, we can obtain the 2D semantic labels under the range view via a 1 ? 1 convolution layer.</p><p>To estimate the labels in 3D space from 2D predictions in range view, we employ k-Nearest Neighborhood in the postprocessing stage. During the spherical projection, there may be multiple points projected into the same grid. We sort the points according to their ranges, and the characteristics of the closer points within the range shall prevail. When recovering 3D information from 2D range residual image, we need to supplement the features of those missing points. In k-NN , the label of each point is jointly determined by its k closest points. Instead of using Euclidean distance, range is employed as the similarity measure so that we can efficiently process data using the sliding windows in 2D space. As described in <ref type="bibr" target="#b4">[5]</ref>, the knearest points within a window can represent the distribution in 3D space very well. In our implementation, we set k = 5 with a 7 ? 7 sliding window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Loss Function</head><p>To facilitate the effective semantic segmentation, we train the proposed neural network by the loss function L with three different terms as follows:</p><formula xml:id="formula_5">L = w 1 L wce + w 2 L ls + w 3 L bd ,<label>(3)</label></formula><p>where L wce is the weighted cross-entropy loss, and L ls is the Lov?sz loss. L bd is the Boundary loss. w 1 , w 2 and w 3 are the weights with respect to each term. In our implementation, we set w 1 = 1, w 2 = 1.5 and w 3 = 1, empirically.</p><p>To account for the multi-class segmentation problem, the weighted cross-entropy loss L wce <ref type="bibr" target="#b32">[33]</ref> is employed to maxi-mize the prediction accuracy for point labels, which is able to balance the distributions among different classes. It weights the cross-entropy loss of every class with the corresponding frequency f i as</p><formula xml:id="formula_6">L wce (y,?) = ? i 1 ? f i p (y i ) log (p (? i )) ,<label>(4)</label></formula><p>where y i represents the ground truth, and? i is prediction. The Lov?sz loss L ls <ref type="bibr" target="#b33">[34]</ref> is used to maximize the intersection-over-union (IoU) score that is commonly used to in performance evaluation on semantic segmentation. Since IoU is discrete and indifferentiable, it needs to be optimized using a derivable surrogate function. We define a vector of pixel errors m(c) of each pixel i on class c with its predicted probability f i (c) ? [0, 1] and ground truth label</p><formula xml:id="formula_7">y i (c) ? {?1, 1}: m i (c) = 1 ? f i (c) if c = y i (c), f i (c) otherwise<label>(5)</label></formula><p>We use the Lov?sz extension <ref type="bibr" target="#b33">[34]</ref> for the vector of errors m(c) to construct the loss ? Jc surrogate to ? Jc . Then, L ls can be formulated as below:</p><formula xml:id="formula_8">L ls = 1 |C| c?C ? Jc (m(c)),<label>(6)</label></formula><p>where |C| denotes the total number of classes. As suggested in <ref type="bibr" target="#b34">[35]</ref>, the pixel-level loss function like crossentropy may not effectively handle the complex boundaries between different classes in remote sensing images that have a wide range and low contrast. To emphasize the boundaries between different objects, we adopt the boundary loss function L bd <ref type="bibr" target="#b34">[35]</ref> for LiDAR semantic segmentation. Given the extracted boundary image y b for ground truth y and? b for the predicted result? in the range view, L bd is defined as below:</p><formula xml:id="formula_9">L bd (y,?) = 1 ? 2P c b R c b P c b + R c b ,<label>(7)</label></formula><p>where P c b and R c b define the precision and recall of predicted boundary image? b to real one y b for class c. The boundary image is computed as follows:</p><formula xml:id="formula_10">y b = pool (1 ? y, ? 0 ) ? (1 ? y) y b = pool (1 ??, ? 0 ) ? (1 ??)<label>(8)</label></formula><p>where pool(?, ?) employs a pixel-wise max-pooling. It operates on the inverted ground truth binary map or predictions with a sliding window of size ? 0 = 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENT</head><p>In this section, we give the details of our experiments and show the results on LiDAR semantic segmentation. Moreover, we compare our proposed approach against the state-of-the-art methods and discuss the results on different settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>The SemanticKITTI dataset <ref type="bibr" target="#b12">[13]</ref> is a large-scale outdoor scene LiDAR dataset, which provides the complete pointwise labels for all 22 sequences in KITTI Odometry Benchmark <ref type="bibr" target="#b35">[36]</ref>. Sequence 00 to 10 are treated as the training sets, and Sequence 11 to 21 are used as test sets. There are 23,201 and 20,351 complete 3D scans for training and testing, respectively. We follow the setting in <ref type="bibr" target="#b12">[13]</ref>, and keep Sequence 08 as the validation set. To evaluate the effectiveness of our proposed approach, we submit the output to the online evaluation website to obtain the results on the testing set without the extra tricks like test time augmentation, fine-tuning on the validation set, or the pre-trained models.</p><p>The SemanticPOSS dataset <ref type="bibr" target="#b17">[18]</ref> is also collected by the LiDAR scanner in outdoor scene, which is more sparse comparing to SemanticKITTI. It contains 2,988 LiDAR scans captured at the campus with large quantity of dynamic instances. SemanticPOSS is divided into six parts with the same size. As in <ref type="bibr" target="#b17">[18]</ref>, we employ the part 3 for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Metric</head><p>To facilitate the fair comparison, we evaluate the performance of different methods with respect to the mean intersection over union metric (mIoU) <ref type="bibr" target="#b12">[13]</ref>, which is defined as below:</p><formula xml:id="formula_11">mIoU = 1 n n c=1 T P c T P c + F P c + F N c .<label>(9)</label></formula><p>For class c, T P c represents the true positives, and F P c denotes false positives. F N c is false negative predictions. In SemanticKITTI benchmark, the single scan task evaluates 19 different classes (n = 19). On the other hand, the multiple scans task evaluates 25 different classes (n = 25), which needs to distinguish more than 6 moving classes comparing to the single-scan challenge. In SemanticPOSS benchmark, 11 different classes are evaluated in the single-scan task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training Settings</head><p>We trained the proposed neural network for 180 epochs using the stochastic gradient descent (SGD) on a PC with two RTX 2080Ti GPUs. The total batch size is 4. Moreover, the initial learning rate is set to 0.01 with a decay of 0.01 at every epoch. We conducted the inference on a single RTX 2080Ti GPU. The height and width of the range residual image are set to H = 64, and W = 2048, respectively. During the training process, we perform data augmentation by randomly rotating, transforming, and flipping the 3D point cloud. In addition, we randomly drop the points at a percentage with a uniform distribution between 0 and 10 before creating the range residual image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Performance Evaluation</head><p>For the quantitative evaluation, we compare our proposed approach against the previous methods on the multiple scans semantic segmentation benchmark, which is our main focus. As shown in <ref type="table" target="#tab_0">Table I</ref>, our presented Meta-RangeSeg method   outperforms the state-of-the-art approach <ref type="bibr" target="#b28">[29]</ref> by 2.6% on SemanticKITTI testing set without extra tricks. It is worthy of mentioning that our approach can process the multiple scans point clouds at a rate of 22Hz while maintaining high accuracy. It is faster than the frame rate (10Hz) of the sensor used in SemanticKITTI dataset.</p><formula xml:id="formula_12">? ? bicyclist ????? person ? ?? ? motorcyclist ? ?????? ? other-vehicle ? ??????? ? truck ? ? ? TangentConv</formula><p>Considering that there are few methods in the multiple scans evaluation, we conduct experiments on the single scan task of SemanticKITTI and SemanticPOSS to illustrate the generalization capability of our proposed approach. For a fair comparison, we only use the range image by excluding the residual image when extracting features under range view for the single scan evaluation. As shown in <ref type="table" target="#tab_0">Table II and Table III</ref>, Meta-RangeSeg outperforms most of previous methods under the same settings, including point-wise and projection-based methods, which demonstrates the effectiveness of our proposed approach on LiDAR semantic segmentation.</p><p>For the qualitative evaluation 1 , <ref type="figure" target="#fig_5">Fig. 5</ref> shows a sample of semantic segmentation results on the SemanticKITTI validation set. The results under range view are generated by our method with the complete semantic labels. To investigate the performance of our approach on multiple scans semantic segmentation, we take the car class including static and moving 1 Supplementary video: https://youtu.be/xUFsmmjZYuA as example to compare the predicted visual results with the ground truth and predictions generated by SpsequenceNet <ref type="bibr" target="#b14">[15]</ref>, as illustrated in <ref type="figure" target="#fig_6">Fig. 6</ref>. It can be observed that our approach can effectively distinguish both static and moving objects with their semantic information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Study</head><p>To examine the improvements of each individual module in our proposed network, we conduct the ablation studies on Sub-SemanticKITTI dataset, which is a subset of the original Se-manticKITTI. The training set of Sub-SemanticKITTI dataset consists of every 8th frame in sequence 00-10 (except 08). The validation set is formed by every 4th frame in sequence 08. As only 1/8 training data and 1/4 validation data are used, we can quickly perform the evaluation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>SpsequenceNet Meta-RangeSeg(Ours) . At each row, the left is the ground truth, the middle is the predictions by SpsquencesNet <ref type="bibr" target="#b14">[15]</ref>, and the right is the results of our proposed Meta-RangeSeg approach. For fair comparison, we treat SalsaNext <ref type="bibr" target="#b5">[6]</ref> with a 5channel range image as the baseline method, which uses the similar backbone network as ours. In our experiments, we report the results on multiple scans semantic segmentation task. As shown in <ref type="table" target="#tab_0">Table IV</ref>, our proposed range residual image (R 2 Image) with 9 channels outperforms the baseline around 0.5% , which demonstrates that the range residual is effective. Moreover, the Meta-kernel block obtains over 0.9% improvement comparing the method without it. Furthermore, FAM performs better than the network without it around 0.9%. Additionally, the boundary loss achieves over 1.8% performance gain, which indicates that the boundary regions are essential to the LiDAR semantic segmentation. Then, we remove each module individually from the complete framework while the results in accuracy drop by 1.6%-2.7%, separately. Finally, it can be observed that our proposed Meta-RangeSeg approach outperforms the baseline over 4.1% with only 1.2% (i.e., 0.08M) extra parameters, which demonstrates the efficacy of each module. Although having added the extra blocks into the backbone, the inference speed of the final model is not greatly affected. This is mainly due to the concise input and our proposed lightweight modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>This paper proposed a novel approach to LiDAR semantic segmentation, which introduced a range residual image representation to capture the spatial-temporal information. Moreover, we employed Meta-Kernel to extract the meta features from the residual image by dynamically learning the weights from the relative Cartesian coordinates and range values. Furthermore, we designed a Feature Aggregation Module to aggregate the features at various scales and levels while emphasizing the range channel. We have conducted extensive evaluations on SemanticKITTI and SemanticPOSS benchmark, whose promising results demonstrated that our proposed Meta-RangeSeg approach not only outperforms the state-of-the-art semantic segmentation methods on multiple scans benchmark but also runs at 22 FPS.</p><p>Since some modules are not fully optimized, our model consumes more memory. In future, we will try to design an effective model that can make use of the connections between each channel in range residual image.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Accuracy vs. inference time. * denotes the results reproduced from the original implementation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Our proposed Meta-RangeSeg framework. (a) Calculate range residual image with nine channels capturing spatial and motion information of raw data. (b) Extract meta features by Meta-Kernel and obtain multi-scale features via U-Net backbone. (c) Aggregate features and get semantic labels in 3D space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Illustration of Meta-Kernel. We sample the range residual image using a 5 ? 5 sliding window, and learn weights dynamically from the relative Cartesian coordinates and range value to extract meta features. the center p</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Illustration of Feature Aggregation Module (FAM). We fistly extract the features of the range channel separately, and then fuse the multi-scale and the range features to obtain the range-guided features. Finally, the meta and range-guided features are aggregated via skip connection, and the 2D label under range view is obtained through 1 ? 1 convolution layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>] 41.6 -84.1 30.4 32.9 20.2 20.7 7.5 0.0 0.0 91.6 64.9 75.3 27.5 85.2 56.5 78.4 50.7 64.8 38.1 53.3 61.5 14.1 15.2 0.2 28.9 37.8 SpSequenceNet [15] 43.1 3 88.5 24.0 26.2 29.2 22.7 6.3 0.0 0.0 90.1 57.6 73.9 27.1 91.2 66.8 84.0 66.0 65.7 50.8 48.7 53.2 41.2 26.2 36.2 2.3 0.1 TemporalLidarSeg [28] 47.0 30* 92.1 47.7 40.9 39.2 35.0 14.4 0.0 0.0 91.8 59.6 75.8 23.2 89.8 63.8 82.3 62.5 64.7 52.6 60.4 68.2 42.8 40.4 12.9 12.4 2.1 TemporalLatticeNet [29] 47.1 6.5 91.6 35.4 36.1 26.9 23.0 9.4 0.0 0.0 91.5 59.3 75.3 27.5 89.6 65.3 84.6 66.7 70.4 57.2 60.4 59.7 41.7 9.4 48.8 5.9 0.0 Meta-RangeSeg(Ours) 49.7 22 90.8 50.0 49.5 29.5 34.8 16.6 0.0 0.0 90.8 62.9 74.8 26.5 89.8 62.1 82.8 65.7 66.5 56.2 64.5 69.0 60.4 57.9 22.0 16.6 2.6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Semantic segmentation results from the range view on the SemanticKITTI validation set (sequence 08) [best view in color]. In each scene, we show the range channel image, predictions by Meta-RangeSeg, and ground truth in turn.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Segmentation results of static and moving cars on the SemanticKITTI validation set (sequence 08). The static cars are labeled in blue, and moving cars are in red [best view in color]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARISONS</head><label>I</label><figDesc>ON THE SEMANTICKITTI MULTIPLE SCANS BENCHMARK. THE ITEM WITH ARROW INDICATES THE MOVING CLASS. VALUES ARE GIVEN AS IOU (%). * DENOTES THE FPS MEASURED ON A TESLA V100 GPU, WHILE OURS ARE TAKEN ON A SINGLE RTX 2080TI GPU.</figDesc><table><row><cell>Methods</cell><cell>mean-IoU</cell><cell>FPS (Hz)</cell><cell>car</cell><cell>bicycle</cell><cell>motorcycle</cell><cell>truck</cell><cell>other-vehicle</cell><cell>person</cell><cell>bicyclist</cell><cell>motorcyclist</cell><cell>road</cell><cell>parking</cell><cell>sidewalk</cell><cell>other-ground</cell><cell>building</cell><cell>fence</cell><cell>vegetation</cell><cell>trunk</cell><cell>terrain</cell><cell>pole</cell><cell>traffic sign</cell><cell>car</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II COMPARISONS</head><label>II</label><figDesc>ON THE SEMANTICKITTI SINGLE SCAN BENCHMARK. ? DENOTES THE SECOND BEST RESULTS. THE TOP-HALF SHOWS THE POINT-WISE METHODS, AND THE BOTTOM-HALF IS PROJECTION-BASED METHODS. * DENOTES THE RESULTS REPRODUCED FROM THE ORIGINAL IMPLEMENTATION. Ours) 64 ? 2048 61.0 ? 26 93.9 50.1 43.8 43.9 43.2 63.7 53.1 18.7 90.6 64.3 74.6 29.2 91.1 64.7 82.6 65.5 65.5 56.3 64.2</figDesc><table><row><cell>Methods</cell><cell>Size</cell><cell>mean-IoU</cell><cell>FPS (Hz)</cell><cell>car</cell><cell>bicycle</cell><cell>motorcycle</cell><cell>truck</cell><cell>other-vehicle</cell><cell>person</cell><cell>bicyclist</cell><cell>motorcyclist</cell><cell>road</cell><cell>parking</cell><cell>sidewalk</cell><cell>other-ground</cell><cell>building</cell><cell>fence</cell><cell>vegetation</cell><cell>trunk</cell><cell>terrain</cell><cell>pole</cell><cell>traffic-sign</cell></row><row><cell>PointNet [1]</cell><cell>50K pts</cell><cell cols="21">14.6 2 46.3 1.3 0.3 0.1 0.8 0.2 0.2 0.0 61.6 15.8 35.7 1.4 41.4 12.9 31.0 4.6 17.6 2.4 3.7</cell></row><row><cell>PointNet++ [2]</cell><cell>50K pts</cell><cell cols="21">20.1 0.1 53.7 1.9 0.2 0.9 0.2 0.9 1.0 0.0 72.0 18.7 41.8 5.6 62.3 16.9 46.5 13.8 30.0 6.0 8.9</cell></row><row><cell>SPLATNet [38]</cell><cell>50K pts</cell><cell cols="21">22.8 1 66.6 0.0 0.0 0.0 0.0 0.0 0.0 0.0 70.4 0.8 41.5 0.0 68.7 27.8 72.3 35.9 35.8 13.8 0.0</cell></row><row><cell>TangentConv [37]</cell><cell>50K pts</cell><cell cols="21">35.9 0.3 86.8 1.3 12.7 11.6 10.2 17.1 20.2 0.5 82.9 15.2 61.7 9.0 82.8 44.2 75.5 42.5 55.5 30.2 22.2</cell></row><row><cell>LatticeNet [30]</cell><cell>50K pts</cell><cell cols="21">52.9 7 92.9 16.6 22.2 26.6 21.4 35.6 43.0 46.0 90.0 59.4 74.1 22.0 88.2 58.8 81.7 63.6 63.1 51.9 48.4</cell></row><row><cell>RandLA-Net [3]</cell><cell>50K pts</cell><cell cols="21">53.9 1.3 94.2 26.0 25.8 40.1 38.9 49.2 48.2 7.2 90.7 60.3 73.7 20.4 86.9 56.3 81.4 61.3 66.8 49.2 47.7</cell></row><row><cell>KPConv [20]</cell><cell>50K pts</cell><cell cols="21">58.8 3.8 96.0 30.2 42.5 33.4 44.3 61.5 61.6 11.8 88.8 61.3 72.7 31.6 90.5 64.2 84.8 69.2 69.1 56.4 47.4</cell></row><row><cell>BAAF-Net [39]</cell><cell>50K pts</cell><cell cols="21">59.9 4.8 95.4 31.8 35.5 48.7 46.7 49.5 55.7 33.0 90.9 62.2 74.4 23.6 89.8 60.8 82.7 63.4 67.9 53.7 52.0</cell></row><row><cell>RangeNet53++ [5]</cell><cell cols="22">64 ? 2048 52.2 12 91.4 25.7 34.4 25.7 23.0 38.3 38.8 4.8 91.8 65.0 75.2 27.8 87.4 58.6 80.5 55.1 64.6 47.9 55.9</cell></row><row><cell>PolarNet [23]</cell><cell cols="22">[480, 360, 32] 54.3 16 93.8 40.3 30.1 22.9 28.5 43.2 40.2 5.6 90.8 61.7 74.4 21.7 90.0 61.3 84.0 65.5 67.8 51.8 57.5</cell></row><row><cell>MINet [40]</cell><cell cols="22">64 ? 2048 55.2 24 90.1 41.8 34.0 29.9 23.6 51.4 52.4 25.0 90.5 59.0 72.6 25.8 85.6 52.3 81.1 58.1 66.1 49.0 59.9</cell></row><row><cell>3D-MiniNet [22]</cell><cell cols="22">64 ? 2048 55.8 28 90.5 42.3 42.1 28.5 29.4 47.8 44.1 14.5 91.6 64.2 74.5 25.4 89.4 60.8 82.8 60.8 66.7 48.0 56.6</cell></row><row><cell>SqueezeSegV3 [21]</cell><cell cols="22">64 ? 2048 55.9 6 92.5 38.7 36.5 29.6 33.0 45.6 46.2 20.1 91.7 63.4 74.8 26.4 89.0 59.4 82.0 58.7 65.4 49.6 58.9</cell></row><row><cell>CNN-LSTM [24]</cell><cell cols="22">[512, 512, 32] 56.9 11 92.6 45.7 49.6 48.6 30.2 53.8 74.6 9.2 90.7 23.3 75.7 17.6 90.0 51.3 87.1 60.8 75.4 63.9 41.5</cell></row><row><cell>SalsaNext [6]</cell><cell cols="22">64 ? 2048 59.5 24 91.9 48.3 38.6 38.9 31.9 60.2 59.0 19.4 91.7 63.7 75.8 29.1 90.2 64.2 81.8 63.6 66.5 54.3 62.1</cell></row><row><cell>FIDNet [41]</cell><cell cols="22">64 ? 2048 59.5 29* 93.9 54.7 48.9 27.6 23.9 62.3 59.8 23.7 90.6 59.1 75.8 26.7 88.9 60.5 84.5 64.4 69.0 53.3 62.8</cell></row><row><cell>Lite-HDSeg [42]</cell><cell cols="22">64 ? 2048 63.8 20 92.3 40.0 55.4 37.7 39.6 59.2 71.6 54.1 93.0 68.2 78.3 29.3 91.5 65.0 78.2 65.8 65.1 59.5 67.7</cell></row><row><cell>Meta-RangeSeg(</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III COMPARISONS</head><label>III</label><figDesc>ON THE SEMANTICPOSS BENCHMARK. VALUES ARE GIVEN AS IOU (%). * DENOTES THE RESULTS REPRODUCED FROM THE ORIGINAL IMPLEMENTATION. Meta-RangeSeg(Ours) 53.7 75.5 19.8 78.7 25.6 27.5 72.3 32.3 49.0 78.0 52.9 79.3</figDesc><table><row><cell>Methods</cell><cell>mean-IoU</cell><cell>people</cell><cell>rider</cell><cell>car</cell><cell>traffic sign</cell><cell>trunk</cell><cell>plants</cell><cell>pole</cell><cell>fence</cell><cell>building</cell><cell>bike</cell><cell>road</cell></row><row><cell>SqueezeSegV2 [4]</cell><cell cols="12">29.8 18.4 11.2 34.9 11.0 15.8 56.3 4.5 25.5 47.0 32.4 71.3</cell></row><row><cell>RangeNet53++ [5]</cell><cell cols="12">28.9 14.2 8.2 35.4 6.8 9.2 58.1 2.8 28.8 55.5 32.2 66.3</cell></row><row><cell>UnpNet [43]</cell><cell cols="12">34.3 17.7 17.2 39.2 9.5 13.8 67.0 5.8 31.1 66.9 40.5 68.4</cell></row><row><cell>MINet [40]</cell><cell cols="12">35.1 20.1 15.1 36.0 15.5 23.4 67.4 5.1 28.2 61.6 40.2 72.9</cell></row><row><cell>SalsaNext [6]</cell><cell cols="12">49.4* 73.4 17.1 73.3 7.0 25.5 69.1 26.9 45.0 77.1 50.1 79.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV ABLATIVE</head><label>IV</label><figDesc>ANALYSIS EVALUATED ON VALIDATION SET (SEQ 08) IN SUB-SEMANTICKITTI DATASET.</figDesc><table><row><cell>Architecture</cell><cell>R 2 Image</cell><cell>Meta-Kernel FAM</cell><cell>Boundary Loss</cell><cell>mIoU</cell><cell>Params</cell><cell>FLOPs</cell></row><row><cell>Baseline</cell><cell></cell><cell></cell><cell></cell><cell>42.8</cell><cell>6.71M</cell><cell>125.74G</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>43.3</cell><cell>6.71M</cell><cell>125.76G</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>44.2</cell><cell>6.66M</cell><cell>115.70G</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>45.1</cell><cell>6.79M</cell><cell>148.64G</cell></row><row><cell>Meta-RangeSeg</cell><cell></cell><cell></cell><cell></cell><cell>45.3</cell><cell>6.78M</cell><cell>147.30G</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>44.2</cell><cell>6.84M</cell><cell>158.70G</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>44.5</cell><cell>6.66M</cell><cell>115.70G</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>46.9</cell><cell>6.79M</cell><cell>148.64G</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Advances in Neural Information Processing Systems</title>
		<meeting>of the Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Randla-net: Efficient semantic segmentation of largescale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Squeezesegv2: Improved model structure and unsupervised domain adaptation for roadobject segmentation from a lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Intl. Conf.on Robotics and Automation</title>
		<meeting>of Intl. Conf.on Robotics and Automation</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4376" to="4382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rangenet++: Fast and accurate lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems</title>
		<meeting>of the IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4213" to="4220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Salsanext: Fast, uncertainty-aware semantic segmentation of lidar point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cortinhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzelepis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aksoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Intl. Symposium on Visual Computing</title>
		<meeting>of Intl. Symposium on Visual Computing</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="207" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9224" to="9232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sparse single sweep lidar point cloud segmentation via learning contextual shape priors from scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the AAAI Conference on Artificial Intelligence</title>
		<meeting>of the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3101" to="3109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cylindrical and asymmetrical 3d convolution networks for lidar-based perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep fusionnet for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Europ. Conf. on Computer Vision</title>
		<meeting>of the Europ. Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Searching efficient 3d architectures with sparse point-voxel convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Europ. Conf. on Computer Vision</title>
		<meeting>of the Europ. Conf. on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="685" to="702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rpvnet: A deep and efficient range-point-voxel fusion network for lidar point cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/CVF Intl. Conf. on Computer Vision</title>
		<meeting>of the IEEE/CVF Intl. Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semantickitti: A dataset for semantic scene understanding of lidar sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/CVF Intl. Conf. on Computer Vision</title>
		<meeting>of the IEEE/CVF Intl. Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9297" to="9307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pointmoseg: Sparse tensor-based end-to-end moving-obstacle segmentation in 3-d lidar point clouds for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="510" to="517" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spsequencenet: Semantic segmentation network on 4d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4574" to="4583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rangedet: In defense of range view for lidar-based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/CVF Intl. Conf. on Computer Vision</title>
		<meeting>of the IEEE/CVF Intl. Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2918" to="2927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Intl. Conf. on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting>of the Intl. Conf. on Medical Image Computing and Computer-Assisted Intervention</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semanticposs: A point cloud dataset with large quantity of dynamic instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Intelligent Vehicles Symposium</title>
		<meeting>of the IEEE Intelligent Vehicles Symposium</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="687" to="693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sensaturban: Learning semantics from urban-scale photogrammetric point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khalid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/CVF Intl. Conf. on Computer Vision</title>
		<meeting>of the IEEE/CVF Intl. Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6411" to="6420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Squeezesegv3: Spatially-adaptive convolution for efficient point-cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Europ. Conf. on Computer Vision</title>
		<meeting>of the Europ. Conf. on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3d-mininet: Learning a 2d representation from point clouds for fast and efficient 3d lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Riazuelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Montesano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="5432" to="5439" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Polarnet: An improved grid representation for online lidar point clouds semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9601" to="9610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hybrid cnn-lstm architecture for lidar point clouds semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="5811" to="5818" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Amvnet: Assertion-based multi-view fusion network for lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">E</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Widjaja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">J</forename><surname>Chong</surname></persName>
		</author>
		<idno>abs/2012.04934</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tornado-net: multiview total variation semantic segmentation with diamond inception module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gerdzhev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Razani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taghavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bingbing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Intl. Conf. on Robotics and Automation</title>
		<meeting>of the IEEE Intl. Conf. on Robotics and Automation</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9543" to="9549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Perception-aware multi-sensor fusion for 3d lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/CVF Intl. Conf. on Computer Vision</title>
		<meeting>of the IEEE/CVF Intl. Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">290</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Lidar-based recurrent 3d semantic segmentation with temporal memory alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Duerr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pfaller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Weigel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beyerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Intl. Conf. on 3D Vision</title>
		<meeting>of the Intl. Conf. on 3D Vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="781" to="790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Abstract flow for temporal semantic segmentation on the permutohedral lattice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Rosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Intl. Conf. on Robotics and Automation</title>
		<meeting>of the IEEE Intl. Conf. on Robotics and Automation</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Latticenet: Fast point cloud segmentation using permutohedral lattices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Rosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Robotics: Science and Systems</title>
		<meeting>of Robotics: Science and Systems</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Temporal segment networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2740" to="2755" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Moving object segmentation in 3d lidar data: A learning-based approach exploiting sequential data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wiesmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="6529" to="6536" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Advances in Neural Information Processing Systems</title>
		<meeting>of the Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The lov?sz-softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Triki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4413" to="4421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Boundary loss for remote sensing imagery semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bokhovkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burnaev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Intl. Symposium on Neural Networks</title>
		<meeting>of the Intl. Symposium on Neural Networks</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="388" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Intl. Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3887" to="3896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Splatnet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2530" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semantic segmentation for real point cloud scenes via bilateral augmentation and adaptive fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1757" to="1767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multiscale interaction for real-time lidar data segmentation on an embedded platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="738" to="745" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fidnet: Lidar point cloud semantic segmentation with fully interpolation decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems</title>
		<meeting>of the IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4453" to="4458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Lite-hdseg: Lidar semantic segmentation using lite harmonic dense convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Razani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taghavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bingbing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Intl. Conf. on Robotics and Automation</title>
		<meeting>of the IEEE Intl. Conf. on Robotics and Automation</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9550" to="9556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rethinking 3-d lidar point cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
