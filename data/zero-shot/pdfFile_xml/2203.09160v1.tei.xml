<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Biasing Like Human: A Cognitive Bias Framework for Scene Graph Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Chang</surname></persName>
							<email>xgchang@seu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Wang</surname></persName>
							<email>wangteng@seu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyin</forename><surname>Sun</surname></persName>
							<email>cysun@seu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Cai</surname></persName>
							<email>wzcai@seu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Biasing Like Human: A Cognitive Bias Framework for Scene Graph Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scene graph generation is a sophisticated task because there is no specific recognition pattern (e.g., looking at and near have no conspicuous difference concerning vision, whereas near could occur between entities with different morphology ). Thus some scene graph generation methods are trapped into most frequent relation predictions caused by capricious visual features and trivial dataset annotations. Therefore, recent works emphasized the "unbiased" approaches to balance predictions for a more informative scene graph. However, human's quick and accurate judgments over relations between numerous objects should be attributed to bias (i.e., experience and linguistic knowledge) rather than pure vision. To enhance the model capability, inspired by the "cognitive bias" mechanism, we propose a novel 3-paradigms framework that simulates how humans incorporate the label linguistic features as guidance of vision-based representations to better mine hidden relation patterns and alleviate noisy visual propagation. Our framework is model-agnostic to any scene graph model. Comprehensive experiments prove our framework outperforms baseline modules in several metrics with minimum parameters increment and achieves new SOTA performance on Visual Genome dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Scene graph generation refers to the vision task that detects objects and recognizes semantic relationships between different objects in an image. With graphbased representation, we could depict the semantic content of images in a structural and efficient way. Such a structural representation of images could greatly benefit downstream vision tasks, including image captioning <ref type="bibr" target="#b6">[7]</ref>  <ref type="bibr" target="#b35">[36]</ref>, visual question answering <ref type="bibr" target="#b41">[42]</ref>  <ref type="bibr" target="#b24">[25]</ref>  <ref type="bibr" target="#b28">[29]</ref>, and video understanding <ref type="bibr" target="#b29">[30]</ref>.</p><p>However, the application of scene graph is confined by full of low semantic predictions in previous scene graph models. The inferior predictions is a consequence of several factors. First, the relationship sample distributions in existing representative datasets are long-tailed. For example, the sum of relations on,in, of in the widely adopted Visual Genome dataset <ref type="bibr" target="#b10">[11]</ref> counts for more than 50% of the total <ref type="figure">(Figure.1a</ref>). Therefore the network shows a strong tendency to head relations and behaves poor on most tail classes. Besides, the image annotation is incomplete. <ref type="bibr">Those</ref>  thus falsely treated as negative samples during learning. As shown in Figure1b, Second, those tail relations (i.e., play, see) typically involve complex semantic meaning and large intra-class variations in images. Large intra-class variations combined with few samples make feature representation learning hard for those tail relations.</p><p>To address the aforementioned challenges, a large number of studies have been conducted to enhance quality of predictions. Concerning long-tailed data distribution issue, recent works strive for designing "debiased" methods, which re-design inference procedures or loss functions to obtain a balance prediction. Nonetheless, these "debiased" methods could be seen as a special form of "reweighting" but in a learnable way. As no extra feature enhancement modules are introduced, these methods improve predictions on tail categories at the expense of those head categories. As for better feature representation learning, different propagation mechanisms have been developed for modeling context <ref type="bibr" target="#b11">[12]</ref>. Nevertheless, it fails to achieve satisfactory performances due to noisy propagation. However, bias should not be alleviated arbitrarily. In human recognition pipeline, "Cognitive bias" <ref type="bibr" target="#b31">[32]</ref>, as a psychological activity, benefits our decisionmaking in uncertain and intricate circumstances with the help of language <ref type="bibr" target="#b30">[31]</ref>. Figure2 illustrates how cognitive bias impacts on relation cognition. Given a ball and woman entities, human first exploits experience of several most frequent occurrence relations between "ball" and "woman", these candidates are served as the alternative choices for later decisions . Next, due to the experience can not represent the current circumstance, more precise guesses can be obtained by focusing on the part that woman contact the ball, which is the typical criteria when determines the relation between a person and a object. Noticed that the criteria is not necessary to be intersection part of two entities, but affected by categories. In the end, by considering all objects in this scene, it is not hard to conclude a shopping scenario. Hence human can choose "hold" as best choice by alleviating univocal meaning of "ball" with restriction "selling".</p><p>In this work, inspired by three cognitive bias paradigms, we propose a language-based framework to enhance feature representations and reduce visual  : Given two categories "woman" and "ball", the relation distribution is generated by the experience of combinations. (Middle): Considering two objects properties (one belongs to goods, another is human-being ), intersect region (i.e. hands) indicates that their relation shell be hold, play rather than carry. (Right): For further determination, Global contexts are incorporated into a single object description by prefixing "selling" to ball, which provides a tendency to prefer hold. feature redundancy , by extracting prior knowledge of experience, linguistic local and global context respectively. Concretely, We first utilize object pair labels to simulate the experience over predicates to same space of prediction likelihoods in a supervised manner. Then, a language map module projects linguistic similarity as a channel-wised attention which focusing on local specific visual patterns that can impact final decisions. Moreover, a scene extractor module refines each object's linguistic representation by the global context. To our best knowledge, we are the first to introduce human cognitive bias in Scene graph generation task, and utilize labels as high dimension representations. We conduct widely and detailed experiments on Visual Genome Dataset to prove framework effectiveness. The main contributions include:</p><p>1. We propose a 3-paradigms cognitive bias framework that simulates human thinking procedure to effectively extracting informative relation patterns from images and remedy the trivial dataset problem. 2. we design 3 cognition-driven language feature representations as the guidance of visual-based message propagation in the form of bias. 3. Our proposed framework is flexible and lightweight that can be plug-in any out-of-shelf scene graph model. And we achieve new SOTA performance in three scene graph tasks on baseline BGNN <ref type="bibr" target="#b11">[12]</ref> with margins. <ref type="figure">(</ref>  <ref type="bibr" target="#b13">[14]</ref> harnessed sequential memory model(LSTM <ref type="bibr" target="#b8">[9]</ref> or GRU <ref type="bibr" target="#b2">[3]</ref>), but failed to adopt edge feature in context formulation. Later works <ref type="bibr" target="#b11">[12]</ref> [26] <ref type="bibr" target="#b37">[38]</ref> harness Graph neural network(GCN <ref type="bibr" target="#b9">[10]</ref>) processing node and edge features respectively, and focusing more on pair-wise information. <ref type="bibr" target="#b11">[12]</ref> applied a multi-stage graph message propagation between proposal entities and relationship representation. In <ref type="bibr" target="#b37">[38]</ref>  <ref type="bibr" target="#b5">[6]</ref>. serving as a plugin to generalized scene graph model.</p><p>Prior Knowledge: Current works addressed the fundamental role of prior knowledge in terms of the long-tail problem. The commonly adopted method is leveraging statistical results <ref type="bibr" target="#b40">[41]</ref> as addictions on the decision layer. For instance, "FREQ" <ref type="bibr" target="#b40">[41]</ref> directly summed prior distribution to model decision layer by counting "subject-relation-object" co-occurrence in dataset. However, the counting approach is hard to mimic real word distribution because of low-quality annotations in datasets. Therefore, an increased number of works strived for leveraging prior knowledge in perception procedure. <ref type="bibr" target="#b18">[19]</ref> designed a language module, which concatenated two retrained word vectors and projected representation to the same space of vision module for minimizing the distance of similar semantic features. <ref type="bibr" target="#b23">[24]</ref> entangled the perception and prior in a single model with shared parameters trained by multi-task learning. <ref type="bibr" target="#b27">[28]</ref> introduced a confidence estimation module to alleviate the error propagation by incorporating confidence estimation in graph node feature updating. <ref type="bibr" target="#b38">[39]</ref> incorporated external commonsense knowledge by unifying the formulation of scene graph and commonsense graph <ref type="bibr" target="#b0">[1]</ref> incorporates co-occurrence of objects and relations to form a knowledge graph as a constraint in graph message propagation. <ref type="bibr" target="#b39">[40]</ref> structured visual commonsense and proposed a cascaded fusion architecture for fusion. Our model combines both the decision layer and the feature processing layer's fusion.</p><p>Scene Graph Debasing: Due to the lang-tail problems, some researchers proposed the "unbiased" concept, which prevents model overfitting to most frequent categories. Early works simply adopted weighted loss or focal loss <ref type="bibr" target="#b14">[15]</ref> for balanced prediction. Later, Yan et al. <ref type="bibr" target="#b36">[37]</ref> designed a training scheme that loss weight is dynamically changed based on predicate's visual feature similarity, which is learned by minimizing the distance of similar predicate features. Nonetheless, vision patterns of relations are highly fickle, hence hard to learn. <ref type="bibr" target="#b26">[27]</ref> presented a general unbiased inference pipeline that utilized Total Direct Effect analysis. However, this approach did not enrich model capability, but also wiped out object labels information, which can bring out beneficial language cues. On the contrary, our approach exhaustively uses labels for assisting vision. Experiments in 4.3 support our hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Given an image I, scene graph generation task aims to predict object coordinates (bounding box) B ? R 4 , classes C ? R as well as relation R between objects.</p><p>The task can further denote as the optimization of P (R, B, C | I). For a graph structure, it normally consists of nodes and edges. We denote n i ? V as i-th object representation. e i,j ? E as edge representation from node i to j,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Framework Architecture</head><p>Our proposed cognition bias (C-bias) framework overall architecture is presented in Figure3a, which is consist of three paradigms. 1) the experience estimation module is to draw the relation distribution on supervision of joint possibility. This distribution is added on final model outputs. 2) the language map module (Figure3b) is to learn local interactions between pair labels and produce channelwised attention for visual features. 3) the scene extractor module (Figure3c) is to globally consider the linguistic context in an image, and then update the initial label embedding. Those three sub-modules construct an enhanced graph representation by introducing label features in the vision-based baseline model at the output, edge, node, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Object Detection Network:</head><p>Concretely, given an image, a pre-trained object detection network is used to obtain a predicted object location setB = {b i } n i=1 and a class set?</p><formula xml:id="formula_0">= {c i } n i=1</formula><p>with number n. Detection network can be written as:</p><formula xml:id="formula_1">P (B,? | I) = detector(I)<label>(1)</label></formula><p>where I is an input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Paradigm 1: Experience Estimation Module:</head><p>This module learns the joint likelihood of two objects over relationships. We expect it can output distributions that base on the experience of correlation between predicates and categories. The estimated experience can be acquired by MLP layers, with the input of pair label categories c i , c j and position embedding p ij . Here, we use different weight w s and w o to transform subject and object labels to their linguistic representations, respectively. The predicted distribution between i-th subject and j-th object d ij ? R Nr can be describe as:</p><formula xml:id="formula_2">p ij = ? p (x i , y i , w i , h i , x j , y j , w j , h j ),<label>(2)</label></formula><formula xml:id="formula_3">d ij = ? ? s (w T s c i ) : ? o (w T o c j ) : p ij .<label>(3)</label></formula><p>Where x, y, w, h are object coordination, width, height. ? p , ? s , ? o , ? are fully connect layer with activation function (e.g. RELU).</p><p>[:] is concatnation operation.</p><p>Here, unlike previous "FREQ" <ref type="bibr" target="#b40">[41]</ref> method that directly counts triblets sub, rel, obj co-occurrence frequency as prediction likelihood weights, we utilize the joint possibility as supervision signals to obtain a comprehensive and accurate relation distribution over tail categories. Suppose the dataset contains N o number of object categories and N r relation categories, the joint possibility of subject i with object j is denoted as:  whether it serves as a subject or object. For instance, Figure4a shows that "man" as the subject is possible having correlation with "eating". Inversely, in Figure4b, it is impossible. Finally, For learning distribution purpose, we choose cosine loss function between target P ij rel and predicted prediction d ij , which is denoted as:</p><formula xml:id="formula_4">P ij rel = s i ? o j .<label>(4)</label></formula><formula xml:id="formula_5">L est = N i,j 1 ? cos(d ij , P ij rel ).<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Paradigm 2: Language Map Module:</head><p>To incorporate the language feature with local linguistic context into visual feature, we design a "language mapping" (LM) operation to generate linguistic attention based on label pairs c i , c j and project it to the same dimensions as the edge (conventionally, it is the visual feature of union area). Say we got two predicted class labels c i and c j , the initial language map x ij is produced by the vector multiplication as follows:</p><formula xml:id="formula_6">x ij = (w T s c i ) T , w T o c j ,<label>(6)</label></formula><p>Where w s , w o ? R Dw are word embedding weights of subjects and objects. , is dot product operation. The language map initially can be regarded as a covariance matrix between subject and object word embeddings, which contains informative correlation information. Then, pooling and 2D-convolution are applied to gain channel-wised linguistic attention. The language map feature f LM ? R C?1?1 can be denoted as Eq.7.</p><formula xml:id="formula_7">f LM = ?(G N conv ...?(G 1 conv (G pooling (x ij )))).<label>(7)</label></formula><p>Where N is number of convlution 2D layers, G pooling can be Avgpooling, ? is RELU function. Finally, Eq.8 is applied to get final language attention bias l ij .</p><formula xml:id="formula_8">l ij = U psampling(f LM (x ij )),<label>(8)</label></formula><formula xml:id="formula_9">e ij = e ij + l ij .<label>(9)</label></formula><p>Where generated l ij ? R C?Dp?Dp is as the same dimension D p as ROIPooling feature e ij . Next, the original edge feature e ij is updated in Eq.9, as an item of bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Paradigm 3: Scene Extractor Module</head><p>The aforementioned paradigms both generate deterministic linguistic representations given the input label pair. However, the same word in different contexts could have different semantic meanings. Hence, the purpose of this module is to extract global semantics context into each category representation. Hence we leverage multi-head self-attention to achieve that. Differing from prior work <ref type="bibr" target="#b19">[20]</ref> taking visual object features as transformer input, this module only takes object labels concatenated with positions for context encoding. Specifically, our module consists of several stacked multi-head attention layers with a Feed Forward network (FF) to update each node (object) feature. Given an image, suppose there are n object proposals that are packed together as input I.</p><formula xml:id="formula_10">I = w T c c i : p i n i .<label>(10)</label></formula><p>Where w c is object's label embedding weights and p i is position embedding of i-th proposal as the same formulation as Eq.2.</p><p>[:] is concatenation operation. Then attention mechanism can be written as:</p><formula xml:id="formula_11">Attention (Q, K, V ) = softmax QK T ? d k V.<label>(11)</label></formula><p>Where Q = IW Q , K = IW K , V = IW V refer to query, key, value respectively, W Q , W K , W V corresponding to parameter matrices, d k is the dimension of key. So we formulate multi-head attention as:</p><formula xml:id="formula_12">Multihead (Q, K, V ) = ? (head 0 , head 1 ...head n ) ,<label>(12)</label></formula><p>where head i = Attention (Q, K, V ) .</p><p>Where ? is a MLP layer with the purpose of merging all attention heads. Finally, scene representation of i-th object s i can be described as:</p><formula xml:id="formula_14">FFN(x) = max (0, xW 1 + b 1 ) W 2 + b 2 ,<label>(14)</label></formula><formula xml:id="formula_15">s i = LayerNorm(x + FFN(x)).<label>(15)</label></formula><p>Where x is output of multi-head attention. W 1 , W 2 , b 1 , b 2 are FPN parameter matrices and biases. Then the i-th node feature n i is updated:</p><formula xml:id="formula_16">n i = s i ? n i .<label>(16)</label></formula><p>Where ? is concatenation operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">Baseline Model</head><p>After edge and object features are updated, any off-the-shelf scene graph generation network can be used. It could be either a message propagating method or a graph neural network, as long as it exerts node or edge features for prediction. This network can be described as: </p><formula xml:id="formula_17">P (R|I, B, C) = SGG(N ,?).<label>(17)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we first introduce the dataset and evaluation metrics we used as well as the implementation details. Then the comprehensive comparison experiments with other scene graph methods and ablation studies on three paradigms are conducted. Finally, we demonstrate quantitative results for illustration purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Protocol</head><p>Datasets: We train and evaluate on filtered Visual Genome split VG150 proposed in <ref type="bibr" target="#b34">[35]</ref> to keep consistency with other studies. The VG150 dataset contains the most frequent 150 objects and 50 relationships with 108k images, in which 70% images are held out for training and 30% for testing. Among the training set, 5000 images are fetched for evaluation.</p><p>Tasks: 1) predicate classification (PredCls) task predicts predicates given the ground truth locations and classes. 2) scene graph classification (SGCls) task recognizes predicates and classes given locations. scene graph generation (SGGen) predicts valid objects (IoU &gt; 0.5) and classes as well as predicates.</p><p>Metrics: Following the convention of the previous studies, We take mean re-call@k, Zero-Shot Recall <ref type="bibr" target="#b18">[19]</ref>, and No Graph Constraint mean Recall <ref type="bibr" target="#b20">[21]</ref> as evaluation metrics. We do not report recall@k <ref type="bibr" target="#b27">[28]</ref> because it mainly focuses on low-semantic relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>All models are training on two NVIDIA Titan XP GPUs. The learning scheduler is WarmupReduceLROnPlateau with decay factor 0.6 and patient 6. The base learning rate is 0.08,0.08,0.06, and batch size is 12. We choose the SGD optimizer for optimization.</p><p>Detector: We use pretrained faster-rcnn <ref type="bibr" target="#b22">[23]</ref> with backbone ResNeXt-101-FPN <ref type="bibr" target="#b7">[8]</ref> . We froze its weights during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C-bias Framework:</head><p>We test BGNN <ref type="bibr" target="#b11">[12]</ref>, Motifs <ref type="bibr" target="#b40">[41]</ref> and G-RCNN <ref type="bibr" target="#b37">[38]</ref> as baseline. For a fair comparison, all configurations in our proposed network will be identical to baseline settings, including node and edge feature dimensions.</p><p>Noticed that for each baseline model, we choose the same configurations for all 3 tasks. We use pre-trained glove.6B <ref type="bibr" target="#b21">[22]</ref> with D w = 200 as the initial word embedding weights W s ,W o for three paradigms. Specifically, for paradigm 1, we use 3 FC layers ? s , ? o , ? p with 1024 neurons and ? is a 2 layers MLP with hidden dimension 4096. For paradigm 2, We choose two 3?3 convolution layers to generate a 256-channels language feature map. The output size D p of ROIAlign <ref type="bibr" target="#b7">[8]</ref> is 7?7. For paradigm 3, We use 4 layers self-attention with 8 heads, of which output 512-dim linguistic-based object features s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Quantitative Results</head><p>Mean Recall: <ref type="table" target="#tab_5">Table 1</ref> shows comparisons between various methods and our C-bias framework on constrained mean recall(mR@K) on VG dataset. We notice a consistent improvement on mR@K on all baseline models. By plugging our framework in BGNN, our framework surpasses prior SOTA performance in three tasks yielded by <ref type="bibr" target="#b36">[37]</ref> and <ref type="bibr" target="#b11">[12]</ref> (mR@100: 37.8%, 16.6%, 12.8% ). It is attributed to valid local and global language representation merging paradigms.</p><p>To compare with baseline BGNN <ref type="bibr" target="#b11">[12]</ref>, our framework gains relative improvements of 17.3% , 31.9% and 35.2% on mean recall@100 metrics. The mR@K improvement should attribute to enhanced feature powered by language's generalization and invariability capability, which reduces noisy propagation between nodes and edges, therefore, elevates tail relation predictions. In terms of Motifs and G-RCNN, relatively minor improvements are obtained from 3 tasks over mR@k. It may caused by lack of pair-wise edge feature (e.g., union features) utilization, In consequence, the second paradigms can only be applied to the last perception layer, leading to poor performance gain. However, we find that some wrong predictions of our framework are more accurate than annotations, which is elaborated in Section4.7.</p><p>No-graph Constraint Mean Recall: <ref type="table" target="#tab_5">Table 1</ref> also reports no graph constraint mean recall(ng-mR) performance on the VG dataset, which reveals consistent superior results on baseline models. With baseline BGNN,ng-mR@100 increases 3.2%, 8.6%, 22.31%. Results reflects our framework's capability of predicting correct predicates with higher confidence when there is no graph constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Zero-shot Recall:</head><p>We also measure zero-shot recall (zR@K) for evaluating the framework generalization capability, results are shown in Table2. In each task, our framework still has noticeable improvements in PredCls tasks, with relative improvement 16.37%, 5.72%, 33.4% on zR@100 in 3 baselines, due to available of ground truth labels. However, the training process shows a relatively contradictory tendency between mR@K and zR@K performance, revealing that those subject-predicate-object combinations that do not occur in the training set have a considerable proportion of high-frequency predicates. Limited by length of paper, more experiments would be posted on supplement materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Predicate Analyze:</head><p>Shown in Figure5, we present each predicate category's mR@100 performance on PredCls task. Among which yellow pillars represent Baseline BGNN, whereas blue ones represent the proposed C-bias framework. Even compared with such a strong baseline model, We still notice more precise predictions that our framework gets, from which 44 of predicates are better recalled by our framework. Besides, there are predicates that the baseline model fails to recall (e.g.across, against, growing on, mounted on, says, walking in, etc.) been successfully hit by the proposed C-bias framework, which supports the claim that our framework successfully inducts relation patterns between objects. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Studies</head><p>To reveal the effectiveness of each paradigm, we incrementally add them to the baseline one by one. For simplicity, we choose BGNN as the baseline for illustration. The results are shown in <ref type="table">Table.</ref>3.</p><p>Paradigm1:We observe that adding the Experience estimation module improves baseline with 6.83%, 15.07%, 32.70% over three tasks on mR@100 respectively, which accounts for relation distribution learned from supervision of joint distribution. Besides, compared with the baseline that using FREQ <ref type="bibr" target="#b40">[41]</ref>, results prove the hypothesis that joint possibility indeed describes real-world distribution more precisely.</p><p>Paradigm2: Furthermore, the Language map module promotes the performance to 37.68, 19.82, 17.15. Noticed that the gap between mR@50 and mR@100 shrinks, indicating a lower false prediction proportion in top guesses. It is expected that channel-wised language map effectively guides union visual feature finding intrinsic object-relation patterns, which in turn leads to the confidence of positive-true predictions boosting.</p><p>Paradigm3: By adding the Scene extractor module, the performances further reach 38.44, 20.87, and 17.24. Through SEM brings a relatively tiny improvement than other modules, it meets our anticipation that global language context mainly serves as the assistance when a label is ambiguous.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Model Size and Runtime:</head><p>We present several model parameters and inference time in Table4. We choose BGNN and MOTIFS as baselines, C-bias and Unbiased <ref type="bibr" target="#b26">[27]</ref> as a plug-in model. Though Unbiased is claimed as a inference method, our framework still shows low computational requirements than Unbiased. For C-bias, parameters of model merely increased 5.33% and 6.17% in two baselines, the performance of mean recall@100 increases 24.88% and 8%, Compare with unbiased, our framework size is smaller but perform better. This advantage comes from label input of 3 paradigms, which is low-dimensional than image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Qualitative Studies</head><p>Beyond numbers, we emphasize the intuitive evaluation because trivial and uncompleted ground truth has traumatized the authority of metrics. Some inspiring findings that we discovered from wrong results of SGGen task in Figure6 reveal several advantages that the C-bias framework has over baseline and annotations, including:</p><p>1. Multi-scaled: An intuitive sensation is that our results generate more relations widespread in the whole scene. Compared to baseline results which are incapable of detecting relations with no overlaps, our C-bias framework successfully recognizes distanced entities' correlations. In <ref type="figure">Figure 6</ref>, the last column of first image shows our framework can notice &lt;man walk on sidewalk &gt; in a small area of the background. 2. Accurate and Informative: The baseline predicts multitudinous less meaningful categories "on, in, of". We speculate that it is caused by massive repeated annotations full in annotations (ground truth graph in the first row of <ref type="figure">Figure 6</ref>), which indeed deteriorate model learning. Intuitively, we <ref type="figure">Fig. 6</ref>: Visualization Results: we present scene graphs generated by annotations, baseline BGNN, ours in three columns, respectively. The relations and entities that neither occur in annotations nor baseline are marked with purple and orange.</p><p>find that our proposed framework can mine meaningful but inconspicuous correlations regardless of trivial annotations. Take last row of Figure6 for example, "letter" are neither part of annotations or baseline results, and &lt;logo on bus&gt; in annotations is not informative. Inspiringly, in the C-bias framework, relation triblets &lt;letter painted on bus&gt; and &lt;logo mounted on bus&gt; are more accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel cognitive bias framework in SGG task to properly mining fickle relation patterns by incorporating local, global context with help of linguistics information. We simulate human cognitive bias paradigms to acquire better relation perception capability, and address crucial role of using label-derivative linguistic features as biases upon visual message propagation. The result shows superiority over previous works.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>An example of three human Cognitive paradigms. (Left)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Overview of our proposed C-bias framework. First, we get predicted object labels and consist object and edge features from ROI features. Than Linguistic map module (LMM) receives object pairs label to generate a local language map that as a channel bias on edge features. Next, Scene extractor module (SEM) takes all labels in global context into consideration and generates divergent linguistic representations to update node features. The node and edge representations are updated before baseline model. Finally, Experience Estimation Module (EEM) yields estimated relation distribution by label pairs, and updates the final relation likelihood.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Distribution discrepancy between subject and object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Where I, B, C is a given image, Object location set, class set.N = {? i } and E = {? ij } are node and edge features enhanced by language. Then, SGG model output and "Paradigm 1" estimated relation distribution D = {d ij } are collectively considered:P (R|I) = SGG(N ,?) + D.(18)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Comparison of mR@100 performance on PredCls task. Orange pillars are BGNN, blue one is our framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>informative but distant relations are typically ignored and Fig. 1: a: predicate sampling frequency in VG dataset. b: an annotation example on VG dataset with numerous tedious relations . c: Mean recall@100 improvement of our framework on three tasks over SOTA.</figDesc><table><row><cell></cell><cell>37.8</cell><cell>38.43</cell><cell>SOTA</cell><cell>Ours</cell></row><row><cell></cell><cell>mR@100</cell><cell></cell><cell>16.58 21.87</cell><cell>12.75</cell><cell>17.24</cell></row><row><cell></cell><cell cols="2">PredCLS</cell><cell>SGCls</cell><cell>SGGen</cell></row><row><cell>a</cell><cell>b</cell><cell></cell><cell>c</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Where s i , o j ? R Nr are statistical marginal distribution of subject and object categories over relation categories (e.g., Figure4). Compare to directly counting N 2 o number of triblet combinations, marginal distribution does not suffer a lot from sample scarcity, thus can generate more accurate distribution. Noticed that the relation distribution of the same object classes is alternated depending on</figDesc><table><row><cell>a man: subject</cell><cell>b man: object</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc>The SGG performances of constraint and no graph constraint mean Recall@K in % on VG dataset. * denotes results come from author's paper. / means results are not available</figDesc><table><row><cell></cell><cell>Model</cell><cell>Method</cell><cell cols="3">PredCls mR@20 mR@50 mR@100</cell><cell cols="3">SGCls mR@20 mR@50 mR@100</cell><cell cols="3">SGGen mR@20 mR@50 mR@100</cell></row><row><cell></cell><cell>PCPL* [37]</cell><cell>/</cell><cell>/</cell><cell>35.2</cell><cell>37.8</cell><cell>/</cell><cell>18.6</cell><cell>19.6</cell><cell>/</cell><cell>9.5</cell><cell>11.7</cell></row><row><cell></cell><cell>IMP* [35]</cell><cell>/</cell><cell>/</cell><cell>15.8</cell><cell>17.2</cell><cell>/</cell><cell>9.3</cell><cell>9.6</cell><cell>/</cell><cell>6.0</cell><cell>7.3</cell></row><row><cell></cell><cell>VCTree* [28]</cell><cell>/</cell><cell>/</cell><cell>15.4</cell><cell>16.6</cell><cell>/</cell><cell>7.4</cell><cell>7.9</cell><cell>/</cell><cell>8.2</cell><cell>9.7</cell></row><row><cell>constraint</cell><cell>G-RCNN [38]</cell><cell cols="2">baseline 13.21 C-bias 15.75</cell><cell>16.46 18.64</cell><cell>17.28 19.80</cell><cell>8.35 8.62</cell><cell>9.67 10.11</cell><cell>10.17 10.8</cell><cell>3.45 3.80</cell><cell>4.89 5.15</cell><cell>5.95 6.24</cell></row><row><cell></cell><cell>Motifs [41]</cell><cell cols="2">baseline 16.26 C-bias 16.61</cell><cell>18.79 20.38</cell><cell>19.69 21.87</cell><cell>7.90 8.50</cell><cell>9.39 9.89</cell><cell>9.97 10.41</cell><cell>3.53 3.91</cell><cell>5.22 5.49</cell><cell>6.65 6.99</cell></row><row><cell></cell><cell>BGNN [12]</cell><cell>baseline C-bias</cell><cell cols="2">26.21 31.30 36.31 30.64</cell><cell>32.76 38.44</cell><cell>13.24 15.80</cell><cell>15.58 20.38</cell><cell>16.58 21.87</cell><cell>7.82 11.63</cell><cell>10.59 14.43</cell><cell>12.75 17.24</cell></row><row><cell></cell><cell>PCPL*</cell><cell>/</cell><cell>/</cell><cell>50.6</cell><cell>62.6</cell><cell>/</cell><cell>26.8</cell><cell>32.8</cell><cell>/</cell><cell>10.4</cell><cell>14.4</cell></row><row><cell></cell><cell>IMP*</cell><cell>/</cell><cell>/</cell><cell>9.8</cell><cell>10.5</cell><cell>/</cell><cell>5.8</cell><cell>6.0</cell><cell>/</cell><cell>3.8</cell><cell>4.8</cell></row><row><cell>no constraint</cell><cell>G-RCNN Motifs</cell><cell cols="2">baseline 21.66 C-bias 23.31 baseline 23.12 C-bias 23.95</cell><cell>35.74 36.38 35.90 37.12</cell><cell>46.02 46.45 47.72 49.67</cell><cell>13.66 13.83 12.90 13.59</cell><cell>20.11 20.77 19.76 20.73</cell><cell>24.71 26.23 25.54 26.08</cell><cell>4.04 4.03 4.31 4.59</cell><cell>6.62 6.66 7.37 7.57</cell><cell>10.11 10.27 10.72 11.55</cell></row><row><cell></cell><cell>BGNN</cell><cell cols="2">baseline 34.41 C-bias 36.94</cell><cell>49.02 51.20</cell><cell>61.04 63.02</cell><cell>17.96 20.10</cell><cell>25.29 28.01</cell><cell>30.98 33.65</cell><cell>8.66 13.57</cell><cell>13.63 17.07</cell><cell>17.97 21.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Zero shot Recall@K evaluation in % on VG dataset</figDesc><table><row><cell cols="2">Model Method</cell><cell cols="2">PredCls zR@50 zR@100</cell><cell cols="2">SGCls zR@50 zR@100</cell><cell cols="2">SGGen zR@50 zR@100</cell></row><row><cell>G-RCNN</cell><cell cols="2">baseline 10.22 C-bias 12.44</cell><cell>12.22 14.22</cell><cell>6.67 4.67</cell><cell>7.56 6.44</cell><cell>0.44 0.44</cell><cell>0.44 0.44</cell></row><row><cell>MOTIFS</cell><cell cols="2">baseline 10.22 C-bias 10.44</cell><cell>11.70 12.37</cell><cell>6.22 5.33</cell><cell>6.22 6.67</cell><cell>0.00 0.00</cell><cell>0.00 0.22</cell></row><row><cell>BGNN</cell><cell cols="2">baseline 4.44 C-bias 6.22</cell><cell>5.33 7.11</cell><cell>3.56 3.56</cell><cell>3.56 4.0</cell><cell>0.44 0.00</cell><cell>0.44 1.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Ablation studies on framework structure</figDesc><table><row><cell>Model</cell><cell cols="6">PredCls mR@50 mR@100 mR@50 mR@100 mR@50 mR@100 SGCls SGGen</cell></row><row><cell>BGNN</cell><cell>30.40</cell><cell>32.76</cell><cell>14.30</cell><cell>16.58</cell><cell>10.70</cell><cell>12.75</cell></row><row><cell>BGNN+EEM</cell><cell>32.75</cell><cell>35.00</cell><cell>17.77</cell><cell>19.08</cell><cell>14.54</cell><cell>16.92</cell></row><row><cell>BGNN+EEM+LMM</cell><cell>35.99</cell><cell>37.68</cell><cell>18.71</cell><cell>19.82</cell><cell>15.10</cell><cell>17.15</cell></row><row><cell cols="5">BGNN+EEM+LMM+SEM 36.31 38.44 20.38 21.87</cell><cell>14.43</cell><cell>17.24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Compariation of model size and runtime</figDesc><table><row><cell>Model</cell><cell>BGNN</cell><cell>BGNN+Ours</cell><cell>BGNN+Unbiased</cell><cell>MOTIFS</cell><cell>MOTIFS+Ours</cell></row><row><cell>Params</cell><cell>341.9M</cell><cell>360.13M</cell><cell>365.746M</cell><cell>367.17M</cell><cell>389.82M</cell></row><row><cell>Relative increase</cell><cell>/</cell><cell>5.33%</cell><cell>6.98%</cell><cell>/</cell><cell>6.17%</cell></row><row><cell>Inference Time(ms)</cell><cell>497.5</cell><cell>538.1</cell><cell>539.2</cell><cell>420.4</cell><cell>450.9</cell></row><row><cell>Mean recall@100</cell><cell>32.76</cell><cell>38.44</cell><cell>34.89</cell><cell>6.65</cell><cell>6.99</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Knowledge-embedded routing network for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6163" to="6171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recovering the unbiased scene graphs from the biased ones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1581" to="1590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<title level="m">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detecting visual relationships with deep relational networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and Pattern recognition</title>
		<meeting>the IEEE conference on computer vision and Pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3076" to="3086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unconditional scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dhamo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farshad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Musatian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16362" to="16371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention-translationrelation network for scalable scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gkanatsios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pitsikalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koutras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Maragos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unpaired image captioning via scene graph alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10323" to="10332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bipartite graph network with adaptive message passing for unbiased scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="11109" to="11119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Factorizable net: an efficient subgraph-based framework for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="335" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scene graph generation from objects, phrases and region captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1261" to="1270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gps-net: Graph property sensing network for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3746" to="3753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fully convolutional scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mortazavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bhanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11546" to="11556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Structure inference net: Object detection using scene-level context and instance-level relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6985" to="6994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="852" to="869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Context-aware scene graph generation with seq2seq transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volkovs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15931" to="15941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Pixels to graphs by associative embedding. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Classification by attention: Scene graph classification with prior knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharifzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Baharlou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10084</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Explainable and explicit visual reasoning over scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8376" to="8384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Energy-based learning for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suhail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Siddiquie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Broaddus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eledath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13936" to="13945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unbiased scene graph generation from biased training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3716" to="3725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to compose dynamic tree structures for visual contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6619" to="6628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Graph-structured representations for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Target adaptive context aggregation for video scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13688" to="13697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Advantages of bias and prejudice: an exploration of their neurocognitive templates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tobe?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroscience &amp; Biobehavioral Reviews</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1047" to="1058" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Judgment under uncertainty: Heuristics and biases: Biases in judgments reveal some heuristics of thinking under uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">185</biblScope>
			<biblScope unit="issue">4157</biblScope>
			<biblScope unit="page" from="1124" to="1131" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exploring context and visual pattern of relationship for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Linknet: Relational embedding for scene graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Scene graph generation by iterative message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5410" to="5419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scene graph captioner: Image captioning based on structural visual representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="477" to="485" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pcpl: Predicate-correlation perception learning for unbiased scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="265" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Graph r-cnn for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="670" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bridging knowledge graphs to generate scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="606" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning visual commonsense for robust scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="642" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Neural motifs: Scene graph parsing with global context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5831" to="5840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">An empirical study on leveraging scene graphs for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12133</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
