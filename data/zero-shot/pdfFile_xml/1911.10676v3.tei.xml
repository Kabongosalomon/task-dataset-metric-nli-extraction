<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attribute Restoration Framework for Anomaly Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqin</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkun</forename><surname>Cao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosen</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
						</author>
						<title level="a" type="main">Attribute Restoration Framework for Anomaly Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>JOURNAL OF L A T E X CLASS FILES, APRIL 2020 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Anomaly detection</term>
					<term>attribute restoration frame- work</term>
					<term>semantic feature embedding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the recent advances in deep neural networks, anomaly detection in multimedia has received much attention in the computer vision community. While reconstruction-based methods have recently shown great promise for anomaly detection, the information equivalence among input and supervision for reconstruction tasks can not effectively force the network to learn semantic feature embeddings. We here propose to break this equivalence by erasing selected attributes from the original data and reformulate it as a restoration task, where the normal and the anomalous data are expected to be distinguishable based on restoration errors. Through forcing the network to restore the original image, the semantic feature embeddings related to the erased attributes are learned by the network. During testing phases, because anomalous data are restored with the attribute learned from the normal data, the restoration error is expected to be large. Extensive experiments have demonstrated that the proposed method significantly outperforms several state-of-thearts on multiple benchmark datasets, especially on ImageNet, increasing the AUROC of the top-performing baseline by 10.1%. We also evaluate our method on a real-world anomaly detection dataset MVTec AD and a video anomaly detection dataset ShanghaiTech.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A NOMALY detection, with broad application in network intrusion detection, credit card fraud detection, and numerous other fields <ref type="bibr">[1]</ref>, has received significant attention among the machine learning community. With the recent advances in deep neural networks, there is a heated topic on anomaly detection in multimedia, e.g., medical diagnosis, defect detection and intrusion detection. In this paper, we focus on anomaly detection of still images. Anomaly detection is a technique used to identify unusual patterns that do not conform to expected behavior. Considering the scarcity and diversity of anomalous data, anomaly detection is usually modeled as a self-supervised learning or one-class classification problem <ref type="bibr">[2]</ref>, i.e., the training dataset contains only normal data and the anomalous data is not available during training.</p><p>Reconstruction-based methods <ref type="bibr">[3]</ref>- <ref type="bibr">[5]</ref> have recently shown great promise for anomaly detection. Autoencoder <ref type="bibr">[6]</ref> is adopted by most reconstruction-based methods, assuming that normal and anomalous samples could lead to significantly different embeddings and thus differences in the corresponding reconstruction errors can be leveraged to differentiate the two types of samples <ref type="bibr">[7]</ref>. However, this assumption may fail for datasets with more complex texture and structure information <ref type="bibr">[8]</ref>. The MSE loss is shown to forces autoencoders to focus on reducing low-level pixel-wise error insensitive to human perception, rather than learning semantic features <ref type="bibr">[9]</ref>, <ref type="bibr">[10]</ref>. As data complexity grows, the extracted low-level features are more likely to be shared between normal and anomalous data, leading to mixed feature embeddings. Under this situation, both normal and anomalous data could be reconstructed properly <ref type="bibr">[11]</ref>, <ref type="bibr">[12]</ref>. To tackle this problem, various attempts have been made to introduce more efficient loss functions rather than the pixel-wise MSE loss. Adversarial training is introduced by adding a discriminator after autoencoders to judge whether its original or reconstructed image <ref type="bibr">[5]</ref>, <ref type="bibr">[13]</ref>. Akcay et al. <ref type="bibr">[4]</ref> adds an extra encoder after autoencoders and leverages an extra MSE loss between the two different embeddings. Despite much progress made along this line, the improvement remains limited, especially for complex datasets. Recent works <ref type="bibr">[14]</ref>, <ref type="bibr">[15]</ref> have shown that reconstruction-based methods fail to extract semantic features effectively. This problem may be attributed to the "information equivalence", i.e. the equivalence between the input and target data, which more likely leads to simple compression of the image rather than learning a semantically meaningful representation <ref type="bibr" target="#b60">[16]</ref>.</p><p>To achieve effective supervision and learn semantic feature embeddings, we start with breaking the information equivalence. Taking the original data as supervision, the input data is obtained by erasing selected information from the original data, creating a gap in information between the input data and the target supervision. To restore input data to the original data, the network is then forced to learn what is erased and how to restore it. The corresponding architecture is shown in <ref type="figure">Figure 1</ref>. In this way, we convert this task from reconstruction into restoration, a type of self-supervised learning tasks potentially effective for extracting semantic features <ref type="bibr" target="#b60">[16]</ref>- <ref type="bibr" target="#b62">[18]</ref>. The design of the information erasing module is critical. If the erased information is very localized and low-level, e.g the Gaussian noise introduced in denoising autoencoders <ref type="bibr" target="#b63">[19]</ref>, it cannot effectively force the model to extract semantic features <ref type="bibr" target="#b60">[16]</ref>. Different from previous approaches, we introduce an Attribute Erasing Module (AEM) to remove certain attributes associated with compact semantic representations (e.g. color and orientation) <ref type="bibr" target="#b62">[18]</ref>, <ref type="bibr" target="#b64">[20]</ref>. Since the restoration requires deeper semantic understandings of the images, the Attribute Restoration Network (ARNet) can effectively extract semantic features and the AEM controls the feature embeddings by model stability tends to be more important than traditional data classification tasks. We train three models, including ARNet, traditional autoencoder <ref type="bibr">[6]</ref> and GANomaly <ref type="bibr">[4]</ref>, respectively on each category of MNIST <ref type="bibr" target="#b101">[57]</ref> and Fashion-MNIST <ref type="bibr" target="#b102">[58]</ref> datasets and test models every 5 epochs along with training. The traditional autoencoder <ref type="bibr">[6]</ref> and GANomaly <ref type="bibr">[4]</ref> are set as our baseline model. The model performance of validation during the training process is shown in <ref type="figure">Figure 9</ref> and <ref type="figure">Figure 10</ref>, from which we can see the performance of our ARNet method always converges in a high position; moreover, ARNet shows the highest performance stability at the end of the training process. L train (x,x) x during the training process is shown in <ref type="figure">Figure 9</ref> and <ref type="figure">Figure 10</ref>, from which we can see the performance of our ARNet method always converges in a high position; moreover, ARNet shows the highest performance stability at the end of the training process. L train (x,x) x <ref type="figure">Fig. 1</ref>: Overview of the attribute restoration framework. During the training phase, to restore the original image, ARNet is forced to learn semantic feature embeddings related to the erased attributes. During the testing phase, the wrong restored attributes caused by the unseen semantic features will enlarge the restoration loss (the car is restored with wrong color and orientation).</p><p>erasing the corresponding information. Besides extracting powerful semantic features, ARNet also benefits from a unique mechanism for image restoration tasks. Normal data can be restored properly as the erased attributes and the embedded features by the restoration network matches, which is satisfied through the training process. However, this match is broken when normal data and anomalous data are different regarding to the erased attribute. In this case, anomalous data can not be restored properly and suffers from high restoration errors.</p><p>To validate the effectiveness of ARNet, we conduct extensive experiments with several benchmarks and compare them with state-of-the-art methods. Our experimental results have shown that ARNet outperforms state-of-the-art methods in terms of model accuracy and model stability for different tasks. To further evaluate with more challenging tasks, we experiment with the large-scale dataset ImageNet <ref type="bibr">[8]</ref> and show that ARNet improves the AUROC of the top-performing baseline by 10.1%. To illustrate that ARNet is adaptable to complex real-world environments, we experiment on a real-world anomaly detection dataset MVTec AD <ref type="bibr" target="#b65">[21]</ref>. We further experiment on a distorted datasets CIFAR-10-C <ref type="bibr" target="#b66">[22]</ref>. The result shows that ARNet is robust when facing low-level corruption and remains effective while other reconstruction-based methods fail, indicating that the ARNet is focused on semantic features. We also conduct T-SNE <ref type="bibr" target="#b67">[23]</ref> visualization of latent spaces to illustrate that ARNet can extract distinctive semantic features. To the best of our knowledge, we are the first to apply image restoration to the anomaly detection problem and show impressive performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Anomaly Detection</head><p>Depend on different tasks, anomaly detection can be roughly divided into two classes: anomaly detection in video <ref type="bibr">[5]</ref>, <ref type="bibr" target="#b68">[24]</ref>- <ref type="bibr" target="#b76">[32]</ref> and anomaly detection in still images. In this paper, we focus on anomaly detection in still images. Suffering from the scarcity and diversity of anomalous data, the vital challenge of anomaly detection is that the training dataset contains only normal data, leading to a lack of supervision. Judging by whether the model can be directly used in anomaly detection, popular methods can be concluded into two types accordingly: one-class classification based approaches and surrogate supervision based approaches. One-class classification based approaches: To distinguish the anomalous data from normal data, some previous conventional methods <ref type="bibr" target="#b77">[33]</ref>- <ref type="bibr" target="#b80">[36]</ref> tended to depict the normal data with statistical approaches. Through training, a distribution function was forced to fit on the features extracted from the normal data to represent them in a shared latent space. During testing, samples mapped to different statistical representations are considered as anomalous.</p><p>Some approaches tackled the anomaly detection problem by finding a hyperplane to separate normal data in the latent space. In OC-SVM <ref type="bibr" target="#b81">[37]</ref>, the normal samples are mapped to the high-dimensional feature space through kernel function to get better aggregated. In the feature space, the coordinate origin is considered as the only anomalous data. Then a maximum margin hyperplane is found in feature space to better separate the mapped data from the origin. To better aggregate the mapped data in latent space, Ruff et al. <ref type="bibr">[2]</ref> optimized the neural network by minimizing the volume of a hyper-sphere which encloses the network representations of the data.</p><p>Other researchers tried to find the hyperplane through generating or introducing extra anomalous data <ref type="bibr" target="#b82">[38]</ref>, <ref type="bibr" target="#b83">[39]</ref>. Lee et al. <ref type="bibr" target="#b82">[38]</ref> used Kullback-Leibler (KL) divergence to guide GAN to generate anomalous data closer to normal data, leading to a better training set for the classification method. Hendrycks et al. <ref type="bibr" target="#b83">[39]</ref> introduced extra data to build a multiclass classification task. The experiment revealed that even though the extra data was in limited quantities and weakly correlated to the normal data, the learned hyperplane was still effective in separating normal data. Surrogate supervision based approaches: Many approaches modeled anomaly detection as an unsupervised learning problem and remedy the lack of supervision by introducing surrogate supervision. The model was trained to optimize the surrogate task-based objective function firstly. Then normal data can be separated with the assumption that anomalous data will result differently in the surrogate task.</p><p>Reconstruction <ref type="bibr">[3]</ref>, <ref type="bibr">[12]</ref>, <ref type="bibr">[13]</ref>, <ref type="bibr" target="#b84">[40]</ref>, <ref type="bibr" target="#b85">[41]</ref> is the most popular surrogate supervision. Based on autoencoders or variation autoencoders, this kind of method compressed normal samples into a lower-dimensional latent space and then reconstructed them to approximate the original input data. It assumed that anomalous samples would be distinguished through relatively high reconstruction errors compared with normal samples. Sakurada et al. <ref type="bibr">[7]</ref> were the first to apply the autoencoder to anomaly detection. This work further indicated that the learned features in the hidden layer of autoencoders were distinguishable between normal and anomalous data. Based on that, Nicolau et al. <ref type="bibr" target="#b86">[42]</ref> introduced density estimation to estimate the different distribution in the latent space of autoencoder. It assumed that anomalous data would hold lower density in latent space. Some recent works <ref type="bibr">[9]</ref>, <ref type="bibr">[10]</ref> indicated mean square error (MSE) loss function, adopted by most reconstruction-based methods, forces the network to focus on pixel-wise error rather than learning semantic features. When dealing with more complex data, the learned low-level features are more likely to be shared and lead to good reconstruction results in both normal and anomalous data.</p><p>To tackle this problem, some recent approaches continued to follow the reconstruction based method by introducing more efficient loss function rather than MSE. Adversarial training is employed to optimize the autoencoder and its discriminator is leveraged to further enlarge the reconstruction error gap between normal and anomalous data <ref type="bibr">[5]</ref>, <ref type="bibr" target="#b74">[30]</ref>. To make the method more robust against noises, Gaussian noise is added to the input training samples and then fed to the encoder. To detect the irregularity in videos, Sabokrou et al. <ref type="bibr" target="#b73">[29]</ref> proposed an architecture to detect and localize the irregularity simultaneously. Training by the normal data only, the model learns to replace the irregularity in the video with a dominant concept, in which this process is noted as image inpainting. Based on <ref type="bibr">[5]</ref>, Akcay et al. <ref type="bibr" target="#b87">[43]</ref> leveraged another encoder to embed the reconstruction results to the subspace where to calculate the reconstruction error. Similarly, Wang et al. <ref type="bibr" target="#b88">[44]</ref> employed adversarial training under a variational autoencoder framework with the assumption that normal and anomalous data follows different Gaussian distribution. Zenati et al. <ref type="bibr" target="#b89">[45]</ref> trained a BiGAN model and employed a discriminator to add supervision to encoder and decoder simultaneously. Gong et al. <ref type="bibr">[11]</ref> augmented the autoencoder with a memory module and developed an improved autoencoder called memory-augmented autoencoder to strengthen reconstructed errors on anomalies. Perera et al. <ref type="bibr" target="#b90">[46]</ref> applied two adversarial discriminators and a classifier on a denoising autoencoder. By adding constraint and forcing each randomly drawn latent code to reconstruct examples like the normal data, it obtained high reconstruction errors for the anomalous data.</p><p>Other approaches tackled this problem by introducing new self-supervised methods as surrogate tasks. Golan et al. <ref type="bibr" target="#b91">[47]</ref> introduced a self-supervised method <ref type="bibr" target="#b92">[48]</ref> into anomaly detection by applying dozens of image geometric transforms and created a self-labeled dataset for transformation classification, assuming that the transformation of anomalous data can not be classified properly. Wang et al. <ref type="bibr">[14]</ref> further introduced a selfsupervised method <ref type="bibr" target="#b93">[49]</ref> into anomaly detection by applying Jigsaw puzzles to extend the above self-labeled dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Self-supervised Learning Methods</head><p>Self-supervised representation learning leverages input data itself as supervision and benefits almost all types of downstream tasks like classification, detection and segmentation. Examples of self-supervised tasks are recognizing the geometric transformation applied to an image <ref type="bibr" target="#b92">[48]</ref>, predicting the relative position between a pair of random patches from an image <ref type="bibr" target="#b94">[50]</ref>, solving Jigsaw puzzles after randomly swapping the image patching <ref type="bibr" target="#b93">[49]</ref>. Some works tackled the self-supervised learning problem through restoration. It assumed that by restoring the damaged image, the network is forced to learn robust feature embeddings. Denoising autoencoders <ref type="bibr" target="#b63">[19]</ref> add alternative corrupting noises to the original data, and require the autoencoder network to undo the damage. Pathak et al. <ref type="bibr" target="#b60">[16]</ref> indicated that the damaged region should have a relatively large size, otherwise the model could restore the damaged image through the local non-semantic feature. Pathak et al. <ref type="bibr" target="#b60">[16]</ref> randomly blanked out a region from the original image and employed an autoencoder to restore. Jenni et al. <ref type="bibr" target="#b61">[17]</ref> tackle the problem by removing and inpainting information at a more abstract level (the internal representation), rather than at the raw data level. Denton <ref type="bibr" target="#b95">[51]</ref> indicated that previous work was difficult to generate large image patches that look realistic. To address this, a low resolution but intact version of the original image was extra fed to the network to guide reconstruction. To be noted that in the restoration framework, the information erasing module is non-parameterized and separated from the network.</p><p>In this paper, we introduce the restoration framework for anomaly detection to control feature embedding and extract semantic features. The information erasing module is different from traditional restoration-based unsupervised learning in this paper. In ARNet, a novel and effective attribute erase module is utilized to erase certain object attribute from the image. In anomaly detection, there are several approaches related to our work. Sabokrou et al. <ref type="bibr">[5]</ref>, <ref type="bibr" target="#b74">[30]</ref> applying Denoising autoencoders <ref type="bibr" target="#b63">[19]</ref>, which can also be considered as restorationbased anomaly detection. Sabokrou et al. <ref type="bibr" target="#b73">[29]</ref> indicated that AVID operates similarly to a denoising network, which replaces the irregularity in the video with a dominant concept, it can be considered as inpainting-based anomaly detection. However, Pathak et al. <ref type="bibr" target="#b60">[16]</ref> indicated that the Gaussian noise introduced in Denoising autoencoders is typically very localized and lowlevel, thus does not require much semantic information to restore. Different from these approaches, by forcing the network to restore the erased attributes, ARNet can effectively control feature embedding and extract semantic features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. ATTRIBUTE RESTORATION FRAMEWORK</head><p>We first formulate the problem of anomaly detection. Let X , X n , and X an denote the sets of entire dataset, normal dataset and anomalous dataset, respectively, where X n ? X an = X and X n ? X an = ?. Given any image x ? X , where x ? R C?H?W , and C, H and W denote the dimensions of image channels, height and width, the goal is to build a model M(?) for discriminating whether x ? X n or x ? X an . To solve the above problem, we propose the attribute restoration framework, which consists of three parts: (1) Attribute Erasing Module (AEM): erase certain attributes of images to create an image restoration task; (2) Attribute Restoration Network (ARNet): use the original images as supervision and the images after erasing certain attributes as inputs to train a model for restoring the images against the attribute absence; (3) Anomaly measurement: establish a link between the image restoration task and the image anomaly detection task. The corresponding structure is shown in <ref type="figure">Figure 2</ref>. to more fields, opening avenues for future research.</p><p>Restoration Loss Enc(?) Dec(?) <ref type="bibr" target="#b61">[17]</ref>  to the addition of more transformations and the exploration of a more intelligent transformation selection strategy. In addition, this way of feature embedding can also be applied to more fields, opening avenues for future research. Restoration Loss Enc(?) Dec(?) <ref type="bibr" target="#b60">[16]</ref>  to the addition of more transformations and the exploration of a more intelligent transformation selection strategy. In addition, this way of feature embedding can also be applied to more fields, opening avenues for future research. Restoration Loss Enc(?) Dec(?) <ref type="bibr" target="#b60">[16]</ref>  Attribute Erasing Module (AEM) aims to erase a set of attributes from the objects, enforcing information inequivalence between input and output data and turn the task into restoration. The effective attribute for anomaly detection should satisfy the following assumptions:</p><p>? The erased attribute should be connected to the semantic information of the normal data and be shared among the normal data.</p><p>? The erased attribute should be either different or connected to different semantic information between normal and anomalous data. If the normal and anomalous data share the same attribute and the attribute is connected to the same semantic information, the ARNet is hard to distinguish the normal data and anomalous data through this attribute, as anomalous data can be restored properly using the shared features learning from the normal data.</p><p>? The attributes can be erased by a module which does not rely on extra dataset or labels; otherwise, this will require an additional training process for attribute erasing.</p><p>We take a concrete example to further reveal the details to design the Attribute Erasing Module. By human prior, the semantic information connected to the orientation is shared within a class but different between classes, e.g. the wheels of the cars are always at the bottom of the images while the circle of the digit number "9" is always at the top of the images, which meets the first and the second condition. To erase the orientation of these objects, we can employ a random rotation operation, which rotates the images with a randomly selected angle. This orientation erasing operation does not need to introduce an additional training process, which also meets the third condition we discussed above.</p><p>The main challenge of unsupervised anomaly detection is that anomalous data is not available during training, leaving no guarantee that the second assumption is satisfied by all kinds of anomalous data. Fortunately, as the chosen attribute is connected to the semantic information, it is hard to be shared between normal and anomalous data (see Section V-B for more discussion). In addition, although some attributes cannot be used to distinguish between normal and anomalous data when the second condition is not satisfied, it only causes the anomaly detection performance of the image restoration task degrading to that of the image reconstruction task. To alleviate this problem, we propose a set of attribute erase operations to increase the probability that at least one attribute could meet the second condition. The Attribute Erasing Module works as follows: Suppose we have a set of attribute erasing operations</p><formula xml:id="formula_0">O = {f O k (?)|k = 1, . . . , K}, where f O k (?) denotes the k-th</formula><p>attribute erasing operation. Given x n ? X n , the data after AEM</p><formula xml:id="formula_1">should bex n = F O (x n ) := f O k (f O k?1 (? ? ? f O1 (x n ))).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Attribute Restoration Network</head><p>We now present the Attribute Restoration Network (ARNet) in detail. ARNet is based on an encoder-decoder framework to restore the original images. In the training phase, givenx n after AEM, the proposed ARNet takes thex n as the inputs, and attempts to inversely restore the original training samples x n . Givenx n , the restored samplex n is expressed a?</p><formula xml:id="formula_2">x n = M(x n ) = Dec(Enc(x n )),<label>(1)</label></formula><p>where M(?) indicates the model of ARNet, while Enc(?) and Dec(?) indicate encoder and decoder of ARNet. Note that while ARNet is employed for the image restoration tasks, it is different from existing autoencoders in that the inputs and outputs are asymmetrical, i.e., ARNet needs to restore attributes erased by the Attribute Erasing Module.</p><p>To train ARNet for effective anomaly detection, a likelihoodbased restoration loss is employed as loss function. 2 loss is utilized to measure the distances between the restored samples and targets since it is smoother and distributes more punishments on the dimensions with larger generation errors. Let the target image be x n , the training loss is formulated as</p><formula xml:id="formula_3">L train = E xn?p(xn) M(x n ) ? x n 2 2 ,<label>(2)</label></formula><p>where ? 2 denotes the 2 norm and p(x n ) indicates the distribution of normal data. To approximate the expectation operation in Eq. (2), in each mini-batch, we randomly select a sample x n and obtainx n = F O (x n ). Then we calculate the average cost between any x n and corresponding M(x n ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Anomaly Measurement</head><p>To establish a link between the image restoration task and the image anomaly detection task, in the test phase, we design a metric based on the restoration error to distinguish whether one sample belongs to the normal set. Both normal and anomalous data are fed into the model, which are utilized together to determine whether a query sample is anomalous. In the test phase, we calculate the restoration error of each input image x for anomaly detection. We suppose that the restorations of normal samples show much smaller errors than the anomalous samples due to the specific image restoration scheme. We note that 1 loss is more suitable to measure the distance between outputs and original images. Let the test sample be x, the anomaly score is formulated as</p><formula xml:id="formula_4">S test (x) = M(F O (x)) ? x 1 ,<label>(3)</label></formula><p>where ? 1 denotes the 1 norm. However, f O k may function through randomization, in which the original fixed operation is reformulated as a random selection f? k from an operation set {f? k,j k |j k = 1, . . . , m k } with size of m k . For example, we employ random rotation to formulate the orientation erasing operation, where the rotation angle is randomly selected from a fixed set, such as several discrete angle options, {0?, 90?, 180?, 270?}. Accordingly, as the F O is the compound function of f O k , F O is reformulated as F?(?) = f? k (f? k?1 (? ? ? f? 1 (?))), where F?(?) is a random selection from the set {F? i (?)|i = 1, . . . , N } with size N = K k=1 m k . Note that, when m k = 1, f O k = f? k . During the test process, we need to traverse all selections F? i (?) and set average restoration error as the anomaly score, which is reformulated as</p><formula xml:id="formula_5">S test (x) = 1 N N i=1 M(F? i (x)) ? x 1 .<label>(4)</label></formula><p>We notice that the restoration errors under some F? i (?) may larger than the others in natural since different tasks have different restoration difficulties. In this case, given the same input sample, different F? i (?) lead to different restoration errors and the final anomaly score may has a bias if we average these restoration errors naively. To make each F? i (?) contributes equally to the final anomaly score, we use the original training data and calculate the mathematical expectation of the restoration error for each F? i (?) as a normalization, and set the final anomaly score as</p><formula xml:id="formula_6">S test (x) = 1 N N i=1 M(F? i (x)) ? x 1 E xn?p(xn) M(F? i (x n )) ? x n 1 ] ,<label>(5)</label></formula><p>where p(x n ) indicates the distribution of normal data, as well as being consistent with the distribution of training set. A normal sample leads to a low anomaly score; the higher value S test (x) obtained, the higher probability for the sample x to be anomalous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Discussion: Restoration vs. Reconstruction</head><p>Both image reconstruction and image restoration tasks can be implemented with an encoder-decoder architecture. The differences are summarized in three folds. First, different from reconstruction, the input and output for ARNet are asymmetric which is achieved with an Attribute Erasing Module. The erased information of anomalous data may not be restored properly through feature embeddings learned from the normal data, leading to high anomaly scores for anomalous data. Secondly, unlike the reconstruction-based methods, especially vanilla AE, which blindly learns uncontrollable features from normal data, the restoration-based framework leverages the attribute erasing to guide the feature embedding and thus enables the embedding of semantic features. Thirdly, in the final anomaly detection phase, the two methods differ in the way to obtain the final anomaly scores. Different from the reconstruction-based methods, for the restoration-based framework, multiple restoration losses produced by multiple attribute erasing operations are weighted and summed to obtain the anomaly scores. These weights can be obtained from the training data, which has been discussed in Section III-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We conduct substantial experiments to validate our method. Under unsupervised anomaly detection settings, the ARNet is first evaluated on multiple commonly used benchmark datasets and the large-scale dataset ImageNet <ref type="bibr">[8]</ref>, which is rarely looked into in previous anomaly detection studies. Next, we conduct experiments on real anomaly detection datasets to evaluate the performance in real-world environments. Then we present the respective effects of different designs (e.g., different types of image-level transformation and loss function design) through ablation study. The stability of our models is validated through monitoring performance fluctuation during the training process and comparing the final performance after convergence in multiple training attempts, all from random weights and with the same training configuration. Finally, the visualization analysis illustrates the efficiency of the attribute restoration framework in anomaly detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiments on Popular Benchmarks</head><p>Datasets. In this part, our experiments involve five popular image datasets: MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100 and ImageNet. For all datasets, the training and test partitions remain as default. In addition, pixel values of all images are normalized to [?1, 1]. We introduce these five datasets briefly as follows:</p><p>? MNIST [54]: consists of 70,000 28 ? 28 handwritten grayscale digit images.</p><p>? Fashion-MNIST <ref type="bibr" target="#b99">[55]</ref>: a relatively new dataset comprising 28 ? 28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category.</p><p>? CIFAR-10 [56]: consists of 60,000 32 ? 32 RGB images of 10 classes, with 6,000 images for per class. There are 50,000 training images and 10,000 test images, divided in a uniform proportion across all classes.</p><p>? CIFAR-100 <ref type="bibr" target="#b100">[56]</ref>: consists of 100 classes, each of which contains 600 RGB images. The 100 classes in the CIFAR-100 are grouped into 20 "superclasses" to make the experiment more concise and data volume of each selected "normal class" larger.</p><p>? ImageNet <ref type="bibr">[8]</ref>: We group the data from the ILSVRC 2012 classification dataset <ref type="bibr">[8]</ref> into 10 superclasses by merging similar category labels using Latent Dirichlet Allocation (LDA) <ref type="bibr" target="#b101">[57]</ref>, a natural language processing method (see appendix for more details). We note that few anomaly detection research has been conducted on ImageNet since its images have higher resolution and more complex background.</p><p>Model configuration. The detailed structure of the model we used can be found in the appendix. We follow the settings in <ref type="bibr" target="#b87">[43]</ref>, <ref type="bibr" target="#b102">[58]</ref>, <ref type="bibr" target="#b103">[59]</ref> and add skip-connections between some layers in encoder and corresponding decoder layers to facilitate the backpropagation of the gradient in an attempt to improve the performance of image restoration. We use stochastic gradient descent (SGD) <ref type="bibr" target="#b104">[60]</ref> optimizer with default hyperparameters in Pytorch. ARNet is trained using a batch size of 32 for 500/T epochs, where T means the number of transformations we used. The learning rate is initially set to 0.1, and is divided by 2 every 50/T epoch.</p><p>In our experiments, we use a attribute erasing operation set which contains two cascade operations: The graying operation erases color information, and the random rotation operation erases objects' orientation. Both of them meet the assumptions we introduced in Section III-A.</p><p>Evaluation protocols. For a dataset with C classes, a batch of C experiments are performed respectively with each of the C classes set as "normal". We then evaluate performance on an independent test set, which contains samples from all classes, including normal and anomalous data. As all classes have equal volumes of samples in our selected datasets, the overall number proportion of normal and anomalous samples is  <ref type="figure">Fig. 3</ref>: Comparison of frames per second (FPS) (horizontal coordinates), GPU memory usages (circular sizes) and AUROC for anomaly detection (vertical coordinates) of various methods testing on CIFAR-10. ARNet takes up a relatively small GPU memory, and its FPS is relatively higher. simply 1 : C ?1. We quantify the model performance using the area under the Receiver Operating Characteristic (ROC) curve metric (AUROC). It is commonly adopted as performance measurement in anomaly detection tasks and eliminates the subjective decision of threshold value to divide the "normal" samples from the anomalous ones. Comparison with State-of-the-art Methods. <ref type="table" target="#tab_4">Table I</ref> provides results on MNIST, Fashion-MNIST, CIFAR-10, ImageNet and CIFAR-100 in detail. Some popular methods are involved in comparison: VAE <ref type="bibr" target="#b96">[52]</ref>, DAGMM <ref type="bibr" target="#b97">[53]</ref>, DSEBM <ref type="bibr">[12]</ref>, ALOCC <ref type="bibr">[5]</ref>, AnoGAN <ref type="bibr">[3]</ref>, ADGAN <ref type="bibr">[13]</ref>, GANomaly <ref type="bibr">[4]</ref>, OCGAN <ref type="bibr" target="#b90">[46]</ref>, GeoTrans <ref type="bibr" target="#b91">[47]</ref> and our baseline backbone AE. Results of VAE, AnoGAN and ADGAN are borrowed from <ref type="bibr">[13]</ref>. Results of DAGMM, DSEBM and GeoTrans are borrowed from <ref type="bibr" target="#b91">[47]</ref>. We use the officially released source codes to fill the incomplete results reported in <ref type="bibr">[4]</ref>, <ref type="bibr">[5]</ref> with our experimental settings. For RGB datasets, such as CIFAR-10 and CIFAR-100, we use graying and random rotation operations tandemly, together with some standard data augmentations (flipping / mirroring / shifting), which is widely used in <ref type="bibr" target="#b105">[61]</ref>, <ref type="bibr" target="#b106">[62]</ref>. For grayscale datasets, such as MNIST and Fashion-MNIST, we only use random rotation transformation, without any data augmentation.</p><p>On all involved datasets, experiment results present that the average AUROC of ARNet outperforms all other methods to different extents. For each individual image class, we also obtain competitive performances, showing effectiveness for anomaly detection. To further validate the effectiveness of our method, we conduct experiments on a subset of the ILSVRC 2012 classification dataset <ref type="bibr">[8]</ref>. <ref type="table" target="#tab_4">Table I</ref> also shows the performance of GANomaly, GeoTrans, baseline AE and our method on ImageNet. As can be seen, our method significantly outperforms the other three methods and maintains performance stability on more difficult datasets. We further compared to two self-supervised approaches, Colorization <ref type="bibr" target="#b62">[18]</ref> and RotNet <ref type="bibr" target="#b92">[48]</ref>, for anomaly detection. For each method, we train the model with the dataset containing only normal data. When testing, we utilize the original loss of each method as the anomaly score, and the sample corresponding to a large loss is considered as anomalous. As shown in <ref type="table" target="#tab_4">Table I</ref>, ARNet outperforms the two self-supervised methods. Computational Cost. We investigate the computational efficiency and the GPU memory cost. For all methods, we test for 10 times on CIFAR-10 (total 10,000 images) with NVIDIA GTX 1080Ti and record the average FPS and the GPU memory costs, the results are shown in <ref type="figure">Figure 3</ref>. ALOCC <ref type="bibr">[5]</ref> has an advantage in the highest computational efficiency (1049fps) but takes up relatively large GPU memory (1291MB). GeoTrans <ref type="bibr" target="#b91">[47]</ref> takes up more GPU memory (1389MB) and suffering from a slow computational efficiency (35fps). ARNet reaches 270fps (5? faster than GeoTrans) and takes only 713MB of GPU memory. Though during the testing phase, ARNet needs to traverse all the transformations functions, ARNet still reaches a considerable efficiency thanks to its light network structure. With the best performance in AUROC, ARNet takes up a relatively small GPU memory with high computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments on Real-world Anomaly Detection</head><p>Previous works <ref type="bibr">[13]</ref>, <ref type="bibr" target="#b91">[47]</ref> experiment on multi-class classification datasets since there is a lack of comprehensive realworld datasets available for anomaly detection. By defining anomalous events as occurrences of different object classes and splitting the datasets based on unsupervised settings, the multiclass datasets can be used for anomaly detection experiments. However, the real anomalous data does not necessarily meet the above settings, e.g., damaged objects. In this section, we experiment on the most recent real-world anomaly detection benchmark dataset MVTec AD <ref type="bibr" target="#b65">[21]</ref>. MVTec anomaly detection dataset. MVTec Anomaly Detection (MVTec AD) dataset <ref type="bibr" target="#b65">[21]</ref> contains 5354 high-resolution color images of different object and texture categories. It contains normal images intended for training and images with anomalies intended for testing. The anomalies manifest themselves in the form of over 70 different types of defects such as scratches, dents, and various structural changes. In this paper, we conduct image-level anomaly detection tasks on the MVTec AD dataset to classify normal and anomalous objects. III: Average area under the ROC curve (AUROC) in % of anomaly detection methods for different components on CIFAR-10. "S", "G" and "R" represent scaling, graying and random rotation operations. The best performing method in each experiment is in bold. TABLE IV: Average area under the ROC curve (AUROC) in % of anomaly detection methods for different losses on CIFAR-10. " 1 " means 1 loss and " 2 " means 2 loss. For example, 2 ? 1 means using 2 loss as training loss to train autoencoders and using 1 loss to calculate restoration error when testing. The best performing method in each experiment is in bold.  Comparison with state-of-the-art methods. <ref type="table" target="#tab_4">Table II</ref> shows that ARNet performs better than baseline AE, GANomaly and GeoTrans. The advantages of ARNet over GeoTrans are growing from ideal datasets to real-world datasets MVTec AD. We conclude that ARNet is more adaptable to complex real-world environments.</p><formula xml:id="formula_7">ci 1 ? 1 1 ? 2 2 ? 2 2 ? 1(OURS</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study</head><p>We study the contribution of the proposed components of ARNet independently. <ref type="table" target="#tab_4">Table III</ref> shows experimental results of ablation study on CIFAR-10. It shows that both graying and random rotation operations improve the performance significantly, especially the random rotation operation. <ref type="table" target="#tab_4">Table IV</ref> shows the ablation study about the selection of restoration loss. It proves that using 2 loss as training loss and using 1 loss to calculate restoration error performs the best. Through the ablation study, we claim that the attribute erasing operations, network architecture and the loss function we used all have independent contributions to boost the model performance.</p><p>We use the image scaling to study the degradation problem of the ARNet caused by ill-selected attribute erasing operation. Downsampling of images can delete part of the image information, but it is not correlated to any object attributes. It will lead the model to learn a linear interpolation process in which both normal and anomalous data can be restored easily by inferring from neighboring pixels. Thus, this operation cannot remove any attribute which satisfies the assumptions introduced in Section III-A. In this case, the AEM module is not functional and the ARNet will degrade into a vanilla AE. We test on CIFAR-10 with a 0.5x scaling and obtain 58.8% AUROC for ARNet, while that of AE is 59.3%, showing that the ARNet <ref type="figure">Fig. 4</ref>: Training process under three methods. Both logs are achieved on the MNIST dataset. It shows the case when the digit "7" is the normal class. We attach complete logs for Fashion-MNIST and MNIST datasets in the appendix.</p><p>indeed degenerates into a vanilla AE with ill-selected attribute erasing operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Model Stability</head><p>Anomaly detection puts higher concerns on the stability of model performance than traditional classification tasks. It is because of the lack of anomalous data makes it impossible to do validation during training. Thus, model stability tends to be more important since without validation there is no way to select the best checkpoint for anomaly detection model in the training phases. The stability of model performance is mainly reflected in three aspects: 1) whether the model can stably reach convergence after acceptable training epochs in one training attempt; 2) whether the model can reach stable performance level in multiple independent training attempts under the same training configuration; 3) whether the model can stably achieve good performance in various datasets and training configurations. <ref type="figure">Figure 4</ref> shows AUC-changing during one run to reveal that our model performs more stably in the late training phase. Thus, through ARNet, a highly reliable model can be achieved through acceptable training epochs in this practically validation-unavailable task. In order to test the stability of multiple training performances, we rerun GeoTrans <ref type="bibr" target="#b91">[47]</ref> and our method for 10 times on MNIST. <ref type="table" target="#tab_9">Table V</ref> shows that GeoTrans suffers a larger performance fluctuation compared with our method. <ref type="table" target="#tab_4">Table I</ref> shows that our method has the strongest stability of this type, as can be seen from its low standard deviation (SD). in % of anomaly detection methods on CIFAR-10-C dataset with five distorted severity <ref type="bibr" target="#b66">[22]</ref>. Each level includes all the 19 distorted categories. The last column shows the average AUROC and its anomaly detection performance degradation compared to the original CIFAR-10 test set in <ref type="figure">Figure I</ref>. The best performing method in each experiment is in bold.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Visualization Analysis</head><p>In order to demonstrate the effectiveness of the attribute restoration framework for anomaly detection in a simple and straightforward way, we visualize some restoration outputs from ARNet, comparing with GANomaly in <ref type="figure">Figure 5</ref> and <ref type="figure">Figure 6</ref>. For MNIST and CIFAR-10, all visualization results are based on the same experimental setting in which the number "6" and the class "horse" are considered normal samples respectively.</p><p>The first column "Ori" represents original images. "I" means images after Attribute Erasing Module. Note that the restoration error is calculated between outputs and original images. Cases with low restoration error with original images are considered normal, otherwise anomalous. For example, the bottom line in <ref type="figure">Figure 5</ref> shows the testing results of number "9". Intuitively, four outputs are far different from "Ori" and thus recognized as anomalous. Except for the number "6", the other numbers get either wrong direction or ambiguous restoration outputs from ARNet. It enlarges the gap of restoration error between normal and anomalous data. However, all the outputs from GANomaly are similar to the ground truth, meaning that it is less capable to distinguish between normal and anomalous data. In addition, for the anomalous cases in <ref type="figure">Figure 6</ref>, restoration errors even larger since the wrong colors the ARNet used for image restoration. All the outputs show that ARNet attempts to restore the input images using the orientation or color distribution of the normal classes learning from the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXTENDED EXPERIMENTS AND DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Can ARNet Extract Semantic Features?</head><p>ARNet has achieved better performance by introducing the restoration framework into anomaly detection. Here we further investigate whether ARNet can extract the semantic features.</p><p>The CIFAR-10-C <ref type="bibr" target="#b66">[22]</ref> is designed to measure a method's robustness to some common image corruptions. It is obtained by applying different corruptions to CIFAR-10. There are in total 19 diverse corruption types and each type of corruption has five levels of severity, resulting in 95 distinct corruptions. With the distortions, the models with less performance drop are more likely to be able to extract semantic features, as the low-level information in the original images has been damaged to a certain extent. We train all models with CIFAR-10 and test them on the CIFAR-10-C test set. <ref type="table" target="#tab_4">Table VI and Table VII</ref> provide the comparison of ARNet with several state-of-the-art methods ALOCC <ref type="bibr">[5]</ref>, GANomaly <ref type="bibr" target="#b87">[43]</ref> and GeoTrans <ref type="bibr" target="#b91">[47]</ref>. As can be seen from <ref type="table" target="#tab_4">Table VI</ref>, ARNet and GeoTrans are robust to corruptions of input images and with only 6.1% and 5.8% performance drop on CIFAR-10-C, respectively, suggesting that the features extracting by ARNet and GeoTrans have more semantic information. On the contrary, both ALOCC and GANomaly have much more significant performance drop in this case, indicating that they rely heavily on the low-level information.</p><p>We further show that the latent representations extracted by ARNet can be used to distinguish samples of different categories through T-SNE analysis, i.e. the extracting features already contain semantic information about the different categories. We first show T-SNE visualization results of CIFAR-10. As shown in <ref type="figure">Figure 7</ref>, feature maps of latent space for ARNet are more discriminative than those of AE and GANomaly, suggesting that ARNet can extract more semantic features than the other two methods. <ref type="figure">Figure 8</ref> further shows a more specific case with handwritten numbers 6 and 9 from MNIST, where numbers 6 are set to be normal. We use the random rotation operation in this task. As can be seen from <ref type="figure">Figure 8</ref>, T-SNE clusters the data into four categories, corresponding to the four degrees of rotations on normal data. For example, the number 6 without rotation and the number 9 rotated 180 degrees are grouped into the same category. In order to restore the handwritten number 6 to the original images with the correct orientation, the decoder needs to simply map these four categories to the one category which has the same orientation. And this simple mapping operation will cause a large image restoration error for the number 9.</p><p>B. Do the attributes easily be shared between normal and anomalous data?</p><p>We have introduced graying and random rotation operations to erase the color distribution and the orientation accordingly. In Sec III-A, the two attributes are required to satisfy the second assumption, i.e. either different or connected to different semantic information between normal and anomalous data. We here argue that the second assumption is true for most cases in real-world scenarios. Taking the color as an example, Zhang et al. <ref type="bibr" target="#b62">[18]</ref> indicated that there are many statistical dependencies between the semantics and textures of grayscale images and their color versions. For each pixel, to restore its color information, the corresponding semantics and textures from the whole image should be taken into consideration. Even the color distribution is similar between normal and anomalous data, e.g. a yellow cat and a yellow car, the connected semantic information is different. Thus, when testing on anomalous data, the network is unable to utilize the unseen semantic information to restore the color properly. The same principle also applies to the orientation, which is connected with more complex semantics and textures. In conclusion, once the first assumption is satisfied, where AEM can erase attribute which connected to semantic information, the second assumption is easy to be satisfied as semantic information is likely to be different among different classes. Therefore, in future research, we do not need to be constrained to the choice of attributes but should consider more on how to extract more powerful semantic features under our attribute restoration framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION AND FUTURE WORK</head><p>In this paper, we propose a novel technique named Attribute Restoration Network (ARNet) for anomaly detection. Attribute Erasing Module is employed to erase certain attributes. The ARNet is forced to learn the attribute related features to restore the original data. The restoration error is expected to be a good indicator of anomalous data. We experiment with two simple but effective attribute erasing operations: graying and random rotation, and show that our method not only outperforms stateof-the-art methods but also achieves high stability. Notably, there are still more operations to explore. These operations are likely to further improve the performance of ARNet for anomaly detection by effectively extracting semantic features. We look forward to the addition of more operations and the exploration of a more intelligent operations selection strategy. In addition, this way to learn semantic feature embeddings can also be applied to more fields, opening avenues for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A INDEX FOR IMAGENET [8]</head><p>We group categories into 10 superclasses by merging similar categories for anomaly detection. <ref type="table" target="#tab_4">Table VIII</ref> shows the specific category index.  <ref type="bibr" target="#b65">[21]</ref> MVTec AD dataset contains 5354 high-resolution color images of different object and texture categories. It contains normal images intended for training and images with anomalies intended for testing. The anomalies manifest themselves in the form of over 70 different types of defects such as scratches, dents, contaminations, and various structural changes. <ref type="table" target="#tab_4">Table IX</ref> shows class names and anomalous types for each categories.    <ref type="bibr" target="#b98">[54]</ref>. Ten sub-images represent the cases where the digit "0"-"9" is set as the normal category by order.</p><p>maxpooling or an upsampling operation, following two 3 ? 3 convolutional layers. Skip-connection operations are added to facilitate the backpropagation of the gradient and improve the performance of image restoration.   <ref type="figure">Fig. 11</ref>: Restoration error maps of AE and ARNet on an anomalous frame of ShanghaiTech. Chasing is the anomalous event in this frame (red bounding box). "G" means graying and "R" means random rotation transformation. ARNet can significantly highlight the anomalous parts in the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX D MODEL STABILITY</head><p>We argue that our proposed method achieves more robust performance. The main challenge in the task of anomaly detection is the lack of negative samples. Without validation, model stability tends to be more important than traditional data classification tasks. We train three models, including ARNet, traditional autoencoder <ref type="bibr">[6]</ref> and GANomaly <ref type="bibr">[4]</ref>, respectively on each category of MNIST <ref type="bibr" target="#b98">[54]</ref> and Fashion-MNIST <ref type="bibr" target="#b99">[55]</ref> datasets and test models every 5 epochs along with training. The traditional autoencoder <ref type="bibr">[6]</ref> and GANomaly <ref type="bibr">[4]</ref> are set as our baseline model. The model performance of validation during the training process is shown in <ref type="figure">Figure 9</ref> and <ref type="figure">Figure 10</ref>, from which we can see the performance of our ARNet method always converges in a high position; moreover, ARNet shows the highest performance stability at the end of the training process.</p><p>APPENDIX E EXPERIMENTS ON VIDEO ANOMALY DETECTION Video anomaly detection, which is distinguished from imagelevel anomaly detection, requires detections of anomalous objects and strenuous motions in the video data. We here experiment on a most recent video anomaly detection benchmark dataset ShanghaiTech <ref type="bibr" target="#b107">[63]</ref>, comparing our methods with other state-of-the-arts. ShanghaiTech. ShanghaiTech <ref type="bibr" target="#b107">[63]</ref> has 13 scenes with complex light conditions and camera angles. It contains 130 anomalous events and over 270, 000 training frames. In the dataset, objects except for pedestrians (e.g., vehicles) and strenuous motion (e.g., fighting and chasing) are treated as anomalies.</p><p>Comparison with state-of-the-art methods. Since our ARNet is designed for image-level anomaly detection, different from some state-of-the-arts <ref type="bibr">[11]</ref>, <ref type="bibr" target="#b107">[63]</ref>, <ref type="bibr" target="#b108">[64]</ref>, we use single frames but not stacking neighbor frames as inputs. In order to apply the random rotation transformation, we resize all the images into 480 ? 480. We here use ResNet34 <ref type="bibr" target="#b105">[61]</ref> as our encoder. Following <ref type="bibr">[11]</ref>, <ref type="bibr" target="#b107">[63]</ref>, <ref type="bibr" target="#b109">[65]</ref>, we obtain the normality score p u of the uth frame by normalizing the errors to range [0, 1]:</p><formula xml:id="formula_8">p u = 1 ? e u ? min u (e u ) max u (e u ) ? min u (e u ) ,<label>(6)</label></formula><p>where e u denotes the restoration error of the uth frame in a video episode. The value of p u closer to 0 indicates the frame is more likely an anomalous frame. <ref type="table" target="#tab_4">Table XI</ref> shows the AUROC values on ShanghaiTech dataset. Results show that our ARNet outperforms all the state-of-the-arts, including some temporal dependent methods <ref type="bibr">[11]</ref>, <ref type="bibr" target="#b107">[63]</ref>, <ref type="bibr" target="#b108">[64]</ref>. Visualization Analysis. <ref type="figure">Figure 11</ref> shows restoration error maps of AE and ARNet on an anomalous frame of ShanghaiTech, in which the highlight regions (regions with high restoration error) are considered as anomalous. In this frame, human chasing is the anomalous event (red bounding box in <ref type="figure">Figure 11 (a)</ref>). Due to good model generalization, AE reconstructs this frame properly even including the anomalous event (human chasing), leading to a low reconstruction error (reconstruction error map almost all black in <ref type="figure">Figure 11</ref> (b)) Thus, AE cannot correctly detect this anomalous event. On the contrary, ARNet can not restore the anomalous region properly and significantly highlights the anomalous regions in the restoration error maps in <ref type="figure">Figure 11  (c and d)</ref>. This is the reason why ARNet outperforms state-ofthe-arts in video anomaly detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>arXiv:1911.10676v3 [cs.CV] 12 Dec 2020 JOURNAL OF L A T E X CLASS FILES,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 : 1 I 2 O 2 I 3 Fig. 6 :</head><label>512236</label><figDesc>Visualization analysis comparing with GANomaly on MNIST. "Ori", "I" and "O" represent original images, inputs and outputs, respectively. Cases with outputs similar to "Ori" are considered normal, otherwise anomalous. All visualization results are based on the number "6" as normal samples.I 0 O 0 I 1 OVisualization analysis comparing with GANomaly on CIFAR-10. "Ori", "I" and "O" represent original images, inputs and outputs, respectively. Cases with outputs similar to "Ori" are considered normal, otherwise anomalous. All visualization results are based on the class "horse" as normal samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 7 :Fig. 8 :</head><label>78</label><figDesc>T-SNE visualization of latent spaces of autoencoder, GANomaly and ARNet on CIFAR-10. The corresponding AUROCs of anomaly detection are marked in the upper left corners. T-SNE visualization of latent spaces of ARNet on number 6 and 9 in the handwritten dataset MNIST. Number 6 is set as the normal class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>9 Fig. 9 :</head><label>99</label><figDesc>Reported Accuracy under the L 1 metric on the test dataset of MNIST</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>9 Fig. 10 :</head><label>910</label><figDesc>Reported Accuracy under the L 1 metric on the test dataset of Fashion-MNIST<ref type="bibr" target="#b99">[55]</ref>. Ten sub-images represent the cases where the class 0 -9 is set as the normal category by order.(a) Frame (b) AE-Conv2D (c) ARNet(G) (d) ARNet(G+R)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In CVPR, 2017. 5 [17] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In CVPR, 2017. 4 [18] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 4, 5 [19] B Ravi Kiran, Dilip Mathew Thomas, and Ranjith Parakkal. An overview of deep learning based methods for unsupervised and semi-supervised anomaly detection in videos. Journal of Imaging, 2018. 2 [20] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009. 4 [21] Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998. 4 [22] Kimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin. Training confidence-calibrated classifiers for detecting out-ofdistribution samples. In ICLR, 2018. 2 [23] Weixin Luo, Wen Liu, and Shenghua Gao. A revisit of sparse coding based anomaly detection in stacked rnn framework.</figDesc><table><row><cell>864</cell><cell></cell></row><row><cell>865</cell><cell></cell></row><row><cell>866</cell><cell></cell></row><row><cell>867</cell><cell></cell></row><row><cell>868</cell><cell></cell></row><row><cell>869</cell><cell></cell></row><row><cell>870</cell><cell></cell></row><row><cell>871</cell><cell></cell></row><row><cell>872</cell><cell></cell></row><row><cell>873</cell><cell></cell></row><row><cell>874</cell><cell></cell></row><row><cell>875</cell><cell></cell></row><row><cell>876</cell><cell></cell></row><row><cell>877</cell><cell></cell></row><row><cell>878</cell><cell></cell></row><row><cell>879</cell><cell></cell></row><row><cell>880</cell><cell></cell></row><row><cell>881</cell><cell></cell></row><row><cell>882</cell><cell></cell></row><row><cell>883</cell><cell></cell></row><row><cell>884</cell><cell></cell></row><row><cell>885</cell><cell></cell></row><row><cell>886 887 888 889 890</cell><cell>In ICCV, 2017. 2, 7 [24] Markos Markou and Sameer Singh. Novelty detection: a review-part 1: statistical approaches. Signal Processing, 2003. 2</cell></row><row><cell>891</cell><cell></cell></row><row><cell>892</cell><cell></cell></row><row><cell>893</cell><cell></cell></row><row><cell>894</cell><cell></cell></row><row><cell>895</cell><cell></cell></row><row><cell>896</cell><cell></cell></row><row><cell>897</cell><cell></cell></row><row><cell>898</cell><cell></cell></row><row><cell>899</cell><cell></cell></row><row><cell>900</cell><cell></cell></row><row><cell>901</cell><cell></cell></row><row><cell>902</cell><cell></cell></row><row><cell>903</cell><cell></cell></row><row><cell>904</cell><cell></cell></row><row><cell>905</cell><cell></cell></row><row><cell>906</cell><cell></cell></row><row><cell>907</cell><cell></cell></row><row><cell>908</cell><cell></cell></row><row><cell>909</cell><cell></cell></row><row><cell>910</cell><cell></cell></row><row><cell>911</cell><cell></cell></row><row><cell>912</cell><cell></cell></row><row><cell>913</cell><cell></cell></row><row><cell>914</cell><cell></cell></row><row><cell>915</cell><cell></cell></row><row><cell>916</cell><cell></cell></row><row><cell>917</cell><cell></cell></row><row><cell></cell><cell>1, 2</cell></row><row><cell>9</cell><cell></cell></row></table><note>[25] Markos Markou and Sameer Singh. Novelty detection: a review-part 2: : neural network based approaches. Signal Processing, 2003. 2 [26] Jonathan Masci, Ueli Meier, Dan Cire?an, and J?rgen Schmid- huber. Stacked convolutional auto-encoders for hierarchical feature extraction. In International Conference on Artificial Neural Networks, 2011. 1 [27] Pramuditha Perera, Ramesh Nallapati, and Bing Xiang. Oc- gan: One-class novelty detection using gans with constrained latent representations. 2019. 2, 4, 5 [28] Marco A. F. Pimentel, David A. Clifton, Clifton Lei, and Lionel Tarassenko. A review of novelty detection. Signal Processing, 2014. 2 [29] Mostafa Rahmani and George K. Atia. Coherence pursuit: Fast, simple, and robust principal component analysis. IEEE Transactions on Signal Processing, 2017. 2 [30] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, 2015. 4 [31] Lukas Ruff, Nico G?rnitz, Lucas Deecke, Shoaib Ahmed Sid- diqui, Robert Vandermeulen, Alexander Binder, Emmanuel M?ller, and Marius Kloft. Deep one-class classification. In ICML, 2018. 1 [32] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San- jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. IJCV, 2015. 2, 3, 4, 5 [33] Mohammad Sabokrou, Mohammad Khalooei, Mahmood Fathy, and Ehsan Adeli. Adversarially learned one-class classifier for novelty detection. In CVPR, 2018.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In CVPR, 2017. 5 [17] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In CVPR, 2017. 4 [18] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 4, 5 [19] B Ravi Kiran, Dilip Mathew Thomas, and Ranjith Parakkal. An overview of deep learning based methods for unsupervised and semi-supervised anomaly detection in videos. Journal of Imaging, 2018. 2 [20] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009. 4 [21] Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998. 4 [22] Kimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin. Training confidence-calibrated classifiers for detecting out-ofdistribution samples. In ICLR, 2018. 2 [23] Weixin Luo, Wen Liu, and Shenghua Gao. A revisit of sparse coding based anomaly detection in stacked rnn framework. Marco A. F. Pimentel, David A. Clifton, Clifton Lei, and Lionel Tarassenko. A review of novelty detection. Signal Processing, 2014. 2 [29] Mostafa Rahmani and George K. Atia. Coherence pursuit: Fast, simple, and robust principal component analysis. IEEE Transactions on Signal Processing, 2017. 2 [30] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, 2015. 4 [31] Lukas Ruff, Nico G?rnitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Robert Vandermeulen, Alexander Binder, Emmanuel M?ller, and Marius Kloft. Deep one-class classification. In ICML, 2018. 1 [32] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. IJCV, 2015. 2, 3, 4, 5 [33] Mohammad Sabokrou, Mohammad Khalooei, Mahmood Fathy, and Ehsan Adeli. Adversarially learned one-class classifier for novelty detection. In CVPR, 2018. 1, 2 9 Fig. 2: Pipeline for anomaly detection with attribute restoration framework with mathematical expression.</figDesc><table><row><cell>In ICCV, 2017. 2, 7</cell></row><row><cell>[24] Markos Markou and Sameer Singh. Novelty detection: a</cell></row><row><cell>review-part 1: statistical approaches. Signal Processing,</cell></row><row><cell>2003. 2</cell></row><row><cell>[25] Markos Markou and Sameer Singh. Novelty detection: a</cell></row><row><cell>review-part 2: : neural network based approaches. Signal</cell></row><row><cell>Processing, 2003. 2</cell></row><row><cell>[26] Jonathan Masci, Ueli Meier, Dan Cire?an, and J?rgen Schmid-</cell></row><row><cell>huber. Stacked convolutional auto-encoders for hierarchical</cell></row><row><cell>feature extraction. In International Conference on Artificial</cell></row><row><cell>Neural Networks, 2011. 1</cell></row><row><cell>[27] Pramuditha Perera, Ramesh Nallapati, and Bing Xiang. Oc-</cell></row><row><cell>gan: One-class novelty detection using gans with constrained</cell></row><row><cell>latent representations. 2019. 2, 4, 5</cell></row><row><cell>[28]</cell></row></table><note>A. Attribute Erasing Module (AEM)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE I :</head><label>I</label><figDesc>Average area under the ROC curve (AUROC) in % of anomaly detection methods. For every dataset, each model is trained on the single class, and tested against all other classes. "SD" means standard deviation among classes. The best performing method is in bold. This operation averages each pixel value along the channel dimension of images.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>avg</cell><cell>SD</cell></row><row><cell>MNIST</cell><cell>VAE [52] ALOCC [5] AnoGAN [3] ADGAN [13] GANomaly [4] OCGAN [46] GeoTrans [47]</cell><cell>92.1 99.5 99.0 99.5 97.2 99.8 98.2</cell><cell>99.9 99.1 99.8 99.9 99.6 99.9 91.6</cell><cell>81.5 92.0 88.8 93.6 85.1 94.2 99.4</cell><cell>81.4 92.1 91.3 92.1 90.6 96.3 99.0</cell><cell>87.9 93.5 94.4 94.9 94.5 97.5 99.1</cell><cell>81.1 84.7 91.2 93.6 94.9 98.0 99.6</cell><cell>94.3 97.5 92.5 96.7 97.1 99.1 99.9</cell><cell>88.6 94.1 96.4 96.8 93.9 98.1 96.3</cell><cell>78.0 87.5 88.3 85.4 79.7 93.9 97.2</cell><cell>92.0 92.8 95.8 95.7 95.4 98.1 99.2</cell><cell>87.7 93.3 93.7 94.7 92.8 97.5 98.0</cell><cell>7.05 4.70 4.00 4.15 6.12 2.10 2.50</cell></row><row><cell></cell><cell>AE OURS</cell><cell>98.8 98.6</cell><cell>99.3 99.9</cell><cell>91.7 99.0</cell><cell>88.5 99.1</cell><cell>86.2 98.1</cell><cell>85.8 98.1</cell><cell>95.4 99.7</cell><cell>94.0 99.0</cell><cell>82.3 93.6</cell><cell>96.5 97.8</cell><cell>91.9 98.3</cell><cell>5.90 1.78</cell></row><row><cell>Fashion-MNIST</cell><cell>DAGMM [53] DSEBM [12] ADGAN [13] GANomaly [4] GeoTrans [47]</cell><cell>42.1 91.6 89.9 80.3 99.4</cell><cell>55.1 71.8 81.9 83.0 97.6</cell><cell>50.4 88.3 87.6 75.9 91.1</cell><cell>57.0 87.3 91.2 87.2 89.9</cell><cell>26.9 85.2 86.5 71.4 92.1</cell><cell>70.5 87.1 89.6 92.7 93.4</cell><cell>48.3 73.4 74.3 81.0 83.3</cell><cell>83.5 98.1 97.2 88.3 98.9</cell><cell>49.9 86.0 89.0 69.3 90.8</cell><cell>34.0 97.1 97.1 80.3 99.2</cell><cell>51.8 86.6 88.4 80.9 93.5</cell><cell>16.47 8.61 6.75 7.37 5.22</cell></row><row><cell></cell><cell>AE OURS</cell><cell>71.6 92.7</cell><cell>96.9 99.3</cell><cell>72.9 89.1</cell><cell>78.5 93.6</cell><cell>82.9 90.8</cell><cell>93.1 93.1</cell><cell>66.7 85.0</cell><cell>95.4 98.4</cell><cell>70.0 97.8</cell><cell>80.7 98.4</cell><cell>80.9 93.9</cell><cell>11.03 4.70</cell></row><row><cell>CIFAR-10</cell><cell>VAE [52] DAGMM [53] DSEBM [12] ALOCC [5] AnoGAN [3] ADGAN [13] GANomaly [4] OCGAN [46] RotNet [48] GeoTrans [47]</cell><cell>62.0 41.4 56.0 62.0 61.0 63.2 93.5 75.7 71.9 74.7</cell><cell>66.4 57.1 48.3 71.7 56.5 52.9 60.8 53.1 94.5 95.7</cell><cell>38.2 53.8 61.9 53.7 64.8 58.0 59.1 64.0 78.4 78.1</cell><cell>58.6 51.2 50.1 56.0 52.8 60.6 58.2 62.0 70.0 72.4</cell><cell>38.6 52.2 73.3 58.7 67.0 60.7 72.4 72.3 77.2 87.8</cell><cell>58.6 49.3 60.5 56.3 59.2 65.9 62.2 62.0 86.6 87.8</cell><cell>56.5 64.9 68.4 61.2 62.5 61.1 88.6 72.3 81.6 83.4</cell><cell>62.2 55.3 53.3 60.5 57.6 63.0 56.0 57.5 93.7 95.5</cell><cell>66.3 51.9 73.9 74.4 72.3 74.4 76.0 82.0 90.7 93.3</cell><cell>73.7 54.2 63.6 67.1 58.2 64.4 68.1 55.4 88.8 91.3</cell><cell>58.1 53.1 60.9 62.2 61.2 62.4 69.5 65.6 83.3 86.0</cell><cell>11.50 5.95 9.10 6.90 5.68 5.56 13.08 9.52 8.82 8.52</cell></row><row><cell></cell><cell>AE OURS</cell><cell>57.1 78.5</cell><cell>54.9 89.8</cell><cell>59.9 86.1</cell><cell>62.3 77.4</cell><cell>63.9 90.5</cell><cell>57.0 84.5</cell><cell>68.1 89.2</cell><cell>53.8 92.9</cell><cell>64.4 92.0</cell><cell>48.6 85.5</cell><cell>59.0 86.6</cell><cell>5.84 5.35</cell></row><row><cell>ImageNet</cell><cell>GANomaly [4] Colorization [18] RotNet [48] GeoTrans [47]</cell><cell>58.9 67.6 70.0 72.9</cell><cell>57.5 62.9 84.1 61.0</cell><cell>55.7 56.8 66.5 66.8</cell><cell>57.9 62.2 82.3 82.0</cell><cell>47.9 64.7 70.3 56.7</cell><cell>61.2 68.5 79.8 70.1</cell><cell>56.8 62.2 80.3 68.5</cell><cell>58.2 63.7 75.1 77.2</cell><cell>49.7 66.5 72.4 62.8</cell><cell>48.8 71.9 82.0 83.6</cell><cell>55.3 64.7 76.2 70.1</cell><cell>4.46 4.18 6.28 8.43</cell></row><row><cell></cell><cell>AE OURS</cell><cell>57.1 71.9</cell><cell>51.3 85.8</cell><cell>47.7 70.7</cell><cell>57.4 78.8</cell><cell>43.8 69.5</cell><cell>54.9 83.3</cell><cell>54.6 80.6</cell><cell>51.3 72.4</cell><cell>48.3 74.9</cell><cell>41.5 84.3</cell><cell>50.8 77.2</cell><cell>5.16 5.77</cell></row><row><cell>Dataset</cell><cell>Method</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell></cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell></row><row><cell></cell><cell>DAGMM [53] DSEBM [12] ALOCC [5] ADGAN [13] GANomaly [4] GeoTrans [47]</cell><cell>43.4 64.0 52.7 63.1 57.9 74.7</cell><cell>49.5 47.9 56.6 54.9 51.9 68.5</cell><cell>66.1 53.7 61.2 41.3 36.0 74.0</cell><cell>52.6 48.4 61.1 50.0 46.5 81.0</cell><cell>56.9 59.7 66.7 40.6 46.6 78.4</cell><cell cols="2">52.4 46.6 50.6 42.8 42.9 59.1</cell><cell>55.0 51.7 63.9 51.1 53.7 81.8</cell><cell>52.8 54.8 66.2 55.4 59.4 65.0</cell><cell>53.2 66.7 50.9 59.2 63.7 85.5</cell><cell>42.5 71.2 73.4 62.7 68.0 90.6</cell><cell>52.7 78.3 71.1 79.8 75.6 87.6</cell></row><row><cell>CIFAR-</cell><cell>AE OURS</cell><cell>66.7 77.5</cell><cell>55.4 70.0</cell><cell>41.4 62.4</cell><cell>49.2 76.2</cell><cell>44.9 77.7</cell><cell cols="2">40.6 64.0</cell><cell>50.2 86.9</cell><cell>48.1 65.6</cell><cell>66.1 82.7</cell><cell>63.0 90.2</cell><cell>52.7 85.9</cell></row><row><cell>100</cell><cell>Method</cell><cell>11</cell><cell>12</cell><cell>13</cell><cell>14</cell><cell>15</cell><cell>16</cell><cell></cell><cell>17</cell><cell>18</cell><cell>19</cell><cell>avg</cell><cell>SD</cell></row><row><cell></cell><cell>DAGMM [53] DSEBM [12] ALOCC [5] ADGAN [13] GANomaly [4] GeoTrans [47]</cell><cell>46.4 62.7 56.9 53.7 57.6 83.9</cell><cell>42.7 66.8 63.6 58.9 58.7 83.2</cell><cell>45.4 52.6 56.0 57.4 59.9 58.0</cell><cell>57.2 44.0 57.9 39.4 43.9 92.1</cell><cell>48.8 56.8 58.2 55.6 59.9 68.3</cell><cell cols="2">54.4 63.1 57.0 63.3 64.4 73.5</cell><cell>36.4 73.0 73.5 66.7 71.8 93.8</cell><cell>52.4 57.7 61.0 44.3 54.9 90.7</cell><cell>50.3 55.5 58.8 53.0 56.8 85.0</cell><cell>50.5 58.8 60.9 54.7 56.5 78.7</cell><cell>6.55 9.36 6.70 10.08 9.94 10.76</cell></row><row><cell></cell><cell>AE OURS</cell><cell>62.1 83.5</cell><cell>59.6 84.6</cell><cell>49.8 67.6</cell><cell>48.1 84.2</cell><cell>56.4 74.1</cell><cell cols="2">57.6 80.3</cell><cell>47.2 91.0</cell><cell>47.1 85.3</cell><cell>41.5 85.4</cell><cell>52.4 78.8</cell><cell>8.11 8.82</cell></row></table><note>? Graying:? Random rotation: This operation rotates x anticlockwise by angle ? around the center of each image channel. The rotation angle ? is randomly selected from a set {0? , 90 ? , 180 ? , 270 ? }.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE II :</head><label>II</label><figDesc>Average area under the ROC curve (AUROC) in % of anomaly detection methods on MVTec AD<ref type="bibr" target="#b65">[21]</ref> dataset. The best performing method in each experiment is in bold.</figDesc><table><row><cell></cell><cell cols="2">Method</cell><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell><cell>14</cell><cell>avg</cell></row><row><cell cols="4">GeoTrans [47] GANomaly [4] AE OURS</cell><cell>74.4 89.2 65.4 94.1</cell><cell>67.0 73.2 61.9 68.1</cell><cell>61.9 70.8 82.5 88.3</cell><cell>84.1 84.2 79.9 86.2</cell><cell>63.0 74.3 77.3 78.6</cell><cell>41.7 79.4 73.8 73.5</cell><cell>86.9 79.2 64.6 84.3</cell><cell>82.0 74.5 86.8 87.6</cell><cell>78.3 75.7 63.9 83.2</cell><cell>43.7 69.9 64.1 70.6</cell><cell>35.9 78.5 73.1 85.5</cell><cell>81.3 70.0 63.7 66.7</cell><cell>50.0 74.6 99.9 100</cell><cell>97.2 65.3 76.9 100</cell><cell>61.1 83.4 97.0 92.3</cell><cell>67.2 76.2 75.4 83.9</cell></row><row><cell></cell><cell></cell><cell cols="2">GeoTrans</cell><cell></cell><cell></cell><cell cols="2">ARNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>AUROC (%)</cell><cell>75 80 70</cell><cell cols="2">1389MB</cell><cell></cell><cell></cell><cell cols="2">GANomaly 713MB</cell><cell></cell><cell>ALOCC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>65</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>458MB</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>60</cell><cell cols="2">951MB</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>55</cell><cell cols="3">ADGAN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1291MB</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>30</cell><cell>60</cell><cell cols="2">90 120 150 180</cell><cell>210 240</cell><cell>270</cell><cell></cell><cell>1000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FPS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>) 57.1 54.9 59.9 62.3 63.9 57.0 68.1 53.8 64.4 48.6 59.3 ARNet+S 72.8 41.8 66.4 57.5 71.0 62.8 68.4 48.5 56.8 31.9 57.8 ARNet+G 67.4 60.9 60.5 67.1 67.0 65.5 70.7 69.3 69.7 61.0 65.6</figDesc><table><row><cell>Method</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>avg</cell></row><row><cell>AE (reconstructionARNet+R ARNet+G+R</cell><cell cols="11">76.1 80.0 83.6 77.1 89.2 83.0 82.6 85.0 90.0 75.9 82.2 78.5 89.8 86.1 77.4 90.5 84.5 89.2 92.9 92.0 85.5 86.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE V :</head><label>V</label><figDesc>Average area under the ROC curve (AUROC) in % of anomaly detection methods on MNIST for ten runs in which digit number "1" is taken as normal data. Our stability is much higher than GeoTrans.</figDesc><table><row><cell>Methods</cell><cell>#1</cell><cell>#2</cell><cell>#3</cell><cell>#4</cell><cell>#5</cell><cell>#6</cell><cell>#7</cell><cell>#8</cell><cell>#9</cell><cell>#10</cell><cell>avg</cell><cell>SD</cell></row><row><cell cols="2">GeoTrans [47] 91.55 OURS 99.93</cell><cell>72.38 99.94</cell><cell>81.26 99.95</cell><cell>82.94 99.94</cell><cell>87.04 99.95</cell><cell>87.95 99.93</cell><cell>87.24 99.93</cell><cell>81.77 99.94</cell><cell>85.51 99.92</cell><cell>85.68 99.93</cell><cell>84.33 99.94</cell><cell>0.01 5.22</cell><cell>.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VI :</head><label>VI</label><figDesc>Average area under the ROC curve (AUROC)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE VII</head><label>VII</label><figDesc></figDesc><table><row><cell cols="6">: Average area under the ROC curve (AUROC) in % of anomaly detection methods on CIFAR-10-C dataset with 19 distorted categories [22]. For each distorted category, we conduct experiments on five levels of distorted severity, and the result is reported in average AUROC. The best performing method in each experiment is in bold.</cell></row><row><cell></cell><cell></cell><cell>ALOCC [5]</cell><cell>GANomaly [4]</cell><cell>GeoTrans [47]</cell><cell>ARNet (ours)</cell></row><row><cell>Noise</cell><cell>Gauss Shot Impulse Speckle</cell><cell>49.0 49.0 49.0 48.9</cell><cell>47.9 48.0 47.8 48.0</cell><cell>75.3 77.5 75.4 77.6</cell><cell>76.9 78.9 76.3 78.7</cell></row><row><cell>Blur</cell><cell>Gauss Defocus Glass Zoom Motion</cell><cell>49.0 49.0 48.7 48.9 49.0</cell><cell>48.2 48.2 48.0 48.2 48.1</cell><cell>82.1 83.3 74.3 83.1 80.0</cell><cell>81.1 82.8 73.0 82.1 80.1</cell></row><row><cell>Weather</cell><cell>Snow Frost Fog Spatter Bright Saturate</cell><cell>49.8 50.0 49.7 49.0 49.7 49.0</cell><cell>49.7 50.3 48.1 48.0 49.8 50.0</cell><cell>79.7 79.9 82.7 82.0 84.5 84.4</cell><cell>82.4 79.5 84.3 81.5 84.5 83.5</cell></row><row><cell>Digital</cell><cell>Contrast Elastic Pixelate JPEG</cell><cell>48.0 48.8 48.9 49.0</cell><cell>50.1 48.0 48.1 48.1</cell><cell>78.3 82.0 82.3 80.0</cell><cell>76.4 82.1 82.6 82.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE VIII :</head><label>VIII</label><figDesc>Index of clustering results for ImageNet.</figDesc><table><row><cell>APPENDIX B INDEX FOR MVTEC AD</cell><cell></cell><cell></cell></row><row><cell>c i</cell><cell>Label</cell><cell>Index</cell></row><row><cell>0 1 2 3 4 5 6 7 8 9</cell><cell>Snake Finch Spider Big cat Beetle Wading bird Monkey Fungus Cat Dog</cell><cell>n01728920, n01728572, n01729322, n01734418, n01737021, n01740131, n01735189 n01530575, n01531178, n01532829, n01534433, n01795545, n01796340 n01773157, n01773549, n01774384, n01775062, n01773797, n01774750 n02128385, n02128925, n02129604, n02130308, n02128757, n02129165 n02165105, n02165456, n02169497, n02177972, n02167151 n02007558, n02012849, n02013706, n02018795, n02006656 n02486261, n02486410, n02488291, n02489166 n12985857, n13037406, n13054560, n13040303 n02123045, n02123394, n02124075, n02123159 n02088364, n02105412, n02106030, n02106166, n02106662, n02106550, n02088466, n02093754, n02091635</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE IX :</head><label>IX</label><figDesc>Class names and anomalous types of MVTec AD.</figDesc><table><row><cell>c i</cell><cell>Class Name</cell><cell>Anomalous Types</cell></row><row><cell>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14</cell><cell>Bottle Capsule Grid Leather Pill Tile Transistor Zipper Cable Carpet Hazelnut Metal nut Screw Toothbrush Wood</cell><cell>broken large, broken small, contamination crack, faulty imprint, poke, scratch, squeeze bent, broken, glue, metal contamination, thread color, cut, fold, glue, poke color, combined, contamination, crack, faulty imprint, pill type, scratch crack, glue strip, gray stroke, oil, rough bent, cut, damaged, misplaced broken teeth, combined, fabric border, fabric interior, rough, split teeth, squeezed teeth bent wire, cable swap, combined, cut inner insulation, cut outer insulation, missing cable, missing wire, poke insulation color, cut, hole, metal contamination, thread crack, cut, hole, print bent, color, flip, scratch manipulated front, scratch head, scratch neck, thread side, thread top defective color, combined, hole, liquid, scratch</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE X :</head><label>X</label><figDesc>Structure of ARNet.Table X shows the model structure of ARNet. It bases on an encoder-decoder framework. It totally has 4 blocks for the encoder and 4 blocks for the decoder. Each block has a</figDesc><table><row><cell>Layer</cell><cell>Input</cell><cell>Output</cell></row><row><cell>3 ? 3 ? 64</cell><cell>x (1 ? H ? W )</cell><cell>x 0?1 (64 ? H ? W )</cell></row><row><cell>3 ? 3 ? 64 MaxPool</cell><cell>x 0?1 x 0?2</cell><cell>x 0?2 (64 ? H ? W ) x 1?1 (64 ? 1/2H ? 1/2W )</cell></row><row><cell>3 ? 3 ? 128</cell><cell>x 1?1</cell><cell>x 1?2 (128 ? 1/2H ? 1/2W )</cell></row><row><cell>3 ? 3 ? 128 MaxPool</cell><cell>x 1?2 x 1?3</cell><cell>x 1?3 (128 ? 1/2H ? 1/2W ) x 2?1 (128 ? 1/4H ? 1/4W )</cell></row><row><cell>3 ? 3 ? 256</cell><cell>x 2?1</cell><cell>x 2?2 (256 ? 1/4H ? 1/4W )</cell></row><row><cell>3 ? 3 ? 256 MaxPool</cell><cell>x 2?2 x 2?3</cell><cell>x 2?3 (256 ? 1/4H ? 1/4W ) x 3?1 (256 ? 1/8H ? 1/8W )</cell></row><row><cell>3 ? 3 ? 512</cell><cell>x 3?1</cell><cell>x 3?2 (256 ? 1/8H ? 1/8W )</cell></row><row><cell>3 ? 3 ? 512 MaxPool</cell><cell>x 3?2 x 3?3</cell><cell>x 3?3 (256 ? 1/8H ? 1/8W ) x 4?1 (256 ? 1/8H ? 1/16W )</cell></row><row><cell>3 ? 3 ? 512</cell><cell>x 4?1</cell><cell>x 4?2 (512 ? 1/16H ? 1/16W )</cell></row><row><cell>3 ? 3 ? 512 UpSample</cell><cell>x 4?2 x 4?3</cell><cell>x 4?3 (512 ? 1/16H ? 1/16W ) up 3?1 (512 ? 1/8H ? 1/8W )</cell></row><row><cell>3 ? 3 ? 256</cell><cell>[up 3?1 , x 3?3 ]</cell><cell>up 3?2 (256 ? 1/8H ? 1/8W )</cell></row><row><cell>3 ? 3 ? 256 UpSample</cell><cell>up 3?2 up 3?3</cell><cell>up 3?3 (256 ? 1/8H ? 1/8W ) up 2?1 (256 ? 1/4H ? 1/4W )</cell></row><row><cell>3 ? 3 ? 128</cell><cell>[up 2?1 , x 2?3 ]</cell><cell>up 2?2 (128 ? 1/4H ? 1/4W )</cell></row><row><cell>3 ? 3 ? 128 UpSample</cell><cell>up 2?2 up 2?3</cell><cell>up 2?3 (128 ? 1/4H ? 1/4W ) up 1?1 (128 ? 1/2H ? 1/2W )</cell></row><row><cell>3 ? 3 ? 64</cell><cell>[up 1?1 , x 1?3 ]</cell><cell>up 1?2 (64 ? 1/2H ? 1/2W )</cell></row><row><cell>3 ? 3 ? 64 UpSample</cell><cell>up 1?2 x 1?3</cell><cell>up 1?3 (64 ? 1/2H ? 1/2W ) up 0?1 (64 ? H ? W )</cell></row><row><cell>3 ? 3 ? 64</cell><cell>[up 0?1 , x 0?2 ]</cell><cell>up 0?2 (64 ? H ? W )</cell></row><row><cell>3 ? 3 ? 64</cell><cell>up 0?2</cell><cell>up 0?3 (64 ? H ? W )</cell></row><row><cell>3 ? 3 ? 3</cell><cell>up 0?3</cell><cell>output (3 ? H ? W )</cell></row><row><cell></cell><cell cols="2">APPENDIX C MODEL STRUCTURE OF ARNET</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE XI :</head><label>XI</label><figDesc>Average area under the ROC curve (AUROC) in % of anomaly detection methods on ShanghaiTech<ref type="bibr" target="#b107">[63]</ref> dataset. The best performing method in each experiment is in bold.</figDesc><table><row><cell>Methods</cell><cell>Temporal Dependency? AUROC</cell></row><row><cell>TSC [63] StackRNN [63] AE-Conv3D [64] MemAE [11]</cell><cell>67.9 68.0 69.7 71.2</cell></row><row><cell>AE-Conv2D [65] OURS</cell><cell>60.9 72.5</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Akcay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Samet Ak? Ay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Breckon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08954</idno>
		<title level="m">Skip-ganomaly: Skip connected and adversarially trained encoder-decoder anomaly detection</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Variational autoencoder based anomaly detection using reconstruction probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwon</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungzoon</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Special Lecture on IE</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<meeting>COMPSTAT&apos;2010</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep learning for anomaly detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghavendra</forename><surname>Chalapathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Chawla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Anomaly detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Anomaly detection with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Kloft</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Anomaly detection over noisy data using learned probability distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleazar</forename><surname>Eskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep anomaly detection using geometric transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izhak</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel. Memorizing normality to detect anomaly: Memoryaugmented deep autoencoder for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vuong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Budhaditya</forename><surname>Saha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning temporal regularity in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmudul</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep anomaly detection with outlier exposure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Akcay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Samet Ak? Ay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Breckon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08954</idno>
		<title level="m">Skip-ganomaly: Skip connected and adversarially trained encoder-decoder anomaly detection</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Variational autoencoder based anomaly detection using reconstruction probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwon</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungzoon</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Special Lecture on IE</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<meeting>COMPSTAT&apos;2010</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deep learning for anomaly detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghavendra</forename><surname>Chalapathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Chawla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Anomaly detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Anomaly detection with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Kloft</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Anomaly detection over noisy data using learned probability distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleazar</forename><surname>Eskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep anomaly detection using geometric transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izhak</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel. Memorizing normality to detect anomaly: Memoryaugmented deep autoencoder for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vuong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Budhaditya</forename><surname>Saha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning temporal regularity in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmudul</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep anomaly detection with outlier exposure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Akcay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Samet Ak? Ay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Breckon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08954</idno>
		<title level="m">Skip-ganomaly: Skip connected and adversarially trained encoder-decoder anomaly detection</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Variational autoencoder based anomaly detection using reconstruction probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwon</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungzoon</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Special Lecture on IE</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<meeting>COMPSTAT&apos;2010</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deep learning for anomaly detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghavendra</forename><surname>Chalapathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Chawla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Anomaly detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Anomaly detection with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Kloft</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Anomaly detection over noisy data using learned probability distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleazar</forename><surname>Eskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep anomaly detection using geometric transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izhak</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel. Memorizing normality to detect anomaly: Memoryaugmented deep autoencoder for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vuong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Budhaditya</forename><surname>Saha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning temporal regularity in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmudul</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep anomaly detection with outlier exposure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Anomaly detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>G?rnitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Processing in Medical Imaging</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Akcay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Adversarially learned one-class classifier for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khalooei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Stacked convolutional auto-encoders for hierarchical feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cire?an</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Anomaly detection using autoencoders with nonlinear dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sakurada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yairi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mlsda Workshop on Machine Learning for Sensory Data Analysis</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B L</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>S?nderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.09300</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Generating images with perceptual similarity metrics based on deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Memorizing normality to detect anomaly: Memoryaugmented deep autoencoder for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1705" to="1714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep autoencoding gaussian mixture model for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lumezanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Image anomaly detection with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Effective end-to-end unsupervised outlier detection via inlier priority of discriminative network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5960" to="5973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4183" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Self-supervised feature learning by learning to spot artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jenni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Towards fine-grained open zero-shot learning: Inferring unseen visual features from attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="944" to="952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Mvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and surface variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">An overview of deep learning based methods for unsupervised and semi-supervised anomaly detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Kiran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Parakkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Imaging</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Sparse coding guided spatiotemporal feature learning for abnormal event detection in large videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="246" to="255" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Anomaly detection based on stacked sparse coding with intraframe classification strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1062" to="1074" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Video anomaly detection and localization based on an adaptive intra-frame classification network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Video anomaly detection and localisation based on the sparsity and reconstruction error of autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoseini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics Letters</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1122" to="1124" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Avid: Adversarial visual irregularity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pourreza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Entezari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="488" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Deep end-to-end oneclass classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Deepanomaly: Fully convolutional neural network for fast anomaly detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Moayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Deep-cascade: Cascading 3d deep neural networks for fast anomaly detection and localization in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Anomaly detection over noisy data using learned probability distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamanishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">I</forename><surname>Takeuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining &amp; Knowledge Discovery</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Coherence pursuit: Fast, simple, and robust principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Atia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Robust pca via outlier pursuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Caramanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanghavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Estimating the support of a high-dimensional distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1443" to="1471" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Training confidence-calibrated classifiers for detecting out-of-distribution samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Deep anomaly detection with outlier exposure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Variational autoencoder based anomaly detection using reconstruction probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Special Lecture on IE</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Learning discriminative reconstructions for unsupervised outlier removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">A hybrid autoencoder and density estimation model for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nicolau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcdermott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Parallel Problem Solving from Nature</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="717" to="726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Skip-ganomaly: Skip connected and adversarially trained encoder-decoder anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ak?ay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>-A. Andtoby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">advae: A self-adversarial variational autoencoder with gaussian anomaly prior knowledge for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.00904</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Efficient gan-based anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zenati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Foo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lecouat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Manek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06222</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Ocgan: One-class novelty detection using gans with constrained latent representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Deep anomaly detection using geometric transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Semi-supervised learning with context-conditional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06430</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Deep structured energy based models for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1100" to="1109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">The mnist database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Citeseer, Tech. Rep</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<meeting>COMPSTAT&apos;2010</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">A revisit of sparse coding based anomaly detection in stacked rnn framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Spatiotemporal autoencoder for video anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Learning temporal regularity in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
