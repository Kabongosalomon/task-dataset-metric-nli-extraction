<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Residual Attention: A Simple but Effective Method for Multi-Label Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Zhu</surname></persName>
							<email>zhuk@lamda.nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Residual Attention: A Simple but Effective Method for Multi-Label Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-label image recognition is a challenging computer vision task of practical use. Progresses in this area, however, are often characterized by complicated methods, heavy computations, and lack of intuitive explanations. To effectively capture different spatial regions occupied by objects from different categories, we propose an embarrassingly simple module, named class-specific residual attention (CSRA). CSRA generates class-specific features for every category by proposing a simple spatial attention score, and then combines it with the class-agnostic average pooling feature. CSRA achieves state-of-the-art results on multi-label recognition, and at the same time is much simpler than them. Furthermore, with only 4 lines of code, CSRA also leads to consistent improvement across many diverse pretrained models and datasets without any extra training. CSRA is both easy to implement and light in computations, which also enjoys intuitive explanations and visualizations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional neural networks (CNNs) have dominated many computer vision tasks, especially in image classification. However, although many network architectures have been proposed for single-label classification, e.g., VGG <ref type="bibr" target="#b26">[27]</ref>, ResNet <ref type="bibr" target="#b14">[15]</ref>, EfficientNet <ref type="bibr" target="#b28">[29]</ref> and VIT <ref type="bibr" target="#b6">[7]</ref>, the progress in multi-label recognition remains modest. In multi-label tasks, the objects' locations and sizes vary a lot and it is difficult to learn a single deep representation that fits all of them.</p><p>Recent studies in multi-label recognition mainly focus on three aspects: semantic relations among labels, object proposals, and attention mechanisms. To explore semantic relations, Bayesian network <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18]</ref>, Recurrent Neural Network (RNN) <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b29">30]</ref> and Graph Convolutional Network (GCN) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref> have been adopted, but they suffer from high computational cost or manually defined adjacency matrices. <ref type="figure">Figure 1</ref>. A simple modification in the testing stage using PyTorch, in which Lambda (or ?) is a hyperparameter that combines global average and max pooling scores. When Lambda (?) is 0, score equals y_avg, which is the score of the baseline model.</p><p>Proposal based methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b18">19]</ref> spend too much time in processing the object proposals. Although attention models are end-to-end and relatively simple, for multi-label classification they resort to complicated spatial attention models <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b10">11]</ref>, which are difficult to optimize, implement, or interpret. Instead, we propose an embarrassingly simple and easy to train class-specific residual attention (CSRA) module to fully utilize the spatial attention for each object class separately, and achieves superior accuracy. The CSRA module has negligible computational cost, too.</p><p>Our motivation came from <ref type="figure">Fig. 1</ref>, in which only 4 lines of code consistently leads to improvement of multi-label recognition, across many diverse pretrained models and datasets, even without any extra training, as detailed in <ref type="table">Table 1</ref>. The only change is to add a global max-pooling on top of the usual global average pooling, but the improvement is consistent. Its advantage is also verified on ImageNet, a single-label recognition task.</p><p>In this paper, we show that this operation, the max pooling among different spatial regions for every class, is in fact a class-specific attention operation, which can be further viewed as a residual component of the class-agnostic global average pooling. Hence, we generalize it to propose a simple class-specific residual attention module (CSRA), and has achieved state-of-the-art performance on four multilabel datasets, namely VOC2007 <ref type="bibr" target="#b8">[9]</ref>, VOC2012 <ref type="bibr" target="#b9">[10]</ref>, MS-COCO <ref type="bibr" target="#b19">[20]</ref> and WIDER-Attribute <ref type="bibr" target="#b18">[19]</ref>. Furthermore, the <ref type="table">Table 1</ref>. An embarrassingly simple, almost zero-cost, and training-free improvement to a diverse set of existing models on 3 multi-label recognition datasets and 1 single-label recognition dataset. <ref type="bibr">Datasets</ref>  proposed CSRA has an intuitive explanation on how spatial attention is integrated into it. Our contributions can be summarized as:</p><p>? An extremely simple but effective method to improve pretrained models without any further training;</p><p>? A simple and effective CSRA module that achieves excellent results on four multi-label recognition datasets;</p><p>? An intuitive interpretation of the proposed attention module (plus visualizations).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We first briefly review recent progresses in multi-label image classification.</p><p>Many methods focus on the semantic relationship between objects or object classes, by using dependency networks <ref type="bibr" target="#b13">[14]</ref>, pairwise co-occurrence adjacency matrices <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b33">34]</ref>, or conditional graphs <ref type="bibr" target="#b17">[18]</ref>. However, inference in graphs can be very difficult due to the exponential label space, and is usually approximated by Gibbs sampling. Pairwise statistics for first-order adjacency matrix construction has attracted much attention recently <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b35">36]</ref>, mainly attributed to the popularity of the Graph Convolutional Network (GCN) <ref type="bibr" target="#b16">[17]</ref>. But, co-occurrence statistics in a small training set is not reliable, and can easily cause overfitting. Besides, high order relationship is beyond what GCN can represent. Recurrent Neural Network (RNN) has also been applied in various studies <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38]</ref> to explore high order label dependencies <ref type="bibr" target="#b37">[38]</ref>. But, the effectiveness of RNN in multi-label tasks is yet to be proved. Also, training RNN usually requires dedicated hyperparameter tuning, making it unsuitable in real applications.</p><p>Generating object proposals <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b30">31]</ref> is another approach. Object proposals are generated by methods like EdgeBoxes <ref type="bibr" target="#b1">[2]</ref> or Selective Search <ref type="bibr" target="#b15">[16]</ref>, then sent to a shared CNN. Finally, a category-wise max-pooling is used to fuse the proposals' scores. Proposals are, however, large in quantity and expensive in computing.</p><p>Attention mechanism is widely adopted in a variety of vision tasks, such as detection <ref type="bibr" target="#b22">[23]</ref> and tracking <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b21">22]</ref>. In multi-label recognition, one representative paradigm is SRN <ref type="bibr" target="#b38">[39]</ref>, which used a spatial regularization network to rectify original predictions. Sarafianos et al. <ref type="bibr" target="#b25">[26]</ref> proposed a similar pipeline to aggregate visual attention, and Durand et al. <ref type="bibr" target="#b7">[8]</ref> generated multiple class heat maps to assemble the prediction scores. These attention methods require deliberate design processes, and cannot be intuitively interpreted. Recently, You et al. <ref type="bibr" target="#b35">[36]</ref> used a cross-domain model to extract class-specific features, but they dropped the classagnostic average pooling. Gao and Zhou <ref type="bibr" target="#b10">[11]</ref> proposed a detection-like pipeline for multi-label image classification, but the inference process is very expensive. There are other complicated attention-based methods, such as knowledge distillation <ref type="bibr" target="#b20">[21]</ref>, visual attention consistency <ref type="bibr" target="#b12">[13]</ref>, and suppressing negative class activation map <ref type="bibr" target="#b27">[28]</ref>.</p><p>Unlike existing attention-based methods that either drop the class-agnostic average pooling or design a complicated and nonintuitive pipeline, this paper proposes a residual attention module, which uses a class-agnostic average pooling and a class-specific spatial pooling to get robust features for multi-label image classification. We want to emphasize that our CSRA is significantly different from the previous attention model SRN <ref type="bibr" target="#b38">[39]</ref>. CSRA reuses the classifier's weights and has no additional parameters, while SRN has a complicated pipeline and many extra parameters. We also generalize CSRA to multi-head attention, which is totally different from the whole structure in SRN. Our CSRA is endto-end trainable, while SRN requires a three stage dedicated model training and finetuning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Class-specific Residual Attention</head><p>To present the proposed CSRA module, we start from the code in <ref type="figure">Fig. 1</ref> and the results in <ref type="table">Table 1.</ref> 3.1. Why global max pooling helps? <ref type="table">Table 1</ref> lists experimental results of various backbone networks on four different datasets. Because only very few previous studies on multi-label recognition release their code or pretrained models, we also trained our own models to obtain baseline results using MobileNet <ref type="bibr" target="#b24">[25]</ref>, ResNet <ref type="bibr" target="#b14">[15]</ref>, EfficientNet <ref type="bibr" target="#b28">[29]</ref> and VIT <ref type="bibr" target="#b6">[7]</ref>, in addition to the GCN <ref type="bibr" target="#b3">[4]</ref> method. In our simple modification, ? was chosen in two ways. In the first ("Varying ?"), ? was tuned for every experiment; while in the second ("Fixed ?"), we always used 0.02 for models in the ResNet family, and 0.2 for all other models. For multi-label tasks, we used mAP as the evaluation metric, and for ImageNet (which is single-label), accuracy ("acc") was used.</p><p>In <ref type="table">Table 1</ref>, "ResNet-cut" was ResNet-101 pretrained on ImageNet with CutMix <ref type="bibr" target="#b36">[37]</ref>; pretrained "ResNet-50/-101" were downloaded from the PyTorch official website. "Effi-cientNet" and "MobileNet" were EfficientNet-B3 <ref type="bibr" target="#b28">[29]</ref> and MobileNet-V2 <ref type="bibr" target="#b24">[25]</ref>, respectively. All the VIT <ref type="bibr" target="#b6">[7]</ref> models were pretrained on ImageNet-21k and finetuned on Ima-geNet with the 224?224 resolution, except "VIT-B16-384" (384?384 resolution). For VIT <ref type="bibr" target="#b6">[7]</ref> models, we dropped the class token and use the final output patch embeddings as the feature tensor, and we interpolated positional embeddings suggested in <ref type="bibr" target="#b6">[7]</ref> when finetuning them with higher resolutions on VOC2007 <ref type="bibr" target="#b8">[9]</ref> and MS-COCO <ref type="bibr" target="#b19">[20]</ref>.</p><p>These results show that simply adding a max pooling can consistently improve multi-label recognition, especially when the baseline model's mAP is not high. In fact, as shown by the "Fixed ?" results, our simple modification is robust to ?, which means that these 4 lines of code is already a practical method to improve multi-label recognition: simple, minimal-cost, training-free, and insensitive to the hyperparameter.</p><p>Why the simple global max pooling helps? First, note that y_max in <ref type="figure">Fig. 1</ref> finds the maximum value among all spatial locations for each category. Hence, it can be viewed as a class-specific attention mechanism. We conjecture it focuses our attention to classification scores at different locations for different object categories. This attention mechanism is, intuitively, very useful for multi-label recognition, especially when there are objects from many classes and with varying sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Residual attention</head><p>The attention interpretation of <ref type="figure">Fig. 1</ref> inspires us to generalize it and design a trainable attention mechanism.</p><p>For a given image I, it is first sent to a feature extractor (the CNN backbone) ? to obtain a feature tensor</p><formula xml:id="formula_0">x ? R d?h?w , where d, h, w are the dimensionality, height,</formula><p>width of the feature tensor:</p><formula xml:id="formula_1">x = ?(I; ?) ,<label>(1)</label></formula><p>in which ? is the parameters of the CNN backbone. For simplicity, we adopt ResNet-101 <ref type="bibr" target="#b14">[15]</ref> and input image resolution 224 ? 224 as an example, if not otherwise specified. As a result, the shape of the feature tensor x is 2048?7?7, which can be decoupled as</p><formula xml:id="formula_2">x 1 , x 2 , . . . , x 49 (x i ? R 2048 ). Next, a</formula><p>fully connected (1 ? 1 convolution) layer is the classifier (FC in <ref type="figure">Fig. 1</ref>), with m i ? R 2048 being the classifier for the i-th class. Constants such as 49 can be changed accordingly when a different setup has been used. Now we define the class-specific attention scores for the i-th class and j-th location as</p><formula xml:id="formula_3">s i j = exp(T x T j m i ) ? 49 k=1 exp(T x T k m i ) ,<label>(2)</label></formula><p>where ? 49 j=1 s i j = 1 and T &gt; 0 is the temperature controlling the scores' sharpness. We can view s i j as the probability of the class i appearing at location j.</p><p>Then, we can define the class-specific feature vector for class i as a weighted combination of the feature tensor, where the attention scores for the i-th class s i k (1 ? k ? 49) are the weights, as</p><formula xml:id="formula_4">a i = 49 k=1 s i k x k .<label>(3)</label></formula><p>In contrast, the classical global class-agnostic feature vector for the entire image is</p><formula xml:id="formula_5">g = 1 49 49 k=1 x k .<label>(4)</label></formula><p>Since g has been widely used and has achieved good results, we treat it as the main feature vector. And we treat a i as class-specific residual features. As illustrated in <ref type="figure">Fig. 2</ref>, by adding these two vectors, we obtain our class-specific residual attention (CSRA) feature f i for the i-th class:</p><formula xml:id="formula_6">f i = g + ?a i .<label>(5)</label></formula><p>This constitutes the proposed CSRA module. Finally, all these class-specific feature vectors are sent to the classifier to obtain the final logit?</p><formula xml:id="formula_7">y ? (y 1 , y 2 , . . . , y C ) = (m T 1 f 1 , m T 2 f 2 , . . . , m T C f C ) ,<label>(6)</label></formula><p>where C means the number of classes. <ref type="figure">Figure 2</ref>. The proposed CSRA module to obtain features and classification results for the i-th class.</p><formula xml:id="formula_8">??? x ? ??? ? ??? ? ??? ?? ??? ? ? 1 49 1 49 1 49 ? ? 6SDWLDO SRROLQJ $YHUDJH SRROLQJ ??? ??? ? ORJLW IRU WKH LWK FODVV</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Explaining the CSRA module</head><p>We first prove that <ref type="figure">Fig. 1</ref> is in fact a special case of our CSRA module. By substituting Eq. (2) till Eq. (5) into Eq. <ref type="formula" target="#formula_7">(6)</ref>, we can easily derive the logit for the i-th class as:</p><formula xml:id="formula_9">y i = m T i g + ?m T i 49 k=1 s i k x k (7) = 1 49 49 k=1 x T k m i + ? 49 k=1 exp(T x T k m i ) ? 49 l=1 exp(T x T l m i ) x T k m i .<label>(8)</label></formula><p>The first term in the right hand side of Eq. <ref type="formula">(7)</ref> is the base logit for the i-th class, which can be written as <ref type="figure">Fig. 1</ref>). Inside the second term, x T k m i is the classification score at location k (1 ? k ? 49) for the i-th class, which is then weighed by the normalized attention score s i k to form the average class-specific score. It is well-known that when T ? ?, the softmax output s </p><formula xml:id="formula_10">1 49 ? 49 k=1 x T k m i (y_avg in</formula><formula xml:id="formula_11">y i = m T i g + ? max(x T 1 m i , . . . , x T k m i ) .<label>(9)</label></formula><p>Obviously, the last term in Eq. (9) exactly corresponds to Lambda * y_max in <ref type="figure">Fig. 1</ref>, and the testing-time modification in <ref type="figure">Fig. 1</ref> is not only the motivation for, but also a special case of CSRA.</p><p>Comparing Eq. (9) with Eq. (8), instead of solely relying on one location for the residual attention, CSRA hinges on residual attention features from all locations. Intuitively, when there are multiple small objects from the same class in the input image, CSRA has a clear edge over global average pooling alone or global max pooling alone.</p><p>More specifically, we have</p><formula xml:id="formula_12">f i = g + ?a i (10) = 49 k=1 ( 1 49 + ?s i k )x k (11) = (1 + ?) 49 k=1 1 49 + ?s i k 1 + ? x k<label>(12)</label></formula><p>where ? ? 1 49 corresponds to a prior term, which is independent of either category i or location k;</p><p>? s i k , the probability of location k occupied by an object from category i (computed based on x k ), corresponds to a data likelihood term, which hinges on both the classifier m i for the i-th class and location k's feature x k . From this perspective, it is both intuitive and reasonable to use this class-specific and data-dependent score as our attention score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Multi-head attention</head><p>The temperature hyperparameter T may be tricky to tune, and it is possible that different classes may need different temperatures (or different attention scores). Thus, we further propose a simple multi-head attention extension to CSRA, as shown in <ref type="figure">Fig. 3</ref>.</p><p>Multiple branches (or heads) of residual attention <ref type="figure">(Fig. 2</ref> / Eq. 7) are used, with each branch utilizing a different temperature T but sharing the same ?. We denote the number of attention heads as H. To rid ourselves of tuning the temperature T , we either choose single head (H = 1) with a fixed temperature T = 1, or use multi-head attention (H &gt; 1) with fixed sequences of temperatures T 1 , T 2 , . . . , T H . Besides H = 1, we also used H = 2, 4, 6, 8. Specifically,</p><p>? When H = 2, T 1 = 1 and T 2 = ? (i.e., max pooling);</p><p>? When H = 4, T 1?3 = 1, 2, 4 and T 4 = ?;</p><p>? When H = 6, T 1?5 = 1, 2, 3, 4, 5 and T 6 = ?;</p><p>? When H = 8, T 1?7 = 1, 2, 3, 4, 5, 6, 7 and T 8 = ?.</p><p>That is, when H &gt; 1, the final T H is always ?, and the other T are chosen in an increasing order. Different T values can bring diversity into the branches, thus producing better <ref type="figure">Figure 3</ref>. Overall pipeline of multi-head CSRA. An Image is first sent to a CNN backbone to get the feature tensor x, which is used to generate multiple score tensors (? R C?h?w , C is the number of classes) by different 1 ? 1 convolutions (FCs). The residual attention defined in Eq. <ref type="formula" target="#formula_9">(8)</ref> is applied to every score tensor to produce different logits? T i (i ? {1, 2, . . . , H},? T i ? R C ), which are then fused to get the final logits? o . The temperature T is different in different branches, but the same ? is shared among them.</p><formula xml:id="formula_13">x ??? ???????? ???;?? ?? ? ??? ? ??? ,PDJH 6FRUH WHQVRU ??? 6FRUH WHQVRU ??? ? FRQY 5HVLGXDO $WWHQWLRQ ? ? ? ? ? ??? ? ? ? ??? ? ? ? ??? ? ? ??? ? ??? ? ? ? FRQY ? ??? ??? ? ??? 5HVLGXDO $WWHQWLRQ ? ? ? 1</formula><p>classification results. In short, there is no need to tune T in CSRA.</p><p>For better convergence speed, we choose to normalize our classifier's weights to unit vectors (i.e., m i ? m i ||m i || ). We will show empirically that this normalization makes no difference in accuracy, but it can lead to faster convergence in the training process.</p><p>The logits?</p><formula xml:id="formula_14">T 1 ,? T 2 , . . . ,? T H (? T i ? R C ) from different</formula><p>heads are added to get the final logits,? o , a?</p><formula xml:id="formula_15">y o = H h=1? T i .<label>(13)</label></formula><p>where T i is the temperature for the i-th head. Finally, the classical binary cross entropy (BCE) loss is used to compute the loss incurred between our prediction y o and the groundtruth labels, and the stochastic gradient descent (SGD) optimization method is used to minimize this loss function. Hence, the proposed CSRA (either single-or multi-head) is simple in structure and easy to implement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>Now, we validate the effectiveness of CSRA and analyze its components empirically. We first describe the general experimental settings, then present our experimental results and compare CSRA with previous state-of-the-art models. Finally, we empirically analyze how the components and hyperparameters influence the performance of our model. We experimented with 4 multi-label datasets: VOC2007 <ref type="bibr" target="#b8">[9]</ref>, VOC2012 <ref type="bibr" target="#b9">[10]</ref>, MS-COCO <ref type="bibr" target="#b19">[20]</ref> and WIDER-Attribute <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental settings</head><p>Training details As aforementioned, we build multi-label recognition models in an end-to-end way by minimizing the binary cross entropy loss using SGD. For data augmentation, we only perform random horizontal flip and random resized crop, following previous work <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref>. When we train baseline models (BCE loss without CSRA), we use an initial learning rate of 0.01 for both the backbone and the classifiers. For training our residual attention models, we choose the learning rate of 0.1 for the CSRA module and classifiers, and 0.01 for the CNN backbone, respectively. We apply the warmup scheduler for training both the baseline models and our CSRA models. The CNN backbone is initialized from various pretrained models, and fine-tuned for 30 epochs on multi-label datasets. The momentum is 0.9, and weight decay is 0.0001. The batch size and input image resolution for the WIDER Attribute dataset <ref type="bibr" target="#b18">[19]</ref> are 64 and 224 ? 224, respectively. For MS-COCO <ref type="bibr" target="#b19">[20]</ref>, VOC2007 <ref type="bibr" target="#b8">[9]</ref> and VOC2012 <ref type="bibr" target="#b9">[10]</ref>, the batch size and input image resolution are 16 and 448 ? 448, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation metrics</head><p>The widely used mean average precision (mAP) is our primary evaluation metric. We set positive threshold as 0.5 and also adopt the overall precision (OP), overall recall (OR), overall F1-measure (OF1), per-category precision (CP), per-category recall (CR) and per-category F1-measure (CF1), following previous multi-label image classification research <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b12">13</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with state-of-the-arts</head><p>VOC2007 VOC2007 <ref type="bibr" target="#b8">[9]</ref> is a widely used multi-label image classification dataset. It has 9,963 images and 20 classes, in which the train-val set has 5,011 images and the test set has 4,952 images. We use the train-val set for training and the test set for evaluation. The input resolution is 448 ? 448.</p><p>CSRA was applied to two backbones: the original ResNet-101, and ResNet-cut pretrained on ImageNet with Cut-Mix <ref type="bibr" target="#b36">[37]</ref>. We also report the mAP of these backbones pretrained on the MS-COCO <ref type="bibr" target="#b19">[20]</ref> dataset. For simplicity, we only use one branch, i.e., H = 1, T = 1, and ? = 0.1. As shown in <ref type="table">Table 2</ref>, CSRA surpasses previous state-of-the-art models.</p><p>VOC2012 VOC2012 [10] contains 11,540 train-val images and 10,991 test images. We train our model on the train-val <ref type="table">Table 2</ref>. Comparisons of mAP (in %) of state-of-the-art models and our CSRA on VOC2007, where "-" means the results were not provided. "ResNet-101" is pretrained and downloaded from the PyTorch official website, "ResNet-cut" is ResNet-101 pretrained on ImageNet with CutMix <ref type="bibr" target="#b36">[37]</ref>. The baseline result of ResNet-101 is from <ref type="bibr" target="#b10">[11]</ref>. "extra data" means pretrained on MS-COCO. The  sets, and evaluate its performance on the official evaluation server. The settings are the same as those in VOC2007: H = 1, T = 1, ? = 0.1 and 448 ? 448 resolution. As shown in <ref type="table">Table 3</ref>, when only using ResNet-101 ImageNet pretrained models, our CSRA has surpassed previous methods. When pretrained on extra data (MS-COCO), the performance of CSRA can be further improved, achieving new state-of-theart performance. <ref type="bibr" target="#b19">[20]</ref> is widely used for segmentation, classification, detection and captioning. We use COCO-2014 in our experiments, which has 82,081 training and 40,137 validation images and 80 object classes. We train our model using three pretrained CNN backbones (ResNet-101, ResNet-cut and VIT-L16) on the train set and evaluate them on the val set. Following <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13]</ref>, we report the precision, recall and F1-measure with and without Top-3 scores. Note that the variation of objects' shapes and sizes are more complicated on MS-COCO than those in both VOC2007 and VOC2012, thus we adopt a larger trade-off parameter ? with multiple heads for better attention. Specifically, when running ResNet-101 and ResNet-cut models, we adopt six attention heads (H = 6), and choose ? = 0.5 and ? = 0.4, respectively. For the VIT-L16 backbone <ref type="bibr" target="#b6">[7]</ref>, we adopt eight heads (H = 8, ? = 1.0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MS-COCO Microsoft COCO</head><p>The results are shown in <ref type="table">Table 4</ref>, in which the upper block lists results of methods using ResNet series models as backbones, and the lower block is for other backbones. It can be seen that when our CSRA module is added to the ResNet-101 model, there is a significant gain, raising mAP from 79.4% to 83.5%, a total of 4.1% improvement. ResNetcut (pretrained with CutMix) plus CSRA has achieved 85.6% mAP, surpassing previous state-of-the-art models by a large margin.</p><p>It is worth mentioning that previous methods such as MCAR <ref type="bibr" target="#b10">[11]</ref> and KSSNet <ref type="bibr" target="#b20">[21]</ref> used complicated and time consuming pipelines. In comparison, our residual attention model is not only effective, but also surprisingly simple.</p><p>When running non-ResNet series models, we choose VIT-L16 pretrained on ImageNet with 224?224 input and finetune it on MS-COCO with 448?448 resolution (we interpolate the positional embeddings as suggested in <ref type="bibr" target="#b6">[7]</ref>). In comparison with ASL [1] that used TResNet <ref type="bibr" target="#b23">[24]</ref>, our residual attention model, VIT-L16 + CSRA, has achieved state-of-the-art performance by increasing mAP from 80.4% to 86.5%, a significant improvement of 6.1%. Note that ASL <ref type="bibr" target="#b0">[1]</ref> used multiple complex data augmentation methods, such as Cutout <ref type="bibr" target="#b5">[6]</ref>, GPU Augmentations <ref type="bibr" target="#b0">[1]</ref> or RandAugment <ref type="bibr" target="#b4">[5]</ref>, while we only used classic simple data augmentation (horizontal flip and random resize crop). When we use the RandAugment <ref type="bibr" target="#b4">[5]</ref> as our data augmentation technique, our CSRA was further improved to 86.9% mAP (denoted as VIT-L16 + CSRA * in <ref type="table">Table 4</ref>) on MS-COCO. When we compare more specific metrics (such as CP, CR and CF1), the proposed CSRA method also has clear advantages.</p><p>WIDER-Attribute WIDER-Attribute <ref type="bibr" target="#b18">[19]</ref> is a pedestrian dataset containing 14 categories (human attributes) of each person. The training and validation sets have 28,345 annotated people and the test set has 29,179 people. Following the conventional settings, we use the train-val set for training and evaluate the performance on the test set. As this dataset has unspecified labels, we set them as negative in the training stage and ignore these unspecified labels in the test stage, following previous settings in <ref type="bibr" target="#b38">[39]</ref>. We adopt VIT <ref type="bibr" target="#b6">[7]</ref> as our backbone and evaluate its performance on this pedestrian dataset with or without the proposed CSRA module. For simplicity, we only use one attention head (H = 1), and choose ? = 0.3, T = 1 in our CSRA model. The input image resolution is 224 ? 224.</p><p>For running VIT-B16 and VIT-L16, we drop the class token and use the final patch embeddings as the feature tensor. As shown in <ref type="table">Table 5</ref>, the improvement of the backbone is large, from 86.3% to 89.0% in the VIT-B16 backbone and from 87.7% to 90.1% in the VIT-L16 backbone. Together with the experimental results on MS-COCO, we have shown that the proposed CSRA module is not only suitable for the classic ResNet backbones, but also for emerging <ref type="table">Table 4</ref>. Comparisons of mAP (in %) and multiple other metrics on MS-COCO. The upper block corresponds to ResNet-101-based models, and the lower block is for other non-ResNet models. The * symbol means using RandAugment <ref type="bibr" target="#b4">[5]</ref>, which was used in ASL <ref type="bibr" target="#b0">[1]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Effects of various components in CSRA</head><p>Finally, we study the effects of various components in the proposed CSRA module.</p><p>Class-agnostic vs. class-specific To further verify whether the class-agnostic average pooling matters much to the final performance, we did a controlled experiment, modifying only the process of calculating the overall feature for the i-th class. When we only apply the average pooling, the overall feature f i = g, which is the same as the baseline method. When we apply only the spatial pooling, the overall</p><formula xml:id="formula_16">feature f i = a i = ? 49 k=1 s i k x k .</formula><p>When the two are combined, f i = g + ?a i , which is the proposed CSRA. <ref type="table" target="#tab_3">Table 6</ref> shows the results of ResNet-cut with one attention head (? = 0.4) on the MS-COCO dataset. The class-specific attention (spatial pooling) is more effective than the classagnostic average pooling. By combining the two, CSRA clearly outperforms both. <ref type="table" target="#tab_3">Table 6</ref> confirms that classspecific attention is important for multi-label recognition. Intuitively, we believe that the proposed CSRA is in particu-  lar valuable when there are many small objects. We resort to visualization of the attention scores to verify this intuition, which are shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. The visualization shows that even when there are multiple persons (including a person occupying only a small number of pixels), the attention score of CSRA (s i j in Eq. 2, with i corresponds to the 'person' class, and j enumerates all locations) effectively captures where are the persons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualizing the attention</head><p>In general, the score maps often accurately localize objects from different categories. More visualizations are shown in the supplementary material of this paper.</p><p>Effect of ? Since we have fixed the sequence of temperature P$3 RQ 06&amp;2&amp;2 /DPEGD ? values, the only hyperparameter in CSRA is ?. We take VIT-L16 and ResNet-cut as our backbone networks, and evaluate the performance of different ? on the MS-COCO and VOC2007 datasets. As show in <ref type="figure" target="#fig_4">Fig. 5</ref>, the performance of VIL-L16 steadily increases to its peak around ? = 1.0, while the ResNet-cut reaches its highest score at ? = 0.1. <ref type="figure" target="#fig_4">Fig. 5</ref> suggests that CSRA is relatively robust to ?, because all these ? values produce mAPs far higher than the baseline method. However, different backbone may need different ?.</p><p>When ? becomes too large, the effect of the average pooling component becomes declining, and the spatial pooling will dominate our model. As shown in <ref type="table" target="#tab_3">Table 6</ref>, spatial pooling alone is inferior to CSRA (which combines average and spatial pooling). Hence, it is possible that a too large ? will lead to a performance drop in our CSRA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of attention heads</head><p>We then test how the number of attention heads is affecting the model's performance. Similarly, we use VIT-L16 and ResNet-cut as our backbone, and evaluate the performance of different number of heads, H. As shown in <ref type="table">Table 7</ref>, the mAP increases steadily when H increases to a large number (6 or 8), demonstrating the effectiveness of the multi-head attention version of CSRA.</p><p>It is also worth noting that even H = 8 leads to very Normalization To test the effect of the normalization of the classifiers m i (Sec. 3.4), we adopt ResNet-101 as our backbone, and evaluate the performance on MS-COCO, because this dataset is assumed to be the most representative multi-label image classification dataset. <ref type="table">Table 8</ref> shows the influence of normalization. Since all mAP results are close to each other, the effect of normalization is limited in terms of mAP (the difference is 0.1 in both H = 2 and H = 4). We use this normalization step not for higher accuracy, but to increase the convergence speed during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>In this paper, we proposed CSRA, a simple but effective pipeline for multi-label image classification. The inspiration for CSRA came from our simple modification in the testing stage, where 4 lines of code brought consistent improvement to a variety of existing models without training. We generalized this modification to capture separate features for every object class, leading to the proposed class-specific residual attention module. A multi-head attention version of CSRA not only improved recognition accuracy, but also removed the dependency on a hyperparameter. CSRA outperformed existing methods on 4 benchmark datasets, albeit being both simpler and more explainable.</p><p>In the future, we will generalize our method to more general image representation learning, and further verify whether our residual attention will be useful in other computer vision tasks, such as object detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>x T k m i ) ? 49 l=1 exp(T x T l m i ) becomes a Dirac delta function, in which the maximum element in s i k (1 ? k ? 49) corresponds to all the probability mass while all other elements are 0. Hence, when T ? ?,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>= 1 ,= 1 .</head><label>11</label><figDesc>and the constant multiplier 1+? can be safely ignored. Hence, the final CSRA feature vector for the i-th class is a weighted combination of the feature tensor x k . The weight for location k, This weight is also a weighted combination of two terms: 1 49 and s i k :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>+</head><label></label><figDesc>symbol means using larger input image resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>One sample image from MS-COCO 2014 (on the left) and the attention scores for the 'person' class overlapped on top of it (on the right). The score map was resized to the same size as that of the input image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>(a) VIT-L16 (H = 8) on MS-COCO P$3 RQ 92&amp;/DPEGD ? (b) ResNet-cut (H = 1) on VOC2007The influence of ? on the MS-COCO dataset using VIL-L16 as the backbone, and on VOC2007 using ResNet-cut. We set classifier num H = 8 and H = 1 for them, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>The highest scores in each block are shown in boldface.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>All</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Top 3</cell><cell></cell></row><row><cell>Methods</cell><cell>mAP</cell><cell>CP</cell><cell>CR</cell><cell>CF1 OP</cell><cell>OR</cell><cell>OF1 CP</cell><cell>CR</cell><cell cols="2">CF1 OP</cell><cell>OR</cell><cell>OF1</cell></row><row><cell>ResNet-101</cell><cell>79.4</cell><cell cols="10">83.4 66.6 74.0 86.8 71.1 78.2 86.2 59.7 70.6 90.5 63.7 74.8</cell></row><row><cell>ResNet-cut</cell><cell>82.1</cell><cell cols="10">86.2 68.7 76.4 88.9 73.1 80.3 88.7 61.3 72.5 92.1 65.2 76.3</cell></row><row><cell>ML-GCN [4]</cell><cell>83.0</cell><cell cols="10">85.1 72.0 78.0 85.8 75.4 80.3 89.2 64.1 74.6 90.5 66.5 76.7</cell></row><row><cell>MS-CMA [36]</cell><cell>83.8</cell><cell cols="10">82.9 74.4 78.4 84.4 77.9 81.0 88.2 65.0 74.9 90.2 67.4 77.1</cell></row><row><cell>KSSNet [21]</cell><cell>83.7</cell><cell cols="5">84.6 73.2 77.2 87.8 76.2 81.5 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MCAR [11]</cell><cell>83.8</cell><cell cols="10">85.0 72.1 78.0 88.0 73.9 80.3 88.1 65.5 75.1 91.0 66.3 76.7</cell></row><row><cell>ResNet-101 + CSRA</cell><cell>83.5</cell><cell cols="10">84.1 72.5 77.9 85.6 75.7 80.3 88.5 64.2 74.4 90.4 66.4 76.5</cell></row><row><cell>ResNet-cut + CSRA</cell><cell>85.6</cell><cell cols="10">86.2 74.9 80.1 86.6 78.0 82.1 90.1 65.7 76.0 91.4 67.9 77.9</cell></row><row><cell>ASL [1]</cell><cell>86.5</cell><cell cols="10">87.2 76.4 81.4 88.2 79.2 81.8 91.8 63.4 75.1 92.9 66.4 77.4</cell></row><row><cell>VIT-L16</cell><cell>80.4</cell><cell cols="10">83.8 67.0 74.5 86.6 72.0 78.6 86.8 60.0 70.1 90.3 64.7 75.4</cell></row><row><cell>VIT-L16 + CSRA</cell><cell>86.5</cell><cell cols="10">88.2 74.4 80.8 88.5 77.4 82.6 91.9 65.8 76.7 92.6 68.2 78.5</cell></row><row><cell>VIT-L16 + CSRA  *</cell><cell>86.9</cell><cell cols="10">89.1 74.2 81.0 89.6 77.1 82.9 92.5 65.8 76.9 93.4 68.1 78.8</cell></row><row><cell cols="5">Table 5. Comparisons of mAP (in %) of state-of-the-art models and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">our CSRA on the WIDER-Attribute dataset.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="3">mAP CF1 OF1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DHC [19]</cell><cell>81.3</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VA [12]</cell><cell>82.9</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SRN [39]</cell><cell>86.2</cell><cell cols="2">75.9 81.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VAA [26]</cell><cell>86.4</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VAC [13]</cell><cell>87.5</cell><cell cols="2">77.6 82.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VIT-B16</cell><cell>86.3</cell><cell cols="2">75.9 81.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VIT-L16</cell><cell>87.7</cell><cell cols="2">78.1 82.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VIT-B16 + CSRA</cell><cell>89.0</cell><cell cols="2">79.4 84.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VIT-L16 + CSRA</cell><cell>90.1</cell><cell cols="2">81.0 85.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">non-convolutional deep networks like vision transformers.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .</head><label>6</label><figDesc>The effect of applying average pooling versus spatial pooling on the MS-COCO dataset.</figDesc><table><row><cell>Backbone</cell><cell cols="2">Method average spatial mAP</cell></row><row><cell>ResNet-cut H = 1, T = 1</cell><cell>1 2 3</cell><cell>82.1 84.2 85.3</cell></row><row><cell cols="2">(a) Original image</cell><cell>(b) Attention image</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .Table 8 .</head><label>78</label><figDesc>The influence of the number of attention heads in CSRA tested on the MS-COCO dataset. H = 1 H = 2 H = 4 H = 6 H = 8 The influence of normalization of classifier on the MS-COCO dataset with the ResNet-101 CNN backbone. The numbers are mAPs under different settings. Normalize H = 2 H = 4 small computational overhead, thanks to the simplicity of our CSRA attention module. On MS-COCO, in the training stage the baseline method (without CSRA) took 3705.6 seconds, while CSRA with 8 heads took 3735.7 seconds, with only 0.8% overhead. The total test time on MS-COCO increased from 142.3 to 153.8 seconds, and this increase (8%) is acceptable when considering the significant improvement in mAP.</figDesc><table><row><cell>VIT-L16</cell><cell>85.8</cell><cell>86.1</cell><cell>86.4</cell><cell>86.4</cell><cell>86.5</cell></row><row><cell cols="2">ResNet-cut 85.3</cell><cell>85.4</cell><cell>85.5</cell><cell>85.6</cell><cell>85.5</cell></row><row><cell cols="2">ResNet-101</cell><cell>with without</cell><cell>83.2 83.1</cell><cell>83.3 83.4</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Protter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14119</idno>
		<title level="m">Asymmetric loss for multi-label classification</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Edge Boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning semantic-specific graph representation for multi-label image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolu</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hefeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="522" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-label image recognition with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Zhao-Min Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">WILDCAT: Weakly supervised learning of deep convnets for image classification, pointwise localization and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Mordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="642" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The PASCAL Visual Object Classes (VOC) Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html.1" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Multi-label image recognition with multi-class attentional regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bin-Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01755</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Human attribute recognition by refining attention heat map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochuan</forename><surname>Hao Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition Letters</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visual attention consistency under image transforms for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochuan</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-label classification using conditional dependency networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suicheng</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence</title>
		<meeting>the International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">1300</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Conditional graphical lasso for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoying</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2977" to="2986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Human attribute recognition by deep hierarchical contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">9910</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-label image classification via knowledge distillation from weakly-supervised detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Shiming Xiang, and Chunhong Pan</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Shi Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.03851</idno>
		<title level="m">Deep attentive tracking via reciprocative learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ThunderNet: Towards realtime generic object detection on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6718" to="6727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">TResNet: High performance GPU-dedicated architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hussam</forename><surname>Lawen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><forename type="middle">Ben</forename><surname>Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1400" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">MobileNetV2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep imbalanced attribute classification using visual attention aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">11215</biblScope>
			<biblScope unit="page" from="680" to="697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fixing localization errors to improve image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisham</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12370</biblScope>
			<biblScope unit="page" from="271" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">EfficientNet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">CNN-RNN: A unified framework for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Beyond object proposals: Random crop pooling for multi-label image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changzhi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5678" to="5688" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-label image recognition by recurrently discovering attentional regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">HCP: A flexible CNN framework for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junshi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1901" to="1907" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Correlative multi-label multi-instance image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="651" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Exploit bounding box annotations for multi-label object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><forename type="middle">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bin-Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="280" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cross-modality attention with semantic graph embedding for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renchun</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingze</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">CutMix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multilabel image classification with regional latent semantic dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2801" to="2813" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning spatial regularization with imagelevel supervisions for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="5513" to="5522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">End-to-end flow correlation tracking with spatial-temporal attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="548" to="557" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
