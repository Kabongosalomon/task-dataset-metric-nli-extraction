<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dense Depth Posterior (DDP) from Single Image and Sparse Range</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
							<email>yanchao.yang@cs.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UCLA Vision Lab University of California</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wong</surname></persName>
							<email>alexw@cs.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UCLA Vision Lab University of California</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
							<email>soatto@cs.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UCLA Vision Lab University of California</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dense Depth Posterior (DDP) from Single Image and Sparse Range</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Code is available at: https://github. com/YanchaoYang/Dense-Depth-Posterior</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a deep learning system to infer the posterior distribution of a dense depth map associated with an image, by exploiting sparse range measurements, for instance from a lidar. While the lidar may provide a depth value for a small percentage of the pixels, we exploit regularities reflected in the training set to complete the map so as to have a probability over depth for each pixel in the image. We exploit a Conditional Prior Network, that allows associating a probability to each depth value given an image, and combine it with a likelihood term that uses the sparse measurements. Optionally we can also exploit the availability of stereo during training, but in any case only require a single image and a sparse point cloud at run-time. We test our approach on both unsupervised and supervised depth completion using the KITTI benchmark, and improve the state-ofthe-art in both.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>There are many dense depth maps that are compatible with a given image and a sparse point cloud. Any pointestimate, therefore, depends critically on the prior assumptions made. Ideally, one would compute the entire posterior distribution of depth maps, rather than a point-estimate. The posterior affords to reason about confidence, integrating evidence over time, and in general, is a (Bayesian) sufficient representation that accounts for all the information in the data.</p><p>Motivating application. In autonomous navigation, a sparse point cloud from lidar may be insufficient to make planning decisions: Is the surface of the road in <ref type="figure">Fig. 1</ref> (middle, better viewed when enlarged) littered with pot-holes, or is it a smooth surface? Points that are nearby in image topology, projecting onto adjacent pixels, may be arbitrarily far in the scene. For instance, pixels that straddle an occluding boundary correspond to large depth gaps in the scene. While the lidar may not measure every pixel, if we know it projects onto a tree, trees tend to stand out from the ground, which informs the topology of the scene. On the other hand, pixels that straddle illumination boundaries, like shadows cast by trees, seldom correspond to large depth discontinuities.</p><p>Depth completion is the process of assigning a depth value to each pixel. While there are several deep learningbased methods to do so, we wish to have the entire posterior estimate over depths. Sparse range measurements serve to ground the posterior estimate in a metric space. This could then be used by a decision and control engine downstream. <ref type="bibr">Figure</ref> 1. An image (top) is insufficient to determine the geometry of the scene; a point cloud alone (middle) is similarly ambiguous. Lidar returns are shown as colored points, but black regions are uninformative: Are the black regions holes in the road surface, or due to radiometric absorption? Combining a single image, the lidar point cloud, and previously seen scenes allows inferring a dense depth map (bottom) with high confidence. Color bar from left to right: zero to infinity.</p><p>Side information. If the dense depth map is obtained by processing the given image and sparse point cloud alone, the quality of the resulting decision or control action could be no better than if the raw data was fed downstream (Data Processing Inequality). However, if depth completion can exploit a prior or aggregate experience from previously seen images and corresponding dense depth maps, then it is possible for the resulting dense depth map to improve the quality of the decision or action, assuming that the training set is representative. To analyze a depth completion algorithm, it is important to understand what prior assumptions, hypotheses or side information is being exploited.</p><p>Goal. We seek methods to estimate the geometry and topology of the scene given an image, a sparse depth map, and a body of training data consisting of images and the associated dense depth maps. Our assumption is that the distribution of seen images and corresponding depth maps is representative of the present data (image and sparse point cloud) once restricted to a sparse domain.</p><p>Our method yields the full posterior over depth maps, which is much more powerful than any point estimate. For instance, it allows reasoning about confidence intervals. We elect the simplest point estimate possible, which is the maximum, to evaluate the accuracy of the posterior. It should be noted, however, that when there are multiple hypotheses with similar posterior, the point estimate could jump from one mode to another, and yet the posterior being an accurate representation of the unknown variable. More sophisticated point estimators, for instance, taking into account memory, or spatial distribution, non-maximum suppression, etc. could be considered, but here we limit ourselves to the simplest one.</p><p>Key idea. While an image alone is insufficient to determine a depth map, certain depth maps are more probable than others given the image and a previously seen dataset. The key to our approach is a conditional prior model P (d|I, D) that scores the compatibility of each dense depth map d with the given image I based on the previously observed dataset D. This is computed using a Conditional Prior Network (CPN) <ref type="bibr" target="#b38">[35]</ref> in conjunction with a model of the likelihood of the observed sparse point cloud z under the hypothesized depth map d, to yield the posterior probability and, from it, a maximum a-posteriori (MAP) estimate of the depth map for benchmark evaluation:</p><formula xml:id="formula_0">d = arg max d P (d|I, z) ? P (z|d)P D (d|I).<label>(1)</label></formula><p>Let D ? R 2 be the image domain, sampled on a regular lattice of dimension N ? M , I : D ? R 3 is a color image, with the range quantized to a finite set of colors, d : D ? R+ is the dense depth map defined on the lattice D, which we represent with an abuse of notation as a vector of dimension M N :</p><formula xml:id="formula_1">d ? R N M + . ? ? D is a sparse subset of the image domain, with cardinality K = |?|, where the function d takes values d(?) = z ? R K + . Fi- nally, D = {d j , I j } n</formula><p>j=1 is a dataset of images I j and their corresponding dense depth maps d j ? R N M + . Since we do not treat D as a random variable but a given set of data, we write it as a subscript. In some cases, we may have additional data available during training, for instance stereo imagery, in which case we include it in the dataset, and discuss in detail how to exploit it in Sect. 3.3.</p><p>Results. We train a deep neural network model to produce an estimate of the posterior distribution of dense depth maps given an image and a sparse point cloud (sparse range map), that leverages a Conditional Prior Network to restrict the hypothesis space, weighted by a classical likelihood term. We use a simple maximum a-posteriori (MAP) estimate to evaluate our approach on benchmark datasets, including the KITTI-unsupervised, where the dense depth map is predicted given an image and a point cloud with 5% pixel coverage, and the KITTI-supervised, where a point cloud with 30% coverage is given for training. We achieve top performance in both. We also validate on additional data in the Supplementary Materials <ref type="bibr" target="#b39">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Semi-Dense Depth Completion. Structured light sensors typically provide dense depth measurements with about 20% missing values; At this density, the problem is akin to inpainting <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b20">19,</ref><ref type="bibr" target="#b29">27]</ref> that use morphological operations <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b25">24]</ref>. The regime we are interested in involves far sparser point clouds (&gt; 90% missing values). Supervised Depth Completion. Given a single RGB image and its associated sparse depth measurements along with dense ground truth, learning-based methods <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr">25,</ref><ref type="bibr" target="#b32">29,</ref><ref type="bibr" target="#b40">37]</ref> minimize the corresponding loss between prediction and ground truth depth. <ref type="bibr" target="#b32">[29]</ref> trains a deep network to regress depth using a sparse convolutional layer that discounts the invalid depth measurements in the input while <ref type="bibr" target="#b14">[14]</ref> proposes a sparsity-invariant upsampling layer, sparsity-invariant summation, and joint sparsity-invariant concatenation and convolution. <ref type="bibr" target="#b7">[7]</ref> treats the binary validity map as a confidence map and adapts normalized convo-lution for confidence propagation through layers. <ref type="bibr" target="#b5">[5]</ref> implements an approximation of morphological operators using the contra-harmonic mean (CHM) filter <ref type="bibr" target="#b24">[23]</ref> and incorporates it as a layer in a U-Net architecture for depth completion. <ref type="bibr" target="#b4">[4]</ref> proposes a deep recurrent auto-encoder to mimic the optimization procedure of compressive sensing for depth completion, where the dictionary is embedded in the neural network. <ref type="bibr" target="#b40">[37]</ref> predicts surface normals and occlusion boundaries from the RGB image, which gives a coarse representation of the scene structure. The predicted surface normals and occlusion boundaries are incorporated as constraints in a global optimization framework guided by sparse depth. Unsupervised Depth Completion. In this problem setting, dense ground truth depth is not available as supervision, so a strong prior is key. <ref type="bibr" target="#b21">[20]</ref> proposes minimizing the photometric consistency loss among a sequence of images with a second-order smoothness prior based on a similar formulation in single image depth prediction <ref type="bibr" target="#b23">[22,</ref><ref type="bibr" target="#b33">30,</ref><ref type="bibr" target="#b41">38]</ref>. Instead of having a separate pose network or using direct visual odometry methods, <ref type="bibr" target="#b21">[20]</ref> uses Perspective-n-Point (PnP) <ref type="bibr" target="#b19">[18]</ref> and Random Sample Consensus (RANSAC) <ref type="bibr" target="#b8">[8]</ref> to obtain pose. We exploit recently introduced method to learn the conditional prior <ref type="bibr" target="#b38">[35]</ref> to take into account scene semantics rather than using a local smoothness assumption.</p><p>Stereo as Supervision. Recent works in view synthesis <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b35">32]</ref> and unsupervised single image depth prediction, <ref type="bibr">[11,</ref><ref type="bibr" target="#b12">12]</ref> propose using view synthesis to hallucinate a novel view image by reconstruction loss. In the case of stereo pairs, <ref type="bibr">[11,</ref><ref type="bibr" target="#b12">12]</ref> propose training networks to predict the disparities of an input image by reconstructing the unseen right view of a stereo pair given the left. In addition to the photometric reconstruction loss, local smoothness is assumed; <ref type="bibr" target="#b12">[12]</ref> additionally proposed edge-aware smoothness and leftright consistency. Although during inference, we assume only one image is given, at training time we may have stereo imagery available, which we exploit as in Sect. 3.3. In this work, we incorporate only the stereo photometric reconstruction term. Despite our network predicting depths and the network [11, 12] predicting disparities, we are able to incorporate this training scheme seamlessly into our approach.</p><p>Exploiting Semantics and Contextual Cues. While methods <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr">25,</ref><ref type="bibr" target="#b32">29,</ref><ref type="bibr" target="#b40">37]</ref> learn a representation for the depth completion task through ground truth supervision, they do not have any explicit modeling of the semantics of the scene. Recently, <ref type="bibr" target="#b27">[26]</ref> explored this direction by predicting object boundary and semantic labels through a deep network and using them to construct locally planar elements that serve as input to a global energy minimization for depth completion. <ref type="bibr" target="#b3">[3]</ref> proposes to complete the depth by anisotropic diffusion with a recurrent convolution network, where the affinity matrix is computed locally from an im-age. <ref type="bibr" target="#b15">[15]</ref> also trains a U-Net for joint depth completion and semantic segmentation in the form of multitask learning in an effort to incorporate semantics in the learning process.</p><p>To address contextual cues and scene semantics, <ref type="bibr" target="#b38">[35]</ref> introduces a Conditional Prior Network (CPN) in the context of optical flow, which serves as a learning scheme for inferring the distribution of optical flow vectors given a single image. We leverage this technique and formulate depth completion as a maximum a-posteriori problem by factorizing it into a likelihood term and a conditional prior term, making it possible to explicitly model the semantics induced regularity of a single image. Even though our method could be applied to sparse-to-dense interpolation for optical flow, where the sparse matches can be obtained using <ref type="bibr" target="#b37">[34,</ref><ref type="bibr" target="#b36">33]</ref>, here we focus our test on depth completion task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In order to exploit a previously observed dataset D, we use a Conditional Prior Network (CPN) <ref type="bibr" target="#b38">[35]</ref> in our framework. Conditional Prior Networks infer the probability of an optical flow given a single image. During training, ground truth optical flow is encoded (upper branch in <ref type="figure" target="#fig_0">Fig. 2-A)</ref>, concatenated with the encoder of an image (lower branch), and then decoded into a reconstruction of the input optical flow.</p><p>In our implementation, the upper branch encodes dense depth, concatenated with the encoding of the image, to produce a dense reconstruction of depth at the decoder, together with a normalized likelihood that can serve as a posterior score. We consider a CPN as a function that, given an image (lower branch input) maps any sample putative depth map (upper branch input) to a positive real number, which represents the conditional probability/prior of the input dense depth map given the image.</p><p>We denote the ensemble of parameters in the CPN as w CP N ; with abuse of notation, we denote the decoded depth with d = w CP N (d, I). When trained with a bottleneck imposed on the encoder (upper branch), the reconstruction error is proportional to the conditional distribution:</p><formula xml:id="formula_2">Q(d, I; w CP N ) = e ? w CP N (d,I)?d ? ? P D (d|I) (2)</formula><p>where ? indicates the specific norm used for calculating Q.</p><p>In Sect. 4.2 and Sect. 5, we show the training details of CPN, and also quantitatively show the effect of different choices of the norm ?. In the following, we assume w CP N is trained, and Q will be used as the conditional prior. For the proof that Q computed by CPN represents the conditional prior as in Eq. <ref type="formula">(2)</ref>, please refer to <ref type="bibr" target="#b38">[35]</ref>.</p><p>In order to obtain a posterior estimate of depth, the CPN needs to be coupled with a likelihood term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Supervised Single Image Depth Completion</head><p>Supervised learning of dense depth assumes the availability of ground truth dense depth maps. In the KITTI depth completion benchmark <ref type="bibr" target="#b32">[29]</ref>, these are generated by accumulating the neighboring sparse lidar measurements. Even though it is called ground truth, the density is only ? 30% of the image domain, whereas the density of the unsupervised benchmark is ? 5%. The training loss in the supervised modality is just the prediction error:</p><formula xml:id="formula_3">L(w) = N j=1 ?(z j , I j ; w) ? d j ? (3)</formula><p>where ? is the map from sparse depth z and image I to dense depth, realized by a deep neural network with parameters w, and ? = 1 fixed in the supervised training.</p><p>Our network structure for ? is detailed in <ref type="figure" target="#fig_0">Fig. 2-B</ref>, which has a symmetric two-branch structure, each encoding different types of input: one sparse depth, the other an image; skip connections are enabled for two branches. Note that our network structure is unique among all the top performing ones on the KITTI depth completion benchmark: We do not use specifically-designed layers for sparse inputs, such as sparsity invariant layers <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b32">29]</ref>. Instead of early fusion of sparse depth and image, our depth defers fusion to decoding, which entails fewer learnable parameters, detailed in <ref type="bibr" target="#b39">[36]</ref>. A related idea is proposed in <ref type="bibr" target="#b15">[15]</ref>; instead of a more sophisticated NASNet block <ref type="bibr" target="#b42">[39]</ref>, we use the more common ResNet block <ref type="bibr" target="#b13">[13]</ref>. Although simpler than competing methods, our network achieves state-of-the-art performance (Sect. 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Unsupervised Single Image Depth Completion</head><p>Supervised learning requires ground truth dense depth, which is hard to come by. Even the "ground truth" provided in the KITTI benchmark is only 30% dense and interpolated from even sparser maps. When only sparse independent measurements of depth are available, for instance from lidar, with less than 10% coverage (e.g. 5% for KITTI), we call depth completion unsupervised as the only input are sensory data, from images and a range measurement device, with no annotation or pre-processing of the data.</p><p>The key to our approach is the use of a CPN to score the compatibility of each dense depth map d with the given image I based on the previously observed data D. In some cases, we may have additional sensory data available during training, for instance, a second image taken with a camera with a known relative pose, such as stereo. In this case, we include the reading from the second camera in the training set D, as described in Sect. 3.3. When only a single image is given, the CPN Eq. <ref type="formula">(2)</ref> is combined with a model of the likelihood of the observed sparse point cloud z under the hypothesized depth map d:</p><formula xml:id="formula_4">P (z|d) ? e ? z?d(?) ? (4)</formula><p>which is simply a Gaussian around the hypothesized depth, restricted to the sparse subset ?, when ? = 2. The overall loss is:</p><formula xml:id="formula_5">L u (w) = ? N j=1 logP (d j |I j , z j , D) = N j=1 z j ? d j (?) ? +? N j=1 w CP N (d j , I j ) ? d j ? = N j=1 z j ? ?(z j , I j ; w)(?) ? + ? N j=1 w CP N (?(z j , I j ; w), I j ) ? ?(z j , I j ; w) ? (5)</formula><p>Note that ?, ? control the actual norm used during training, as well as the modeling of the likelihood and conditional distribution. We experiment with these parameters in Sect. 5.1, and show our quantitative analysis there.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Disparity Supervision</head><p>Some datasets come with stereo imagery. We want to be able to exploit it, but without having to require its availability at inference time. We exploit the strong relation between depth and disparity. In addition to the sparse depth z and the image I, we are given a second image I as part of a stereo pair, which is rectified (standard pre-processing), to first-order we assume that there exists a displacement s = s(x), x ? D such that</p><formula xml:id="formula_6">I(x) ? I (x + s)<label>(6)</label></formula><p>which is the intensity constancy constraint. We model, again simplistically, disparity s as s = F B/d, where F is the focal length and B is the baseline (distance between the optical centers) of the cameras. Hence, we can synthesize disparity s from the predicted dense depth d, thus to constrain the recovery of 3-d scene geometry. More specifically, we model the likelihood of seeing I given I, d as:</p><formula xml:id="formula_7">P (I |I, d) ? e ? x I(x) ? I (x + s(d(x))) ? 2<label>(7)</label></formula><p>However, the validity of the intensity constancy assumption is affected by complex phenomena such as translucency, transparency, inter-reflection, etc. In order to mitigate the error in the assumption, we could also employ a perceptual metric of structural similarity (SSIM) <ref type="bibr" target="#b34">[31]</ref>. SSIM scores corresponding 3 ? 3 patches p(x), p (x) ? centered at x in I and I , respectively, to measure their local structural similarity. Higher scores denote more similarity; hence we can subtract the scores from 1 to form a robust version of Eq. <ref type="bibr" target="#b7">(7)</ref>. We use P raw (I |I, d) and P ssim (I |I, d) to represent the probability of I given I, d measured in raw photometric value and SSIM score respectively. When the stereo pair is available, we can form the conditional prior as follows by applying conditional independence: P (d|I, I , D) ? P (I |I, d, D)P (d|I, D)</p><p>= P (I |I, d)P D (d|I) <ref type="bibr" target="#b8">(8)</ref> Similar to the training loss Eq. (5) for the unsupervised single image depth completion setting, we can derive the loss for the stereo setting as follows:</p><formula xml:id="formula_8">L s (w) = ? N j=1 logP (d j |I j , I j , z j , D) = L u (w) + ? j,x I j (x) ? I j (x + s(d j (x))) (9)</formula><p>where d j = ?(z j , I j ; w) and L u is the loss defined in Eq. <ref type="bibr" target="#b5">(5)</ref>. Note that, the above summation term is the instantiation for P raw <ref type="figure">(I |I, d)</ref>, which can also be replaced by the SSIM counterpart. Rather than choosing one or the other, we compose the two with tunable parameters ? c and ? s , our final loss for stereo setting depth completion is:</p><formula xml:id="formula_9">L s (w) = L u (w) + ? c ? c + ? s ? s<label>(10)</label></formula><p>with ? c represents the raw intensity summation term in Eq. (9), and ? s for the SSIM counterpart. Next, we elaborate our implementation details and evaluate the performance of our proposed method in different depth completion settings.  <ref type="bibr" target="#b21">[20]</ref> marginally performs better than us on MAE by 0.8%. We note that <ref type="bibr" target="#b21">[20]</ref> achieves this performance using photometric supervision. When including our photometric term (Eq. <ref type="formula" target="#formula_0">(10)</ref>), we outperform <ref type="bibr" target="#b21">[20]</ref> on every metric and achieve state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Network architecture</head><p>We modify the public implementation of CPN <ref type="bibr" target="#b38">[35]</ref> by replacing the input of the encoding branch with a dense depth map. Fusion of the two branches is simply a concatenation of the encodings. The encoders have only convolutional layers, while the decoder is made of transposed convolutional layers for upsampling.</p><p>Our proposed network, unlike the base CPN, as seen in <ref type="figure" target="#fig_0">Fig. 2</ref>-A, contains skip connections between the layers of the depth encoder and the corresponding decoder layers, which makes the network symmetric. We also use ResNet blocks <ref type="bibr" target="#b13">[13]</ref> in the encoders instead of pure convolutions. A stride of 2 is used for downsampling in the encoder and the number of channels in the feature map after each encoding layer is [64 * k, 128 * k, 256 * k, 512 * k, 512 * k]. In all our experiments, we use k = 0.25 for the depth branch, and k = 0.75 for the image branch, taking into consideration that an RGB image has three channels while depth map only has one channel. Our network has fewer parameters than those based on early fusion (e.g. <ref type="bibr" target="#b21">[20]</ref> used ?27.8M parameters in total; where as we only use ?18.8M). We provide an example comparing our network architecture and that of <ref type="bibr" target="#b21">[20]</ref> in the Supplementary Materials <ref type="bibr" target="#b39">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training Procedure</head><p>We begin by detailing the training procedure for CPN. Once learned, we apply CPN as part of our training loss and do not need it during inference. In order to learn the conditional prior of the dense depth maps given an image, we require a dataset with images and corresponding dense depth maps. We are unaware of any real-world dataset for outdoor scenes that meets our criterion. Therefore, we train the CPN using the Virtual KITTI dataset <ref type="bibr" target="#b10">[10]</ref>. It contains 50 high-resolution monocular videos with a total of 21, 260 frames, together with ground truth dense depth maps, generated from five different virtual worlds under different lighting and weather conditions. The original Virtual KITTI im- <ref type="figure">Figure 3</ref>. This plot shows the empirical study on the choice of norms ?, ? in the likelihood term and the conditional prior term respectively. Each curve is generated by varying ? in Eq. (5) with fixed ?, ?. And the performance is measured in RMSE.</p><p>age has a large resolution of 1242 ? 375, which is too large to feed into a normal commercial GPU. So we crop it to 768 ? 320 and use a batch size of 4 for training. The initial learning rate is set to 1e ?4 , and is halved every 50,000 steps 300,000 steps in total.</p><p>We implement our approach using TensorFlow <ref type="bibr" target="#b0">[1]</ref>. We use Adam <ref type="bibr" target="#b16">[16]</ref> to optimize our network with the same batch size and learning rate schedule as the training of CPN. We apply histogram equalization and also randomly crop the image to 768 ? 320. We additionally apply random flipping both vertical and horizontal to prevent overfitting. In the case of unsupervised training, we also perform a random shift within a 3 ? 3 neighborhood to the sparse depth input and the corresponding validity map. We use ? = 0.045, ? = 1.20 for Eq. (9), and the same ? is applied with ? c = 0.15, ? s = 0.425 for Eq. (10). We choose ? = 1 and ? = 2, but as one may notice in Eq. (2), the actual conditional prior also depends on the choice of the norm ?. To show the reasoning behind our choice, we will present as an empirical study in <ref type="figure">Fig. 3</ref> to show the effects of the different pairing of norms with a varying ? by evaluating each model on the RMSE metric.</p><p>In the next section, we report representative experiments in both the supervised and unsupervised benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate our approach on the KITTI depth completion benchmark <ref type="bibr" target="#b32">[29]</ref>. The dataset provides ? 80k raw image frames and corresponding sparse depth maps. The sparse depth maps are the raw output from the Velodyne lidar sensor, each with a density of about 5%. The ground truth depth map is created by accumulating the neighboring 11 raw lidar scans, with roughly 30% pixels annotated. We use the officially selected 1,000 samples for validation and we apply our method to 1,000 testing samples, with which we submit to the official KITTI website for evaluation. We additionally perform an ablation study on the effects of the sparsity of the input depth measurements on the NYUv2 indoor dataset <ref type="bibr" target="#b30">[28]</ref> in the Supplementary Materials <ref type="bibr" target="#b39">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Norm Selection</head><p>As seen in Eq. (5), ?, ? control the actual norms (penalty functions) applied to the likelihood term and conditional prior term respectively, which in turn determine how we model the distributions. General options are from the binary set {1, 2}. i.e. {L 1 , L 2 }, however, there is currently no agreement on which one is better suited for the depth completion task. <ref type="bibr" target="#b21">[20]</ref> shows ? = 2 gives significant improvement for their network, while both <ref type="bibr" target="#b32">[29,</ref><ref type="bibr" target="#b15">15]</ref> claim to have better performance when ? = 1 is applied. In our approximation of the posterior in Eq. (5), the choice of the norms gets more complex as the modeling (norm) of the conditional prior will also depend on the likelihood model. Currently, there is no clear guidance on how to make the best choice, as it may also depend on the network structure.</p><p>Here we try to explore the characteristic of different norms, at least for our network structure, by conducting an empirical study on a simple version (channel number of features reduced) of our depth completion network using different combinations of ? and ?. As shown in <ref type="figure">Fig. 3</ref>, the performance on the KITTI depth completion validation set varies in a wide range with different ?, ?. Clearly for our depth completion network, L 1 is always better than L 2 on the likelihood term. And the lowest RMSE is achieved when a L 2 is also applied on the conditional prior term. Thus the best coupling is ? = 1, ? = 2 for Eq. (5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Supervised Depth Completion</head><p>We evaluate the proposed Depth Completion Network described in Sect. 3.1 on the KITTI depth completion benchmark. We show a quantitative comparison between our approach and the top performers on the benchmark in Tab. 1. Our approach achieves the state-of-the-art in three metrics by outperforming <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b15">15]</ref>, who each held the stateof-the-art in different metrics on the benchmark. We improve over <ref type="bibr" target="#b15">[15]</ref> in iRMSE and iMAE by 2.3% and 9.5%, respectively, and <ref type="bibr" target="#b7">[7]</ref> in MAE by 11.9%. <ref type="bibr" target="#b21">[20]</ref> performs better on the RMSE metric by 2.6%; however, we outperform <ref type="bibr" target="#b21">[20]</ref> by 24.3%, 28.9% and 17.8% on the iRMSE, iMAE and MAE metrics, respectively. Note in the online table of KITTI depth completion benchmark 1 , all methods are solely ranked by the RMSE metric, which may not fully reflect the performance of each method. Thus we propose to rank all methods by averaging over the rank numbers on each metric, and the overall ranking is shown in the last column of Tab. 1. Not surprisingly, our depth completion network gets the smallest rank number due to its generally good performance on all metrics. <ref type="figure" target="#fig_1">Fig. 4</ref> shows a qualitative comparison of our method to the top performing method on the test set of the KITTI benchmark. We see that our method produces depths that  <ref type="bibr" target="#b21">[20]</ref> on KITTI depth completion test set in the supervised setting. Image and validity map of the sparse measurements (1st column), dense depth results and corresponding error map of <ref type="bibr" target="#b21">[20]</ref> (2nd column) and our results and error map (3rd column). Warmer color in the error map denotes higher error. The yellow rectangles highlight the regions for detailed comparison. Note that our network consistently performs better on fine and far structures and our completed dense depth maps have less visual artifacts. are more consistent with the scene with fewer artifacts (e.g. grid-like structures <ref type="bibr" target="#b21">[20]</ref>, holes in objects <ref type="bibr" target="#b7">[7]</ref>). Also, our network performs consistently better on fine and far structures, which may be traffic signs and poles on the roadside that provide critical information for safe driving as shown in the second row in <ref type="figure" target="#fig_1">Fig. 4</ref>. More in the Supplementary <ref type="bibr" target="#b39">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Unsupervised Depth Completion</head><p>We show that our network can also be applied to unsupervised setting using only the training loss Eq. (5) to achieve the state-of-the-art results as well. We note that the simplest way for the network to minimize the data term is to directly copy the sparse input to the output, which will make the learning inefficient. To facilitate the training, we change the stride of the first layer from 1 to 2 and replace the final layer of the decoder with a nearest neighbor upsampling.</p><p>We show a quantitative comparison (Tab. 2) between our method and that of <ref type="bibr" target="#b21">[20]</ref> along with an ablation study on our loss function. We note that the results of <ref type="bibr" target="#b21">[20]</ref> are achieved using their full model, which includes their multi-view photometric term. Our approach using just Eq. (5) is able to outperform <ref type="bibr" target="#b21">[20]</ref> in every metric with the exception of MAE where <ref type="bibr" target="#b21">[20]</ref> marginally beats us by 0.8%. By applying our reconstruction loss Eq. (9), we outperform <ref type="bibr" target="#b21">[20]</ref> in every metric. Moreover, our full model Eq. (10) further improves over all other variants and is state-of-the-art in unsupervised depth completion. We present a qualitative comparison between our approach and that of <ref type="bibr" target="#b21">[20]</ref> in <ref type="figure" target="#fig_2">Fig. 5</ref>. Visually, we observe the results of <ref type="bibr" target="#b21">[20]</ref> still contain the artifacts as seen before. The artifacts, i.e. circles, as detailed in <ref type="figure" target="#fig_2">Fig. 5</ref>, are signs that their network is probably overfitted to the input sparse depth, due to the lack of semantic regularity. Our approach, however, does not suffer from these artifacts; instead, our predictions are globally correct and consistent with the scene geometry.  <ref type="bibr" target="#b21">[20]</ref> on the KITTI depth completion test set in the unsupervised setting. Image and validity map of the sparse measurements (1st column), dense depth results and corresponding error map of <ref type="bibr" target="#b21">[20]</ref> (2nd column) and ours (3rd column). Warmer color in the error map denotes higher error. Yellow rectangles highlight the regions for detailed comparison. Note again that our network consistently performs better on fine and far structures and our completed dense depth maps have less visual artifacts (this includes the circle in the center of their prediction, row 1, column 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>In this work, we have described a system to infer a posterior probability over the depth of points in the scene corresponding to each pixel, given an image and a sparse aligned point cloud. Our method leverages a Conditional Prior Network, that allows the association of a probability to each depth value based on a single image, and combines it with a likelihood term for sparse depth measurements. Moreover, we exploit the availability of stereo imagery in constructing a photometric reconstruction term that further constrains the predicted depth to adhere to the scene geometry.</p><p>We have tested the approach both in a supervised and unsupervised setting. It should be noted that the difference between "supervised" and "unsupervised" in the KITTI benchmark is more quantitative than qualitative: the former has about 30% coverage in depth measurements, the latter about 5%. We show in Tab. 1 and 2 that our method achieves state-of-the-art performance in both supervised and unsupervised depth completion on the KITTI benchmark. Although we outperform other methods on score metrics that measures the deviation from the ground truth, we want to emphasize that our method does not simply produce a point estimate of depth, but provides a confidence measure, that can be used for more downstream processing, for instance for planning, control and decision making.</p><p>We have explored the effect of various hyperparameters, and are in the process of expanding the testing to realworld environments, where there could be additional errors and uncertainty due to possible time-varying misalignment between the range sensor and the camera, or between the two cameras when stereo is available, faulty intrinsic camera calibration, and other nuisance variability inevitably present on the field that is carefully weeded out in evaluation benchmarks such as KITTI. This experimentation is a matter of years, and well beyond the scope of this paper. Here we have shown that a suitably modified Conditional Prior Network can successfully transfer knowledge from prior data, including synthetic ones, to provide context to input range values for inferring missing data. This is important for downstream processing as the context can, for instance, help differentiate whether gaps in the point cloud are free space or photometrically homogeneous obstacles, as discussed in our motivating example in <ref type="figure">Fig. 1.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Supplementary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">An Ablation Study on NYUv2 Indoor Dataset</head><p>The goal of this ablation study is to see how the performance of our depth completion network changes while we vary its input depth density. By density we mean the number of valid depth measurements divided by the total number of pixels in an image. We choose 100k samples from the training set of NYUv2 <ref type="bibr" target="#b30">[28]</ref> indoor dataset as our training set, and we separately select 500 samples as the test set. Each of the sample has dimension 480x640. During training we use a batch size of 1, and an initial learning rate of 1e-4, which is halved every 25000 steps until 100000 total number of steps. The performance with respect to the given density reported in <ref type="table">Table 3</ref> is obtained at the end of the training, and the density is in percentage. As shown in <ref type="table">Table 3</ref>, we also observe the performance degeneration phenomenon as observed in others <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b22">21]</ref>, which confirms again the negative effect of decreasing the density of the input sparse depth.  <ref type="table">Table 3</ref>. A quantitative evaluation of our proposed network structure on the NYUv2 <ref type="bibr" target="#b30">[28]</ref> dataset with varying input sparse depth density. We denote density as percentage of input sparse depth with respect to the input image. The input sparse depth measurements were randomly sampled from the dense depth maps provided. It is apparent from the experiment that decreasing the density of the input depth causes the performance of the model to degrade.</p><p>7.2. Detail of ? s in Eq. <ref type="bibr" target="#b10">(10)</ref> We state P ssim (I |I, d) as the likelihood counterpart to Eq. (7) for stereo setting. Here we show its detailed form. We denote g(x, y; I, I ) as the function to compute a SSIM score given two images I, I and two pixel locations x, y, each on different images. Then we have: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">On the Network Complexity</head><p>In regards to the number of parameters used in the network versus the prior-art <ref type="bibr" target="#b21">[20]</ref>: Although our network has two branches that become fused at a later stage, we in fact have fewer parameters than that of <ref type="bibr" target="#b21">[20]</ref>. Here we use the second layer as an example. After early fusion, the input to the second layer for Ma et al. <ref type="bibr" target="#b21">[20]</ref>   <ref type="bibr" target="#b21">[20]</ref>. This same exercise can be applied similarly to other layers to compare the number of parameters used.</p><p>To summarize for the reader, <ref type="bibr" target="#b21">[20]</ref> contains a total of ?27.8M parameters for their network; whereas, we only use ?18.8M parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">More Visual Comparisons</head><p>In <ref type="figure">Fig. 6</ref>, we show more comparisons to the top method <ref type="bibr" target="#b21">[20]</ref> on KITTI depth completion benchmark in the unsupervised setting. Note the "O" shape artifacts in the results of Ma et al. <ref type="bibr" target="#b21">[20]</ref> were due to the lack of knowledge about the scene geometry. Such is not the case in our results as we employ a more sophisticated scene prior that discounts the unlikely artifacts that is not compatible with the scene depicted in the RGB image.</p><p>In <ref type="figure">Fig. 7</ref>, we show more comparisons to the state-of-theart methods <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b3">3]</ref> on the supervised depth completion task. Please see details therein.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.">CPN Applied to Monocular Video</head><p>Indeed our experiment is performed on the stereo setting; whereas video-based supervision, if properly leveraged, can exploit larger baselines. We have conducted the ablation study, shown in the table below. The use of CPN as a prior gives improvement (rows 1, 2), and our method in the monocular setting still outperforms <ref type="bibr" target="#b21">[20]</ref>   <ref type="table">Table 4</ref>. Ablation study using different supervision and prior.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>(A): the architecture of the Conditional Prior Network (CPN) to learn the conditional of the dense depth given a single image. (B): Our proposed Depth Completion Network (DCN) for learning the mapping from a sparse depth map and an image to a dense depth map. Connections within each encoder/decoder block are omitted for simplicity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative comparison to Ma et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative comparison to Ma et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>P</head><label></label><figDesc>ssim (I |I, d) ? e ? x 1 ? g(x, x + s(d(x)); I, I ) ? 2 (11) with d j = ?(z j , I j ; w), the detailed loss as defined in Eq. (10) is:L s (w) = L u (w) + ? c j,x I j (x) ? I j (x + s(d j (x))) + ? s j,x 1 ? g(x,x + s(d j (x)); I j , I j )<ref type="bibr" target="#b12">(12)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative results on the supervised KITTI depth completion benchmark. Our method achieves state of the art performance in three metrics, iRMSE, iMAE, and MAE.<ref type="bibr" target="#b21">[20]</ref> performs better than us by 2.6% on the RMSE metric; however, we outperform<ref type="bibr" target="#b21">[20]</ref> on all other metrics by 24.3%, 28.9% and 17.8% on the iRMSE, iMAE and MAE, respectively. The last column is the average rank over ranks on all the four metrics.</figDesc><table><row><cell>Method</cell><cell cols="3">iRMSE iMAE RMSE</cell><cell>MAE</cell><cell>Rank</cell></row><row><cell cols="2">Dimitrievski [6] 3.84</cell><cell>1.57</cell><cell cols="3">1045.45 310.49 13.0</cell></row><row><cell>Cheng [3]</cell><cell>2.93</cell><cell>1.15</cell><cell cols="3">1019.64 279.46 7.5</cell></row><row><cell>Huang [14]</cell><cell>2.73</cell><cell>1.13</cell><cell>841.78</cell><cell cols="2">253.47 6.0</cell></row><row><cell>Ma [20]</cell><cell>2.80</cell><cell>1.21</cell><cell>814.73</cell><cell cols="2">249.95 5.5</cell></row><row><cell>Eldesokey [7]</cell><cell>2.60</cell><cell>1.03</cell><cell>829.98</cell><cell cols="2">233.26 4.75</cell></row><row><cell>Jaritz [15]</cell><cell>2.17</cell><cell>0.95</cell><cell>917.64</cell><cell cols="2">234.81 3.0</cell></row><row><cell>Ours</cell><cell>2.12</cell><cell>0.86</cell><cell>836.00</cell><cell cols="2">205.40 1.5</cell></row></table><note>R 3?3 +</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>has dimension b ? h ? w ? 64, and the output of second layers in the network of Ma et al. has dimension b ? h 2 ? w 2 ? 128 so number of parameters used for the second layer is going to be 3 ? 3 ? 64 ? 128 = 73728. However, in our depth completion network, the sparse depth branch has input dimension b ? h ? w ? 16 and so the sparse depth branch has 3 ? 3 ? 16 ? 32 = 4608 parameters. The image branch has input dimension b ? h ? w ? 48, outso the image branch has 3 ? 3 ? 48 ? 96 = 41472 parameters. In total, the second layer in our depth completion network contains only 46080 trainable parameters. Much less that those in Ma et al</figDesc><table><row><cell>output dimension b ? ? 32, put dimension b ? h 2 ? w 2 h 2 ? w 2 ? 96,</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.cvlibs.net/datasets/kitti/eval_ depth.php?benchmark=depth_completion</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensor-Figure 6. Qualitative comparisons to Ma</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>et al. [20] in unsupervised setting. Note the &quot;O&quot; shape artifacts occur due to a lack of knowledge about the scene regularity in [20], which however is not present in our completed depth due to a more sophisticated scene prior</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">More qualitative comparisons to Spade-RGBsD [15] (second column) and CSPN [3] (third column) in supervised setting. Yellow rectangles highlight detailed comparisons. flow: A system for large-scale machine learning</title>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
	<note>Figure 7</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Efficient spatio-temporal hole filling strategy for kinect depth maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Camplani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Salgado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Three-dimensional image processing (3DIP) and applications Ii</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">8290</biblScope>
			<biblScope unit="page">82900</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Depth estimation via affinity learned with convolutional spatial propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep convolutional compressed sensing for lidar depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08949</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning morphological operators for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dimitrievski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veelaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Philips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advanced Concepts for Intelligent Vision Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="450" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning morphological operators for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dimitrievski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veelaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Philips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Concepts for Intelligent Vision Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Propagating confidences through cnns for sparse data regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eldesokey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11913</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deepstereo: Learning to predict new views from the world&apos;s imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Neulander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5515" to="5524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Virtual worlds as proxy for multi-object tracking analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Bg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="740" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="7" to="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Hms-net: Hierarchical multi-scale sparsity-invariant network for sparse depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.08685</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sparse and dense data with cnns: Depth completion and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">De</forename><surname>Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wirbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Perrotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nashashibi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">In defense of classical image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Fast depth completion on the cpu</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1802.00036</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Epnp: An accurate o (n) solution to the pnp problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">155</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Depth enhancement via lowrank matrix completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3390" to="3397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Selfsupervised sparse-to-dense: Self-supervised depth completion from lidar and monocular camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Cavalheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00275</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sparse-to-dense: Depth prediction from sparse depth samples and a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05522</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A learning framework for morphological operators using counter-harmonic mean</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Angulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Mathematical Morphology and Its Applications to Signal and Image Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="329" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">High-resolution lidar-based depth mapping using bilateral filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Premebida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asvadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Nunes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05614</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sbnet: Sparse blocks network for fast inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semantically guided depth upsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinggera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Layer depth denoising and completion for structured-light rgb-d cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><forename type="middle">S</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1187" to="1194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<title level="m">Sparsity invariant cnns. 2017 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning depth from monocular videos using direct methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2022" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="842" to="857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Coarse-to-fine region selection and matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sundaramoorthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5051" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">S2f: Slow-to-fast interpolator flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2087" to="2096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Conditional prior networks for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Dense depth posterior (ddp) from single image and sparse range</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10034</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep depth completion of a single rgb-d image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="7" to="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
