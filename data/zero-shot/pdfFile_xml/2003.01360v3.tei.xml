<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DiPE: Deeper into Photometric Errors for Unsupervised Learning of Depth and Ego-motion from Monocular Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hualie</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laiyan</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenglong</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Huang</surname></persName>
						</author>
						<title level="a" type="main">DiPE: Deeper into Photometric Errors for Unsupervised Learning of Depth and Ego-motion from Monocular Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised learning of depth and ego-motion from unlabelled monocular videos has recently drawn great attention, which avoids the use of expensive ground truth in the supervised one. It achieves this by using the photometric errors between the target view and the synthesized views from its adjacent source views as the loss. Despite significant progress, the learning still suffers from occlusion and scene dynamics. This paper shows that carefully manipulating photometric errors can tackle these difficulties better. The primary improvement is achieved by a statistical technique that can mask out the invisible or nonstationary pixels in the photometric error map and thus prevents misleading the networks. With this outlier masking approach, the depth of objects moving in the opposite direction to the camera can be estimated more accurately. To the best of our knowledge, such scenarios have not been seriously considered in the previous works, even though they pose a higher risk in applications like autonomous driving. We also propose an efficient weighted multi-scale scheme to reduce the artifacts in the predicted depth maps. Extensive experiments on the KITTI dataset show the effectiveness of the proposed approaches. The overall system achieves state-oftheart performance on both depth and ego-motion estimation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hualie Jiang, Laiyan Ding, Zhenglong Sun, and Rui Huang * Abstract-Unsupervised learning of depth and ego-motion from unlabelled monocular videos has recently drawn great attention, which avoids the use of expensive ground truth in the supervised one. It achieves this by using the photometric errors between the target view and the synthesized views from its adjacent source views as the loss. Despite significant progress, the learning still suffers from occlusion and scene dynamics. This paper shows that carefully manipulating photometric errors can tackle these difficulties better. The primary improvement is achieved by a statistical technique that can mask out the invisible or nonstationary pixels in the photometric error map and thus prevents misleading the networks. With this outlier masking approach, the depth of objects moving in the opposite direction to the camera can be estimated more accurately. To the best of our knowledge, such scenarios have not been seriously considered in the previous works, even though they pose a higher risk in applications like autonomous driving. We also propose an efficient weighted multi-scale scheme to reduce the artifacts in the predicted depth maps. Extensive experiments on the KITTI dataset show the effectiveness of the proposed approaches. The overall system achieves state-oftheart performance on both depth and ego-motion estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The depth and ego-motion estimation is the core problem in Simultaneous Localization And Mapping (SLAM). Recently, Monocular Depth Estimation (MDE) attracts much attention, as it can be flexibly used in many applications, such as autonomous mobile robotics and AR/VR. Tracking the 6-DoF motion for a moving camera is also critical for these applications. Traditional supervised methods require expensively-collected ground truth, resulting in limited ability in generalization. By contrast, unsupervised learning from monocular videos <ref type="bibr" target="#b0">[1]</ref> is a much more generalizable solution.</p><p>The unsupervised learning models usually contain two networks for predicting the depth map of the target view, and the motion between the target view and its temporally adjacent views. With the network output, the target view can be reconstructed by the adjacent source views with image warping, and the resulted photometric loss can be used as the supervisory signal for learning. However, the image reconstruction is usually destroyed by between-view occlusion and scene dynamics, as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, and the resulting incorrect supervision harms the network learning. Occlusion: point P is visible in C t (camera view in time t) but occluded in C t+1 , and to achieve photometric consistency (matching P instead of Q), P is estimated with shorter depth (Q ), making the foreground object blur. (b) Co-directional motion: if point M moves forward like camera C (from t ? 1 to t), it is likely estimated with farther depth (M t?1 ), producing 'dark holes'. (c) Contra-directional motion: when point M moves backward opposite to camera C (from t to t + 1), it is estimated with shorter depth (M t+1 ).</p><p>The theory of how minimizing between-view reconstruction errors affects the depth estimation of occluded regions and the common forward and backward moving objects is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. Many methods have been proposed to cope with the occlusion and dynamics, and considerable improvement has been made. For example, the effect of 'dark holes' by the co-directionally moving objects has been tackled in the latest work <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. However, as shown in <ref type="figure">Fig. 4</ref> the latest models make significant underestimation of the depth for the contra-directionally moving objects. To the best of our knowledge, the inaccuracy of such objects has not been reported in the literature, which may cause trouble in practical applications. For instance, in autonomous driving, if the distance of oncoming cars is underrated, unnecessary braking or avoiding may be executed.</p><p>This issue can be largely avoided by our proposed outlier masking technique, which helps to exclude the occluded and moving regions, especially the oncoming objects. The technique is driven by our observation that the photometric errors of occluded and dynamic regions are much larger. In theory, the visible background usually dominates the scenes The outlier masking can exclude many invisible and nonstatic pixels, particularly those belonging to contra-moving objects, thus predicting a more accurate depth map. Without outlier masking, the oncoming vehicle is predicted to be very close, and the foreground object boundary significantly dilates. and the invisible or moving pixels are inconsistent with the background, thus making their errors difficult to optimize. Besides, we also propose an efficient weighted multi-scale scheme to reduce artifacts and work with the outlier masking to produce better depth maps.</p><p>The effectiveness of our two main contributions, as mentioned above, is experimentally proven on the driving KITTI dataset. Together with a simple baseline model and some other masking practices, we build an overall state-of-theart unsupervised monocular depth and ego-motion estimation system, called DiPE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In recent years, deep Convolutional Neural Networks (CNN) have boosted the performance of MDE. One typical approach is using a deep CNN to densely regress the ground truth depth obtained with physical sensors <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>.</p><p>Other approaches can be categorized as combining deep learning with graphical models <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> or casting MDE as a dense classification problem <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. However, models trained on publicly available datasets with ground truth depth, like the NYUDepthV2 <ref type="bibr" target="#b14">[15]</ref> or KITTI <ref type="bibr" target="#b15">[16]</ref>, usually do not generalize well to real scenarios.</p><p>Instead of depending on ground truth, unsupervised learning schemes adopt more available resources, the stereo images <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> or adjacent monocular video frames <ref type="bibr" target="#b0">[1]</ref> to construct the supervisory signal. Specifically, the loss is the photometric difference between a view and its synthesis. The synthesis can be computed from the additional view by its estimated depth and the known or estimated pose between the two views. To take advantage of both spatial and temporal cues, stereo videos are exploited for training in <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Compared with stereo images, the monocular videos are more generalized and available, thus this paper focuses on the latter one.</p><p>The first method training with monocular videos, SfM-Learner <ref type="bibr" target="#b0">[1]</ref> adopts an additional Pose CNN to estimate the relative motion between sequential views to make view synthesis attainable. However, the photometric consistency between nearby views is usually unsatisfied due to occlusion and moving objects. To improve this advantageous framework, many methods have been proposed, which can be mainly classified as: masking photometric errors <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, joint learning with optical flow <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, modelling object motion <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. The masking strategies also do not necessarily guarantee flexibility. Some masking techniques, such as the explainability mask <ref type="bibr" target="#b0">[1]</ref> and the uncertainty map <ref type="bibr" target="#b20">[21]</ref> also requires an extra network to learn. Joint learning with the optical flow has to construct a new network for learning optical flow to explain or compensate for the photometric inconsistency caused by occlusion or scene dynamics. Similarly, modelling object motion also requires additional modules to estimate the segmentation and motion of objects. Different from the above methods, the overlap and blank masks geometrically derived from the image warping process <ref type="bibr" target="#b21">[22]</ref> is a light-weight design for occlusion. A simpler method for occlusion is the minimum reprojection in Monodepth2 <ref type="bibr" target="#b3">[4]</ref>, which takes the minimum photometric errors from all source views, thus is also a masking technique. Monodepth2 also adopts a auto-masking technique for moving objects in a close speed with the camera. This simple and efficient masking strategy has been proved effective by Monodepth2, compared with other state-of-the-art methods. However, the oncoming moving objects, have not been noticed and solved. The outlier masking method is proposed in this paper for such objects. Further, our outlier masking technique can help the minimum reprojection to recover a more accurate boundary for the foreground objects in predicted depth maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY A. Preliminaries</head><p>The monocular unsupervised learning scheme is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. A training sample contains the target frame I t at time t and some source frames I s at nearby times, s ? S.</p><formula xml:id="formula_0">Conventionally, S = {t?1, t+1} or {t?2, t?1, t+1, t+2}.</formula><p>Suppose that K is the shared intrinsic matrix of these frames. With the predicted depth D t and transformation T t,s , the synthesis from the source view s to the target view t can be expressed as,</p><formula xml:id="formula_1">I s?t = I s proj(D t , T t?s , K) ,<label>(1)</label></formula><p>where is the differentiable bilinear sampling operator <ref type="bibr" target="#b26">[27]</ref> and proj() is the operation projecting the pixel p t in the target image to the point p s in the source image,</p><formula xml:id="formula_2">p s KT t?s D(p t )K ?1 p t ,<label>(2)</label></formula><p>where p t and p s are expressed in homogeneous coordinates.</p><p>In this paper, we adopt the popular combination of L1 and SSIM by <ref type="bibr" target="#b17">[18]</ref> to compute the photometric errors,</p><formula xml:id="formula_3">PE(I a , I b ) = 0.85 1 ? SSIM(I a , I b ) 2 + 0.15 I a ? I b 1 ,<label>(3)</label></formula><p>In addition, an edge-aware smoothness term is usually also applied in unsupervised training. We use the one by <ref type="bibr" target="#b3">[4]</ref>,</p><formula xml:id="formula_4">L es = mean |? x d * t | e ?|?xIt| + |? y d * t | e ?|?yIt| ,<label>(4)</label></formula><p>where d * t = d t /d t is the mean-normalized inverse depth from <ref type="bibr" target="#b27">[28]</ref> to discourage shrinking of the estimated depth. Both losses are applied in 4 scales to avoid gradient locality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Outlier Masking</head><p>As has been discussed, the image reconstruction can be damaged by some adverse factors, such as occlusion and scene dynamics. Therefore a portion of pixels in the photometric error map is invalid, and the incorporation of them in training can be misleading. We have the observation that most pixels are visible and stationary, and other occluded and moving pixels always produce more significant photometric errors. The outlier masking technique is based on this observation, which is simple but effective. The outlier mask is automatically determined by the statistical information of photometric errors. Specifically,, we first compute the mean and standard deviation of pixel photometric errors from all source images for every training sample,</p><formula xml:id="formula_5">? = mean{PE(I t , I s?t )|s ? S},<label>(5)</label></formula><formula xml:id="formula_6">? = std{PE(I t , I s?t )|s ? S}.<label>(6)</label></formula><p>Then, we compute an outlier mask for the photometric error map PE(I t , I s?t ), where l and u are the lower and upper thresholds. We can use the computed mask to exclude the possible occluded or moving regions, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. By visualizing the resulted masks during training, we find that it is good to set u as 0.5 because a higher value cannot sufficiently mask the moving objects, and a lower value can mask out many stationary objects. Besides, l is set as 1 to mask some pixels with very small photometric errors because these pixels usually belong to homogeneous regions and not very valuable for network training. This selection for u and l can retain the principal photometric errors for optimizing, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p><formula xml:id="formula_7">M ol s = ? ? l? &lt; PE(I t , I s?t ) &lt; ? + u?,<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Weighted Multi-Scale Scheme</head><p>To avoid getting stuck in local minima due to the gradient locality of the bilinear sampler <ref type="bibr" target="#b26">[27]</ref>, the unsupervised learning models usually predict 4 scale depth maps ( <ref type="figure" target="#fig_1">Fig. 2)</ref> and compute multi-scale photometric losses for training. However, it has been pointed out that this scheme tends to produce 'holes' in large low-texture regions in the intermediate lower resolution depth maps, as well as texture-copy artifacts <ref type="bibr" target="#b3">[4]</ref>. To alleviate this phenomenon, Monodepth2 <ref type="bibr" target="#b3">[4]</ref> adopts a full resolution multi-scale scheme, i.e., to upsample the multi-scale depth maps to the full resolution, perform the image warping using the full-resolution images, and compute photometric losses at the full resolution.</p><p>However, we find that this full-resolution scheme considerably increases the computation and GPU memory during training. To suppress the phenomenon without raising training overhead, we propose a weighted multi-scale scheme to devalue the low-resolution photometric losses and lighten the disadvantage they bring. Explicitly, we define a scale factor f &lt; 1 to compute the weight for the scale r,</p><formula xml:id="formula_8">w r = f r ,<label>(8)</label></formula><p>where r ? {0, 1, 2, 3}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Integrated Objective Function</head><p>Although the proposed outlier masking technique can exclude most irregular pixels, it has some failure cases. For example, the outlier masking cannot eliminate the pixels that move out of the image boundary, as illustrated by the bottom of the outlier mask in <ref type="figure" target="#fig_1">Fig. 2</ref>. In fact, it is easy to mask the out-of-box pixels by the principled masking technique <ref type="bibr" target="#b28">[29]</ref>, which only retains the pixels that are reprojected inside the image box of the source images. Besides, the outlier masking cannot mask out the objects with a very close speed to the camera, as these objects are usually estimated to the maximum depth, and the corresponding photometric errors can exactly lie in the statistical inlier region. As illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, the car in the same lane is not well masked in the outlier mask. Fortunately, the auto-masking technique in Monodepth2 <ref type="bibr" target="#b3">[4]</ref> can handle such cases and after including this masking method, the car in the front is not estimated to be very far away. Moreover, we also find that our outlier masking can collaborate with the minimum reprojection in <ref type="bibr" target="#b3">[4]</ref> well to produce more accurate foreground object boundaries in the predicted depth maps. Therefore, we build a baseline with these three techniques.</p><p>The auto-masking excludes the pixels that hold larger photometric errors by reconstruction than the direct photometric error between the target view and the source view. The minimum reprojection is also a masking technique, and it only retains the pixels with the minimum photometric error among all source views. We express the masks of these three masking methods for the photometric error map where p s is calculated by Eqn. 2. Then we can compute the final mask for the photometric error map PE(I t , I s?t ) by combining three type masks,</p><formula xml:id="formula_9">M = M ol s ? M p s ? M a s ? M mr s ,<label>(12)</label></formula><p>where ? represents the element-wise logical conjunction. Finally, the overall objective function is computed by,</p><formula xml:id="formula_10">L = ? r f r s M s P s #{M s = 1} + ? r e r L r es ,<label>(13)</label></formula><p>where we denote PE(I t , I s?t ) as P s , ? and ? are weights to balance the two types of losses, and e is a weighting factor for the edge-aware smoothness loss from different scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>We implement the proposed approaches based on Mon-odepth2 <ref type="bibr" target="#b3">[4]</ref> and maintain the most basic experimental settings. The depth CNN is a fully convolutional encoderdecoder network with an input/output resolution of 640?192. The Pose CNN is a stand CNN with fully connected layer to regress the 6-Dof relative camera motion. Both networks use a ResNet18 <ref type="bibr" target="#b31">[32]</ref> pretrained on ImageNet <ref type="bibr" target="#b32">[33]</ref> as backbone for all of the experiments. In depth estimation experiments, as Monodetph2 <ref type="bibr" target="#b3">[4]</ref>, we only use the nearby 2 frames (S = {t?1, t+1}) and the pair-input Pose <ref type="figure" target="#fig_1">(Fig. 2)</ref>. In ego-motion estimation, however, we also experiment with the all-input Pose CNN with the nearby 4 frames (S = {t ? 2, t ? 1, t + 1, t + 2}) and 2 frames (S = {t ? 1, t + 1}) for <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b28">[29]</ref>.</p><p>The hyper-parameter ?, ?, and e in the final loss function are empirically set to 1, 0.001, and 0.5. The factor f of the weighted multi-scale scheme is chosen as 0.25 by examining several values in the validation set. DiPE is also trained for 20 epochs using Adam <ref type="bibr" target="#b33">[34]</ref>. As our weighted multiscale scheme consumes less memory, DiPE is trained with a bigger batch size of 16 than 12 in Monodepth2 and the training spends only 9 hours on a single Titan Xp while Monodepth2 uses 12 hours. DiPE also uses an initial learning rate of 10 ?4 but divides it by 5 after 15 and 18 epochs, whereas Monodepth2 divides it by 10 only after 15 epochs. As the outlier masking further reduces the errors for training and decreasing the learning rate can help DiPE converges better. Monodepth2 uses the same intrinsic parameters for all training samples by approximating the principal point of the camera to the image center and averaging the focal length on the whole dataset. More precisely, we use the calibrated intrinsic parameters for every training samples, and when performing horizontal flips in data augmentation, the horizontal coordinate of the principal point is also flipped.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. KITTI Eigen Split</head><p>We adopt the standard Eigen split <ref type="bibr" target="#b4">[5]</ref> of the KITTI dataset <ref type="bibr" target="#b15">[16]</ref> in the monocular depth estimation experiments. Following Zhou et al. <ref type="bibr" target="#b0">[1]</ref>, we use a subset of the training set that contains no static frames for training. There are 39,810, 4,424, and 697 samples for training, validation, and test. We also only use about one-tenth (432) of the validation set for validation, because we evaluate all the validation samples after every epoch rather than evaluate a batch of validation samples for certain steps, and this is better for monitoring the training process without spending too much time on validation. In evaluation, every predicted depth map is aligned to the ground truth depth map by multiplying the median value ratio <ref type="bibr" target="#b0">[1]</ref> as other unsupervised monocular methods, and we also adopt the conventional metrics and cropping region in <ref type="bibr" target="#b4">[5]</ref>, and the standard depth cap 80m <ref type="bibr" target="#b17">[18]</ref>. There are 4 error metrics, namely, absolute relative error (Abs Rel), square relative error (Sq Rel), root mean square error (RMSE) and the root mean square error in log space (RMSE log). Other 3 accuracy metrics are the percentages of pixels where the ratio (?) between the estimated depth and ground truth depth smaller than 1.25, 1.25 2 and 1.25 3 .</p><p>1) Performance Comparison: We quantitatively and qualitatively compare the results of our model and other stateof-the-art methods. The quantitative results are shown in <ref type="table" target="#tab_1">Table I</ref>    <ref type="bibr" target="#b15">[16]</ref>. Three categories of methods, which perform training with the depth, stereo images. and monocular video frames, respectively, are compared. In each category, the best results are in bold. Legend: D -depth supervision; S -unsupervised stereo supervision; M -unsupervised mono supervision; ?-newer results from the respective online implementations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>SfMLearner <ref type="bibr" target="#b0">[1]</ref> GeoNet <ref type="bibr" target="#b22">[23]</ref> EPC++ <ref type="bibr" target="#b1">[2]</ref> Struct2depth <ref type="bibr" target="#b2">[3]</ref> Monodepth2 <ref type="bibr" target="#b3">[4]</ref> DiPE(Ours) <ref type="figure">Fig. 4</ref>: Qualitative comparison. Our model DiPE produces very high-quality depth maps, and it reduces most artifacts due to occlusion and scene dynamics. More importantly, recent models, for instance, joint learning with optical flow <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b1">[2]</ref>, e.g.oncoming cars, including modelling object motion <ref type="bibr" target="#b2">[3]</ref> and auto-masking moving objects <ref type="bibr" target="#b3">[4]</ref>, underestimate the depth for the objects moving in an opposite direction, e.g.oncoming cars, while our DiPE succeeds (the second column  2) Ablation Study: We also perform ablation experiments to examine the effectiveness of our contributions. As mentioned in Section III-D, the baseline model uses the three existing masking techniques, i.e., the principled masking, auto-masking, and minimum reprojection. We experiment with four possible combinations of whether including our two contributions, the weighted multi-scale scheme, and the outlier masking technique. The results are shown in <ref type="table" target="#tab_1">Table II</ref>.</p><p>It can be observed that, our two contributions can obviously improve the performance individually, and the performance gain when they combine together is more than double of their separate performance gain, which indicates that the two techniques can collaborate well. Furthermore, the weighted multi-scale scheme also helps DiPE address the artifacts better than Monodepth2 <ref type="bibr" target="#b3">[4]</ref>. DiPE can handle the two failure cases in Monodepth2, as illustrated in <ref type="figure">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. KITTI Odometry</head><p>To prove the effectiveness of our DiPE model in the ego-motion estimation, we also experiment on the official odometry split of the KITTI dataset <ref type="bibr" target="#b15">[16]</ref>. We use three different input settings for the ego-motion network, with the number of frames as 2, 3, and 5, respectively. For training the ego-motion network with input as 2 or 3 frames, we use the DiPE based on the baseline model. However, for the 5-frame-input network, we do not adopt the minimum reprojection technique, because it almost masks out all the pixels from the source views with indexes of t ? 2 and t + 2 and the motion estimation for these two views is inferior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Sequence 09 Sequence 10 # frames ORB-SLAM <ref type="bibr" target="#b34">[35]</ref> 0.014?0.008 0.012?0.011 -SfMLearner <ref type="bibr" target="#b0">[1]</ref> 0.021?0.017 0.020?0.015 5 DF-Net <ref type="bibr" target="#b23">[24]</ref> 0.017?0.007 0.015?0.009 5 GeoNet <ref type="bibr" target="#b22">[23]</ref> 0.012?0.007 0.012?0.009 5 DiPE (Ours) 0.012?0.006 0.012?0.008 5 DDVO <ref type="bibr" target="#b27">[28]</ref> 0.045?0.108 0.033?0.074 3 Vid2Depth <ref type="bibr" target="#b28">[29]</ref> 0.013?0.010 0.012?0.011 3 EPC++ <ref type="bibr" target="#b1">[2]</ref> 0.013?0.007 0.012?0.008 3 DiPE (Ours) 0.012?0.006 0.012?0.008 3 Monodepth2 <ref type="bibr" target="#b3">[4]</ref> 0.017?0.008 0.015?0.010 2 DiPE (Ours) 0.013?0.006 0.012?0.008 2 For evaluation, we adopt the commonly used metric proposed by Zhou et al. <ref type="bibr" target="#b0">[1]</ref>, i.e., the Absolute Trajectory Error (ATE) <ref type="bibr" target="#b34">[35]</ref> in 5-frame snippets. The results are shown in <ref type="table" target="#tab_1">Table III</ref> and the results of other models are taken from their corresponding papers. Among models with the three different input settings, DiPE achieves the best performance. Notably, in the setting of the pair-input ego-motion network, DiPE significantly outperforms Monodepth2 <ref type="bibr" target="#b3">[4]</ref>. Besides, there is no significant performance difference among different motion network settings for DiPE, so DiPE is robust to different motion network input settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we have demonstrated that carefully processing the photometric errors for unsupervised learning of depth and ego-motion from monocular videos can successfully solve the intrinsic difficulties, i.e., the occlusion and scene dynamics. We have introduced the outlier masking technique to exclude the irregular photometric errors that may mislead the network learning. This technique is useful in tackling occlusion and scene dynamics, especially for contradirectionally moving objects. Moreover, we have proposed an efficient and effective weighted multi-scale scheme to avoid the artifacts brought by multi-scale training. Unlike other methods that introducing extra modules, our approaches are simple, as they can be very easily incorporated in the unsupervised geometry learning framework. We have experimentally proven the effectiveness of our two contributions and built a new state-of-the-art model, DiPE, on both monocular depth and ego-motion estimation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>The detriment of occlusion and dynamics. (a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The Unsupervised Learning Flow and Effect of Outlier Masking. (a) Depth CNN: A standard fully convolutional U-net that predicts the multi-scale depth maps for the target image. Pose CNN: A standard CNN that inputs the target view and one source view and predicts their relative motion. With D t and T t?s by the networks, the synthesized image I s?t from the source view I s to the target view I t be differentiablly warped. The photometric errors between I t and I s?t can work as the training objective for both the Depth CNN and Pose CNN. (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>The variation of the photometric error distribution. The photometric error distribution of a validation sample changes during training. Before training, even with a little long tail, the errors are distributed somewhat evenly. After 2 epochs, the majority of errors converge to the lower bound, and a notable long tail forms. Next, the errors under the upper bound continue to decrease and converge, but the errors in the long tail do not change much.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>=</head><label></label><figDesc>PE(I t , I s?t ) as M p s , M a s and M mr s , PE(I t , I s?t ) &lt; PE(I t , I s ), (10) M mr s = PE(I t , I s?t ) ? min s PE(I t , I s?t ), (11)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and the results of other methods are taken from the corresponding papers. The comparison is mainly among the unsupervised monocular training methods, but some</figDesc><table><row><cell>Method</cell><cell>Train</cell><cell cols="3">Error metric ? Abs Rel Sq Rel RMSE</cell><cell>RMSE log</cell><cell cols="3">Accuracy metric ? ? &lt; 1.25 ? &lt; 1.25 2 ? &lt; 1.25 3</cell></row><row><cell>Eigen et al. [5]</cell><cell>D</cell><cell>0.203</cell><cell>1.548</cell><cell>6.307</cell><cell>0.282</cell><cell>0.702</cell><cell>0.890</cell><cell>0.890</cell></row><row><cell>Liu et al. [10]</cell><cell>D</cell><cell>0.201</cell><cell>1.584</cell><cell>6.471</cell><cell>0.273</cell><cell>0.680</cell><cell>0.898</cell><cell>0.967</cell></row><row><cell>Kuznietsov et al. [30]</cell><cell>DS</cell><cell>0.113</cell><cell>0.741</cell><cell>4.621</cell><cell>0.189</cell><cell>0.862</cell><cell>0.960</cell><cell>0.986</cell></row><row><cell>DORN [14]</cell><cell>D</cell><cell>0.072</cell><cell>0.307</cell><cell>2.727</cell><cell>0.120</cell><cell>0.932</cell><cell>0.984</cell><cell>0.994</cell></row><row><cell>Garg [17]</cell><cell>S</cell><cell>0.152</cell><cell>1.226</cell><cell>5.849</cell><cell>0.246</cell><cell>0.784</cell><cell>0.921</cell><cell>0.967</cell></row><row><cell>Monodepth R50 [18] ?</cell><cell>S</cell><cell>0.133</cell><cell>1.142</cell><cell>5.533</cell><cell>0.230</cell><cell>0.830</cell><cell>0.936</cell><cell>0.970</cell></row><row><cell>SuperDepth [31]</cell><cell>S</cell><cell>0.112</cell><cell>0.875</cell><cell>4.958</cell><cell>0.207</cell><cell>0.852</cell><cell>0.947</cell><cell>0.977</cell></row><row><cell>Monodepth2 [4]</cell><cell>S</cell><cell>0.109</cell><cell>0.873</cell><cell>4.960</cell><cell>0.209</cell><cell>0.864</cell><cell>0.948</cell><cell>0.975</cell></row><row><cell>SfMLearner [1] ?</cell><cell>M</cell><cell>0.183</cell><cell>1.595</cell><cell>6.709</cell><cell>0.270</cell><cell>0.734</cell><cell>0.902</cell><cell>0.959</cell></row><row><cell>Vid2Depth [29]</cell><cell>M</cell><cell>0.163</cell><cell>1.240</cell><cell>6.220</cell><cell>0.250</cell><cell>0.762</cell><cell>0.916</cell><cell>0.968</cell></row><row><cell>DF-Net [24]</cell><cell>M</cell><cell>0.150</cell><cell>1.124</cell><cell>5.507</cell><cell>0.223</cell><cell>0.806</cell><cell>0.933</cell><cell>0.973</cell></row><row><cell>GeoNet [23] ?</cell><cell>M</cell><cell>0.149</cell><cell>1.060</cell><cell>5.567</cell><cell>0.226</cell><cell>0.796</cell><cell>0.935</cell><cell>0.975</cell></row><row><cell>DDVO [28]</cell><cell>M</cell><cell>0.151</cell><cell>1.257</cell><cell>5.583</cell><cell>0.228</cell><cell>0.810</cell><cell>0.936</cell><cell>0.974</cell></row><row><cell>EPC++ [2]</cell><cell>M</cell><cell>0.141</cell><cell>1.029</cell><cell>5.350</cell><cell>0.216</cell><cell>0.816</cell><cell>0.941</cell><cell>0.976</cell></row><row><cell>Struct2depth '(M)' [3]</cell><cell>M</cell><cell>0.141</cell><cell>1.026</cell><cell>5.291</cell><cell>0.215</cell><cell>0.816</cell><cell>0.945</cell><cell>0.979</cell></row><row><cell>Gordon et al.[26]</cell><cell>M</cell><cell>0.128</cell><cell>0959</cell><cell>5.230</cell><cell>0.212</cell><cell>0.845</cell><cell>0.947</cell><cell>0.976</cell></row><row><cell>Monodepth2 [4]</cell><cell>M</cell><cell>0.115</cell><cell>0.903</cell><cell>4.863</cell><cell>0.193</cell><cell>0.877</cell><cell>0.959</cell><cell>0.981</cell></row><row><cell>DiPE (Ours)</cell><cell>M</cell><cell>0.112</cell><cell>0.875</cell><cell>4.795</cell><cell>0.190</cell><cell>0.880</cell><cell>0.960</cell><cell>0.981</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Quantitative Results. All the methods are trained and evaluated on the Eigen split [5] of the KITTI dataset</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Ablation Experiments. There are 4 model variants with monocular training on the Eigen split<ref type="bibr" target="#b4">[5]</ref> of the KITTI dataset<ref type="bibr" target="#b15">[16]</ref>. The baseline model adopts existing principled masking, auto-masking and minimum reprojection techniques. Other 3 models include either one or both of our two contributions, the outlier making and weighted multi-scale methods. Artifacts. DiPE can solve the artifacts better and success in the two failure cases by Monodepth2 as is reported in the paper of Monodepth2<ref type="bibr" target="#b3">[4]</ref>.or even better performance to the models of the other two categories.Fig 4 demonstratesthe qualitative comparison among the predicted depth maps by DiPE and many stateof-the-art unsupervised monocular training methods. The predicted depth maps of other models are either shared by the authors or obtained by running the codes provided by the authors. DiPE handles the the scene dynamics and artifacts better than other methods. More results between DiPE and Monodepth2<ref type="bibr" target="#b3">[4]</ref> about oncoming vehicles can be seen from the attached video https://youtu.be/UH8f-WkxVmU.</figDesc><table><row><cell>Baseline</cell><cell>Baseline</cell></row><row><cell>DiPE</cell><cell>DiPE</cell></row><row><cell>Fig. 5:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>Visual odometry results on the odometry split of the KITTI<ref type="bibr" target="#b15">[16]</ref> dataset. Results show the average absolute trajectory error, and standard deviation, in meters.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1851" to="1858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Every pixel counts++: Joint learning of geometry and motion with 3d holistic understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Depth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Third AAAI Conference on Artificial Intelligence (AAAI-19)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3828" to="3838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV), 2016 Fourth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Geocuedepth: exploiting geometric structure cues to estimate depth from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="17" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">High quality monocular depth estimation via a multi-scale network and a detail-preserving objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1920" to="1924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards unified depth and semantic prediction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2800" to="2809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2024" to="2039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-scale continuous crfs as sequential deep networks for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Estimating depth from monocular images as classification using deep fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Monocular depth estimation with hierarchical fusion of dilated cnns and soft-weighted-sum inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="328" to="339" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Bg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="740" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weerasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="340" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Undeepvo: Monocular visual odometry through unsupervised deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7286" to="7291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Supervising the new with the old: learning sfm from sfm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Klodt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="698" to="713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised learning of monocular depth and ego-motion using multiple masks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4724" to="4730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Geonet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1983" to="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Df-net: Unsupervised joint learning of depth and flow using cross-task consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="36" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07804</idno>
		<title level="m">Sfm-net: Learning of structure and motion from video</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Depth from videos in the wild: Unsupervised monocular depth learning from unknown cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning depth from monocular videos using direct methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2022" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5667" to="5675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semi-supervised deep learning for monocular depth map prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>St?ckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6647" to="6655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Superdepth: Self-supervised, super-resolved monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ambru?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9250" to="9256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Orb-slam: a versatile and accurate monocular slam system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M M</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on robotics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1147" to="1163" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
