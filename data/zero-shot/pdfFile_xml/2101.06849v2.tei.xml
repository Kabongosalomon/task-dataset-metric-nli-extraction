<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CFC-Net: A Critical Feature Capturing Network for Arbitrary-Oriented Object Detection in Remote Sensing Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ming</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjuan</forename><surname>Miao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Dong</surname></persName>
						</author>
						<title level="a" type="main">CFC-Net: A Critical Feature Capturing Network for Arbitrary-Oriented Object Detection in Remote Sensing Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Object detection</term>
					<term>deep learning</term>
					<term>convolutional neural networks (CNNs)</term>
					<term>critical features</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Object detection in optical remote sensing images is an important and challenging task. In recent years, the methods based on convolutional neural networks have made good progress. However, due to the large variation in object scale, aspect ratio, and arbitrary orientation, the detection performance is difficult to be further improved. In this paper, we discuss the role of discriminative features in object detection, and then propose a Critical Feature Capturing Network (CFC-Net) to improve detection accuracy from three aspects: building powerful feature representation, refining preset anchors, and optimizing label assignment. Specifically, we first decouple the classification and regression features, and then construct robust critical features adapted to the respective tasks through the Polarization Attention Module (PAM). With the extracted discriminative regression features, the Rotation Anchor Refinement Module (R-ARM) performs localization refinement on preset horizontal anchors to obtain superior rotation anchors. Next, the Dynamic Anchor Learning (DAL) strategy is given to adaptively select high-quality anchors based on their ability to capture critical features. The proposed framework creates more powerful semantic representations for objects in remote sensing images and achieves high-performance real-time object detection. Experimental results on three remote sensing datasets including HRSC2016, DOTA, and UCAS-AOD show that our method achieves superior detection performance compared with many state-of-the-art approaches. Code and models are available at https://github.com/ming71/CFC-Net.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>O BJECT detection in optical remote sensing images is a vital computer vision technique which aims at classifying and locating objects in remote sensing images. It is widely used in crop monitoring, resource exploration, environmental monitoring, military reconnaissance, etc. With the explosive growth of available remote sensing data, identifying objects of interest from massive amounts of remote sensing images has gradually become a challenging task. Most of the traditional methods use handcrafted features to identify objects <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref>. Although much progress has been made, there are still problems such as low-efficiency, insufficient robustness, and poor performance.</p><p>In recent years, the development of convolution neural networks (CNNs) has greatly improved the performance of object detection. Most CNN-based detection frameworks first extract features through convolution operation, and then preset a series of prior boxes (anchors) on the feature maps. Subsequently, classification and regression will be performed on these anchors to obtain the bounding boxes of objects. The powerful ability to automatically extract features of CNN makes it possible to achieve efficient object detection on massive images. Currently, the CNN-based models have been widely used in the object detection in remote sensing images, such as road detection <ref type="bibr" target="#b5">[6]</ref>, vehicle detection <ref type="bibr" target="#b6">[7]</ref>, airport detection <ref type="bibr" target="#b7">[8]</ref>, and ship detection <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Although CNN-based approaches have made good progress, they are often directly derived from generic object detection frameworks. It is difficult for these methods to detect objects with a wide variety of scales, aspect ratios, and orientations in remote sensing images. For example, the orientation of objects varies greatly in remote sensing imagery, while the mainstream generic detectors utilize predefined horizontal anchors to predict these rotated ground-truth (GT) boxes. The horizontal boxes often contain a lot of background which may mislead the detection. There are some approaches that use rotated anchors to locate arbitrary-oriented objects <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b16">[17]</ref>. But it is hard for rotation anchors to achieve good spatial alignment with GT boxes, and they can not ensure to provide sufficiently good semantic information for classification and regression. Some recent researches address the above problems by designing more powerful feature representations <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b20">[21]</ref>. However, they only focus on a certain type of characteristics of remote sensing targets, such as rotation invariant features <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>and scale sensitive features <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. They cannot automatically extract and utilize more complex and discriminative features. Another commonly used method is to manually set a large number of anchors covering different aspect ratios, scales, and orientations to achieve better spatial alignment with targets. In this way, sufficient high-quality anchors can be obtained and better performance can be achieved. Nevertheless, excessive preset anchors bring about three problems: <ref type="bibr" target="#b0">(1)</ref> Most anchors are backgrounds that cannot be used for bounding box regression, which leads to severely redundant calculation. <ref type="bibr" target="#b1">(2)</ref> The parameters of the prior anchors need to be careful manually set, otherwise, they would not obtain good alignment with GT boxes. <ref type="bibr" target="#b2">(3)</ref> There are a large number of low-quality negative samples in the excessive laid anchors which are not conducive to network convergence. The abovementioned issues lead to the fact that densely preset anchors are still unable to effectively handle the difficulties of remote sensing object detection.</p><p>To figure out how the complex variabilities of remote sensing objects make it difficult to achieve high-performance detection, in this paper we introduce the essential concept named critical features, which indicates discriminative features required for accurate classification or localization. Taking the classification task as an example, most anchor-based detectors treat the anchors in <ref type="figure" target="#fig_0">Fig. 1</ref>(a) and <ref type="figure" target="#fig_0">Fig. 1(b)</ref> as positive samples, that is, the IoU between these anchors and GT boxes is higher than 0.5. But the anchor in <ref type="figure" target="#fig_0">Fig. 1(b)</ref> does not capture the discriminative features of the island and bow which are necessary to identify the ship B. Although this anchor achieves accurate localization, it leads to incorrect classification results, thereby degrading detection performance. Furthermore, by visualizing the features extracted by CNN, it is found that the critical features required to identify objects for classification and regression are not always evenly distributed on the object, but may be on local areas such as the bow and stern (see <ref type="figure" target="#fig_1">Fig. 2</ref>(a) and <ref type="figure" target="#fig_1">Fig. 2(b)</ref>). The preset anchors need to capture these critical features to achieve accurate detection. This is similar to the conclusion of some previous work <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b21">[22]</ref>. However, the mainstream rotation detectors are more likely to select anchors with high IoU with GT boxes as positives, but ignore high-quality anchors that contain critical features, which eventually leads to the unstable training process and poor performance. The distribution of the localization ability for all candidates is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>(c) to support this viewpoint. It can be seen that only 74% of positive anchors can achieve high-quality detection (with output IoU larger than 0.5) after regression, which indicates that even the positive anchors still cannot guarantee precise localization. We attribute this phenomenon to the fact that some of the selected positives do not capture the critical features required by the regression task. Besides, as shown in <ref type="figure" target="#fig_1">Fig. 2(d)</ref>, surprisingly more than half of the anchors (about 58% in this case) that achieve accurate detection are regressed from samples that are divided as negatives. It means that a large number of negative anchors capture the critical features well but have not been effectively utilized at all. The inconsistency between the training sample division and the regression results will further lead to a gap between the classification scores and localization accuracy of the detections. Based on the above observations, we conclude that one of the key issues in object detection in remote sensing imagery is whether the anchors can capture the critical features of the objects.</p><p>In this paper, based on the viewpoint of the significance of critical features discussed above, the Critical Feature Capturing Network (CFC-Net) is proposed to achieve highperformance object detection in optical remote sensing imagery. Specifically, CFC-Net first uses a well-designed Polarization Attention Module (PAM) to generate different feature pyramids for classification and regression tasks, and then we can obtain task-specific critical features that are more discriminative as well as easy to be captured. Next, the Rotation Anchor Refinement Module (R-ARM) refines the preset horizontal anchors to better capture the regression critical features to obtain high-quality rotation anchors. Finally, in the training process, the Dynamic Anchor Learning (DAL) strategy is adopted to select the high-quality anchors that capture critical features as positives to ensure superior detection performance after training. Due to the proper construction and utilization of critical features, CFC-Net achieves the state-ofthe-art detection performance using only one anchor, which makes it became a both high-performance and memory-saving method. The code is available to facilitate future research.</p><p>The contributions of this article are summarized as follows: 1) We point out the existence of critical features through experiments, and interpret common challenges for object detection in remote sensing imagery from this perspective.</p><p>2) A novel object detection framework CFC-Net is proposed to extract the critical features and utilize highquality anchors that capture the critical features to achieve superior detection performance. 3) Polarized attention is proposed to construct task-specific critical features. Decoupled critical features provide more useful semantic information for individual tasks, which is beneficial to accurate classification and regression. 4) The dynamic anchor selection strategy selects highquality anchors that capture the critical regression features to bridge the inconsistency between classification and regression, and thus greatly improves the performance of detection. The rest of this article is organized as follows. Section II introduces the related work of object detection. Section III elaborates on the proposed method. Section IV shows the experimental results and analysis. Finally, conclusions are drawn in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Object detection in remote sensing images has a wide range of application scenarios and has been receiving extensive attention in recent years. Most of the early traditional methods use handcraft features to detect remote sensing objects, such as shape and texture features <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, scale-invariant features <ref type="bibr" target="#b1">[2]</ref>, and saliency <ref type="bibr" target="#b2">[3]</ref>. For instance, Zhu et al. <ref type="bibr" target="#b3">[4]</ref> achieves accurate ship detection based on shape and texture features. Eikvil et al. <ref type="bibr" target="#b4">[5]</ref> utilizes spatial geometric properties and gray level features for vehicle detection in satellite images. These approaches have achieved satisfactory performance for specific scenes, but their low efficiency and poor generalization make it hard to detect objects in complex scenarios.</p><p>Recently, with the great success of convolution neural networks, generic object detection has been strongly promoted. Mainstream CNN-based object detection methods can be classified into two categories: one-stage detectors and two-stage detectors. The two-stage detectors first generate a series of proposals, and then perform classification and regression on these regions to obtain the detection results [23]- <ref type="bibr" target="#b24">[25]</ref>. These algorithms usually have high accuracy but slow inference speed. The one-stage detectors, such as the YOLO series <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b27">[28]</ref> and SSD <ref type="bibr" target="#b28">[29]</ref>, directly conduct classification and regression on the prior anchors without region proposal generation. Compared with the two-stage detectors, one-stage methods have relatively low accuracy, but are faster and can achieve real-time object detection.</p><p>Deep learning methods have been widely used in object detection in remote sensing images. A series of CNN-based approaches have been proposed and achieved good performance. However, some methods are directly developed from the generic object detection framework <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b29">[30]</ref>, which detect objects with horizontal bounding box. It is hard for the horizontal box to distinguish densely arranged remote sensing targets and is prone to misdetection. To solve this problem, some studies introduced an additional orientation dimension to achieve the oriented object detection <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref>. For example, Liu et al. <ref type="bibr" target="#b10">[11]</ref> integrates the angle regression into the YOLOv2 <ref type="bibr" target="#b26">[27]</ref> to detect rotated ships. R 2 PN <ref type="bibr" target="#b11">[12]</ref> detects rotated ships by generating oblique region of interest (RoI). RR-CNN <ref type="bibr" target="#b12">[13]</ref> uses the rotated RoI pooling layer, which makes the RoI feature better aligned with the orientation of the object to ensure accurate detection. However, in order to have a higher overlap with the rotated objects, these methods preset densely arranged rotation anchors. Most of the anchors have no intersection with the targets, which brings a lot of redundant computation and the severe imbalance problem. Some work alleviates the issue by setting fewer anchors but still maintaining detection performanc <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b30">[31]</ref>. RoI Transformer <ref type="bibr" target="#b13">[14]</ref> adopts horizontal anchors to learn the rotated RoI through spatial transformation, and thus a few horizontal anchors work well for oriented object detection. R 3 Det [31] achieves state-of-the-art performance through cascade regression and feature alignment is performed on horizontal anchors. Despite the success of these methods, it is still difficult for horizontal anchors to match the rotation objects and the number of preset anchors is still large. Different from the previous work, our CFC-Net uses only one anchor for faster inference and achieves high-quality rotation object detection.</p><p>There are also some methods trying to construct better feature representation to alleviate the difficulty of anchor matching caused by large scale, shape, and orientation variations <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b31">[32]</ref>. For instance, ORN <ref type="bibr" target="#b18">[19]</ref> performs feature extraction through the rotated convolution kernel to achieve rotation invariance. RICNN <ref type="bibr" target="#b17">[18]</ref> optimizes the feature representation by learning a rotation-invariant layer. FMSSD <ref type="bibr" target="#b20">[21]</ref> aggregates the context information in different scales to cope with the multi-scale objects in large-scale remote sensing imagery. Li et al. <ref type="bibr" target="#b14">[15]</ref> proposed a shape-adaptive pooling to extract the features of the ships with various aspect ratios, and then multilevel features are incorporated to generate compact feature representation for ship detection. RRD <ref type="bibr" target="#b15">[16]</ref> observes that shared features degrade performance due to the incompatibility of the classification and regression tasks, and thus the rotation-invariant and rotation-sensitive features are constructed for classification and regression tasks, respectively. But these work only pays attention to a certain aspect of the object characteristics, and cannot comprehensively cover the discriminative features required for object detection. According to the proposed concept of critical features, we believe that the detection performance depends on whether the prior anchors effectively capture these critical features, not limited to the rotation-invariant features or scale-invariant features. Therefore, the clear and easy-to-capture powerful critical feature representation is very important for object detection. The proposed CFC-Net extracts and utilizes tasksensitive critical features for classification and regression tasks respectively so that the detector obtains substantial performance improvements from the more discriminative critical feature representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>The overall structure of CFC-Net is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. It uses ResNet-50 as the backbone network. Firstly, we build multi-scale feature pyramids through FPN <ref type="bibr" target="#b32">[33]</ref>, and then the decoupled features that are sensitive to classification and regression are generated through the proposed PAM. Subsequently, anchor refinement is conducted via R-ARM to obtain the high-quality rotation candidates based on the critical regression features. Finally, through the DAL strategy, anchors that capture critical features are dynamically selected as positive samples for training. In this way, the inconsistency between classification and regression can be alleviated and thus the detection performance can be effectively improved. The details of the proposed CFC-Net are elaborated below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Polarization Attention Module</head><p>In most object detection frameworks, both classification and regression rely on the shared features. However, as mentioned in <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b33">[34]</ref>, the shared features degrade performance owing to the incompatibility between the two tasks. For example, the regression branch of detectors needs to be sensitive to change of the angle so as to achieve accurate orientation prediction, while classification branch is supposed to have the same response to different angles. Therefore, rotationinvariant features are beneficial to classification task, but it is not conducive to bounding box regression.</p><p>We propose Polarization Attention Module (PAM) to avoid the feature interference between different tasks and effectively extract the task-specific critical features. The overall structure of PAM is shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. Firstly, we build separate feature pyramids for different tasks, which is called dual FPN. Next, a well-designed polarization attention mechanism is applied to enhance the representation ability of features. Through the polarization function, different branches generate the discriminative features required for respective tasks. Specifically, for classification, we tend to select high-response global features to reduce noise interference. For regression, we pay more attention to the features of object boundaries and suppress the influence of irrelevant high activation regions.</p><p>Given input feature F ? R C?H?W , we construct tasksensitive features as follows:</p><formula xml:id="formula_0">M = M c (F) ? M s (F), F = M + ?(?(M)) F + F,<label>(1)</label></formula><p>where ? and represent tensor product and elementwise multiplication, respectively. ? denotes sigmoid function. Firstly, we extract channel-wise attention map M c and spatial attention map M s from input features through convolution operations. The purpose of channel attention is to extract the channel-wise relationship of the feature maps. The weight of each channel is extracted by global average pooling and fully connected layers as:</p><formula xml:id="formula_1">M c (F) = ? (W 1 (W 0 (F gap ))) ,<label>(2)</label></formula><p>where F gap is obtained from input feature F via global average pooling, W 0 ? R C/r?C and W 1 ? R C?C/r are the weights of the fully connected layers. ? represents the sigmoid function. Correspondingly, spatial attention is used to model the dependencies between pixels of the input image. It is computed as: kernels with different aspect ratios are used to better detect slender objects such as ships and bridges.</p><p>Next, the attention response map M for a specific task is obtained by multiplying the two attention maps. On this basis, we further build the powerful task-sensitive critical feature representation through the task-specific polarization function ?(?). For classification, the features are expected to pay more attention to the high-response part on feature maps, and ignore the part of less important clues which may be used for localization or or may bring interference noise. We use the following excitation function to achieve the function:</p><formula xml:id="formula_2">? cls (x) = 1 1 + e ??(x?0.5) ,<label>(4)</label></formula><p>where ? is the modulation factor used to control the intensity of feature activation (set to 15 in our experiment). Since the high-response area of critical classification features is enough to achieve accurate classification, there is no need to pursue too much information. Consequently, the effect of high-response critical classification features are excited, while irrelevant features with attention weight less than 0.5 are suppressed. In this way, the classifier is able to pay less attention to the difficult-to-classify areas and reduce the risk of overfitting and misjudgment.</p><p>Meanwhile, for the regression branch, the critical features are often scattered on the edges of object. We expect that the feature maps focus on as many visual clues as possible for object localization, such as object contours and contextual information. To this end, we use the following depression function to process the input features:</p><formula xml:id="formula_3">? reg (x) = x if x &lt; 0.5, 1 ? x otherwise.<label>(5)</label></formula><p>Different from the classification task, a strong response to a patch of the object edge is not conducive to locating the entire object. In Eq.(5), the depression function suppresses the area with the high response in the regression feature, which enforces the model to seek potential visual clues to achieve accurate localization. The curves of polarization function ?(?) are shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. Finally, the polarization attention weighted features are combined with the original feature pyramid to better extract the critical features. As described in Eq.(1), the attention weighted features, the input features F, and the attention response map M are merged by element-wise summation to obtain powerful feature representations for accurate object detection. The proposed PAM greatly improves detection performance via optimizing the representation of critical features. The explainable visualization results are shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. It can be seen that PAM can effectively extract the critical features required for different tasks. For example, the extracted regression critical features are evenly distributed on the object, which is helpful to identify the object boundary and accurately localize the target. The classification critical features are concentrated more on the most recognizable part of an object to avoid interference from other parts of the object, and thus the classification results will be more accurate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Rotation Anhcor Refinement Module</head><p>In the existing anchor-based object detectors, classification and regression are performed on densely preset anchors. It is difficult to achieve alignment between anchors and rotation objects owing to the large variation in the scale and orientation of the remote sensing objects. To solve this problem, we proposed a rotation anchor refinement module (R-ARM) to generate high-quality candidates based on critical regression features to reduce the reliance on the priori geometric knowledge of anchors. Given the regression-sensitive feature map extracted by PAM, R-ARM refines the initial anchors to obtain the rotated anchors that better align with the critical regression features. The regions of these high-quality anchors capture the discriminative and semantic features of the object boundary, which helps to achieve accurate localization.</p><p>The architecture of R-ARM is shown in <ref type="figure" target="#fig_5">Fig. 6</ref>. We preset A initial horizontal anchors at each position of the feature map, which is represented as (x, y, w, h). (x, y) is the center coordinate, and w, h denote the width and height of the anchor, respectively. R-ARM regresses the additional angle ? and the box offsets of the prior anchor to get the rotation anchor which is expressed as (x, y, w, h, ?). R-ARM enables anchors to generate refined rotated boxes that are well aligned with the ground-truth objects, and would simultaneously help to capture more critical features for subsequent detection layers. Specifically, we predict offsets t r = (t x , t y , t w , t h , t ? ) for anchor refinement, which are represented as follows:</p><formula xml:id="formula_4">t r x = (x ? x a ) /w a , t r y = (y ? y a ) /h a , t r w = log (w/w a ) , t r h = log (h/h a ) , t r ? = tan (? ? ? a ) ,<label>(6)</label></formula><p>where x and x a are for the refined box and anchor respectively (likewise for y, w, h, ?).</p><p>In CFC-Net, we set A = 1. It means that only one initial anchor is used, and thus we do not need to carefully set the hyperparameters of angle, aspect ratio, and scale for anchors like the current anchor-based methods, due to the special design of R-ARM after the PAM. Note also that we do not integrate classification prediction in R-ARM as some cascade regression approaches <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b34">[35]</ref>. This is owing to the following considerations:</p><p>1. Classification in the refining stage is not accurate enough, and thus it is easy to mistakenly exclude the potential high-quality candidates, resulting in a poor recall of detections. 2. As mentioned in Section I, there is a gap between classification and regression. The high classification score does not guarantee accurate localization. The training sample selection based on classification confidence in anchor refinement will further degrade the detection performance.</p><p>Compared with previous one-stage detectors, CFC-Net needs fewer predefined anchors, but achieves better detection performance with the R-ARM. As illustrated in <ref type="figure" target="#fig_4">Fig. 5</ref>, guided by the critical regression features generated by PAM, the initial square anchor produces a more accurate rotated candidate via R-ARM. The refined anchor aligns well with the highresponse region that captures critical features, which provides an effective semantic prior for subsequent localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Dynamic Anchor Learning</head><p>In the previous sections, we have introduced the critical feature extraction structure and high-quality anchor generation in CFC-Net. However, the misalignment between classification and regression tasks still exists, that is, the high classification scores can not guarantee the accurate localization of the detections. This issue has been widely discussed in many studies <ref type="bibr" target="#b35">[36]</ref>- <ref type="bibr" target="#b38">[39]</ref>, and some of the work attributed it to the regression uncertainty <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b38">[39]</ref>, which reveals that the localization results obtained by the regression are not completely credible. We believe that the gap between classification and regression mainly comes from unreasonable training sample selection <ref type="bibr" target="#b39">[40]</ref>, and further solve this problem from the perspective of critical features.</p><p>Current detectors usually select positive anchors in the label assignment for training according to the IoU between anchors and GT boxes. For simplicity, we denote the IoU between anchors and GT boxes as IoU in , while the IoU between the predicted boxes and GT boxes as IoU out . The selected positive anchors are supposed to have good semantic information which is conducive to object localization. However, although there is a positive correlation between the classification score and the IoU in (see <ref type="figure">Fig. 7(a)</ref>), the high IoU in does not guarantee good localization potential of the anchors, and as shown in <ref type="figure">Fig. 7(b)</ref>, there is only a weak correlation between the classification confidence and localization capability of predicted boxes. We believe that one of the main causes is that the samples selected according to the IoU in do not align well with the critical features of the objects.</p><p>To resolve the above problems, a Dynamic Anchor Learning (DAL) method is adopted to select samples with strong critical feature capturing ability in the training phase. DAL consists of two parts: dynamic anchor selection (DAS) and matchingsensitive loss (MSL). The rest of this section will elaborate on the implementation of the two strategies.</p><p>Firstly, we adopt a new standard called matching degree to guide training sample division. It is defined as follows:</p><formula xml:id="formula_5">md = ? ? IoU in + (1 ? ?) ? IoU out ? u ? ,<label>(7)</label></formula><p>in which IoU in and IoU out are the IoUs between the anchor box and the GT box before and after regression, respectively. ? and ? are hyperparameters used to weight the influence of different items. u is the penalty term used to suppress the uncertainty during the regression process. The matching degree combines the prior information of spatial alignment, critical feature alignment ability, and regression uncertainty of the anchor to measure its localization capacity. Specifically, for a predefined anchor and its assigned GT box, IoU in is the measure of initial spatial alignment, while IoU out can be used to indicate the critical feature alignment ability. Intuitively, higher IoU out means that the anchor better captures critical regression features and has a stronger localization potential. However, actually, this indicator is unreliable due to the regression uncertainty. It is possible that some high-quality anchors with high IoU in but low IoU out would be mistakenly judged as negative samples <ref type="bibr" target="#b39">[40]</ref>. Therefore, in Eq. <ref type="formula" target="#formula_5">(7)</ref> we further introduce the penalty term u to alleviate the influence from regression uncertainty. It is defined as follows:</p><formula xml:id="formula_6">u = |IoU in ? IoU out |,<label>(8)</label></formula><p>The change of IoU after regression indicates the probability of incorrect anchor assessment, and we use this to measure regression uncertainty. Uncertainty suppression item u imposes a distrust penalty on samples with excessive IoU change after regression to ensure a reasonable training sample selection. We will confirm in the experimental part that the suppression of uncertainty during regression is the key to take advantage of the critical feature information.</p><p>With the evaluation of the matching degree, we can conduct better training sample selection. We first calculate the matching degree between all anchors and GT boxes in the images, and then candidates with matching degree higher than a certain threshold (set to 0.6 in our experiment) are selected as positive samples, while the rest are negatives. Next, for targets that are not assigned with any positives, the candidate with the highest matching degree will be selected as a positive sample.</p><p>The matching degree measures the ability of feature alignment, and thus the division of positive and negative samples is more reasonable, which would alleviate the misalignment between the classification and regression. It can be seen from <ref type="figure" target="#fig_4">Fig. 5</ref> that DAL dynamically selects anchors that capture the critical regression features for bounding box regression. These high-quality candidates can obtain accurate localization performance after the regression, thereby alleviating the inconsistency before and after the regression, and alleviating the misalignment between classification and regression tasks.</p><p>We further integrate matching degree into the training process to construct a matching-sensitive loss (MSL) to achieve high-performance detection. The classification loss is as follows: <ref type="bibr" target="#b8">(9)</ref> in which N n and N p inidcates the number of all negative and positive anchors, respectively. ? n and ? p respectively represent negative and positive samples. F L(?) is focal loss defined as RetinaNet <ref type="bibr" target="#b40">[41]</ref>. p * is the classification label for anchor (p * = 1 if it is positive, while p * = 0 otherwise). w j represents the weighting factor, which is utilized to distinguish positive candidates with different localization ability. For a given target g, we first calculate its matching degrees (denoted by md) with all preset anchors, among which we then select the matching degrees of positives (denoted by md pos , and md pos ? md). Assuming that the maximum value of md pos is md max , we define a compensation value ?md as follows:</p><formula xml:id="formula_7">L cls = 1 N n i??n F L (p i , p * i ) + 1 N p j??p (w j + 1) ? F L p j , p * j ,</formula><formula xml:id="formula_8">?md = 1 ? md max .<label>(10)</label></formula><p>Subsequently, ?md is added to the matching degree of all positive candidates to obtain the weighting factor: w = md pos + ?md.</p><p>The weighting factor improves the contribution of the positive samples to the loss during the training process. In this way, the classification branch can discriminate anchors with different capabilities to capture critical features. Compared with the commonly used method that treats all positive anchors equally, this discriminative approach helps to distinguish positive samples of different localization ability. By involving the localization information of anchors into the classification loss, the classifier can output more reliable classification confidence to select the detections with good localization, thereby bridging the gap between classification and regression.</p><p>Since matching degree measures the localization ability of anchors, it can be further used to promote high-quality localization. The matching-sensitive regression loss is defined as follows:</p><formula xml:id="formula_10">L reg = 1 N p j??p w j ? L smooth L 1 t j , t * j ,<label>(12)</label></formula><p>where L smooth L 1 represents the smooth-L 1 loss <ref type="bibr" target="#b23">[24]</ref>. t and t * are offsets for the predicted boxes and target boxes, respectively. The weighted regression loss can adaptively pay more attention to the samples with high localization potential rather than good initial spatial alignment, and thus better detection performance would be achieved after the training. It can be seen from <ref type="figure">Fig. 8(a)</ref> that the detectors trained with normal smooth-L 1 loss exhibits a weak correlation between the classification score and localization ability of the detections, which causes the predictions selected by the classification confidence to be unreliable. After training with a matching-sensitive loss function, as shown in <ref type="figure">Fig. 8(b)</ref>, the detection with better localization performance will also achieve higher classification confidence, facilitating the selection of high-quality detection based on the classification score. The above analysis confirms the effectiveness of the matching-sensitive loss. Dynamic anchor selection strategy and matching-sensitive loss can also be employed to the anchor refinement stage, and thus the multitask loss for CFC-Net is defined as follows: <ref type="bibr" target="#b12">(13)</ref> where L cls (p, p * ), L ref (t r , t * ), and L reg (t, t * ) are the classification loss, anchor refinement loss, and regression loss, respectively. t r , t denotes the predicted offsets of refined anchors and detection boxes, respectively. t * represents the offsets of GT boxes. These loss items are balanced via parameters ? 1 and ? 2 (we set ? 1 = ? 2 = 0.5 in our experiments).</p><formula xml:id="formula_11">L = L cls (p, p * ) + ? 1 L ref (t r , t * ) + ? 2 L reg (t, t * ) ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>Experiments are conducted on three public remote sensing datasets: HRSC2016, DOTA, and UCAS-AOD. The groundtruth boxes in these datasets are annotated with oriented bounding box.</p><p>HRSC2016 <ref type="bibr" target="#b41">[42]</ref> is a high resolution remote sensing ship detection dataset with a total of 1061 images. The image sizes range from 300?300 to 1500?900. The entire dataset is divided into training set, validation set, and test set, including 436, 181, and 444 images, respectively. The images are resized to two scales of 416?416 and 800?800 in our experiments.</p><p>DOTA <ref type="bibr" target="#b42">[43]</ref> is the largest publicly available dataset for oriented object detection in remote sensing images. DOTA includes 2806 aerial images with 188,282 annotated instances. There are 15 categories in total, including plane (PL), baseball diamond (BD), bridge (BR), ground track field (GTF), small vehicle (SV), large vehicle (LV), ship (SH), tennis court (TC), basketball court (BC), storage tank (ST), soccer ball field (SBF), roundabout (RA), harbor (HA), swimming pool (SP) and helicopter (HC). Note that images in DOTA are too large, we crop the original images into 800?800 patches with the stride 200 for training and testing.</p><p>UCAS-AOD <ref type="bibr" target="#b43">[44]</ref> is an aerial aircraft and car detection dataset, which contains 1510 images collected from Google Earth. It includes 1000 planes images and 510 cars images in total. Since there is no official division of this dataset. we randomly divide it into training set, validation set, and test set as 5:2:3. All images in UCAS-AOD are resized to 800?800 in the experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>The backbone of our CFC-Net is ResNet-50 <ref type="bibr" target="#b44">[45]</ref>. The model is pre-trained on the ImageNet and fine-tuned on remote sensing image datasets. We utilize the feature pyramid of P 3 , P 4 , P 5 , P 6 , P 7 to detect multi-scale objects. For each position of the feature map, only one anchor is set to regress the nearby objects. We use random flipping, rotation, and HSV jittering for data augmentation. We take matching degree threshold of positives to be 0.4 for the refinement stage, while 0.6 for detection stage for high-quality detections.</p><p>The mean Average Precision (mAP) defined in PASCAL VOC object detection challenge <ref type="bibr" target="#b45">[46]</ref> is used as the evaluation metric for all experiments. For a fair comparison with other methods, HRSC2016 dataset and UCAS-AOD dataset use the mAP metric defined in PASCAL VOC 2007 challenge, while DOTA adopts PASCAL VOC 2012 definition. Our ablation studies are conducted on the HRSC2016 dataset since remote sensing ships often have a large aspect ratio and scale variation, which are major challenges for object detection in optical remote sensing images. In the ablation studies, all images are scaled to 416?416 without data augmentation.</p><p>We train the model with the batch size set to 8 on RTX 2080Ti GPU. The network is trained with Adam optimizer. The learning rate is set to 1e-4 and is divided by 10 at each decay step. The total iterations of HRSC2016, UCAS-AOD, and DOTA are 10k, 5k, and 40k, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Evaluation of different components:</head><p>We conduct a component-wise experiment on HRSC2016 dataset to verify the contribution of the proposed components. The experimental results are shown in <ref type="table" target="#tab_0">Table I</ref>. Since only one anchor is preset, it is difficult to capture the critical features required to identify the object, so the baseline model only achieves the mAP of 70.5%. Using the PAM, the detection performance is increased by 5.7%, indicating that the PAM module effectively constructs more powerful feature representations, so that even one preset anchor can make good use of critical features to <ref type="figure">Fig. 9</ref>. Visualization results of critical features for classification and regression tasks. The left side of each pair of images shows the distribution of critical features for classification task, while the right side for regression task.</p><p>achieve accurate detection. The performance of the model is improved by 8.2% using DAL, due to its ability of selecting high-quality anchors with good critical feature alignment in the learning process. The simultaneous use of DAL and PAM achieves a mAP of 83.8%, indicating that the two methods do not conflict and can effectively improve the detection performance. The proposed R-ARM refines the horizontal anchors to obtain high-quality rotated anchors. It further improves the performance by 2.5%. Finally, CFC-Net reaches the mAP of 86.3% with an increase of 15.8% compared with the naive model, proving the effectiveness of our framework.</p><p>2) Evaluation of PAM: To verify the effect of the proposed PAM, we conduct some comparative experiments on HRSC2016 dataset. The results are shown in <ref type="table" target="#tab_0">Table II</ref>. By using dual FPN to extract independent features for classification and regression branches, the detection performance is improved by 1.6% compared with the baseline model. Although dual FPN seperates features for different tasks and improves performance, the features are not fully utilized. When we adopt the attention mechanism based on dual FPN, a further improvement of 2.8% is achieved. It indicates that the attention mechanism enables the features of different branches to better respond to the discriminative parts of the objects. Through the processing of the polarization function, the discriminative parts of the critical classification features are strengthened, while the high response regions in the critical regression features are suppressed to find more clues to further improve localization results. The improvement of 1.3% based on the attention-based model confirms our viewpoint. These experiments prove that the proposed components of PAM can effectively improve the detection performance.</p><p>Some visualization results are shown in <ref type="figure">Fig. 9</ref>. It can be seen that the heatmap induced by PAM accurately responds to the area of task-sensitive critical features. The discriminative areas required for classification are often concentrated in the local part of objects, such as the stern and bow of ships. Meanwhile, the clues required for regression are more likely to be distributed on the edge of the objects or the contextual information.</p><p>3) Evaluation of DAL: We conduct component-wise experiments to verify the contribution of the DAL. The experimental results are shown in <ref type="table" target="#tab_0">Table III</ref>, in which input IoU, output IoU and regression uncertainty are denoted by the three terms in Eq. <ref type="formula" target="#formula_5">(7)</ref>, respectively. For the variants with output IoU, ? is set to 0.8 for stable training, and the detection performance slightly increases from 70.5% to 71.3%. It indicates that using output IoU alone is insignificant for training sample selection.</p><p>With the suppression of regression uncertainty, the prior space alignment and posterior critical feature alignment can work together to dramatically improve the performance by 5.7% compared with the baseline. Furthermore, matching degree guided loss function effectively distinguishes anchors with differential localization capability, and thus model using the matching sensitivity loss function achieves the mAP of 78.7%, 4) Evaluation of R-ARM: Based on DAL and PAM, we further conduct experiments to verify the effect of the proposed R-ARM and explore the influence of the number of refinement stages. For the model without R-ARM, we set the matching degree threshold of positives to 0.4. For the one-stage refinement model, the thresholds of the refinement stage and the detection stage are set to 0.4 and 0.6, respectively. The thresholds are set to 0.4, 0.6, and 0.8 for two-stage refinement module. As shown in <ref type="table" target="#tab_0">Table IV</ref>, with one-stage R-ARM, the performance is increased by 2.5%. It can be attributed to the fact that the refined proposals learned from horizontal anchors provide high-quality samples, and these candidates are better aligned with critical features of objects. However, adopting two-stage R-ARM drops the performance by 1.8% compared with the one-stage R-ARM. It may be that as the threshold increases in detection stage, the number of positives that higher than the current matching degree threshold decreases sharply, leading to insufficient positive samples and a serious imbalance of positives and negatives. Thus we use one stage R-ARM in CFC-Net. 5) Hyper-parameters: In order to find suitable hyperparameter settings, we conduct parameter sensitivity experiments, and the results are shown in <ref type="table" target="#tab_2">Table V</ref>. As the ? is reduced appropriately, the effect of feature alignment increases, and the mAP increases. For example, on condition that ? is equal to 4, as ? decreases from 0.9 to 0.5, the mAP increases from 72.1% to 78.7%. It indicates that under the premise of uncertainty suppression, the feature alignment represented by the IoU out is conducive to selecting anchors with high localization capa- bilities. However, when ? is extremely small, the performance drops sharply (like ? = 4), because the anchors selected by the dominant output IoU may contain too many falsepositive samples. In this case, prior space alignment can help alleviate this problem and make anchor selection more stable. In addition, as ? decreases, the ability to suppress disturbance samples is stronger, but it may also suppress the mining of potential positives, resulting in performance degradation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Main Results and Analysis</head><p>1) Results on HRSC2016: HRSC2016 contains lots of remote sensing oriented ships with a large aspect ratio, scales and arbitrary orientations. Our method achieves competitive performances on HRSC2016 dataset. As shown in <ref type="table" target="#tab_0">Table VI</ref>, 'aug' represents using data augmentation, 'ms' denotes multiscale training and testing, and NA is the number of preset anchors at each location of feature maps. The proposed CFC-Net achieves the mAP of 86.3% when input images are rescaled to 416?416 without data augmentation, which is comparable to many previous advanced methods. With data augmentation and the input image resized to 800?800, our method reaches the mAP of 88.6%, which is better than many recent methods. Further using multi-scale training and testing, our method achieves state-of-the-art performance on HRSC2016 dataset among the compared methods, reaching the mAP of 89.7%.  It is worth mentioning that our approach uses only one horizontal anchor at each position of feature maps, but outperforms the frameworks with a large number of anchors. These results show that it is unnecessary to preset a large number of rotated anchors for oriented object detection. Instead, the more important thing is to select high-quality anchors and capture the critical features for object recognization. For instance, the anchors in <ref type="figure" target="#fig_0">Fig. 10</ref> have low IoUs with targets in the images and will be regarded as negatives in most detectors. But they actually have a strong potential for accurate localization. CFC-Net effectively utilizes these anchors to achieve efficient and accurate prediction. Note that our model is a single-stage detector, and the feature maps used are P 3 ? P 7 . Compared with the P 2 ? P 6 for two-stage detectors, the total amount of positions that need to set anchor is fewer, so the inference speed is faster. With the input image resized to 800?800, our model reaches 28 FPS on RTX 2080 Ti GPU.</p><p>2) Results on DOTA: We compare the proposed approach with other state-of-the-art methods on DOTA dataset. As shown in <ref type="table" target="#tab_0">Table VIII</ref>, we achieve the mAP of 73.50%, which reaches the best performance among the compared methods. Some detection results on DOTA are shown in <ref type="figure" target="#fig_0">Fig. 11</ref>. It can be seen from the illustration that even though only one anchor is used, our CFC-Net still accurately detects densely arranged small objects (such as ships, small vehicles, and large vehicles in the third row). In addition, the proposed detector also adapts well to the scale variations and accurately locates objects of different scales. Take the second one (from the left) in the second row for example, the precise detections of both large-scale roundabout and small vehicles at different scales are achieved through the feature pyramid with only one prior anchor at each location. Besides, as shown in the third figure and the fifth figure in the first row, our method can use a few square anchors to detect objects with very large aspect ratios (such as bridges and harbors here), These detections indicate that it is not essential for preset anchors to have a good spatial alignment with the objects, while the key is to effectively identify and capture the critical features of the objects. The utilized matching degree measures the critical feature capturing ability of anchors, and on this basis, the DAL strategy performs a more reasonable selection of training samples to achieve high-quality detection.</p><p>3) Results on UCAS-AOD: Experimental results in <ref type="table" target="#tab_0">Table  VII</ref> show that our CFC-Net achieves the best performance among the compared detectors, reaching the mAP of 89.49%. Note that the original YOLOv3 <ref type="bibr" target="#b27">[28]</ref> and RetinaNet <ref type="bibr" target="#b40">[41]</ref> are proposed for generic object detection, and the objects are annotated with horizontal bounding box. To make a fair comparison, we introduce an additional angle dimension and perform angle prediction to achieve rotation object detection. Our method outperforms the other compared single-stage detectors, and even better than some advanced two-stage detectors. Besides, the detection performance of small vehicles is excellent, which indicates that our method is robust to densely arranged small objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this article, we introduce the concept of critical features and prove its importance for high-performance object detection through experiments and observations. On this basis, a Critical Feature Capturing network (CFC-Net) is proposed to optimize the one-stage detector from three aspects: feature representation, anchor refinement, and training sample selection. Specifically, decoupled classification and regression critical features are extracted through the polarization attention mechanism module based on dual FPN. Next, the rotation anchor refinement is performed on one preset anchor to obtain high-quality rotation anchors, which can be better aligned with critical features. Finally, matching degree is adopted to measure the ability of anchors to capture critical features, so as to select positive candidates with high localization potential. As a result, the inconsistency between classification and regression is alleviated and high-quality detection performance can be achieved. Extensive experiments on three remote sensing datasets verify the effectiveness of the proposed method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Illustration of the role of critical features in classification task. Predicted bounding boxes (green) are regressed from predefined anchor boxes (red). The ground truth classes of (a) and (b) are marked as A and B respectively, while the predicted object categories are all A. Only the anchors that capture the critical features required to identify the object (such as island and bow here) can achieve the correct classification prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>arXiv:2101.06849v2 [cs.CV] 16 Aug 2021 Analysis of the importance of critical features. (a)-(b) Discriminative feature activation map in object detection. (c) The proportion of positive samples with high-quality detections among all positives. (d) The proportion of high-quality detections that regressed from negatives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Framework of the proposed CFC-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Illustration of the PAM module. SA denotes spatial attention, CA represents channel-wise attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Illustration of function of the proposed modules in the detection pipeline. The yellow area represents the center of the high-quality anchors</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Illustration of the R-ARM module. A denotes the number of anchors preset at each position of feature map, which is set to 1 in CFC-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Analysis of the classification and regression capabilities of anchors that use input IoU for label assignment. (a) without MSL (b) with MSL The correlation between the output IoU and classification score with and without MSL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>Detection results on HRSC2016 dataset with our method. The red boxes and green boxes indicate the anchor boxes and detection results respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .</head><label>11</label><figDesc>Visualization of detection results on DOTA dataset with our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I EFFECTS</head><label>I</label><figDesc>OF EACH COMPONENT OF CFC-NET.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Different Variants</cell><cell></cell></row><row><cell>with PAM?</cell><cell>?</cell><cell></cell><cell>?</cell><cell></cell><cell></cell></row><row><cell>with DAL?</cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell></cell></row><row><cell>with R-ARM?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell></row><row><cell>mAP</cell><cell>70.5</cell><cell>76.2</cell><cell>78.7</cell><cell>83.8</cell><cell>86.3</cell></row><row><cell></cell><cell></cell><cell>TABLE II</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">ABLATION STUDY OF THE PROPOSED PAM.</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Different Variants</cell><cell></cell></row><row><cell>+ dual FPN</cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell></cell></row><row><cell>+ attention</cell><cell></cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell></row><row><cell cols="2">+ polarization function</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell></row><row><cell>mAP</cell><cell></cell><cell>70.5</cell><cell>72.1</cell><cell>74.9</cell><cell>76.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE III ABLATION</head><label>III</label><figDesc>STUDY OF DAL.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Different Variants</cell><cell></cell></row><row><cell>with Input IoU?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>with Output IoU?</cell><cell>?</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Uncertainty Supression?</cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell></row><row><cell>Matching Sensitive Loss?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell></row><row><cell>mAP</cell><cell>70.5</cell><cell>71.3</cell><cell>76.2</cell><cell>78.7</cell></row><row><cell cols="2">TABLE IV</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">ABLATION STUDY OF THE PROPOSED R-ARM.</cell><cell></cell></row><row><cell>refinement stages</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell></cell></row><row><cell>mAP</cell><cell>83.8</cell><cell>86.3</cell><cell cols="2">84.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE V ANALYSIS</head><label>V</label><figDesc>OF INFLUENCE OF DIFFERENT HYPERPARAMETERS.</figDesc><table><row><cell>?</cell><cell>?</cell><cell cols="2">mAP ?</cell><cell>?</cell><cell>mAP</cell><cell>?</cell><cell>?</cell><cell>mAP</cell></row><row><cell></cell><cell>0.2</cell><cell>76.2</cell><cell></cell><cell>0.2</cell><cell>69.8</cell><cell></cell><cell>0.2</cell><cell>47.4</cell></row><row><cell></cell><cell>0.3</cell><cell>72.7</cell><cell></cell><cell>0.3</cell><cell>78.5</cell><cell></cell><cell>0.3</cell><cell>76.9</cell></row><row><cell>2</cell><cell>0.5</cell><cell>70.0</cell><cell>3</cell><cell>0.5</cell><cell>74.4</cell><cell>4</cell><cell>0.5</cell><cell>78.7</cell></row><row><cell></cell><cell>0.7</cell><cell>71.7</cell><cell></cell><cell>0.7</cell><cell>69.4</cell><cell></cell><cell>0.7</cell><cell>77.3</cell></row><row><cell></cell><cell>0.9</cell><cell>43.9</cell><cell></cell><cell>0.9</cell><cell>69.1</cell><cell></cell><cell>0.9</cell><cell>72.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE VI</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">COMPARISONS WITH DIFFERENT METHODS ON HRSC2016 DATASET.</cell></row><row><cell></cell><cell>Methods</cell><cell></cell><cell cols="2">Backbone</cell><cell cols="2">Size</cell><cell>NA</cell><cell>mAP</cell></row><row><cell cols="2">Two-stage:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">R 2 CNN [47]</cell><cell cols="2">ResNet101</cell><cell cols="2">800?800</cell><cell>21</cell><cell>73.1</cell></row><row><cell cols="3">RC1&amp;RC2 [42]</cell><cell></cell><cell>VGG16</cell><cell>-</cell><cell></cell><cell>-</cell><cell>75.7</cell></row><row><cell cols="3">RRPN [48]</cell><cell cols="2">ResNet101</cell><cell cols="2">800?800</cell><cell>54</cell><cell>79.1</cell></row><row><cell cols="3">R 2 PN [12]</cell><cell></cell><cell>VGG16</cell><cell>-</cell><cell></cell><cell>24</cell><cell>79.6</cell></row><row><cell cols="3">RoI Trans. [14]</cell><cell cols="2">ResNet101</cell><cell cols="2">512?800</cell><cell>5</cell><cell>86.2</cell></row><row><cell cols="3">Gliding Vertex [49]</cell><cell cols="2">ResNet101</cell><cell cols="2">512?800</cell><cell>5</cell><cell>88.2</cell></row><row><cell cols="2">Single-stage:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>RRD [16]</cell><cell></cell><cell></cell><cell>VGG16</cell><cell cols="2">384?384</cell><cell>13</cell><cell>84.3</cell></row><row><cell cols="3">R 3 Det [31]</cell><cell cols="2">ResNet101</cell><cell cols="2">800?800</cell><cell>21</cell><cell>89.3</cell></row><row><cell cols="3">R-RetinaNet [41]</cell><cell cols="2">ResNet101</cell><cell cols="2">800?800</cell><cell>121</cell><cell>89.2</cell></row><row><cell></cell><cell>CFC-Net</cell><cell></cell><cell cols="2">ResNet50</cell><cell cols="2">416?416</cell><cell>1</cell><cell>86.3</cell></row><row><cell cols="3">CFC-Net (aug)</cell><cell cols="2">ResNet50</cell><cell cols="2">800?800</cell><cell>1</cell><cell>88.6</cell></row><row><cell cols="3">CFC-Net (aug)</cell><cell cols="2">ResNet101</cell><cell cols="2">800?800</cell><cell>1</cell><cell>89.5</cell></row><row><cell cols="3">CFC-Net (aug + ms)</cell><cell cols="2">ResNet101</cell><cell cols="2">800?800</cell><cell>1</cell><cell>89.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE VII PERFORMANCE</head><label>VII</label><figDesc>EVALUATION OF OBB TASK ON DOTA DATASET. 69.12 17.17 63.49 34.20 37.16 36.20 89.19 69.60 58.96 49.40 52.52 46.69 44.80 46.30 52.93 R-DFPN [50] 80.92 65.82 33.77 58.94 55.77 50.94 54.78 90.33 66.34 68.66 48.73 51.76 55.10 51.32 35.88 57.94 R 2 CNN [47] 80.94 65.67 35.34 67.44 59.92 50.91 55.81 90.67 66.92 72.39 55.06 52.23 55.14 53.35 48.22 60.67 RRPN [48] 88.52 71.20 31.66 59.30 51.85 56.19 57.25 90.81 72.84 67.38 56.69 52.84 53.08 51.94 53.58 61.01 ICN [51] 81.36 74.30 47.70 70.32 64.89 67.82 69.98 90.76 79.06 78.20 53.64 62.90 67.02 64.17 50.23 68.16 RoI Trans. [14] 88.64 78.52 43.44 75.92 68.81 73.68 83.59 90.74 77.27 81.46 58.39 53.54 62.83 58.93 47.67 69.56 CAD-Net [32] 87.80 82.40 49.40 73.50 71.10 63.50 76.70 90.90 79.20 73.30 48.40 60.90 62.00 67.00 62.20 69.90 DRN [52] 88.91 80.22 43.52 63.35 73.48 70.69 84.94 90.14 83.85 84.11 50.12 58.41 67.62 68.60 52.50 70.70 O 2 -DNet [53] 89.31 82.14 47.33 61.21 71.32 74.03 78.62 90.76 82.23 81.36 60.93 60.17 58.21 66.98 61.03 71.04 R 3 Det [31] 89.54 81.99 48.46 62.52 70.48 74.29 77.54 90.80 81.39 83.54 61.97 59.82 65.44 67.46 60.05 71.69 SCRDet [54] 89.98 80.65 52.09 68.36 68.36 60.32 72.41 90.85 87.94 86.86 65.02 66.68 66.25 68.24 65.21 72.61 CFC-Net (ours) 89.08 80.41 52.41 70.02 76.28 78.11 87.21 90.89 84.47 85.64 60.51 61.52 67.82 68.02 50.09 73.50</figDesc><table><row><cell>Methods</cell><cell>PL</cell><cell>BD</cell><cell>BR</cell><cell>GTF</cell><cell>SV</cell><cell>LV</cell><cell>SH</cell><cell>TC</cell><cell>BC</cell><cell>ST</cell><cell>SBF</cell><cell>RA</cell><cell>HA</cell><cell>SP</cell><cell>HC</cell><cell>mAP</cell></row><row><cell>FR-O [43]</cell><cell>79.09</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VIII DETECTION</head><label>VIII</label><figDesc>RESULTS ON UCAS-AOD DATASET.</figDesc><table><row><cell>Methods</cell><cell>car</cell><cell>airplane</cell><cell>mAP</cell></row><row><cell>YOLOv3 [28]</cell><cell>74.63</cell><cell>89.52</cell><cell>82.08</cell></row><row><cell>R-RetinaNet [41]</cell><cell>84.64</cell><cell>90.51</cell><cell>87.57</cell></row><row><cell>FR-O [43]</cell><cell>86.87</cell><cell>89.86</cell><cell>88.36</cell></row><row><cell>RoI Trans. [14]</cell><cell>88.02</cell><cell>90.02</cell><cell>89.02</cell></row><row><cell>CFC-Net</cell><cell>89.29</cell><cell>88.69</cell><cell>89.49</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">M s (F) = ? c 3?3 (cat((c 3?3 , c 1?3 d , c 3?1 d , c 3?3 d )(F))) , (3) in which c 3?3 represents convolution of a 3 ? 3 filter. c 1?3 d , c 3?1 d , c 3?3 drespectively denote dilated convolution of different kernel sizes. cat denotes concatenation of features. Dilated convolution is adopted here to expand the receptive field of the convolution kernels. At the same time, convolution</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic target detection in high-resolution remote sensing images using a contour-based spatial model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="886" to="890" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient, simultaneous detection of multi-class geospatial targets based on visual saliency modeling and discriminative learning of sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="37" to="48" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Object detection in optical remote sensing images based on weakly supervised learning and high-level feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3325" to="3337" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A novel hierarchical method of ship detection from spaceborne optical image based on shape and texture features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on geoscience and remote sensing</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3446" to="3456" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Classification-based vehicle detection in high-resolution satellite images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Eikvil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Aurdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="72" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Road detection and centerline extraction via deep recurrent convolutional neural network UU-Net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="7209" to="7220" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vehicle detection in remote sensing images leveraging on simultaneous super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="676" to="680" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-layer abstraction saliency for airport detection in SAR images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="9820" to="9831" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inshore ship detection based on convolutional neural network in optical satellite images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4005" to="4015" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">HSF-Net: Multiscale deep feature embedding for ship detection in optical remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7147" to="7161" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented ship detection framework in optical remote-sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="937" to="941" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Toward arbitrary-oriented ship detection with rotated region proposal and discrimination networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1745" to="1749" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rotated region based CNN for ship detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="900" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning RoI transformer for oriented object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2849" to="2858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A novel CNNbased method for accurate ship detection in HR optical remote sensing images via rotated bounding box</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rotation-sensitive regression for oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5909" to="5918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rotationaware and multi-scale convolutional neural network for object detection in remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="page" from="294" to="308" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning rotation-invariant convolutional neural networks for object detection in VHR optical remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7405" to="7415" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Oriented response networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="519" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-scale object detection in remote sensing imagery with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS journal of photogrammetry and remote sensing</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="3" to="22" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fmssd: Feature-merged singleshot detection for multiscale objects in large-scale remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3377" to="3390" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A novel inshore ship detection via ship head classification and body boundary determination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE geoscience and remote sensing letters</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1920" to="1924" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">YOLO9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">YOLOv3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Weakly supervised learning based on coupled convolutional neural networks for aircraft detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="5553" to="5563" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">R3det: Refined single-stage detector with feature refinement for rotating object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05612</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">CAD-Net: A context-aware detection network for objects in remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Revisiting the sibling head in object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11" to="563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cascaded detection framework based on a novel backbone network and feature fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3480" to="3491" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bounding box regression with uncertainty for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2888" to="2897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Gaussian yolov3: An accurate and fast object detector using localization uncertainty for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="502" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Acquisition of localization confidence for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="784" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Towards safe autonomous driving: Capture uncertainty in the deep neural network for lidar 3d vehicle detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 21st International Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3266" to="3273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Dynamic anchor learning for arbitrary-oriented object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.04150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A high resolution optical satellite image dataset for ship recognition and some new baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Pattern Recognition Applications and Methods</title>
		<meeting>the International Conference on Pattern Recognition Applications and Methods</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="324" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">DOTA: A large-scale dataset for object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3974" to="3983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Orientation robust object detection in aerial images using deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3735" to="3739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">R2CNN: Rotational region cnn for orientation robust scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09579</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented scene text detection via rotation proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3111" to="3122" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Gliding vertex on the horizontal bounding box for multi-oriented object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Automatic ship detection in remote sensing images from google earth of complex scenes based on multiscale rotation dense feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">132</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Towards multi-class object detection in unconstrained remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Azimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bahmanyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>K?rner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Reinartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="150" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dynamic refinement network for oriented and densely packed object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Oriented objects as pairs of middle lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">169</biblScope>
			<biblScope unit="page" from="268" to="279" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Scrdet: Towards more robust detection for small, cluttered and rotated objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8232" to="8241" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
