<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visformer: The Vision-friendly Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengsu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Niu</surname></persName>
							<email>niujianwei@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Hangzhou Innovation Institute of Beihang University</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">Zhengzhou University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuefeng</forename><surname>Liu</surname></persName>
							<email>liuxuefeng@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
							<email>longhuiwei@pku.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Xidian University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Visformer: The Vision-friendly Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The past year has witnessed the rapid development of applying the Transformer module to vision problems. While some researchers have demonstrated that Transformerbased models enjoy a favorable ability of fitting data, there are still growing number of evidences showing that these models suffer over-fitting especially when the training data is limited. This paper offers an empirical study by performing step-by-step operations to gradually transit a Transformer-based model to a convolution-based model. The results we obtain during the transition process deliver useful messages for improving visual recognition. Based on these observations, we propose a new architecture named Visformer, which is abbreviated from the 'Vision-friendly Transformer'. With the same computational complexity, Visformer outperforms both the Transformer-based and convolution-based models in terms of ImageNet classification accuracy, and the advantage becomes more significant when the model complexity is lower or the training set is smaller. The code is available at https://github. com/danczs/Visformer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the past decade, convolution used to play a central role in the deep learning models <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b14">15]</ref> for visual recognition. This situation starts to change when the Transformer <ref type="bibr" target="#b34">[35]</ref>, a module that originates from natural language processing <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b24">25]</ref>, is transplanted to the vision scenarios. It was shown in the ViT model <ref type="bibr" target="#b13">[14]</ref> that an image can be partitioned into a grid of patches and the Transformer is directly applied upon the grid as if each patch is a visual word. ViT requires a large amount of training data (e.g., the ImageNet-21K <ref type="bibr" target="#b11">[12]</ref> or the JFT-300M dataset), arguably because the Transformer is equipped with long-range attention and interaction and thus is prone to over-fitting. The follow-up efforts <ref type="bibr" target="#b33">[34]</ref> improved ViT to some extent, but these models still perform badly especially under limited training data or moderate data augmentation compared with convolution-based models. On the other hand, vision Transformers can achieve much better performance than convolution-based models when trained with large amount of data. Namely, vision Transformers have higher 'upper-bound' while convolution-based models are better in 'lower-bound'. Both upper-bound and lower-bound are important properties for neural networks. Upper-bound is the potential to achieve higher performance and lower-bound enables networks to perform better when trained with limited data or scaled to different complexity.</p><p>Based on the observation of lower-bound and upperbound on Transformer-based and convolution-based networks, the main goal of this paper is to identify the reasons behind the difference, by which we can design networks with higher lower-bound and upper-bound. The gap between Transformer-based and convolution-based networks can be revealed with two different training settings on Ima-geNet. The first one is the base setting. It is the standard setting for convolution-based models, i.e., the training schedule is shorter and the data augmentation only contains basic operators such as random-size cropping <ref type="bibr" target="#b31">[32]</ref> and flipping. The performance under this setting is called base performance in this paper. The other one is the training setting used in <ref type="bibr" target="#b33">[34]</ref>. It is carefully tuned for Transformer-based models, i.e., the training schedule is longer and the data augmentation is stronger (e.g., RandAugment <ref type="bibr" target="#b10">[11]</ref>, Cut-Mix <ref type="bibr" target="#b40">[41]</ref>, etc., have been added). We use the elite performance to refer to the accuracy produced by it.</p><p>We take DeiT-S <ref type="bibr" target="#b33">[34]</ref> and ResNet-50 <ref type="bibr" target="#b14">[15]</ref> as the examples of Transformer-based and convolution-based models. As shown in <ref type="table" target="#tab_0">Table 1</ref>, Deit-S and ResNet-50 employ comparable FLOPs and parameters. However, they behave very differently trained on the full data under these two settings. Deit-S has higher elite performance, but changing the setting from elite to base can cause a 10%+ accuracy drop for DeiT-S. ResNet-50 performs much better under the base setting, yet the improvement for the elite setting is merely 1.3%. This motivates us to study the difference between these models. With these two settings, we can roughly estimate the lower-bound and upper-bound of the models. The methodology we use is to perform step-by-step operations to gradually transit one model into another, by which we can identify the properties of modules and designs in these two networks. The entire transition process, taking a total of 8 steps, is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Specifically, from DeiT-S to ResNet-50, one should (i) use global average pooling (not the classification token), (ii) introduce step-wise patch embeddings (not large patch flattening), (iii) adopt the stage-wise backbone design, (iv) use batch normalization <ref type="bibr" target="#b19">[20]</ref> (not layer normalization <ref type="bibr" target="#b0">[1]</ref>), (v) leverage 3 ? 3 convolutions, (vi) discard the position embedding scheme, (vii) replace self-attention with convolution, and finally (viii) adjust the network shape (e.g., depth, width, etc.). After a thorough analysis on the reasons behind the results, we absorb all the factors that are helpful to visual recognition and derive the Visformer, i.e., the Visionfriendly Transformer.</p><p>Evaluated on ImageNet classification, Visformer claims better performance than the competitors, DeiT and ResNet, as shown in <ref type="table" target="#tab_0">Table 1</ref>. With the elite setting, the Visformer-S model outperforms DeiT-S and ResNet-50 by 2.12% and 3.46%, respectively, under a comparable model complexity. Different from Deit-S, Visformer-S also survives two extra challenges, namely, when the model is trained with 10% labels (images) and 10% classes. Visformer-S even performs better than ResNet-50, which reveals the high lower-bound of Visformer-S. Additionally, for tiny models, Visformer-Ti significantly outperforms Deit-Ti by more than 6%.</p><p>The contribution of this paper is three-fold. First, for the first time, we introduce the lower-bound and upperbound to investigate the performance of Transformer-based vision models. Second, we close the gap between the Transformer-based and convolution-based models by a gradual transition process and thus identify the properties of the designs in the Transformer-based and convolution-based models. Third, we propose the Visformer as the final model that achieves satisfying lower-bound and upper-bound and enjoys good scalability at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Image classification is a fundamental task in computer vision. In the deep learning era, the most popular method is to use deep neural networks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b14">15]</ref>. One of the fundamental units to build such networks is convolution, where a number of convolutional kernels are used to capture repeatable local patterns in the input image and intermediate data.</p><p>To reduce the computational costs as well as alleviate the risk of over-fitting, it was believed that the convolutional kernels should be of a small size, e.g., 3 ? 3. However, this brings the difficulty for faraway contexts in the image to communicate with each other -this is partly the reason that the number of layers has been increasing. Despite stacking more and more layers, researchers consider another path which is to use attention-based approaches to ease the propagation of visual information.</p><p>Since Transformers achieved remarkable success in natural language processing (NLP) <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b24">25</ref>], many efforts have been made to introduce Transformers to vision tasks. These works mainly fall into two categories. The first category consists of pure attention models <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36]</ref>. These models usually only utilize self-attention and attempt to build vision models without convolutions. However, it is computationally expensive to relate all pixels with self-attention for realistic full-sized images. Thus, there has some interest in forcing self-attention to only concentrate on the pixels in local neighborhoods (e.g., SASA <ref type="bibr" target="#b26">[27]</ref>, LR-Net <ref type="bibr" target="#b17">[18]</ref>, SANet <ref type="bibr" target="#b42">[43]</ref>). These methods replace convolutions with local self-attentions to learn local relations and achieve promising results. However, it requires complex engineering to efficiently apply self-attention to every local region in an image. Another way to solve the complexity problem is to apply self-attention to reduced resolution. These methods either reduce the resolution and color space first <ref type="bibr" target="#b9">[10]</ref> or regard image patches rather pixels as tokens (i.e., words) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b33">34]</ref>. However, resolution reduction and patch flattening usually make it more difficult to utilize the local prior in natural images. Thus, these methods usually obtain suboptimal results <ref type="bibr" target="#b9">[10]</ref> or require huge dataset <ref type="bibr" target="#b13">[14]</ref> and heavy augmentation <ref type="bibr" target="#b33">[34]</ref>.</p><p>The second category contains the networks built with not only self-attentions but also convolutions. Self-attention was first introduced to CNNs by non-local neural networks <ref type="bibr" target="#b36">[37]</ref>. These networks aim to capture global dependencies in images and videos. Note that non-local neural networks are inspired by the classical non-local method in vision tasks <ref type="bibr" target="#b4">[5]</ref> and unlike those in Transformers, the self-attentions in non-local networks are usually not equipped with multi-heads and position embedding <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b22">23]</ref>. Afterwards, Transformers achieve remarkable success in NLP tasks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref> and, therefore, self-attentions that inherits NLP settings (e.g., multi-heads, position encodings, classification token, etc.) are combined with convolutions to improve vision tasks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b1">2]</ref>. A common combination is to utilize convolutions first and apply self-attention afterwards <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b29">30]</ref>. <ref type="bibr" target="#b13">[14]</ref> builds hybrids of self-attention and convolution by adding a ResNet backbone before Transformers. Besides utilizing convolution in early layers, Bot-Net <ref type="bibr" target="#b29">[30]</ref> designs bottleneck cells for self-attention. Additionally, self-attention has been used in many downstream vision tasks (detection <ref type="bibr" target="#b6">[7]</ref>, segmentation <ref type="bibr" target="#b8">[9]</ref>) and low vision tasks <ref type="bibr" target="#b7">[8]</ref>. These methods mostly utilize both self-attentions and convolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Transformer-based and convolution-based visual recognition models</head><p>Recognition is the fundamental task in computer vision. This work mainly considers image classification, where the input image is propagated through a deep network to derive the output class label. Most deep networks are designed in a hierarchical manner and composed of a series of layers.</p><p>We consider two popular layers named convolution and Transformer. Convolution originates from the intuition to capture local patterns which are believed more repeatable than global patterns. It uses a number of learnable kernels to compute the responses of the input to different patterns, for which a sliding window is moved along both axes of the input data and the inner-product between the data and kernel is calculated. In this paper, we constrain our study in the scope of residual blocks, a combination of 2 or 3 convolutional layers and a skip-connection. Non-linearities such as activation and normalization are inserted between the neighboring convolutional layers.</p><p>On the other hand, Transformer originates from natural language processing and aims to frequently formulate the relationship between any two elements (called tokens) even when they are far from each other. This is achieved by generating three features for each token, named the query, key, and value, respectively. Then, the response of each token is calculated as a weighted sum over all the values, where the weights are determined by the similarity between its query and the corresponding keys. This is often referred to as multi-head self-attention (MHSA), followed by other operations including normalization and linear mapping.</p><p>Throughout the remaining part, we consider DeiT-S <ref type="bibr" target="#b33">[34]</ref> and ResNet-50 <ref type="bibr" target="#b14">[15]</ref> as the representative of Transformerbased and convolution-based models, respectively. Besides the basic building block, there are also differences in de-sign, e.g., ResNet-50 has a few down-sampling layers that partition the model into stages, but the number of tokens remains unchanged throughout DeiT-S. The impact of these details will be elaborated in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Settings: The base and elite performance</head><p>Although DeiT-S reports a 80.1% accuracy which is higher than 78.7% of ResNet-50, we notice that DeiT-S has changed the training strategy significantly, e.g., the number of epochs is enlarged by more than 3? and the data augmentation becomes much stronger. Interestingly, DeiT-S seems to heavily rely on the carefully-tuned training strategy, and other Transformer-based models including ViT <ref type="bibr" target="#b13">[14]</ref> and PIT <ref type="bibr" target="#b7">[8]</ref> also reported their dependency on other factors, e.g., a large-scale training set. In what follows, we provide a comprehensive study on this phenomenon.</p><p>We evaluate all classification models on the ImageNet dataset <ref type="bibr" target="#b27">[28]</ref> which has 1K classes, 1.28M training images and 50K testing images. Each class has roughly the same number of training images. This is one of the most popular datasets for visual recognition.</p><p>There are two settings to optimize each recognition model. The first one is named the base setting which is widely adopted by convolution-based networks. Specifically, the model is trained for 90 epochs with the SGD optimizer. The learning rate starts with 0.2 for batch size 512 and gradually decays to 0.00001 following the cosine annealing function. A moderate data augmentation strategy with random-size cropping <ref type="bibr" target="#b31">[32]</ref> and flipping is used. The second one is named the elite setting which has been verified effective to improve the Transformer-based models. The Adamw optimizer with an initial learning rate of 0.0005 for batch size 512 is used. The data augmentation and regularization strategy is made much stronger to avoid overfitting, for which intensive operations including RandAugment <ref type="bibr" target="#b10">[11]</ref>, Mixup <ref type="bibr" target="#b41">[42]</ref>, CutMix <ref type="bibr" target="#b40">[41]</ref>, Random Erasing <ref type="bibr" target="#b43">[44]</ref>, Repeated Augmentation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref> and Stochastic Depth <ref type="bibr" target="#b18">[19]</ref> are used. Correspondingly, the training lasts 300 epochs, much longer than that of the base setting.</p><p>Throughout the remaining part of this paper, we refer to the classification accuracy under the base and elite settings as base performance and elite performance, respectively. We expect the numbers to provide complementary views for us to understand the studied models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The transition from DeiT-S to ResNet-50</head><p>This subsection displays a step-by-step process in which we gradually transit a model from DeiT-S to ResNet-50. There are eight steps in total. The key steps are illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, and the results, including the base and elite performance and the model statistics, are summarized in <ref type="table" target="#tab_2">Table 2</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Using global average pooling to replace the classification token</head><p>The first step of the transition is to remove the classification token and add global average pooling to the Transformerbased models. Unlike the convolution-based models, Transformers usually add a classification token to the inputs and utilize the corresponding output token to perform classification, which is inherited from NLP tasks <ref type="bibr" target="#b12">[13]</ref>. As a contrast, the classification features in convolution-based models are obtained by conducting global average pooling in the space dimension. By removing the classification token, the Transformer can be equivalent translated to the convolutional version as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Specifically, the patch embedding operation is equivalent to a convolution whose kernel size and stride is the patch size <ref type="bibr" target="#b13">[14]</ref>. The shape of the intermediate features can be naturally converted from a sequence of tokens (i.e., words) to a bundle feature maps and the tokens become the vector in channel dimension (illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. The linear layers in MHSA and MLP blocks are equivalent to 1 ? 1 convolutions.</p><p>The performance of the obtained network (Net1) is shown in <ref type="table" target="#tab_2">Table 2</ref>. As can be seen, this transition can substantially improve the base performance. Our further experiments show that adding global pooling itself can improve the base performance from 64.17% to 69.44%. In other words, the global average pooling operation which is widely used in convolution-based models since NIN <ref type="bibr" target="#b23">[24]</ref>, enables the network to learn more efficiently under moderate augmentation. Furthermore, this transition can slightly improve the elite performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Replacing patch flattening with step-wise patch embedding</head><p>DeiT and ViT models directly encode the image pixels with a patch embedding layer which is equivalent to a convolution with large kernel size and stride (e.g., <ref type="bibr" target="#b15">16</ref>). This operation flattens the image patches to a sequence of tokens so that Transformers can handle images. However, patch flattening impairs the position information within each patch and makes it more difficult to extract the patterns within patches. To solve this problem, existing methods usually attach a preprocessing module before patch embedding. The preprocessing module can be a feature extraction convnet <ref type="bibr" target="#b13">[14]</ref> or a specially designed Transformer <ref type="bibr" target="#b39">[40]</ref>. We found that there is a rather simple solution, which is factorizing the large patch embedding to step-wise small patch embeddings. Specifically, We first add the stem layer in ResNet to the Transformer, which is a 7 ? 7 convolution layer with a stride of two. The stem layer can be seen as a 2 ? 2 patching embedding operation with pixel overlap (i.e., 7 ? 7 kernel size). Since the patch size in the original DeiT model is 16, we still need to embed 8 ? 8 patches after the stem. We further factorize the 8 ? 8 patch embedding to a 4 ? 4 embedding and a 2 ? 2 embedding, which are 4 ? 4 and 2 ? 2 convolution layers with stride 4 and 2 in the perspective of convolution. Additionally, we add an extra 2?2 convolution to further upgrade the patch size from 16? 16 to 32 ? 32 before classification. These patch embedding layers can also be seen as the down-sampling layers and we double the channel numbers after embedding following the practice in convolution-based models. By utilizing step-wise embeddings, the position prior within patches is encoded into features. As a result, the model can learn patterns more efficiently. As can be seen in <ref type="table" target="#tab_2">Table 2</ref>, this transition can significantly improve the base performance and elite performance of the network. It indicates that step-wise embedding is a better choice than larger patch embedding in Transformer-based models. Additionally, this transition is computationally efficient and only introduces about 4% extra FLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Stage-wise design</head><p>In this section, we split networks into stages like ResNets. The blocks in the same stage share the same feature resolution. Since step-wise embeddings in the last transition have split the network into different stages, the transition in this section is to reassign the blocks to different stages as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. However, unlike convolution blocks, the complexity of self-attention blocks increases by O N 4 with respect to the feature size. Thus we only insert blocks to the 8 ? 8, 16 ? 16 and 32 ? 32 patch embedding stages, which correspond to 28 ? 28, 14 ? 14 and 7 ? 7 feature resolutions respectively for 224 ? 224 inputs. Additionally, we halve the head dimension and feature dimension before self-attention in 28 ? 28 stage to ensure that the blocks in different stages utilize similar FLOPs.</p><p>This transition leads to interesting results. The base performance is further improved. It is conjectured that the stage-wise design leverages the image local priors and thus can perform better under moderate augmentation. However, the elite performance of the network decreases markedly.</p><p>To study reasons, we conduct ablation experiments and find that self-attention does not work well in very large resolutions. We conjecture that large resolution contains too many tokens and it is much more difficult for self-attention to learn relations among them. We will detail it in section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Replacing LayerNorm with BatchNorm</head><p>Transformer-based models usually normalize the features with LayerNorm <ref type="bibr" target="#b0">[1]</ref>, which is inherited from NLP tasks <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b12">13]</ref>. As a contrast, convolution-based models like ResNets usually utilize BatchNorm <ref type="bibr" target="#b19">[20]</ref> to stabilize the training process. LayerNorm is independent of batch size and more friendly for specific tasks compared with BatchNorm, while BatchNorm usually can achieve better performance given appropriate batch size <ref type="bibr" target="#b37">[38]</ref>. We replace all the LayerNorm layers with BatchNorm layers and the results show that BatchNorm performs better than LayerNorm. It can improve both the base performance and elite performance of the network.</p><p>In addition, we also try to add BatchNorm to Net2 to further improve the elite performance. However, this Net2-BN network suffers from convergence problems. This may explain why BatchNorm is not widely used in the pure selfattention models. But for our mixed model, BatchNorm is a reliable method to advance performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.5">Introducing 3 ? 3 convolutions</head><p>Since the tokens of the network are present as feature maps, it is natural to introduce convolutions with kernel sizes larger than 1 ? 1. The specific meaning of large kernel convolution is illustrated at the bottom right of <ref type="figure" target="#fig_0">Figure 1</ref>. When global self-attentions attempt to build the relations among all the tokens (i.e., pixels), convolutions focus on relating the tokens within local neighborhoods. We chose to insert 3 ? 3 convolutions between the 1 ? 1 convolutions in feedforward blocks, which transforms the MLP blocks into bottleneck blocks as exhibited at the top right of <ref type="figure" target="#fig_0">Figure 1</ref>. Note that the channel numbers of the 3 ? 3 convolution layers are tuned to ensure that the FLOPs of the feed-forward blocks are nearly unchanged. The obtained bottleneck blocks are similar to the bottleneck blocks in ResNet-50, although they have different bottleneck ratios (i.e., the factor of reducing the channel numbers before the 3 ? 3 convolution). We replace the MLP blocks with bottleneck blocks in all three stages.</p><p>Not surprisingly, 3 ? 3 convolutions which can leverage the local priors in images further improve the network base performance. The base performance (77.37%) becomes comparable with ResNet-50 (77.43%). However, the elite performance decreases by 0.82%. We conduct more experiments to study the reasons. Instead of adding 3 ? 3 convolutions to all stages, we insert 3 ? 3 convolutions to different stages separately. We observe that 3 ? 3 convolutions only work well on the high-resolution features. We conjecture that leveraging local relations is important for the highresolution features in natural images. For the low-resolution features, however, local convolutions become unimportant when equipped with global self-attention. We will detail it in section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.6">Removing position embedding</head><p>In Transformer-based models, position embedding is proposed to encode the position information inter tokens. In the transition network, we utilize learnable position embedding as in <ref type="bibr" target="#b12">[13]</ref> and add them to features after patch embeddings. To approaching ResNet-50, position embedding should be removed.</p><p>The results are exhibited in <ref type="table" target="#tab_2">Table 2</ref>. The base performance is almost unchanged and the elite performance declines slightly (0.29%). As a comparison, We test to remove the position embedding of DeiT-S and elite performance decreases significantly by 3.95%. It reveals that position embedding is less important in the transition model than that in the pure Transformer-based models. It is because that the position prior inter tokens is preserved by the feature maps and convolutions with spatial kernels can encode and leverage it. Consequently, the harm of removing position embedding is remarkably reduced in the transition network. It also explains why convolution-based models do not need position embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.7">Replacing self-attention with feed-forward</head><p>In this section, we remove the self-attention blocks in each stage and utilize a feed-forward layer instead, so that the network becomes a pure convolution-based network. To keep the FLOPs unchanged, several bottleneck blocks are added to each stage. After the replacement, the obtained network consists of bottleneck blocks like ResNet-50.</p><p>The performance of the obtained network (Net7) is shown in <ref type="table" target="#tab_2">Table 2</ref>. The pure convolution-based network performs much worse both in base performance and elite performance. It indicates that self-attentions do drive neural networks to higher elite performance and is not responsible for the poor base performance in ViT or DeiT. It is possible to design a self-attention network with high base performance and elite performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.8">Adjusting the shape of network</head><p>There are still many differences between Net7 and ResNet-50. First, the shape of Net7 is different from ResNet-50. Their depths, widths, bottleneck ratios and block numbers in network stages are different. Second, they normalize the features in different positions. Net7 only normalizes input features in a block, while ResNet-50 normalizes features after each convolutional layer. Third, ResNet-50 down-samples the features with bottleneck blocks but Net7 utilizes a single convolution layer (i.e., patch embedding layer). In addition, Net7 employs a few more FLOPs. Nevertheless, both these two networks are convolution-based networks. The performance gap between these two networks can be attributed to architecture design strategy.</p><p>As shown in <ref type="table" target="#tab_2">Table 2</ref>, the base performance is improved after transition. It demonstrates that ResNet-50 has better network architecture and can perform better with fewer FLOPs. However, ResNet-50 obtains worse elite performance. It indicates that the inconsistencies between base performance and elite performance exist not only in selfattention models but also in pure convolution-based networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Summary: the Visformer model</head><p>We aim to build a network with high base performance and elite performance. The transition study has shown that there are some inconsistencies between base performance and elite performance. The first problem is the stage-wise design, which increases the base performance but decreases the elite performance. To study the reasons, we replace the self-attention blocks with bottleneck blocks in each stage separately for Net5, by which we can estimate the importance of self-attention in different stages. The results are shown in <ref type="table" target="#tab_3">Table 3</ref>. The replacement of self-attention in all three stages reduces both the base performance and the elite performance. There is a trend that self-attentions in lower resolutions play more important roles than those in higher resolutions. Additionally, replacing the self-attentions in the first stage almost has no effect on the network performance. Larger resolutions contain much more tokens and  we conjecture that it is more difficult for self-attentions to learn relations among them. The second problem is adding 3 ? 3 convolutions to the feed-forward blocks, which decreases the elite performance by 0.82%. Based on Net4, we replace MLP blocks with bottleneck blocks in each stage separately. As can be seen in <ref type="table">Table 4</ref>, although all stages obtain improvements in base performance, only the first stage benefits from bottleneck blocks in elite performance. The 3 ? 3 convolutions are not necessary for the other two low-resolution stages when self-attentions already have a global view in these positions. On the high-resolution stage, for which self-attentions have difficulty in handling all tokens, the 3 ? 3 convolutions can provide improvement.</p><p>Integrating the observation above, we propose the Visformer as vision-friendly, Transformer-based models. The detailed architectures are shown in <ref type="table" target="#tab_5">Table 5</ref>. Besides the positive transitions, Visformer adopts the stage-wise design for higher base performance. But self-attentions are only utilized in the last two stages, considered that self-attention in the high-resolution stage is relatively inefficient even when the FLOPs are balanced. Visformer employs bottleneck blocks in the first stage and utilizes group 3 ? 3 convolutions in bottleneck blocks inspired by ResNeXt <ref type="bibr" target="#b38">[39]</ref>. We also introduce BatchNorm to patch embedding modules as in CNNs. We name Visformer-S to denote the model that directly comes from DeiT-S. In addition, we can adjust the complexity by changing the output dimensionality of multihead attentions. Here, we shrink the dimensionality by half and derive the Visformer-Ti model, which requires around 1/4 computational costs of the Visformer-S model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluating the Visformer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison to the state-of-the-arts</head><p>We first compare Visformer against DeiT, the direct baseline. Results are summarized in <ref type="table">Table 6</ref>. Using comparable computational costs, the Visformer models outperform the corresponding DeiT models significantly. Specifically, the advantages of Visformer-S and Visformer-Ti over DeiT-S and DeiT-Ti under the elite setting are 2.12% and 6.41%, while under the base setting, the numbers grow to 14.08% and 10.47%, respectively. In other words, the advantage becomes more significant under the base setting, which is more frequently used for visual recognition.</p><p>We then compare Visformer to other Transformer-based approaches in <ref type="table">Table 7</ref>. At the tiny level, Visformer-Ti significantly outperforms other vision Transformer models. For larger models, Visformer-S performs much better than the models with similar FLOPs. Other models usually need to utilize much more FLOPs to achieve comparable performance. As for the state-of-the-art EfficientNet convnets, our models are below the EfficientNets with similar FLOPs. However, EfficientNets is computing inefficient on GPUs. The results in <ref type="table">Table 8</ref> show that our model is significantly faster than EfficientNet-b3 which performance is slightly worse than our model. Our model is as efficient as DeiT-S and ResNet-50 but with a rather better performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training with limited data</head><p>Last but not least, we evaluate the performance of Visformer in the scenario with limited training data, which we consider is an important ability of being vision-friendly,  <ref type="table">Table 9</ref>. Comparison among Visformer, DeiT, and ResNet, in terms of classification accuracy (%) using limited training data. The elite setting with 300 epochs is used for all models.</p><p>while prior Transformer-based models mostly required abundant training data <ref type="bibr" target="#b13">[14]</ref>.</p><p>Four subsets of ImageNet are used, with 10% and 1% randomly chosen classes (all data), and with 10% and 1% randomly chosen images (all classes), respectively. To challenge the models, we still use the elite setting with 300 epochs (not extended). As shown in <ref type="table">Table 9</ref>, it is observed that the DeiT-S model reports dramatic accuracy drops in all the four tests (note that the accuracy of using only 10% and 1% classes should be much higher if epochs are extended). In comparison, Visformer remains robust in these scenarios, showing its potential of being used for visual recognition with limited data.</p><p>In tiny level, ResNet-50-55% is obtained by reducing the channel numbers (like other tiny models) to 55% (so that the FLOPs, 1.3G, is similar to Visformer-Ti and Deit-Ti). The conclusion is similar: Visformer-Ti is still the best overall model, and the advantage is slightly enlarged because the risk of over-fitting has been reduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>This paper presents Visformer, a Transformer-based model that is friendly to visual recognition. We propose to use two protocols, the base and elite setting, to evaluate the performance of each model. To study the reason why Transformer-based models and convolution-based models behave differently, we decompose the gap between these models and design an eight-step transition procedure that bridges the gap between DeiT-S and ResNet-50. By absorbing the advantages and discarding the disadvantages, we obtain the Visformer-S model that outperforms both DeiT-S and ResNet-50. Visformer also shows a promising ability when it is transferred to a compact model and when it is evaluated on small datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The transition process that starts with DeiT and ends with ResNet-50. To save space, we only show three important movements. The first movement converts DeiT from the Transformer to convolution view (Section 3.3.1). The second movement replaces the patch flattening module with step-wise patch embedding (elaborated in Section 3.3.2) and introduces the stage-wise design (Section 3.3.3) . The third movement replaces the self-attention module with convolution (Section 3.3.7). The upper-right area shows a relatively minor modifications, inserting 3 ? 3 convolution (Section 3.3.5). The lower-right area compares the receptive fields of a 3 ? 3 convolution and self-attention. This figure is best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>77.29 (-0.08) 80.13 (-0.02) Net5-DS2 77.34 (-0.02) 79.75 (-0.40) Net5-DS3 77.05 (-0.32) 79.59 (-0.56)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The comparison among ResNet-50, DeiT-S, and the proposed Visformer-S model in ImageNet classification. Although DeiT-S performs well under the elite setting, its performance drops dramatically when the base setting is used or when fewer data are used for training. In comparison, Visformer-S is friendly to both the base and elite settings, and reports smaller accuracy drops using a limited number of training data. Please refer to the main texts for the detailed settings.</figDesc><table><row><cell cols="2">Network</cell><cell cols="3">ResNet-50 DeiT-S Visformer-S</cell></row><row><cell cols="2">FLOPs (G)</cell><cell>4.1</cell><cell>4.6</cell><cell>4.9</cell></row><row><cell cols="2">Parameters (M)</cell><cell>25.6</cell><cell>21.8</cell><cell>40.2</cell></row><row><cell>Full</cell><cell>base setting</cell><cell>77.43</cell><cell>63.12</cell><cell>77.20</cell></row><row><cell>data</cell><cell>elite setting</cell><cell>78.73</cell><cell>80.07</cell><cell>82.19</cell></row><row><cell cols="2">Part of 10% labels</cell><cell>58.37</cell><cell>40.41</cell><cell>58.74</cell></row><row><cell>data</cell><cell>10% classes</cell><cell>89.90</cell><cell>80.06</cell><cell>90.06</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>The classification accuracy on ImageNet during the transition procedure from DeiT-S to ResNet-50. Both the base setting and the elite setting are considered (for the details, see Section 3.2), and we mark the positive modifications in red and the negative modifications in blue. Note that a modification can impact the base and elite performance differently. Though the number of parameters increases considerably at the intermediate status, the computational costs measured by FLOPs does not change significantly.</figDesc><table><row><cell>Model Name</cell><cell>added</cell><cell>removed</cell><cell>base perf.</cell><cell cols="3">elite perf. FLOPs (G) Params (M)</cell></row><row><cell>DeiT-S</cell><cell>-</cell><cell></cell><cell>64.17</cell><cell>80.07</cell><cell>4.60</cell><cell>22.1</cell></row><row><cell>Net1</cell><cell cols="4">global average pooling classification token 69.81 (+5.64) 80.16 (+0.09)</cell><cell>4.57</cell><cell>22.0</cell></row><row><cell>Net2</cell><cell cols="4">step-wise embeddings large patch embedding 73.01 (+3.20) 81.35 (+1.19)</cell><cell>4.77</cell><cell>23.9</cell></row><row><cell>Net3</cell><cell>stages-wise design</cell><cell>-</cell><cell cols="2">75.76 (+2.75) 80.19 (-1.14)</cell><cell>4.79</cell><cell>39.5</cell></row><row><cell>Net4</cell><cell>batch norm</cell><cell>layer norm</cell><cell cols="2">76.49 (+0.73) 80.97 (+0.78)</cell><cell>4.79</cell><cell>39.5</cell></row><row><cell>Net5</cell><cell>3 ? 3 convolution</cell><cell>-</cell><cell cols="2">77.37 (+0.88) 80.15 (-0.82)</cell><cell>4.76</cell><cell>39.2</cell></row><row><cell>Net6</cell><cell>-</cell><cell cols="3">position embedding 77.31 (-0.06) 79.86 (-0.29)</cell><cell>4.76</cell><cell>39.0</cell></row><row><cell>Net7</cell><cell>convolution</cell><cell>self-attention</cell><cell cols="2">76.24 (-1.07) 79.01 (-0.85)</cell><cell>4.83</cell><cell>45.0</cell></row><row><cell>ResNet-50</cell><cell cols="2">network shape adjustment</cell><cell cols="2">77.43 (+1.19) 78.73 (-0.28)</cell><cell>4.09</cell><cell>25.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Impact of replacing the self-attention blocks with the bottleneck blocks in each stage of Net5. These experiments are performed individually.</figDesc><table><row><cell cols="3">Network base perf.(%) elite perf.(%)</cell></row><row><cell>Net4</cell><cell>76.49</cell><cell>80.97</cell></row><row><cell cols="3">Net4-S1 77.02 (+0.53) 81.10 (+0.13)</cell></row><row><cell cols="3">Net4-S2 76.55 (+0.06) 80.50 (-0.47)</cell></row><row><cell cols="3">Net4-S3 76.82 (+0.33) 80.44 (-0.53)</cell></row><row><cell>Net5</cell><cell cols="2">77.37 (+0.88) 80.15 (-0.82)</cell></row><row><cell cols="3">Table 4. Impact of replacing the MLP layers with the bottleneck</cell></row><row><cell cols="3">blocks in each stage of Net4. These experiments are performed</cell></row><row><cell>individually.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>? 7, 16, stride 2 7 ? 7, 32, stride 2 emb. 28 ? 28 4 ? 4, 96, stride 4 4 ? 4, 192, stride 4</figDesc><table><row><cell></cell><cell>output size</cell><cell></cell><cell cols="2">Visformer-Ti</cell><cell></cell><cell></cell><cell cols="2">Visformer-S</cell></row><row><cell cols="5">? stem 112 ? 112 7 s1 28 ? 28 ? ? ? (group = 8) 1 ? 1, 192 ? 3 ? 3, 384 ? ? ? ?7</cell><cell cols="4">? ? ? ? (group = 8) 1 ? 1, 384 ? 3 ? 3, 768 ? ? ? ?7</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1 ? 1, 96</cell><cell></cell><cell></cell><cell></cell><cell>1 ? 1, 192</cell></row><row><cell cols="9">emb. 14 ? 14 2 ? 2, 192, stride 2 2 ? 2, 384, stride 2</cell></row><row><cell></cell><cell></cell><cell cols="2">? MHSA, 192</cell><cell>?</cell><cell cols="4">? MHSA, 384</cell><cell>?</cell></row><row><cell>s2</cell><cell>14 ? 14</cell><cell>?</cell><cell>1 ? 1, 768</cell><cell>? ?4</cell><cell cols="2">?</cell><cell cols="2">1 ? 1, 1536</cell><cell>? ?4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1 ? 1, 192</cell><cell></cell><cell></cell><cell></cell><cell>1 ? 1, 384</cell></row><row><cell>emb.</cell><cell>7 ? 7</cell><cell cols="7">2 ? 2, 384, stride 2 2 ? 2, 768, stride 2</cell></row><row><cell></cell><cell></cell><cell cols="2">? MHSA, 384</cell><cell>?</cell><cell cols="3">? MHSA, 768</cell><cell>?</cell></row><row><cell>s3</cell><cell>7 ? 7</cell><cell>?</cell><cell>1 ? 1, 1536</cell><cell>? ?4</cell><cell>?</cell><cell cols="2">1 ? 1, 3072</cell><cell>? ?4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1 ? 1, 384</cell><cell></cell><cell></cell><cell></cell><cell>1 ? 1, 768</cell></row><row><cell></cell><cell>1 ? 1</cell><cell cols="7">global average pool, 1000-d fc, softmax</cell></row><row><cell></cell><cell>FLOPs</cell><cell></cell><cell cols="2">1.3 ? 10 9</cell><cell></cell><cell></cell><cell cols="2">4.9 ? 10 9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>The configuration for constructing the Visformer-Ti and Visformer-S models, where 'emb.' stands for feature embedding, and 's1'-'s3' indicate the three stages with different spatial resolutions.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .Table 7 .Table 8 .</head><label>678</label><figDesc>The comparison of base and elite performance as well as the FLOPs between Visformer and DeiT, the direct baseline. Comparison among our method and other Transformerbased vision models. '*' indicates that we re-run the model using the elite setting. 'KD' stands for knowledge distillation<ref type="bibr" target="#b15">[16]</ref>. Comparison of inference efficiency among Visformer-S and other models. A batch size of 32 is used for testing. Besides EfficientNet-B3, other models are trained using the elite setting.</figDesc><table><row><cell>Network</cell><cell cols="2">base perf. (%)</cell><cell cols="2">elite perf. (%)</cell><cell>FLOPs (G)</cell></row><row><cell>Visformer-Ti</cell><cell cols="2">74.34</cell><cell cols="2">78.62</cell><cell>1.3</cell></row><row><cell>DeiT-Ti</cell><cell cols="2">63.87</cell><cell cols="2">72.21</cell><cell>1.3</cell></row><row><cell>Visformer-S</cell><cell cols="2">77.20</cell><cell cols="2">82.19</cell><cell>4.9</cell></row><row><cell>DeiT-S</cell><cell cols="2">63.12</cell><cell cols="2">80.07</cell><cell>4.6</cell></row><row><cell>Methods</cell><cell></cell><cell cols="2">Top-1(%)</cell><cell>FLOPs (G)</cell><cell>Params (M)</cell></row><row><cell>ResNet-18 [15]</cell><cell></cell><cell>69.8</cell><cell></cell><cell>1.8</cell><cell>11.7</cell></row><row><cell>DeiT-Ti [34]</cell><cell></cell><cell>72.2</cell><cell></cell><cell>1.3</cell><cell>5.7</cell></row><row><cell>DeiT-Ti (KD) [34]</cell><cell></cell><cell>74.6</cell><cell></cell><cell>1.3</cell><cell>5.7</cell></row><row><cell>PVT-Ti [36]</cell><cell></cell><cell>75.1</cell><cell></cell><cell>1.9</cell><cell>13.2</cell></row><row><cell cols="2">Visformer-Ti (ours)</cell><cell>78.6</cell><cell></cell><cell>1.3</cell><cell>10.3</cell></row><row><cell>ResNet-50 [15]</cell><cell></cell><cell>76.2</cell><cell></cell><cell>4.1</cell><cell>25.6</cell></row><row><cell>ResNet-50  *  [15]</cell><cell></cell><cell>78.7</cell><cell></cell><cell>4.1</cell><cell>25.6</cell></row><row><cell cols="2">RegNetY-4GF [26]</cell><cell>79.4</cell><cell></cell><cell>4.0</cell><cell>20.6</cell></row><row><cell cols="2">RegNetY-8GF [26]</cell><cell>79.9</cell><cell></cell><cell>8.0</cell><cell>39.2</cell></row><row><cell cols="2">RegNetY-4GF  *  [26]</cell><cell>80.0</cell><cell></cell><cell>4.0</cell><cell>20.6</cell></row><row><cell>DeiT-S [34]</cell><cell></cell><cell>79.8</cell><cell></cell><cell>4.6</cell><cell>21.8</cell></row><row><cell>DeiT-S  *  [34]</cell><cell></cell><cell>80.1</cell><cell></cell><cell>4.6</cell><cell>21.8</cell></row><row><cell>DeiT-B [34]</cell><cell></cell><cell>81.8</cell><cell></cell><cell>17.4</cell><cell>86.3</cell></row><row><cell>PVT-S [36]</cell><cell></cell><cell>79.8</cell><cell></cell><cell>3.8</cell><cell>24.5</cell></row><row><cell>PVT-Medium [36]</cell><cell></cell><cell>81.2</cell><cell></cell><cell>6.7</cell><cell>44.2</cell></row><row><cell>T2T-ViT t -14 [40]</cell><cell></cell><cell>80.7</cell><cell></cell><cell>5.2</cell><cell>21.5</cell></row><row><cell>T2T-ViT t -19 [40]</cell><cell></cell><cell>81.4</cell><cell></cell><cell>8.4</cell><cell>39.0</cell></row><row><cell cols="2">BoTNet-S1-59 [30]</cell><cell>81.7</cell><cell></cell><cell>7.3</cell><cell>33.5</cell></row><row><cell cols="2">Visformer-S (ours)</cell><cell>82.2</cell><cell></cell><cell>4.9</cell><cell>40.2</cell></row><row><cell>Methods</cell><cell></cell><cell>Top-1 (%)</cell><cell cols="2">FLOPs (G)</cell><cell>Batch Time (ms)</cell></row><row><cell>ResNet-50  *</cell><cell></cell><cell>78.7</cell><cell></cell><cell>4.1</cell><cell>34.2</cell></row><row><cell>DeiT-S  *</cell><cell></cell><cell>80.1</cell><cell></cell><cell>4.6</cell><cell>36.9</cell></row><row><cell>RegNetY-4GF  *</cell><cell></cell><cell>80.0</cell><cell></cell><cell>4.0</cell><cell>40.2</cell></row><row><cell cols="2">EfficientNet-B3 [33]</cell><cell>81.6</cell><cell></cell><cell>1.8</cell><cell>48.3</cell></row><row><cell>Visformer-S (ours)</cell><cell></cell><cell>82.2</cell><cell></cell><cell>4.9</cell><cell>36.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>90.06 91.60 58.74 16.56 DeiT-S 80.07 80.06 73.40 40.41 6.94 ResNet-50 78.73 89.90 93.20 58.37 13.59 Visformer-Ti 78.62 89.48 90.60 55.14 11.79 Deit-Ti 72.33 78.72 74.40 38.44 6.53 ResNet-50-55% 72.84 87.10 91.40 51.48 10.68</figDesc><table><row><cell>Network</cell><cell>100% classes</cell><cell>10% classes</cell><cell>1% classes</cell><cell>10% images</cell><cell>1% images</cell></row><row><cell>Visformer-S</cell><cell>82.19</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was supported by the National Key R&amp;D Program of China (2017YFB1301100), National Natural Science Foundation of China (61772060, U1536107, 61472024, 61572060, 61976012, 61602024), and the CERNET Innovation Project (NGII20160316).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">ton. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lambdanetworks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08602</idno>
		<title level="m">Modeling long-range interactions without attention</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3286" to="3295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multigrain: a unified image embedding for classes and instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartomeu</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gcnet: Non-local networks meet squeeze-excitation networks and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00364</idno>
		<title level="m">Pre-trained image processing transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Transunet: Transformers make strong encoders for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieneng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangde</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1691" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Augment your batch: Improving generalization through instance repetition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8129" to="8138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Local relation networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3464" to="3473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deep learning. nature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural architecture search for lightweight non-local networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieru</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochen</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10297" to="10306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10428" to="10436" />
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Doll?r</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Standalone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05909</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11605</idno>
		<title level="m">Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani. Bottleneck transformers for visual recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<title level="m">Alexandre Sablayrolles, and Herv? J?gou. Training data-efficient image transformers &amp; distillation through attention</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05431</idno>
		<title level="m">Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokensto-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Exploring self-attention for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10076" to="10085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
