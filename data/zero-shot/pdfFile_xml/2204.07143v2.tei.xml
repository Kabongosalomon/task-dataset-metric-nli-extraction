<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neighborhood Attention Transformer Self Attention (ViT) Window Self Attention (Swin) Shifted Window Self Attention (Swin) Neighborhood Attention (NAT)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hassani</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SHI Lab @ U of Oregon &amp; UIUC</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Picsart AI Research (PAIR)</orgName>
								<address>
									<addrLine>3 Meta</addrLine>
									<country>Facebook AI</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Walton</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SHI Lab @ U of Oregon &amp; UIUC</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Picsart AI Research (PAIR)</orgName>
								<address>
									<addrLine>3 Meta</addrLine>
									<country>Facebook AI</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SHI Lab @ U of Oregon &amp; UIUC</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Picsart AI Research (PAIR)</orgName>
								<address>
									<addrLine>3 Meta</addrLine>
									<country>Facebook AI</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SHI Lab @ U of Oregon &amp; UIUC</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Picsart AI Research (PAIR)</orgName>
								<address>
									<addrLine>3 Meta</addrLine>
									<country>Facebook AI</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neighborhood Attention Transformer Self Attention (ViT) Window Self Attention (Swin) Shifted Window Self Attention (Swin) Neighborhood Attention (NAT)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: An illustration of receptive fields in Self Attention, Shifted Window Self Attention, and our Neighborhood Attention. Self Attention has the same maximum receptive field for each query token. Shifted Window Self Attention divides inputs into sub-windows and performs self attention, and is followed by another layer with a shift to the input with attention masked for the shifted-out-of-place pixels (gray area). Neighborhood Attention localizes the receptive field to a neighborhood around each token, introducing local inductive biases without needing extra operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We present Neighborhood Attention Transformer (NAT), an efficient, accurate and scalable hierarchical transformer that works well on both image classification and downstream vision tasks. It is built upon Neighborhood Attention (NA), a simple and flexible attention mechanism that localizes the receptive field for each query to its nearest neighboring pixels. NA is a localization of selfattention, and approaches it as the receptive field size increases. It is also equivalent in FLOPs and memory usage to Swin Transformer's shifted-window attention given the same receptive field size, while being less constrained. Furthermore, NA includes local inductive biases, which eliminate the need for extra operations such as pixel shifts. Experimental results on NAT are competitive; NAT-Tiny reaches 83.2% top-1 accuracy on ImageNet with only 4.3 GFLOPs and 28M parameters, 51.4% mAP on MS-COCO and 48.4% mIoU on ADE20k. We open-sourced our checkpoints, code and CUDA kernel at: https://github.com/SHI-Labs/Neighborhood-Attention-Transformer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional neural networks (CNNs) <ref type="bibr" target="#b15">[16]</ref> have been the de facto architecture for computer vision models across different applications for years. AlexNet <ref type="bibr" target="#b14">[15]</ref> showed their usefulness on ImageNet <ref type="bibr" target="#b6">[7]</ref> and many others followed suit, with architectures such as VGG <ref type="bibr" target="#b23">[24]</ref>, ResNet <ref type="bibr" target="#b12">[13]</ref>, and Ef-ficientNet <ref type="bibr" target="#b24">[25]</ref>. Transformers <ref type="bibr" target="#b28">[29]</ref> on the other hand, were originally proposed as attention-based models for natural language processing (NLP), trying to exploit the sequential structure of language. They were the basis upon which BERT <ref type="bibr" target="#b7">[8]</ref> and GPT <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b0">1]</ref> were built, and they continue to be the state of the art architecture in NLP. In late 2020, Vision Transformer (ViT) <ref type="bibr" target="#b8">[9]</ref> was proposed as an image classifier using only a Transformer Encoder operating on an embedded space of image patches, mostly for largescale training. A number of other methods followed, attempting to increase data efficiency <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>, eventually making such Transformer-like models the state of the art in ImageNet-1k classification (without pre-training on large-scale datasets such as JFT-300M). These highperforming attention-based methods are all based on Transformers, which were originally intended for language processing. Self-attention has a linear complexity with respect to the embedding dimension (excluding linear projections), but a quadratic complexity with respect to the number of tokens. In the scope of vision, the number of tokens is typically in linear correlation with image resolution. As a result, higher image resolution results in a quadratic increase in complexity and memory usage in models strictly using self-attention, such as ViT. This has been one of the problems that prevented such models from being easily applicable to downstream vision tasks, such as object detection and semantic segmentation, in which image resolutions are usually much larger than classification. Another problem is that convolutions benefit from inductive biases such as locality, translational equivariance, and 2-dimensional neighborhood structure, while dot-product self attention is a global 1-dimensional operation by definition. While MLP layers in vision transformers are local and translationally equivariant, the rest of those inductive biases have to be learned with large sums of data <ref type="bibr" target="#b8">[9]</ref> or advanced training techniques and augmentations <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b10">11]</ref>. Local attention modules were therefore applied to alleviate this issue. Swin <ref type="bibr" target="#b17">[18]</ref> was one of the first hierarchical vision transformers based on local self attention. Its design and the shiftedwindow self attention allowed it to be easily applicable to downstream tasks, as they made it computationally feasible, while also boosting performance through the additional biases injected. HaloNet <ref type="bibr" target="#b27">[28]</ref> also explored a local attention block, building on SASA <ref type="bibr" target="#b22">[23]</ref>, and found that a combination of local attention blocks and convolutions resulted in the best performance, due to the best trade-off between memory requirements and translational equivariance.</p><p>In this paper, we propose Neighborhood Attention (NA) and build Neighborhood Attention Transformer (NAT) on top of it, which achieves competitive results across vision tasks. NA is a localization of dot-product self attention, limiting each query token's receptive field to a fixed-size neighborhood around its corresponding tokens in the keyvalue pair. Smaller neighborhoods result in more local attention, while larger ones yield more global attention. This design enables control of receptive fields in a way to balance between translational invariance and equivariance. Our efforts are inspired by the localized nature of convolutions and how they created more local inductive biases that are beneficial to vision tasks. It is different from self attention being applied to local windows (Swin), and can be thought of as a convolution with a content-dependant kernel.</p><p>To summarize, our main contributions are:</p><p>1. Proposing Neighborhood Attention (NA): A simple and flexible attention mechanism for vision, which localizes the receptive field for each token to its neighborhood. We compare this module in terms of complexity and memory usage to self attention, window self attention, and convolutions.</p><p>2. Introducing Neighborhood Attention Transformer (NAT), a new efficient, accurate and scalable hierarchical transformer made of levels of neighborhood attention layers. Each level is followed by a downsampling operation, which reduces spatial size by half. A similar design can be seen in many recent attentionbased models, such as Swin <ref type="bibr" target="#b17">[18]</ref>. Unlike those models, NAT utilizes small-kernel overlapping convolutions for embedding and downsampling, as opposed to non-overlapping ones. NAT also introduces a more efficient set of architecture configurations, compared to prior arts such as Swin.</p><p>3. Demonstrating NAT's effectiveness on both classification and downstream vision tasks including object detection and semantic segmentation. We observe that NAT can outperform not only Swin, but also new convolutional contenders <ref type="bibr" target="#b18">[19]</ref>. NAT-Tiny can reach 83.2% top-1 accuracy on ImageNet with only 4.3 GFLOPs and 28M parameters, and 51.4% bounding box mAP on MS-COCO and 48.4% mIoU on ADE20k.</p><p>4. Open-sourcing a new CUDA-supported PyTorch extension for fast and efficient computation of windowbased attention mechanisms. In addition to supporting 2D Neighborhood Attention module, the extension will also allow for customized padding values, strides, and dilated neighborhoods, as well as 1D and 3D data. <ref type="figure">Figure 2</ref>: An illustration of the query-key-value structure of Neighborhood Attention (NA) vs Self Attention (SA) for a single pixel. SA allows each token to attend all other tokens, while NA localizes each token's receptive field to a neighborhood around itself. This is different from the existing window self attention (WSA) in Swin <ref type="bibr" target="#b17">[18]</ref>, which divides self attention into subwindows. Both WSA and NA have a linear computational cost and memory usage with respect to resolution, as opposed to self attention's quadratic cost and memory. However, NA has the added advantage of limiting each pixel to its neighborhood directly at no extra computational cost, thus not requiring pixel shifts to introduce crosswindow interactions. NA is also not limited to working on inputs divisible by the window size like window attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>In this section, we briefly review the multi-headed self attention (MHSA) mechanism <ref type="bibr" target="#b28">[29]</ref>, and review some of the notable vision transformers and transformer-like architectures <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26]</ref>, as well as some of the notable local attentionbased vision transformers <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28]</ref>, and a recent CNN which provides an up-to-date baseline for attention-based models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Multi-Headed self attention</head><p>Scaled Dot-Product Attention was defined by Vaswani et al. <ref type="bibr" target="#b28">[29]</ref> as an operation on a query and a set of key-value pairs. The dot product of query and key is computed and scaled. Softmax is applied to the output in order to normalize attention weights, and is then applied to the values. It can be expressed as follows:</p><formula xml:id="formula_0">Attention(Q, K, V ) = sof tmax QK T ? d k V,<label>(1)</label></formula><p>where d k is the key dimension. Multi-headed self attention <ref type="bibr" target="#b28">[29]</ref> is applying the dot-product attention function multiple times with different embeddings, hence forming attention heads. Self attention by definition takes in an input sequence and uses it as both the query and key-value pairs. Given an input X ? R M ?D , where M is the number of tokens and D is the embedding dimension, this operation has a complexity of O(M 2 D) and a space complexity of O(M 2 ) for the attention weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Vision Transformer</head><p>Dosovitskiy et al. <ref type="bibr" target="#b8">[9]</ref> proposed a Transformer-based image classifier that merely consists of a Transformer encoder <ref type="bibr" target="#b28">[29]</ref> and an image tokenizer, named Vision Transformer. Previous works, such as DETR <ref type="bibr" target="#b2">[3]</ref>, explored a hybrid of CNNs and Transformer models for detection. ViT on the other hand proposed a model that would only rely on a single non-overlapping convolutional layer (patching and embedding). ViT was pre-trained primarily on the private JFT-300M dataset, and was shown to outperform state-of-the-art CNNs on many benchmarks. However, it was also added that when ViT is pre-trained on mediumscale datasets, such as ImageNet-1k and ImageNet-21k, it no longer achieves competitive results. This was attributed to the lack of inductive biases that are inherent to CNNs, which the authors argued is trumped by large-scale training. While this effectively proved ViT inferior in medium-scale training, it provided empirical evidence that Transformerbased models outperform CNNs in larger scales. ViT paved the way for many more vision transformers, and attentionbased models in general, that followed and transferred it to medium-scale learning <ref type="bibr" target="#b25">[26]</ref>, and even small-scale learning on much smaller datasets <ref type="bibr" target="#b10">[11]</ref>. Touvron et al. <ref type="bibr" target="#b25">[26]</ref> extended the study of Vision Transformers by exploring data efficiency. Their Data-efficient image Transformer model performed significantly better than ViT with very few architectural changes, and through the use of advanced augmentations and training techniques. They explored knowledge transfer <ref type="bibr" target="#b13">[14]</ref> through attention by introducing a distillation token, and a hard distillation loss. As for the choice of teacher model, they explored both Transformer-based models, as well as CNNs. It was shown that a convolutional teacher model can improve performance more significantly, as it is arguably transferring inductive biases DeiT lacks. Their experiments highlighted the true potential of a Transformer-based image classifier in the medium-sized data regime, while also inspiring many more to utilize their training techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Models using local attention</head><p>Shifted Window (Swin) Attention <ref type="bibr" target="#b17">[18]</ref> was introduced by Liu et al. as one of the first window-based self attention mechanisms. This mechanism partitions input feature maps and applies self attention to each partition separately. This operation has a more favorable complexity, and can be parallelized. These so-called Window Attentions are followed by Shifted Window Attentions, which apply the same operation but with a shift in pixels prior to the window partitioning stage, in order to introduce connections across the extracted windows, while still maintaining the efficiency. Additionally, the proposed model, Swin Transformer, produces pyramid-like feature maps, reducing spatial dimensionality while increasing depth. This structure has been commonly used in CNNs over the years, and is why Swin can be easily integrated with other networks for application to downstream tasks, such as detection and segmentation. Swin outperformed DeiT with a convolutional teacher, at ImageNet-1k classification. Moreover, Swin Transformer is the state of the art method in object detection on the MSCOCO test set, and was until recently the state of the art in semantic segmentation on ADE20K. Vaswani et al. <ref type="bibr" target="#b27">[28]</ref> proposed HaloNet, building on top of Stand Alone Self-Attention (SASA) <ref type="bibr" target="#b22">[23]</ref>, which operated like a convolution with same zero padding, and extracted key-value pairs by striding the feature map. HaloNet's attention mechanism consists of 3 stages: blocking, haloing, and attention. Input feature maps are blocked into non-overlapping subsets, which will serve as queries. Followed by that, neighboring blocks of equal size are extracted, which will serve as keys and values. The extracted queries and key-value pairs are then sent through self attention. HaloNet was shown to be effective at both reducing cost and improving performance, especially when used in conjunction with convolutional layers in the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Recent Convolutional models</head><p>Liu et al. <ref type="bibr" target="#b18">[19]</ref> recently proposed a new CNN architecture influenced by models such as Swin, dubbed ConvNeXt. These models are not attention-based, and manage to outperform Swin across different vision tasks. This work has since served as a new CNN baseline for fair comparison of convolutional models and attention-based models.</p><p>We propose Neighborhood Attention, which by design localizes the receptive field to a window around each query, and therefore would not require additional techniques such as the cyclic shift used by Swin. We introduce a hierarchical transformer-like model with this attention mechanism, dubbed Neighborhood Attention Transformer, and demonstrate its performance compared to Swin on image classification, object detection, and semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we introduce Neighborhood Attention, a localization of self attention (see Eq. 1) considering the structure of visual data. This not only reduces computational cost compared to self attention, but also introduces local inductive biases, similar to that of convolutions. This operation works with a neighborhood size L, which when at a minimum sets each pixel to attend to only a 1-pixel neighborhood around itself (creating a 3 ? 3 square window). We also show that this definition is equal to self attention when the neighborhood size is at its maximum (i.e. the size of the input). Therefore, if the neighborhood size exceeds or matches the feature map size, neighborhood attention and self attention will have equivalent outputs given the same input. We then introduce our model, Neighborhood Attention Transformer (NAT), which uses this new mechanism instead of self attention. In addition, NAT utilizes a multi-level hierarchical design, similar to Swin <ref type="bibr" target="#b17">[18]</ref>, meaning that feature maps are downsampled between levels, as opposed to all at once. Unlike Swin however, NAT uses overlapping convolutions to downsample feature maps, as opposed to non-overlapping (patched) ones. This creates a slight increase in FLOPs and parameters, which we remedy by proposing new configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Neighborhood Attention</head><p>Inspired by how convolutions introduce neighborhood biases and locality, we introduce Neighborhood Attention. This mechanism is designed to allow each pixel in feature maps to only attend to its neighboring pixels. As neighborhood is dependant on a size, so will our new mechanism. We denote the neighborhood of a pixel at (i, j) with ?(i, j), which is a fixed-length set of indices of pixels nearest to (i, j). For a neighborhood of size L ? L, ?(i, j) = L 2 . Therefore, neighborhood attention on a single pixel can be defined as follows:</p><formula xml:id="formula_1">NA(X i,j ) = softmax Q i,j K T ?(i,j) + B i,j scale V ?(i,j) ,<label>(2)</label></formula><p>where Q, K, V are linear projections of X, similar to Eq. 1. B i,j denotes the relative positional bias, which is added to each attention weight based on its relative position. This operation can further be extended to all pixels (i, j), which results in a form of localized attention. However, if the function ? maps each pixel to all pixels (L 2 is equal to feature map size), this will be equivalent to self attention (with the additional positional bias). That is because ?(i, j) will include all possible pixels when the neighborhood bounds go beyond input size. As a result, K ?(i,j) = K and V ?(i,j) = V , and by removing the bias term, Eq. 1 is derived. Neighborhood attention is computationally cheap. Its complexity is linear with respect to resolution, unlike self attention's, which is quadratic. Additionally, its complexity is also linear with respect to neighborhood size. ?, which maps a pixel to a set of neighboring pixels, can be easily produced with a raster-scan sliding window operation, similar to convolutions. Each pixel is simply mapped to a set of neighboring pixels and itself. An illustration of this operation is presented in <ref type="figure">Figure 2</ref>. This operation is repeated for every pixel in the feature map. As for corner pixels that cannot be centered, the neighborhood is expanded to maintain receptive field size. This is a key design choice, which allows NA to generalize to self attention as neighborhood size grows towards the feature map resolution. Expanded neighborhood is achieved by continuing to pick the L 2 nearest neighboring pixels to the original. For example, for L = 3, each query will end up with 9 key-value pixels surrounding it (a 3 ? 3 grid with the query positioned in the center). For a corner pixel, the neighborhood is another 3 ? 3 grid, but with the query not positioned in the center (See <ref type="figure" target="#fig_4">Fig 8)</ref>.</p><p>Note on Implementation: Since no operations exist in deep learning libraries that directly replicate NA, and generating key-values for every query is highly inefficient and memory-consuming, we wrote our own CUDA kernel for NA (see Appendix B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Neighborhood Attention Transformer</head><p>NAT embeds inputs using 2 consecutive 3 ? 3 convolutions with 2 ? 2 strides, resulting in a spatial size 1/4th the size of the input. This is similar to using a patch and embedding layer with 4 ? 4 patches, but it utilizes overlapping convolutions instead of non-overlapping ones. On the other hand, using overlapping convolutions would increase cost, and two convolutions incurs more parameters. However, we handle that by reconfiguring the model, which results in a better trade-off. NAT consists of 4 levels, each followed by a downsampler (except the last). Downsamplers decrease cut spatial size in half, while doubling the number of channels. We use 3 ? 3 convolutions with 2 ? 2 strides, instead of 2 ? 2 non-overlapping convolutions that Swin uses (patch merge). Since the tokenizer downsamples by a factor of 4, our model produces feature maps of sizes . This change is motivated by previous successful CNN structures, and it allows for easier transfer of pre-trained models to downstream tasks. It was also motivated by the recent success of other hierarchical attention-based methods, such as PVT <ref type="bibr" target="#b29">[30]</ref>, ViL <ref type="bibr" target="#b36">[37]</ref>, and Swin Transformer <ref type="bibr" target="#b17">[18]</ref>. Additionally, we utilize Lay-erScale <ref type="bibr" target="#b26">[27]</ref> for stability when training larger models. An illustration of the overall network architecture is presented in <ref type="figure">Figure 4</ref>. We present a summary of different NAT variants and their key differences in <ref type="table" target="#tab_2">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Complexity analysis</head><p>Neighborhood Attention has the same number of FLOPs as window-based self attention mechanisms, such as Swin <ref type="bibr" target="#b17">[18]</ref>. We present a complexity analysis in this subsection and discuss their memory usages. We also present a complexity analysis, comparing neighborhood attention to convolutions, showing that it involves fewer operations, but uses more memory. For simplicity, we exclude attention heads and work with single-headed attention. Note that we denote neighborhood and window sizes as L in order to avoid confusion with other notations, such as the attention key, K. Given input feature maps of shape H ? W ? C, where C is the number of channels, H and W are feature map height and width respectively, the QKV linear projections have a cost of O 3HW C 2 , which is the same for self-attention, Swin's window self attention, and neighborhood attention. Swin divides the queries, keys, and values into H L ? W L windows of shape L ? L ? C, then applies self attention on each window, which would cost O( H L W L CL 4 ), which is simplified into O HW CL 2 . Memory consumption of attention weights, which are of shape H L ? W L ?L 2 ? L 2 , is therefore O HW CL 2 . In NA, each query token Q i,j has keys K ?(i,j) , and values V ?(i,j) , both of size L ? L?C, which means the cost to compute attention weights is O HW CL 2 , similar to Swin. The attention weights will also be in the shape of H ? W ? L 2 , as each of the H ? W queries attends to L 2 keys. As a result, the memory required to store attention weights would also be the same as Swin, O HW CL 2 . As for convolutions, computational cost is O HW C 2 L 2 , and the memory usage would be only O (HW C) to store the output. While convolutions are more memory-efficient, their complexity is quadratic with respect to channels and linear with respect to window size (L?L). By solving the inequality between the complexities of a 2D convolution and a 2D neighborhood attention, for L &gt; 1, it can be concluded that the latter grows less quickly than the former as the number of channels is increased. Specifically for L = 3, NA is more efficient for all C &gt; 3, which in practice is usually the case. For L ? 5, NA is more efficient for all C &gt; 1. Therefore, it can be concluded that 2D NA is less computationally complex than a 2D convolution in practical scenarios, while only suffering from the additional memory usage due to the QKV projections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We demonstrate NAT's applicability and effectiveness by conducting experiments across different vision tasks, such as image classification, object detection, and semantic segmentation. We used a 7 ? 7 window size, similar to Swin.  <ref type="figure">Figure 4</ref>: An overview of our model, NAT, with its hierarchical design. The model starts off with a convolutional downsampler, then moves on to 4 sequential levels, each consisting of multiple NAT Blocks, which are transformer-like encoder layers. Each layer is comprised of a multi-headed neighborhood attention (NA), a multi-layered perceptron (MLP), Layer Norm (LN) before each module, and skip connections. Between the levels, feature maps are downsampled to half their spatial size, while their depth is doubled. This allows for easier transfer to downstream tasks through feature pyramids.  <ref type="table">Table 2</ref>: Comparison of computational cost and memory consumption between self attention, patched self attention, neighborhood attention, and convolution, with respect to input sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NAT Block Neighborhood Attention Transformer Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Module Computation Memory</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self attention</head><formula xml:id="formula_2">O 3HW C 2 + 2H 2 W 2 C O 3HW C + H 2 W 2 2D Window attention (Swin) O 3HW C 2 + 2HW CL 2 O 3HW C + HW L 2 2D Neighborhood attention O 3HW C 2 + 2HW CL 2 O 3HW C + HW L 2 2D Convolution O HW C 2 L 2 O (HW C)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Classification</head><p>We trained our variants on ImageNet-1k <ref type="bibr" target="#b6">[7]</ref> in order to compare to other transformer-based and convolutional image classifiers. This dataset continues to be one of the few benchmarks for medium-scale image classification, containing roughly 1.28M training, 50K validation, and 100K test images, categorized into 1000 classes. We train NAT with the commonly used timm <ref type="bibr" target="#b30">[31]</ref> (Apache License v2), and use the conventional augmentations (CutMix <ref type="bibr" target="#b34">[35]</ref>, Mixup <ref type="bibr" target="#b35">[36]</ref>, RandAugment <ref type="bibr" target="#b5">[6]</ref>, and Random Erasing <ref type="bibr" target="#b37">[38]</ref>) and training techniques used in methods we compare to <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. We follow Swin's <ref type="bibr" target="#b17">[18]</ref> training configuration (learning rate, iteration-wise cosine schedule, and other hyperparameters). Following convention, we train the model for 300 epochs, 20 of which warm up the learning rate, while the rest decay according to the scheduler, and do 10 additional cooldown epochs <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Object Detection</head><p>We trained Mask <ref type="bibr" target="#b11">[12]</ref> and Cascade Mask R-CNN <ref type="bibr" target="#b1">[2]</ref> on MS-COCO <ref type="bibr" target="#b16">[17]</ref>, with NAT backbones, which were pre-trained on ImageNet. We followed Swin <ref type="bibr" target="#b17">[18]</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Semantic Segmentation</head><p>We also trained UPerNet <ref type="bibr" target="#b31">[32]</ref> on ADE20K <ref type="bibr" target="#b38">[39]</ref>, with ImageNet-pretrained backbones.</p><p>We followed Swin's configuration for training ADE20k, and used mmsegmentation <ref type="bibr" target="#b4">[5]</ref> (Apache License v2). Addition-ally, and following standard practice, input images are randomly resized and cropped at 512 ? 512 when training. Segmentation results on ADE20K are presented in <ref type="table" target="#tab_6">Table 6</ref>. It is noticeable that NAT-Mini outperforms Swin-Tiny, and also comes very close to ConvNeXt-Tiny. NAT-Tiny outperforms ConvNeXt-Tiny significantly, while also slightly Note that both NAT-Small and NAT-Base bear fewer FLOPs with them compared to their Swin and ConvNeXt counterparts, while their performance is within the same region. It is also noteworthy that Swin especially suffers from more FLOPs even beyond the original difference due to the fact that the image resolution input in this task specifically (512 ? 512) will not result in feature maps that are divisible by 7?7, Swin's window size, which forces the model to pad input feature maps with zeros to resolve that issue, prior to every attention operation. NAT does not require this, as feature maps of any size are compatible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation study</head><p>In order to evaluate the effects of each different component compared to Swin Transformer <ref type="bibr" target="#b17">[18]</ref>, we conducted ablations on the attention module, the downsamplers, and the configurations. Attention ablations are presented in <ref type="table">Table 7</ref>. Swin's window self attention is accompanied by a cyclic shift, which introduces out-of-window interactions. Because neighborhood attention computes dynamic key-value pairs for every query, it would not need any shift in pixels to operate. The model configurations used in this study follow Swin-Tiny, with 96 channels spread across 3 heads in the first level, doubling after every level, and 2 layers per level, except the third level, which has 6 layers. Our second ablation is on the model itself, where we start with Swin-Tiny, and replace the patched tokenizer and downsamplers with overlapping convolutions. This change only involves using two 3 ? 3 convolutions with 2 ? 2 strides, instead of one 4 ? 4 convolution with 4 ? 4 strides, along with changing the downsamplers from 2?2 convolutions with 2?2 strides to 3 ? 3 convolutions with the same stride. As it can be noticed, model performance jumps up by just under 0.5% in accuracy, while the number of parameters and FLOPs also see a noticeable increase. We then switch to our Tiny configuration, with fewer heads, smaller inverted bottlenecks, and more layers. This results in an improvement of over 0.9% in accuracy, while reducing the number of parameters and FLOPs to below the baseline. We finally switch the attention mechanism with NA, and see an improvement of just under 0.5% in accuracy. This, along with the study conducted just on attention mechanisms, suggests that not only is NA more powerful under similar conditions, it also is affected more significantly by introducing more inductive biases into the model, through overlapping convolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Saliency analysis</head><p>In an effort to further illustrate the differences between attention mechanisms and models, we present salient maps from ViT-Base, Swin-Base, and NAT-Base. We selected a few images from the ImageNet validation set, sent them through the three models, and created the salient maps based on the outputs, which are presented in <ref type="table" target="#tab_5">Table 5</ref>. All images are correctly predicted (Bald Eagle, Acoustic Guitar, Hummingbird, Steam Locomotive) except ViT's Acoustic Guitar which predicts Stage. From these salient maps we can see that all models have relatively good interpretability, though they focus on slightly different areas. NAT appears to be slightly better at edge detection, which we believe is due to the localized attention mechanism, that we have presented in this work, as well as the convolutional downsamplers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Transformer-based vision models have received significant attention from the research community in computer vision, especially since the introduction of ViT <ref type="bibr" target="#b8">[9]</ref>. Some works focused on data efficiency, with minor changes in the architecture <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b9">10]</ref>, while others focused on efficiency and transferrability to downstream tasks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b9">10]</ref>. In this paper, we introduced an alternate way of localizing self attention with respect to the structure of data, which computes key-value pairs dynamically for each token, along with a more data efficient configuration of models. This helps create a model that utilizes both the power of attention, as well as the efficiency and inductive biases of convolutions. We've shown the power of such a model in image classification, in which it outperforms both Swin Transformer and ConvNeXt significantly. Additionally, we've shown that it can be seamlessly applied to downstream tasks, where it also outperforms or competes with those existing methods. We have open sourced our CUDA kernel, along with extensions of NA to 1-D. We will release updates soon with support for 3-D data, custom padding, stride, and dilation, which we have built on top of the existing set of kernels. We hope to ease the possibility of doing research in such methods by providing efficient implementations that are currently non-existent in deep learning libraries. We will be conducting experiments on even larger NAT variants, as well as ImageNet-21k pretraining in future versions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ViT-B</head><p>Swin-B NAT-B <ref type="figure">Figure 5</ref>: Salient Maps of selected ImageNet validation set images, comparing ViT-Base, Swin-Base, and NAT-Base. The ground truths for these images are: Bald Eagle, Acoustic Guitar, Hummingbird, and Steam Locomotive, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Memory and Throughput Analysis</head><p>We present memory usage and throughputs for each NAT variant, alongside Swin and ConvNeXt, in <ref type="table" target="#tab_3">Table 3</ref>. Memory usage was profiled with MemLab (MIT License) 1 , itself built on top of PyTorch's memory statistics, and includes model weights, and a single mini batch of size 256.</p><p>ConvNeXt is generally faster than Swin, as they pointed out in the paper <ref type="bibr" target="#b18">[19]</ref>, especially when running using the channels last memory format (not only is channels last more efficient, PyTorch also calls a cuDNN DWConv kernel, as opposed to their native one, which is significantly faster). Due to the fact that our proposed NAT architecture is relatively deeper than Swin's, more kernel calls occur, and therefore the throughput decreases further. NAT with an architecture similar to Swin is even faster than Swin itself (See <ref type="table" target="#tab_8">Table 9</ref>). Nevertheless, NAT maintains similar or better throughput at scale (larger models and/or larger resolutions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation details</head><p>In order to implement Neighborhood Attention, we initially started with a version based on existing operations in PyTorch <ref type="bibr" target="#b19">[20]</ref>. Excluding corner pixels (those that cannot have an L ? L window around surrounding them), the rest can be extracted using any raster-scan operator. Typically, any such operator with a stride of 1 will yield the desired neighborhoods for pixels that have L?1 2 pixels on each of their 4 sides (as discussed, we constrain L to be an odd number greater than 1). In PyTorch, the unfold function is the most straightforward solution. As for the corner pixels, simply repeating some of those extracted windows will yield their expected neighborhoods, as their neighborhood consists of the same pixels. This repetition can be done easily with PyTorch's replicated padding, which pads a tensor by repeating edge cells. The combination of these two operations (unfold then replicated_pad) on the keyvalue pair tensors (H ? W ? C) will yield two tensors of shape H ? W ? C ? L ? L (one L ? L for each channel and pixel). Neighborhood Attention can be computed by plugging these into (2) in the place of K and V. An upside 1 https://github.com/Stonesjtu/pytorch_memlab. of this is that the matrix multiplications can be batched and parallelized, just like Self Attention and Window Self Attention. However, this implementation is highly inefficient, because it needs to extract windows, itself very inefficient, temporarily store the extracted windows, and call matrix multiplication on these very large tensors, just to replicate Neighborhood Attention.</p><p>In order to resolve this issue, we wrote custom CUDA kernels for the QK and AV operations separately (leaving operations such as softmax and linear projections to the original kernels for efficiency). The kernels were written as PyTorch extensions, and therefore can be integrated seamlessly with PyTorch modules. The QK kernel can additionally take in the relative positional biases as a tensor, and apply them as attention weights are being computed, making it slightly more efficient. We observed that with these kernels, training a Tiny variant similar to Swin-T takes about 15% the time as the original pure torch implementation (1.2 days vs 9.3 days), and consumes about 15% the amount of memory with 64 samples on every GPU (10GB vs 67GB). We've also observed that its memory usage is even slightly below a Swin model with the same configuration (see <ref type="table" target="#tab_8">Table 9</ref>), since there are a few technical differences: 1. Swin computes relative positional biases with indices, NAT has it built into the kernel computing QK. 2. Swin consumes additional memory for the pixel shift and other necessary operations.</p><p>That said, the kernel is still in early stages of development and would require much additional optimization to reach its optimal performance. We developed a "tiled" implementation of NA, which is based on concepts used in efficient convolution CUDA kernels. This implementation utilizes shared memory within threadblocks, which decrease DRAM accesses, which results in a significant speedup. We will release the latest version of the kernel on GitHub, which not only allows the community to use our method, but also opens up the possibility of contributions from engineers who could optimize our kernel further. We additionally will release extensions of NA to 1-D and 3-D data, as well as different variations of NA, including dilated NA, valid padding, zero padding (SASA <ref type="bibr" target="#b22">[23]</ref>), and strided NA, which we have built on top of the existing set of kernels.   The neighborhood size is expanded at the corners to keep the receptive field size identical to the rest of the feature map. The alternative to this would have been smaller neighborhoods (zero padding at the corners, similar to SASA <ref type="bibr" target="#b22">[23]</ref>). This choice over zero padding was primarily due to its properties (equivalence to self attention when neighborhood and feature map sizes match), but also due to stronger performance. Additionally, the expanded neighborhood keeps the receptive field the same for all tokens, which is a better design choice from the attention point of view.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>ImageNet-1k classification performance versus compute, with bubble size representing the number of parameters. NAT outperfoms both Swin and ConvNeXt in classification with fewer FLOPs, and a similar number of parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>'s training settings, using mmdetection [4] (Apache License v2), and trained with the same accelerated 3? LR schedule. The results are presented in tables 4 and 5. NAT-Mini outperforms Swin-Tiny with Mask R-CNN, while falling slightly short to it with Cascade Mask R-CNN, all while having significantly fewer FLOPs. NAT-Tiny outperforms both its Swin and ConvNeXt counterparts, again with slightly fewer FLOPs, with both Mask and Cascade Mask R-CNN. NAT-Small and NAT-Base can reach similar-level performance with both detectors compared to their Swin and ConvNeXt counterparts, while being noticeably more efficient.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Throughput vs model accuracy plot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Memory usage vs model accuracy plot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The neighborhood size is expanded at the corners to keep the receptive field size identical to the rest of the feature map. The alternative to this would have been smaller neighborhoods (zero padding at the corners, similar to SASA [23]). This choice over zero padding was primarily due to its properties (equivalence to self attention when neighborhood and feature map sizes match), but also due to stronger performance. Additionally, the expanded neighborhood keeps the receptive field the same for all tokens, which is a better design choice from the attention point of view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>A summary of NAT Configurations. Channel double after every level until the final one.</figDesc><table><row><cell>Variant</cell><cell cols="5">Layers Dim ? Heads MLP ratio # Params FLOPs</cell></row><row><cell cols="2">NAT-Mini 3, 4, 6, 5</cell><cell>32 ? 2</cell><cell>3</cell><cell>20 M</cell><cell>2.7 G</cell></row><row><cell cols="2">NAT-Tiny 3, 4, 18, 5</cell><cell>32 ? 2</cell><cell>3</cell><cell>28 M</cell><cell>4.3 G</cell></row><row><cell cols="2">NAT-Small 3, 4, 18, 5</cell><cell>32 ? 3</cell><cell>2</cell><cell>51 M</cell><cell>7.8 G</cell></row><row><cell cols="2">NAT-Base 3, 4, 18, 5</cell><cell>32 ? 4</cell><cell>2</cell><cell>90 M</cell><cell>13.7 G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>ImageNet Top-1 validation accuracy comparison at 224?224 resolution (no extra data or pretraining). Peak memory usage and throughput are measured with a batch size of 256 on a single NVIDIA A100 GPU.</figDesc><table><row><cell>Model</cell><cell cols="5">Top-1 # Params FLOPs Throughput (imgs/sec) Memory (GB)</cell></row><row><cell>NAT-Mini</cell><cell>81.8%</cell><cell>20 M</cell><cell>2.7 G</cell><cell>2135</cell><cell>2.4</cell></row><row><cell>Swin-Tiny [18]</cell><cell>81.3%</cell><cell>28 M</cell><cell>4.5 G</cell><cell>1730</cell><cell>4.8</cell></row><row><cell cols="2">ConvNeXt-Tiny [19] 82.1%</cell><cell>28 M</cell><cell>4.5 G</cell><cell>2491</cell><cell>3.4</cell></row><row><cell>NAT-Tiny</cell><cell>83.2%</cell><cell>28 M</cell><cell>4.3 G</cell><cell>1541</cell><cell>2.5</cell></row><row><cell>Swin-Small [18]</cell><cell>83.0%</cell><cell>50 M</cell><cell>8.7 G</cell><cell>1059</cell><cell>5.0</cell></row><row><cell cols="2">ConvNeXt-Small [19] 83.1%</cell><cell>50 M</cell><cell>8.7 G</cell><cell>1549</cell><cell>3.5</cell></row><row><cell>NAT-Small</cell><cell>83.7%</cell><cell>51 M</cell><cell>7.8 G</cell><cell>1051</cell><cell>3.7</cell></row><row><cell>Swin-Base [18]</cell><cell>83.5%</cell><cell>88 M</cell><cell>15.4 G</cell><cell>776</cell><cell>6.7</cell></row><row><cell cols="2">ConvNeXt-Base [19] 83.8%</cell><cell>89 M</cell><cell>15.4 G</cell><cell>1107</cell><cell>4.8</cell></row><row><cell>NAT-Base</cell><cell>84.3%</cell><cell>90 M</cell><cell>13.7 G</cell><cell>783</cell><cell>5.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>Backbone</cell><cell cols="5"># Params FLOPs Throughput (FPS) AP b AP b</cell><cell>50</cell><cell>AP b</cell><cell>75</cell><cell cols="2">AP m AP m</cell><cell>50</cell><cell>AP m</cell><cell>75</cell></row><row><cell>NAT-Mini</cell><cell>40 M</cell><cell>225 G</cell><cell>54.1</cell><cell>46.5</cell><cell cols="2">68.1</cell><cell cols="2">51.3</cell><cell>41.7</cell><cell>65.2</cell><cell>44.7</cell></row><row><cell>Swin-Tiny [18]</cell><cell>48 M</cell><cell>267 G</cell><cell>45.1</cell><cell>46.0</cell><cell cols="2">68.1</cell><cell cols="2">50.3</cell><cell>41.6</cell><cell>65.1</cell><cell>44.9</cell></row><row><cell>ConvNeXt-Tiny [19]</cell><cell>48 M</cell><cell>262 G</cell><cell>52.0</cell><cell>46.2</cell><cell cols="2">67.0</cell><cell cols="2">50.8</cell><cell>41.7</cell><cell>65.0</cell><cell>44.9</cell></row><row><cell>NAT-Tiny</cell><cell>48 M</cell><cell>258 G</cell><cell>44.5</cell><cell>47.7</cell><cell cols="2">69.0</cell><cell cols="2">52.6</cell><cell>42.6</cell><cell>66.1</cell><cell>45.9</cell></row><row><cell>Swin-Small [18]</cell><cell>69 M</cell><cell>359 G</cell><cell>31.7</cell><cell>48.5</cell><cell cols="2">70.2</cell><cell cols="2">53.5</cell><cell>43.3</cell><cell>67.3</cell><cell>46.6</cell></row><row><cell>NAT-Small</cell><cell>70 M</cell><cell>330 G</cell><cell>34.8</cell><cell>48.4</cell><cell cols="2">69.8</cell><cell cols="2">53.2</cell><cell>43.2</cell><cell>66.9</cell><cell>46.5</cell></row></table><note>COCO Object detection and instance segmentation performance with Mask R-CNN. FLOPS are with respect to an input resolution of (1280, 800). Throughput is measured at the same resolution on a single NVIDIA A100 GPU.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>COCO Object detection and instance segmentation performance with Cascade Mask R-CNN. FLOPS are with respect to an input resolution of (1280, 800). Throughput is measured at the same resolution on a single NVIDIA A100 GPU.</figDesc><table><row><cell>Backbone</cell><cell cols="5"># Params FLOPs Throughput (FPS) AP b AP b</cell><cell>50</cell><cell>AP b</cell><cell>75</cell><cell cols="2">AP m AP m</cell><cell>50</cell><cell>AP m</cell><cell>75</cell></row><row><cell>NAT-Mini</cell><cell>77 M</cell><cell>704 G</cell><cell>27.8</cell><cell>50.3</cell><cell cols="2">68.9</cell><cell cols="2">54.9</cell><cell>43.6</cell><cell>66.4</cell><cell>47.2</cell></row><row><cell>Swin-Tiny [18]</cell><cell>86 M</cell><cell>745 G</cell><cell>25.1</cell><cell>50.4</cell><cell cols="2">69.2</cell><cell cols="2">54.7</cell><cell>43.7</cell><cell>66.6</cell><cell>47.3</cell></row><row><cell>ConvNeXt-Tiny [19]</cell><cell>86 M</cell><cell>741 G</cell><cell>27.3</cell><cell>50.4</cell><cell cols="2">69.1</cell><cell cols="2">54.8</cell><cell>43.7</cell><cell>66.5</cell><cell>47.3</cell></row><row><cell>NAT-Tiny</cell><cell>85 M</cell><cell>737 G</cell><cell>24.9</cell><cell>51.4</cell><cell cols="2">70.0</cell><cell cols="2">55.9</cell><cell>44.5</cell><cell>67.6</cell><cell>47.9</cell></row><row><cell>Swin-Small [18]</cell><cell>107 M</cell><cell>838 G</cell><cell>20.3</cell><cell>51.9</cell><cell cols="2">70.7</cell><cell cols="2">56.3</cell><cell>45.0</cell><cell>68.2</cell><cell>48.8</cell></row><row><cell>ConvNeXt-Small [19]</cell><cell>108 M</cell><cell>827 G</cell><cell>23.0</cell><cell>51.9</cell><cell cols="2">70.8</cell><cell cols="2">56.5</cell><cell>45.0</cell><cell>68.4</cell><cell>49.1</cell></row><row><cell>NAT-Small</cell><cell>108 M</cell><cell>809 G</cell><cell>21.7</cell><cell>52.0</cell><cell cols="2">70.4</cell><cell cols="2">56.3</cell><cell>44.9</cell><cell>68.1</cell><cell>48.6</cell></row><row><cell>Swin-Base [18]</cell><cell>145 M</cell><cell>982 G</cell><cell>17.3</cell><cell>51.9</cell><cell cols="2">70.5</cell><cell cols="2">56.4</cell><cell>45.0</cell><cell>68.1</cell><cell>48.9</cell></row><row><cell>ConvNeXt-Base [19]</cell><cell>146 M</cell><cell>964 G</cell><cell>19.5</cell><cell>52.7</cell><cell cols="2">71.3</cell><cell cols="2">57.2</cell><cell>45.6</cell><cell>68.9</cell><cell>49.5</cell></row><row><cell>NAT-Base</cell><cell>147 M</cell><cell>931 G</cell><cell>18.6</cell><cell>52.3</cell><cell cols="2">70.9</cell><cell cols="2">56.9</cell><cell>45.1</cell><cell>68.3</cell><cell>49.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Semantic segmentation performance on ADE20k. FLOPS are with respect to an input resolution of (2048, 512). Throughput is measured at the same resolution on a single NVIDIA A100 GPU.</figDesc><table><row><cell>Backbone</cell><cell cols="5"># Params FLOPs Throughput (FPS) mIoU mIoU(ms)</cell></row><row><cell>ResNet101 [13]</cell><cell>47 M</cell><cell>-</cell><cell>-</cell><cell>38.8</cell><cell>-</cell></row><row><cell>DeiT-S [26, 18]</cell><cell>52 M</cell><cell>1094 G</cell><cell>-</cell><cell>44.0</cell><cell>-</cell></row><row><cell>NAT-Mini</cell><cell>50 M</cell><cell>900 G</cell><cell>24.5</cell><cell>45.1</cell><cell>46.4</cell></row><row><cell>Swin-Tiny [18]</cell><cell>60 M</cell><cell>946 G</cell><cell>21.3</cell><cell>44.5</cell><cell>45.8</cell></row><row><cell>ConvNeXt-T [19]</cell><cell>60 M</cell><cell>939 G</cell><cell>23.3</cell><cell>46.0</cell><cell>46.7</cell></row><row><cell>NAT-Tiny</cell><cell>58 M</cell><cell>934 G</cell><cell>21.4</cell><cell>47.1</cell><cell>48.4</cell></row><row><cell>Swin-Small [18]</cell><cell>81 M</cell><cell>1040 G</cell><cell>17.0</cell><cell>47.6</cell><cell>49.5</cell></row><row><cell>ConvNeXt-Small [19]</cell><cell>82 M</cell><cell>1027 G</cell><cell>19.1</cell><cell>48.7</cell><cell>49.6</cell></row><row><cell>NAT-Small</cell><cell>82 M</cell><cell>1010 G</cell><cell>17.9</cell><cell>48.0</cell><cell>49.5</cell></row><row><cell>Swin-Base [18]</cell><cell>121 M</cell><cell>1188 G</cell><cell>14.6</cell><cell>48.1</cell><cell>49.7</cell></row><row><cell>ConvNeXt-Base [19]</cell><cell>122 M</cell><cell>1170 G</cell><cell>16.4</cell><cell>49.1</cell><cell>49.9</cell></row><row><cell>NAT-Base</cell><cell>123 M</cell><cell>1137 G</cell><cell>15.6</cell><cell>48.5</cell><cell>49.7</cell></row><row><cell cols="3">more efficient. NAT-Small outperforms Swin-Small on</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">single-scale performance, while matching the multi-scale</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">performance. NAT-Base similarly performs on-par with</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Swin-Base, while falling slightly short of ConvNeXt-Base.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :Table 8 :</head><label>78</label><figDesc>Ablation study on Neighborhood Attention vs Window Self Attention from Swin. Ablation study on NAT, with Swin-T as the baseline. Through overlapping convolutions, and our NAT configuration, we boost Swin classification accuracy significantly with fewer parameters and FLOPS. Swapping SWSA with NA results in an improvement of almost 0.5% in accuracy, while swapping it with SASA results in a slight decrease in accuracy.</figDesc><table><row><cell>Attention</cell><cell></cell><cell></cell><cell cols="2">Positional biases</cell><cell cols="2">Top-1</cell><cell># Params FLOPs</cell></row><row><cell cols="3">Shifted Window Self Attention [18]</cell><cell>None</cell><cell></cell><cell cols="3">80.1% (+ 0.0%) 28.26 M 4.51 G</cell></row><row><cell cols="2">Neighborhood Attention</cell><cell></cell><cell>None</cell><cell></cell><cell cols="3">80.6% (+ 0.5%) 28.26 M 4.51 G</cell></row><row><cell cols="2">Window Self Attention [18]</cell><cell></cell><cell cols="5">Relative Pos. Bias. 80.2% (+ 0.0%) 28.28 M 4.51 G</cell></row><row><cell cols="8">Shifted Window Self Attention [18] Relative Pos. Bias. 81.3% (+ 1.1%) 28.28 M 4.51 G</cell></row><row><cell cols="3">Stand Alone Self Attention [23]</cell><cell cols="5">Relative Pos. Bias. 81.6% (+ 0.3%) 28.28 M 4.51 G</cell></row><row><cell cols="2">Neighborhood Attention</cell><cell></cell><cell cols="5">Relative Pos. Bias. 81.8% (+ 0.5%) 28.28 M 4.51 G</cell></row><row><cell>Model</cell><cell cols="7">Attention Tokenizer Downsampler Layers Heads MLP Ratio Top-1 # Params FLOPs</cell></row><row><cell>Swin-T</cell><cell cols="2">SWSA [18] Patch</cell><cell>Patch</cell><cell></cell><cell>2,2,6,2</cell><cell>3</cell><cell>4</cell><cell>81.29% 28.3 M 4.5 G</cell></row><row><cell>Overlapping Swin-T</cell><cell cols="2">SWSA [18] Conv</cell><cell>Conv</cell><cell></cell><cell>2,2,6,2</cell><cell>3</cell><cell>4</cell><cell>81.78% 30.3 M 4.9 G</cell></row><row><cell cols="3">NAT-like Overlapping Swin-T SWSA [18] Conv</cell><cell>Conv</cell><cell cols="3">3,4,18,5 2</cell><cell>3</cell><cell>82.72% 27.9 M 4.3 G</cell></row><row><cell>NAT-T (zero padding)</cell><cell cols="2">SASA [23] Conv</cell><cell>Conv</cell><cell cols="3">3,4,18,5 2</cell><cell>3</cell><cell>82.54% 27.9 M 4.3 G</cell></row><row><cell>NAT-T</cell><cell>NA</cell><cell>Conv</cell><cell>Conv</cell><cell cols="3">3,4,18,5 2</cell><cell>3</cell><cell>83.20% 27.9 M 4.3 G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>NA and SWSA memory usage comparison with the two configurations: Swin's and NAT's. Memory usage is the peak CUDA memory usage for a single mini batch of size 256. Throughput is estimated calculated by running each method on the ImageNet validation set with the same batch size at full precision.</figDesc><table><row><cell>Model</cell><cell cols="9">Attention Tokenizer Downsampler Layers Heads MLP Ratio Top-1 Throughput (imgs/sec) Memory (GB)</cell></row><row><cell>Swin-T</cell><cell cols="2">SWSA [18] Patch</cell><cell>Patch</cell><cell>2,2,6,2</cell><cell>3</cell><cell>4</cell><cell>81.29%</cell><cell>1730</cell><cell>4.8</cell></row><row><cell>Swin-Like NAT-T</cell><cell>NA</cell><cell>Patch</cell><cell>Patch</cell><cell>2,2,6,2</cell><cell>3</cell><cell>4</cell><cell>81.80%</cell><cell>2021</cell><cell>4.0</cell></row><row><cell cols="3">NAT-like Overlapping Swin-T SWSA [18] Conv</cell><cell>Conv</cell><cell cols="2">3,4,18,5 2</cell><cell>3</cell><cell>82.72%</cell><cell>1320</cell><cell>3.0</cell></row><row><cell>NAT-T</cell><cell>NA</cell><cell>Conv</cell><cell>Conv</cell><cell cols="2">3,4,18,5 2</cell><cell>3</cell><cell>83.20%</cell><cell>1541</cell><cell>2.5</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement:</head><p>We thank Picsart AI Research (PAIR), Meta/Facebook AI, and IARPA for their generous support to make this work possible.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark</title>
		<idno>2020. 7</idno>
		<ptr target="https://github.com/open-mmlab/mmsegmentation" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09681</idno>
		<title level="m">Cross-covariance image transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Escaping the big data paradigm with compact transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Walton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abulikemu</forename><surname>Abuduweili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05704</idno>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A convnet for the 2020s</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stand-alone selfattention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR, 2019. 1</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17239</idno>
		<title level="m">Alexandre Sablayrolles, Gabriel Synnaeve, and Herv? J?gou. Going deeper with image transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Scaling local self-attention for parameter efficient visual backbones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="418" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokensto-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13112</idno>
		<title level="m">Vision outlooker for visual recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Multi-scale vision longformer: A new vision transformer for high-resolution image encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15358</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="633" to="641" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
