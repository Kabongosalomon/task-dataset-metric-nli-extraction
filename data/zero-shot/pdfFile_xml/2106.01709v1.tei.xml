<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SIRE: Separate Intra-and Inter-sentential Reasoning for Document-level Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Zeng</surname></persName>
							<email>zengs@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">The MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Software and Microelectronics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">The MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Wangxuan Institute of Computer Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">The MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SIRE: Separate Intra-and Inter-sentential Reasoning for Document-level Relation Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Document-level relation extraction has attracted much attention in recent years. It is usually formulated as a classification problem that predicts relations for all entity pairs in the document. However, previous works indiscriminately represent intra-and inter-sentential relations in the same way, confounding the different patterns for predicting them. Besides, they create a document graph and use paths between entities on the graph as clues for logical reasoning. However, not all entity pairs can be connected with a path and have the correct logical reasoning paths in their graph. Thus many cases of logical reasoning cannot be covered. This paper proposes an effective architecture, SIRE, to represent intra-and inter-sentential relations in different ways. We design a new and straightforward form of logical reasoning module that can cover more logical reasoning chains. Experiments on the public datasets show SIRE outperforms the previous state-of-the-art methods. Further analysis shows that our predictions are reliable and explainable. Our code is available at https:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation Extraction (RE) is an important way of obtaining knowledge facts from natural language text. Many recent advancements <ref type="bibr" target="#b17">(Sahu et al., 2019;</ref><ref type="bibr" target="#b29">Yao et al., 2019b;</ref><ref type="bibr" target="#b12">Nan et al., 2020;</ref> manage to tackle the document-level relation extraction (doc-level RE) that extracts semantic relations among entities across multiple sentences. Due to its strong correlation with real-world scenarios, doc-level RE has attracted much attention in the field of information extraction.</p><p>The doc-level RE task is usually formulated as a classification problem that predicts possible rela-  <ref type="bibr" target="#b29">(Yao et al., 2019b)</ref> for illustration of intra-and inter-sentential relations. Sentence numbers, entity mentions, and supporting evidence involved in these relation instances are colored. Other mentions are underlined for clarity. tions for all entity pairs, using the information from the entire document. It has two different kinds of relations: intra-sentential relation and inter-sentential relation. We show examples of these two kinds of relations in <ref type="figure" target="#fig_0">Figure 1</ref>. When two entities have mentions co-occurred in the same sentence, they may express intra-sentential relations. Otherwise, they may express inter-sentential relations.</p><p>Previous methods do not explicitly distinguish these two kinds of relations in the design of the model and use the same method to represent them. However, from the perspective of linguistics, intrasentential relations and inter-sentential relations are expressed in different patterns. For two intrasentential entities, their relations are usually expressed from local patterns within their co-occurred sentences. As shown in the first example in <ref type="bibr">Figure 1,</ref><ref type="bibr">(Polar Music,</ref><ref type="bibr">country of origin,</ref><ref type="bibr">Swedish)</ref> and (Wembley Arena, located in, London) can be inferred based solely on the sentence they reside in, i.e., sentences 1 and 6 respectively. Unlike intrasentential relations, inter-sentential relations tend to be expressed from the global interactions across multiple related sentences, also called supporting evidence. Moreover, cross-sentence relations usually require complex reasoning skills, e.g., logical reasoning. As shown in the second example in <ref type="figure" target="#fig_0">Figure 1</ref>, (S?o Paulo, continent, South America) can be inferred from the other two relation facts expressed in the document: (S?o Paulo, country, Brazil) and <ref type="bibr">(Brazil, continent, South America)</ref>. So the different patterns between intra-and intersentential relations show that it would be better for a model to treat intra-and inter-sentential relations differently. However, previous works usually use the information from the whole document to represent all relations, e.g., 13 sentences for predicting (Polar Music, country of origin, Swedish) in the first example in <ref type="figure" target="#fig_0">Figure 1</ref>. We argue that this will bring useless noises from unrelated sentences that misguide the learning of relational patterns.</p><p>Besides, previous methods  treat logical reasoning as a representation learning problem. They construct a document graph from the input document using entities as nodes. And the paths between two entities on their graphs, usually passing through other entities, could be regarded as clues for logical reasoning. However, since not all entity pairs can be connected with a path and have the correct logical reasoning paths available on the graph, many cases of logical reasoning cannot be covered. So their methods are somehow limited, and we should consider a new form of logical reasoning to better model and cover all possible reasoning chains.</p><p>In this paper, we propose a novel architecture called Separate Intraand inter-sentential REasoning (SIRE) for doc-level RE. Unlike previous works in this task, we introduce two different methods to represent intra-and inter-sentential relations respectively. For an intra-sentential relation, we utilize a sentence-level encoder to represent it in every co-occurred sentence. Then we get the final representation by aggregating the relational representations from all co-occurred sentences. This will encourage intra-sentential entity pairs to focus on the local patterns in their co-occurred sen-tences. For an inter-sentential relation, we utilize a document-level encoder and a mention-level graph proposed by  to capture the document information and interactions among entity mentions, document, and local context. Then, we apply an evidence selector to encourage intersentential entity pairs to selectively focus on the sentences that may signal their cross-sentence relations, i.e., finding supporting evidence. Finally, we develop a new form of logical reasoning module where one relation instance can be modeled by attentively fusing the representations of other relation instances in all possible logical chains. This form of logical reasoning could cover all possible cases of logical reasoning in the document.</p><p>Our contributions can be summarized as follows:</p><p>? We propose an effective architecture called SIRE that utilizes two different methods to represent intra-sentential and inter-sentential relations for doc-level RE.</p><p>? We come up with a new and straightforward form of logical reasoning module to cover all cases of logical reasoning chains.</p><p>We evaluate our SIRE on three public doc-level RE datasets. Experiments show SIRE outperforms the previous state-of-the-art models. Further analysis shows SIRE could produce more reliable and explainable predictions which further proves the significance of the separate encoding. </p><formula xml:id="formula_0">i } l i=1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Intra-and Inter-sentential Relation Representation Module</head><p>As is discussed in Sec. 1, for two intra-sentential entities, their relations are usually determined by the local patterns from their co-occurred sentences, while for two inter-sentential entities, their relations are usually expressed across multiple related sentences that can be regarded as the supporting evidence for their relations. So in this module, we utilize two different methods to represent intrasentential and inter-sentential relations separately.  <ref type="figure">Figure 2</ref>: The architecture of SIRE. In the mention-level graph, the number in each circle is its sentence number. Mention nodes with the same color belong to the same entity. Different types of edges are in different styles of line. Our model uses different methods to represent intra-and inter-sentential relations and the self-attention mechanism to model the logical reasoning process. We use the logical reasoning chain:e A ? e B ? e C for illustration.</p><p>Our methods encourage intra-sentential entity pairs to focus on their co-occurred sentences as much as possible and encourage inter-sentential entity pairs to selectively focus on the sentences that may express their cross-sentence relations. We use three parts to represent the relation between two entities: head entity representation, tail entity representation and context representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Intra-sentential Relation Representation Module</head><p>Encoding. We use a sentence-level encoder to capture the context information for intra-sentential relations and produce contextualized word embedding for each word. Formally, we convert the i-th sentence S i containing n i words w S i j n i j=1 into a sequence of vectors g S i j n i j=1 .</p><p>For each word w in S i , we first concatenate its word embedding with entity type embedding and co-reference embedding 1 :</p><formula xml:id="formula_1">x = [E w (w); E t (t); E c (c)]<label>(1)</label></formula><p>where E w (?) , E t (?) and E c (?) denote the word embedding layer, entity type embedding layer and 1 The existing doc-level RE datasets annotate which mentions belong to the same entity. So for each word in the document, it may belong to the i-th entity or non-entity in the document. We embed this co-reference information between entity mention (surface words) and entity (an abstract concept) into the initialized representation of a word. co-reference embedding layer, respectively. t and c are named entity type and entity id. <ref type="bibr">2</ref> Then the vectorized word representations are fed into the sentence-level encoder to obtain the sentence-level context-sensitive representation for each word:</p><formula xml:id="formula_2">[g S i 1 , . . . , g S i n i ] = f S enc ([x S i 1 , . . . , x S i n i ])<label>(2)</label></formula><p>where the f S enc denotes sentence-level encoder, which can be any sequential encoder. We will also get the sentence representation s S i for sentence S i from this encoder. For LSTM, s S i is the hidden state of the last time step; for BERT, s S i is the output representation of the special marker <ref type="bibr">[CLS]</ref>. Representing. For i-th entity pair (e i,h , e i,t ) which expresses intra-sentential relations, where e i,h is the head entity and e i,t is the tail entity, their mentions co-occur in C sentences S co?occur = {S i 1 , S i 2 , . . . , S i C } once or many times. In j-th co-occurred sentence S i j , we use the entity mentions in S i j to represent head and tail entity. And we define that the context representation of this relation instance in S i j is the top K words correlated with the relations of these two mentions.</p><p>Specifically, head entity mention ranging from sth to t-th word is represented as the average of the words it contains: e</p><formula xml:id="formula_3">S i j i,h = 1 t?s+1 t k=s g S i j</formula><p>k , so is the tail entity mention e S i j i,t . Then, we concatenate the representations of head and tail entity mentions and use it as a query to attend all words in S i j and compute relatedness score for each word in S i j :</p><formula xml:id="formula_4">s i,k = ?((W intra ? [e S i j i,h ; e S i j i,t ]) T ? g S i j k ) (3) ? i,k = Sof tmax(s i,k )<label>(4)</label></formula><p>where [?; ?] is a concatenation operation. W intra ? R d?2d is a parameter matrix. ? is an activation function (e.g., ReLU). Then, we average the representations of top K related words to represents the context information c i for intra-sentential entity pair (e i,h , e i,t ) in S i j . In order to make W intra trainable during computing gradient, we also add an item which is the weighted average representation of all words:</p><formula xml:id="formula_5">c S i j i = ?? 1 K k?topK(? i, * ) g S i j k +(1??)? n i j t ? i,t g S i j t (5)</formula><p>where ? is a hyperparameter and we use 0.9 here to force model to focus on the top K words but still consider the subtle influence from other words.</p><p>Next, we concatenate the three parts obtained above to form the relational representation of intrasentential entity pair (e i,h , e i,t ) in S i j and further average the representations in all co-occured sentences S co?occur to get our final relation representation r i for intra-sentential entity pair (e i,h , e i,t ) 3 :</p><formula xml:id="formula_6">r i = 1 C S i j ?S co?occur [e S i j i,h ; e S i j i,t ; c S i j i ]<label>(6)</label></formula><p>This way, we could force the intra-sentential entity pairs to focus on the semantic information from their co-occurred sentences and ignore the noise information from other sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Inter-sentential Relation</head><p>Representation Module Encoding. According to the nature of intersentetential relation, we use a document-level encoder to capture the global interactions for intersentential relations and produce contextualized word embedding for each word. Formally, we convert a document D containing m words w D Same as the embedding for intra-sentential relations, we use Equation 1 to embed each word in the document. Then the vectorized word representations are fed into the document-level encoder to obtain document-level context-sensitive representation for each word:</p><formula xml:id="formula_7">[g D 1 , . . . , g D m ] = f D enc ([x D 1 , . . . , x D m )<label>(7)</label></formula><p>where f D enc denotes the document-level encoder. And we will also get the document representation d D from this encoder.</p><p>To further enhance the document interactions, we utilize the mention-level graph (MG) proposed by . MG in  contains two different nodes: mention node and document node. Each mention node denotes one particular mention of an entity. Furthermore, MG also has one document node that aims to model the document information. We argue that this graph only contains nodes concerning prediction, i.e., the mentions of the entities and document information. However, it does not contain the local context information, which is crucial for the interaction among entity mentions and the document. So we introduce a new type of node: sentence node and its corresponding new edges to infuse the local context information into MG.</p><p>So there are four types of edges 4 in MG: Intra-Entity Edge: Mentions referring to the same entity are fully connected. This models the interactions among mentions of the same entity. Inter-Entity Edge: Mentions co-occurring in the same sentence are fully connected. This models the interactions among different entities via cooccurrences of their mentions. Sentence-Mention Edge: Each sentence node connects with all entity mentions it contains. This models the interactions between mentions and their local context information. Sentence-Document Edge: All sentence nodes are connected to the document node. This models the interactions between local context information and document information, acting as a bridge between mentions and document.</p><p>Next, we apply Relational Graph Convolutional Network (R-GCN, Schlichtkrull et al., 2017) on MG to aggregate the features from neighbors for each node. Given node u at the l-th layer, the graph convolutional operation can be defined as:</p><formula xml:id="formula_8">h (l+1) u = ReLU ? ? t?T v?N t u {u} 1 c u,t W (l) t h (l) v ? ? (8) where T is a set of different types of edges, W (l) t ? R d?d is a trainable parameter matrix. N t</formula><p>u denotes a set of neighbors for node u connected with t-th type edge. c u,t = |N t u | is a normalization constant. We then aggregate the outputs of all R-GCN layers to form the final representation of node u:</p><formula xml:id="formula_9">m u = ReLU (W u ? [h (0) u ; h (1) u ; . . . ; h (N ) u ]) (9) where W u ? R d?N d is a trainable parameter ma- trix. h (0)</formula><p>u is the initial representation of node u. For a mention ranging from the s-th word to the t-th word in the document, h</p><formula xml:id="formula_10">(0) u = 1 t?s+1 t j=s g D j ;</formula><p>for i-th sentence node, it is initialized with s S i from sentence-level encoder; for the document node, it is initialized with d D from document-level encoder.</p><p>Representing. We argue that inter-sentential relations can be inferred from the following information sources: 1) the head and tail entities themselves; 2) the related sentences that signal their cross-sentence relations, namely supporting evidences; 3) reasoning information such as logical reasoning, co-reference reasoning, world knowledge, etc. We here only consider the first two information and leave the last in Sec. 2.2.</p><p>Different from intra-sentential relations, intersentential relations tend to be expressed from the global interactions. So for the i-th entity pair (e i,h , e i,t ) which expresses inter-sentential relation, the head entity representation e i,h and the tail entity representation and e i,t are defined as the average of their entity mentions from MG:</p><formula xml:id="formula_11">e i = 1 N j?M (e i ) m j<label>(10)</label></formula><p>where the M (e i ) is the mention set of e i . And we apply an evidence selector with attention mechanism <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref> to encourage the inter-sentential entity pair to selectively focus on the sentences that express their cross-sentence relations. This process could be regarded as finding supporting evidence for their relations. So the context representation c i for inter-sentential entity pair (e i,h , e i,t ) is the weighted average of the sentence representations from MG:</p><formula xml:id="formula_12">P (S k |e i,h , e i,t ) = ?(W k ? [e i,h ; e i,t ; m S k ]) (11) ? i,k = P (S k |e i,h , e i,t ) l P (S l |e i,h , e i,t )<label>(12)</label></formula><formula xml:id="formula_13">c i = l k ? i,k ? m S k<label>(13)</label></formula><p>where W k ? R 1?2d is a trainable parameter matrix. ? is a sigmoid function. Next, the final relation representation for intersentential entity pair (e i,h , e i,t ) should be:</p><formula xml:id="formula_14">r i = [e i,h ; e i,t ; c i ]<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Logical Reasoning Module</head><p>In this module, we focus on logical reasoning modeling. As mentioned in Sec. 1, previous works usually use the paths between each entity pair as the clues for logical reasoning. Furthermore, they concatenate the path representations with entity pair representations to predict relations. However, since not all entity pairs are connected with a path and have the correct logical reasoning paths in their graph, many cases of logical reasoning cannot be covered. So their methods are somehow limited.</p><p>In this paper, we utilize self-attention mechanism <ref type="bibr" target="#b22">(Vaswani et al., 2017)</ref> to model logical reasoning. Specifically, we can get the relational representations for all entity pairs from the above sections. For i-th entity pair (e h , e t ), we can assume there is a two-hop logical reasoning chains: e h ? e k ? e t in the document, where e k can be any other entities in the document except e h and e t . So (e h , e t ) can attend to all the relational representations of other entity pairs including (e h , e k ) and (e k , e t ), termed as R att . Finally, the weighted sum of R att can be treated as a new relational representation for (e h , e t ), which considers all possible two-hop logical reasoning chains in the document. 5</p><formula xml:id="formula_15">r new i = r k ?Ratt?{r i } ? k ? r k (15) ? k = Sof tmax((W att ? r i ) T ? r k )<label>(16)</label></formula><p>where W att ? R 3d?3d is a parameter matrix. In this way, the path in the previous works could be converted into the individual attention on every entity pair in the logical reasoning chains. We argue that this form of logical reasoning is simpler and more scalable because it will consider all possible logical reasoning chains without connectivity constraints in the graph structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Classification Module</head><p>We formulate the doc-level RE task as a multi-label classification task:</p><formula xml:id="formula_16">P (r|e i,h , e i,t ) = sigmoid (W 1 ?(W 2 r i + b 1 ) + b 2 ) (17) where W 1 , W 2 , b 1 , b 2 are trainable</formula><p>parameters, ? is an activation function (e.g., ReLU). We use binary cross entropy as objective to train our SIRE:</p><formula xml:id="formula_17">L rel = ? D?C h =t r i ?R I (r i = 1) log P (r i |e i,h , e i,t ) + I (r i = 0) log (1 ? P (r i |e i,h , e i,t ))<label>(18)</label></formula><p>where C denotes the whole corpus, R denotes relation type set and I (?) refers to indicator function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>We evaluate our proposed model on three document-level RE datasets: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Settings</head><p>In our SIRE implementation, we use 3 layers of GCN, use ReLU as our activation function, and set the dropout rate to 0.3, learning rate to 0.001. We train SIRE using AdamW <ref type="bibr" target="#b10">(Loshchilov and Hutter, 2019)</ref> as optimizer with weight decay 0.0001 and implement SIRE under PyTorch <ref type="bibr" target="#b13">(Paszke et al., 2017)</ref> and DGL <ref type="bibr" target="#b26">(Wang et al., 2019b)</ref> frameworks. We implement two settings for our SIRE. SIRE-GloVe uses GloVe (100d, <ref type="bibr" target="#b15">Pennington et al., 2014)</ref> and BiLSTM (512d, <ref type="bibr" target="#b19">Schuster and Paliwal, 1997)</ref> as word embedding and encoder, respectively. SIRE-BERT use BERT-base <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref> as encoder on DocRED, cased BioBERT-Base v1.1 as the encoder on CDR/GDA, and the learning rate for BERT parameters is set to 1e ?5 and learning rate for other parameters remains 1e ?3 . Detailed hyperparameter settings are in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Baselines and Evaluation Metrics</head><p>We use the following models as our baselines: <ref type="bibr" target="#b29">Yao et al. (2019b)</ref> propose the BiLSTM (Schuster and Paliwal, 1997) as the encoder on DocRED and use the output from the encoder to represent all entity pairs to predict relations. <ref type="bibr" target="#b25">Wang et al. (2019a)</ref> propose BERT to replace the BiLSTM as the encoder on DocRED. Moreover, they also propose BERT-Two-Step, which first predicts whether two entities have a relation and then predicts the specific target relation. <ref type="bibr" target="#b21">Tang et al. (2020)</ref> propose the hierarchical inference networks HIN-GloVe and HIN-BERT, which make full use of multi-granularity inference information including entity level, sentence level, and document level to infer relations.</p><p>Similar to <ref type="bibr" target="#b25">Wang et al. (2019a)</ref>, <ref type="bibr" target="#b30">Ye et al. (2020)</ref> propose a language representation model called CorefBERT as encoder on DocRED that can capture the coreferential relations in context. <ref type="bibr" target="#b12">Nan et al. (2020)</ref> propose the LSR-GloVe and LSR-BERT to dynamically induce the latent dependency tree structure to better model the document interactions for prediction.  propose a global-to-local network GLRE, which encodes the document information in terms of entity global and local representations as well as context relation representations.  propose the graph aggregationand-inference networks GAIN-GloVe and GAIN-  inter4intra denotes using the inter-sentential module also for intra-sentential entity pairs.</p><p>Model CDR GDA BRAN <ref type="bibr" target="#b23">(Verga et al., 2018)</ref> 62.1 -EoG  63.6 81.5 LSR <ref type="bibr" target="#b12">(Nan et al., 2020)</ref> 64.8 82.2 GLRE-BioBERT  68.5 -SIRE-BioBERT 70.8 84.7 BERT which utilize two levels of graph structures: mention-level graph and entity-level graph to capture document interactions and conduct path logical reasoning mechanism, respectively. <ref type="bibr" target="#b23">Verga et al. (2018)</ref> propose a self-attention encoder BRAN to consider interactions across mentions and relations across sentence boundaries.</p><p>Following the previous works <ref type="bibr" target="#b29">(Yao et al., 2019b;</ref>, we use the F1 and Ign F1 as the evaluation metrics to evaluate the overall performance of a model. The Ign F1 metric calculates F1 excluding the common relation facts in the training and dev/test sets. We also use the intra-F1 and inter-F1 metrics to evaluate a model's performance on intra-sentential relations and inter-sentential relations on the dev set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results</head><p>The performances of SIRE and baseline models on the DocRED dataset are shown in <ref type="table" target="#tab_4">Table 1</ref>. Among the model not using BERT encoding, SIRE outperforms the previous state-of-the-art model by 0.88/1.38 F1/Ign F1 on the test set. Among the model using BERT encoding, SIRE outperforms the previous state-of-the-art models by 1.18/0.81 F1/Ign F1 on the test set. The improvement on Ign F1 is larger than that on F1. This shows SIRE has a stronger generalization ability on the unseen relation instances. On intra-F1 and inter-F1, we can observe that SIRE is better than the previous models that indiscriminately represent the intra-and intersentential relations in the same way. This demonstrates that representing intra-and inter-sentential relations in different methods is better than representing them in the same way. The improvement on intra-F1 is greater than the improvement on inter-F1. This shows that SIRE mainly improves the performance of intra-sentential relations. The performances of SIRE and baseline models on the CDR/GDA dataset are shown in <ref type="table" target="#tab_5">Table 2</ref>, which are consistent with the improvement on DocRED.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Ablation Study</head><p>To further analyze SIRE, we also conduct ablation studies to illustrate the effectiveness of different modules in SIRE. We show the results in <ref type="table" target="#tab_4">Table 1</ref>. 1) the importance of the logical reasoning module: When we discard the logical reasoning module, the performance of SIRE-GloVe decreases by 0.41 F1 on the DocRED test set. This shows the effectiveness of our logical reasoning module, which can better model the reasoning information in the document. Moreover, it drops significantly on inter-F1 and drops fewer points on intra-F1. This shows our logical reasoning module mainly improves the performance of the inter-sentential relations that usually require reasoning skills. 2) Ablation on context representations in Eq. 6 and Eq. 14: When we remove the context representations in intra-and inter-sentential relational representations, the performance of SIRE-GloVe on the DocRED test set drops by 1.81 F1. This shows context information (top K words for intra, evidence sentences for inter) is important for both intra-and inter-sentential relation representation.</p><p>3) Using the inter-sentential module also for intra-sentential entity pairs: In this experiment, we do not distinguish these two types of relations, using the encoding method for inter-sentential to encode all entity pairs, and remain the logical reasoning module unchanged. The performance of SIRE-GloVe drops by 2.66/2.13 F1/intra-F1 on the DocRED test set. This confirms the motivation that we cannot use global information to learn the local patterns for intra-sentential relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Reasoning Performance</head><p>Furthermore, we evaluate the reasoning ability of our model on the development set in <ref type="table" target="#tab_7">Table 3</ref>. We use infer-F1 as the metric that considers only twohop positive relation instances in the dev set. So it will naturally exclude many cases that do not belong to the two-hop logical reasoning process to strengthen the evaluation of reasoning performance. As <ref type="table" target="#tab_7">Table 3</ref> shows, SIRE is superior to previous models in handling the two-hop logical reasoning process. Moreover, after removing the logical reasoning module, out SIRE drops signif-  icantly on infer-F1. This shows that our logical reasoning module plays a crucial role in modeling the logical reasoning process. <ref type="figure">Figure 3</ref> shows the prediction cases of our SIRE. In intra-sentential relations, the top 4 words related to the relations of three entity pairs conform with our intuition. Our model correctly find the words by using Eq.5 that trigger the relations of these entity pairs. In inter-sentential relations, the supporting evidence that the model finds, i.e., sentences 1 and 2, indeed expresses the relations between S?o Paul and South America. We also conduct logical reasoning in terms of the logical reasoning chains: S?o Paul? other-entity ? South America. Our SIRE could focus on the correct logical reasoning chains: S?o Paul? Brazil ? South America. These cases show the predictions of SIRE are explainable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Case Study</head><p>Document-level relation extraction. Many recent efforts <ref type="bibr" target="#b14">Peng et al., 2017;</ref><ref type="bibr" target="#b5">Gupta et al., 2019;</ref><ref type="bibr" target="#b20">Song et al., 2018;</ref><ref type="bibr" target="#b6">Jia et al., 2019;</ref><ref type="bibr" target="#b29">Yao et al., 2019b;</ref><ref type="bibr" target="#b25">Wang et al., 2019a;</ref><ref type="bibr" target="#b21">Tang et al., 2020;</ref><ref type="bibr" target="#b12">Nan et al., 2020;</ref><ref type="bibr" target="#b2">Dai et al., 2020)</ref> manage to tackle the document-level relation extraction. Most of them use graph-based models, such as Graph Convolutional Networks <ref type="bibr">(GCNs, Kipf and Welling, 2017;</ref><ref type="bibr" target="#b18">Schlichtkrull et al., 2017)</ref> that has been used in many natural language processing tasks <ref type="bibr" target="#b11">(Marcheggiani and Titov, 2017;</ref><ref type="bibr" target="#b28">Yao et al., 2019a;</ref>. They construct a graph structure from the input document. This graph uses the word, mentions or entities as nodes and uses heuristic rules and semantic dependencies as edges. They use this graph to model document information and interactions and to predict possible relations for all entity pairs. <ref type="bibr" target="#b12">Nan et al. (2020)</ref> proposed a latent structure induction to induce the dependency tree in the document dynamically.  proposed a double graph-based graph aggregationand-inference network that constructs two graphs: mention-level graph and entity-level graph. They use the former to capture the document information and interactions among entity mentions and document and use the latter to conduct path-based logical reasoning. However, these works do not explicitly distinguish the intra-and inter-sentential relation instances in the design of the model and use the same way to encode them. So the most significant difference between our model and previous models is that we treat intra-sentential and intersentential relations differently to conform with the relational patterns for their prediction.</p><p>Reasoning in relation extraction. Reasoning problem has been extensively studied in the field of question answering <ref type="bibr" target="#b4">(Dhingra et al., 2018)</ref>. However, few works manage to tackle this problem in the document-level relation extraction task.  is the first to propose the explicit way of relational reasoning on doc-level RE, which mainly focuses on logical reasoning. They use the paths on their entity-level graph to provide clues for logical reasoning. However, since not all entity pairs are connected with a path and have the correct logical reasoning paths in their graph, their methods are somehow limited. In this work, we design a new form of logical reasoning to cover more cases of logical reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Intra-and inter-sentential relations are two types of relations in doc-level RE. We propose a novel architecture, SIRE, to represent these two relations in different ways separately in this work. We introduce a new form of logical reasoning module that models logical reasoning as a self-attention among representations of all entity pairs. Experiments show that our SIRE outperforms the previous state-of-the-art methods. The detailed analysis demonstrates that our predictions are explainable. We hope this work will have a positive effect on future research regarding new encoding schema, a more generalizable and explainable model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Hyperparameter settings</head><p>We use the development set to manually tune the optimal hyperparameters for SIRE, based on the Ign F1 score. Experiments are run on NVIDIA-RTX-3090-24GB GPU. Hyperparameter settings for SIRE-GloVe, SIRE-BERT on DocRED are listed in <ref type="table" target="#tab_9">Table 4</ref>, 5, respectively. The values of hyperparameters we finally adopted are in bold. Note that we do not tune all the hyperparameters.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Two examples from DocRED</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>a sequence of vectors g D j m j=1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Needs You" is a song performed by Australian recording artist and songwriter Kylie Minogue, taken from her seventh studio album Light Years (2000). Relation: performer Lark Force was an Australian Army formation established in March 1941 during World War II for service in New Britain and New Ireland. Relation: inception Lake Hiawatha is one of the few lakes through which Minnehaha Creek flows , and the last one before it reaches Minnehaha Falls and then the Mississippi River. Relation: mouth of the watercourse Inter-sentential relation instances [1] (0.87) IBM Research-Brazil is one of twelve research laboratories comprising IBM Research, its first in South America. [2] (0.66) It was established in June 2010, with locations in S?o Paulo and Rio de Janeiro. [3] (0.01) Research focuses on Industrial Technology and Science, ? [4] (0.04) The new lab, IBM's ninth ? [5] (0.38) In collaboration with Brazil's government, it will help IBM to develop ? Relation: continent Logical reasoning attention weight: (S?o Paulo, Brazil) 0.32 (S?o Paulo, June 2010) 0.03 ? (Brazil, South America) 0.45 (June 2010, South America) 0.02 ?Figure 3: Cases for illustrating the reliable and explainable predictions of our SIRE. Head entities, tail entities, and sentence numbers along with the scores from evidence selector are colored in blue, red, green, respectively. In intra-sentential relations, words with pink background color are the top 4 words from Equation 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>We use standard split of the dataset, 23, 353 documents for training, 5, 839 for development, and 1, 000 for testing.</figDesc><table><row><cell>DocRED: The largest human-annotated document-</cell></row><row><cell>level relation extraction dataset was proposed by</cell></row><row><cell>Yao et al. (2019b). It is constructed from Wikipedia</cell></row><row><cell>and Wikidata and contains 96 types of relations,</cell></row><row><cell>132, 275 entities, and 56, 354 relational facts in to-</cell></row><row><cell>tal. Documents in DocRED have about 8 sentences</cell></row><row><cell>on average. More than 40.7% relation facts can</cell></row><row><cell>only be extracted from multiple sentences. 61.1%</cell></row><row><cell>relation instances require various reasoning skills</cell></row><row><cell>such as logical reasoning. 93.4% intra-sentential</cell></row><row><cell>relations can be inferred based solely on their co-</cell></row><row><cell>occurred sentences. We show two examples from</cell></row><row><cell>DocRED in Figure 1. We follow the standard split</cell></row><row><cell>of the dataset, 3, 053 documents for training, 1, 000</cell></row><row><cell>for development, and 1, 000 for testing.</cell></row><row><cell>CDR (BioCreative V): The Chemical-Disease Re-</cell></row><row><cell>actions dataset was created by Li et al. (2016) man-</cell></row><row><cell>ually. It contains one type of relation: Chemical-</cell></row><row><cell>Induced-Disease between chemical and disease en-</cell></row><row><cell>tities. We follow the standard split of the dataset,</cell></row><row><cell>500 documents for training, 500 for development,</cell></row><row><cell>and 500 for testing.</cell></row></table><note>GDA (DisGeNet): The Gene-Disease-Associations dataset was introduced by Wu et al. (2019). It con- tains one type of relation: Gene-Induced-Disease between gene and disease entities.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Performance on DocRED. Models above the double line do not use pre-trained model. LR Module is the logical reasoning module. context denotes context representations in Eq. 6 and Eq. 14.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Performance on CDR and GDA.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Infer-F1 results on dev set of DocRED. P: Precision, R: Recall.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Settings for SIRE-GloVe.</figDesc><table><row><cell>Hyperparameter</cell><cell>Value</cell></row><row><cell>Batch Size</cell><cell>16, 32</cell></row><row><cell>Learning Rate</cell><cell>0.001</cell></row><row><cell>Activation Function</cell><cell>ReLU, Tanh</cell></row><row><cell>Positive v.s. Negative Ratio</cell><cell>1, 0.5, 0.25</cell></row><row><cell>Entity Type Embedding Size</cell><cell>128</cell></row><row><cell>Coreference Embedding Size</cell><cell>128</cell></row><row><cell>Dropout</cell><cell>0.3, 0.5, 0.7</cell></row><row><cell>Layers of GCN</cell><cell>1, 2, 3</cell></row><row><cell>GCN Hidden Size</cell><cell>1024</cell></row><row><cell>Weight Decay</cell><cell>0.0001</cell></row><row><cell>?</cell><cell>0.9</cell></row><row><cell>Numbers of Parameters</cell><cell>307M</cell></row><row><cell>Training Time</cell><cell>24 hours</cell></row><row><cell>Hyperparameter Search Trials</cell><cell>30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Settings for SIRE-BERT.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For those words not belonging to any entity, we introduce None entity type and id.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">If a head entity mentioned N times in a sentence, we will get N intra-sentential relational representations for each of the other tail entities in this sentence.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Note that we remove the mention-document edges of original MG in and substitute them by introducing mention-sentence and sentence-document edges.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">This can be scaled to muti-hop logical reasoning by increasing the self-attention layers. We only consider two-hop logical reasoning in this paper following.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the reviewers for their thoughtful and constructive comments. This paper is supported by the National Key R&amp;D Program of China under Grand No.2018AAA0102003, the National Science Foundation of China under Grant No.61936012 and 61876004.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Connecting the dots: Document-level neural relation extraction with edge-oriented graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenia</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1498</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4925" to="4936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Coarse-to-fine entity representations for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.02507</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural models for reasoning over multiple mentions using coreference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2007</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="42" to="48" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural relation extraction within and across sentence boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pankaj</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subburam</forename><surname>Rajaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Runkler</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33016513</idno>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6513" to="6520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Document-level n-ary relation extraction with multiscale representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1370</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3693" to="3704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Biocreative V CDR task corpus: a resource for chemical disease relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueping</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Sciaky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Hsuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><forename type="middle">Peter</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><forename type="middle">J</forename><surname>Mattingly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">C</forename><surname>Wiegers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1093/database/baw068</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Database</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Evaluating text coherence at sentence and paragraph levels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sennan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Language Resources and Evaluation Conference</title>
		<meeting>the 12th Language Resources and Evaluation Conference</meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1695" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Encoding sentences with graph convolutional networks for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1159</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1506" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reasoning with latent structure refinement for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoshun</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Sekulic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.141</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1546" to="1557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS-W</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cross-sentence n-ary relation extraction with graph LSTMs. Transactions of the Association for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="101" to="115" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction beyond the sentence boundary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1171" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inter-sentence relation extraction with document-level graph convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenia</forename><surname>Sunil Kumar Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ananiadou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4309" to="4316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael Sejr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06103</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
		<idno type="DOI">10.1109/78.650093</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">N-ary relation extraction using graphstate LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1246</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2226" to="2235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">HIN: hierarchical inference network for documentlevel relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengzhu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangxia</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Yin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-47426-3_16</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining -24th Pacific-Asia Conference</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-05-11" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="197" to="209" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Simultaneously self-attending to all mentions for full-abstract biological relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1080</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="872" to="884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Global-to-local neural networks for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Difeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ermei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.303</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3711" to="3721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fine-tune bert for docred with two-step process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christfried</forename><surname>Focke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Sylvester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nilesh</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11898</idno>
	</analytic>
	<monogr>
		<title level="m">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mufei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Deep graph library: Towards efficient and scalable deep learning on graphs. ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">RENET: A deep learning approach for extracting gene-disease associations from literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hing-Fung</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak Wah</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lam</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-17083-7_17</idno>
	</analytic>
	<monogr>
		<title level="m">Research in Computational Molecular Biology -23rd Annual International Conference, RECOMB 2019</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-05" />
			<biblScope unit="volume">11467</biblScope>
			<biblScope unit="page" from="272" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengsheng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33017370</idno>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-01-27" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="7370" to="7377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">DocRED: A large-scale document-level relation extraction dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1074</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="764" to="777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Coreferential Reasoning Learning for Language Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaju</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.582</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7170" to="7186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Double graph based reasoning for documentlevel relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.127</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1630" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
