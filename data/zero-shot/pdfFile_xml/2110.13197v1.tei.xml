<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Nested Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing Institute for General Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Purdue University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Nested Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural network (GNN)'s success in graph classification is closely related to the Weisfeiler-Lehman (1-WL) algorithm. By iteratively aggregating neighboring node features to a center node, both 1-WL and GNN obtain a node representation that encodes a rooted subtree around the center node. These rooted subtree representations are then pooled into a single representation to represent the whole graph. However, rooted subtrees are of limited expressiveness to represent a nontree graph. To address it, we propose Nested Graph Neural Networks (NGNNs). NGNN represents a graph with rooted subgraphs instead of rooted subtrees, so that two graphs sharing many identical subgraphs (rather than subtrees) tend to have similar representations. The key is to make each node representation encode a subgraph around it more than a subtree. To achieve this, NGNN extracts a local subgraph around each node and applies a base GNN to each subgraph to learn a subgraph representation. The whole-graph representation is then obtained by pooling these subgraph representations. We provide a rigorous theoretical analysis showing that NGNN is strictly more powerful than 1-WL. In particular, we proved that NGNN can discriminate almost all r-regular graphs, where 1-WL always fails. Moreover, unlike other more powerful GNNs, NGNN only introduces a constantfactor higher time complexity than standard GNNs. NGNN is a plug-and-play framework that can be combined with various base GNNs. We test NGNN with different base GNNs on several benchmark datasets. NGNN uniformly improves their performance and shows highly competitive performance on all datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph is an important tool to model relational data in the real world. Representation learning over graphs has become a popular topic of machine learning in recent years. While network embedding methods, such as DeepWalk <ref type="bibr" target="#b0">[1]</ref>, can learn node representations well, they fail to generalize to whole-graph representations, which are crucial for applications such as graph classification, molecule modeling, and drug discovery. On the contrary, although traditional graph kernels <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref> can be used for graph classification, they define graph similarity often in a heuristic way, which is not parameterized and lacks some flexibility to deal with features.</p><p>In this context, graph neural networks (GNNs) have regained people's attention and become the state-of-the-art graph representation learning tool <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>. GNNs use message passing to propagate features between connected nodes. By iteratively aggregating neighboring node features to the center node, GNNs learn node representations encoding their local structure and feature information. These node representations can be further pooled into a graph representation, enabling graph-level tasks such as graph classification. In this paper, we will use message passing GNNs to denote this class <ref type="figure" target="#fig_0">Figure 1</ref>: The two original graphs G1 and G2 are non-isomorphic. G1 is composed of two triangles, while G2 is a hexagon. However, both 1-WL and message passing GNNs cannot differentiate them, since all nodes in the two graphs share identical rooted subtrees at any height (see the rooted subtrees around v1 and v2 in the middle block for example). In comparison, we can discriminate the two graphs by comparing their height-1 rooted subgraphs around any nodes. For example, the height-1 rooted subgraph around v1 is a closed triangle, but the height-1 rooted subgraph around v2 is an open triangle (see the red boxes in the right block).</p><p>of GNNs based on repeated neighbor aggregation <ref type="bibr" target="#b17">[18]</ref>, in order to distinguish them from some high-order GNN variants <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref> where the effective message passing happens between high-order node tuples instead of nodes.</p><p>GNNs' message passing scheme mimics the 1-dimensional Weisfeiler-Lehman (1-WL) algorithm <ref type="bibr" target="#b21">[22]</ref>, which iteratively refines a node's color according to its current color and the multiset of its neighbors' colors. This procedure essentially encodes a rooted subtree around each node into its final color, where the rooted subtree is constructed by recursively expanding the neighbors of the root node. One critical reason for GNN's success in graph classification is because two graphs sharing many identical or similar rooted subtrees are more likely classified into the same class, which actually aligns with the inductive bias that two graphs are similar if they have many common substructures <ref type="bibr" target="#b22">[23]</ref>.</p><p>Despite this, rooted subtrees are still limited in terms of expressing all possible substructures that can appear in a graph. It is likely that two graphs, despite sharing a lot of identical rooted subtrees, are not similar at all because their other substructure patterns are not similar. Take the two graphs G 1 and G 2 in <ref type="figure" target="#fig_0">Figure 1</ref> as an example. If we apply 1-WL or a message passing GNN to them, the two graphs will always have the same representation no matter how many iterations/layers we use. This is because all nodes in the two graphs have identical rooted subtrees across all tree heights. However, the two graphs are quite different from a holistic perspective. G 1 is composed of two triangles, while G 2 is a hexagon. The intrinsic reason for such a failure is that rooted subtrees have limited expressiveness for representing general graphs, especially those with cycles.</p><p>To address this issue, we propose Nested Graph Neural Networks (NGNNs). The core idea is, instead of encoding a rooted subtree, we want the final representation of a node to encode a rooted subgraph (local h-hop subgraph) around it. The subgraph is not restricted to be of any particular graph type such as tree, but serves as a general description of the local neighborhood around a node. Rooted subgraphs offer much better representation power than rooted subtrees, e.g., we can easily discriminate the two graphs in <ref type="figure" target="#fig_0">Figure 1</ref> by only comparing their height-1 rooted subgraphs.</p><p>To represent a graph with rooted subgraphs, NGNN uses two levels of GNNs: base (inner) GNNs and an outer GNN. By extracting a local rooted subgraph around each node, NGNN first applies a base GNN to each node's subgraph independently. Then, a subgraph pooling layer is applied to each subgraph to aggregate the intermediate node representations into a subgraph representation. This subgraph representation is used as the final representation of the root node. Rather than encoding a rooted subtree, this final node representation encodes the local subgraph around it, which contains more information than a subtree. Finally, all the final node representations are further fed into an outer GNN to learn a representation for the entire graph. <ref type="figure">Figure 2</ref> shows one NGNN implementation using message passing GNNs as the base GNNs and a simple graph pooling layer as the outer GNN.</p><p>One may wonder that the base GNN seems to still learn only rooted subtrees if it is message-passingbased. Then why is NGNN more powerful than GNN? One key reason lies in the subgraph pooling</p><formula xml:id="formula_0">E B C D A F B A F B C A F B C D F E C D E D F E B C A F E B C D A F</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Perform message passing within each subgraph</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extract a rooted subgraph around each node</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Use the outputs from inner GNNs as the final node representations of roots</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Apply subgraph pooling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inner (base) GNNs</head><p>Outer GNN</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph regression</head><p>Apply graph pooling to get a graph representation <ref type="figure">Figure 2</ref>: A particular implementation of the NGNN framework. It first extracts (copies) a rooted subgraph (height=1) around each node from the original graph, and then applies a base GNN with a subgraph pooling layer to each rooted subgraph independently to learn a subgraph representation. The subgraph representation is used as the root node's final representation in the original graph. Then, a graph pooling layer is used to summarize the final node representations into a graph representation. layer. Take the height-1 rooted subgraphs (marked with red boxes) around v 1 and v 2 in <ref type="figure" target="#fig_0">Figure 1</ref> as an example. Although v 1 and v 2 's height-1 rooted subtrees are still the same, their neighbors (labeled by 1 and 2) have different height-1 rooted subtrees. Thus, applying a one-layer message passing GNN plus a subgraph pooling as the base GNN is sufficient to discriminate G 1 and G 2 .</p><p>The NGNN framework has multiple exclusive advantages. Firstly, it allows freely choosing the base GNN, and can enhance the base GNN's representation power in a plug-and-play fashion. Theoretically, we proved that NGNN is more powerful than message passing GNNs and 1-WL by being able to discriminate almost all r-regular graphs (where 1-WL always fails). Secondly, by extracting rooted subgraphs, NGNN allows augmenting the initial features of a node with subgraphspecific structural features such as distance encoding <ref type="bibr" target="#b23">[24]</ref> to improve the quality of the learned node representations. Thirdly, unlike other more powerful graph neural networks, especially those based on higher-order WL tests <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b24">25]</ref>, NGNN still has linear time and space complexity w.r.t. graph size like standard message passing GNNs, thus maintaining good scalability. We demonstrate the effectiveness of the NGNN framework in various synthetic/real-world graph classification/regression datasets. On synthetic datasets, NGNN demonstrates higher-than-1-WL expressive power, matching very well with our theorem. On real-world datasets, NGNN consistently enhances a wide range of base GNNs' performance, achieving highly competitive results on all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notation and problem definition</head><p>We consider the graph classification/regression problem. Given a graph G = (V, E) where V = {1, 2, . . . n} is the node set and E ? V ? V is the edge set, we aim to learn a function mapping G to its class or target value y. The nodes and edges in G can have feature vectors associated with them, denoted by x i (for node i) and e ij (for edge (i, j)), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Weisfeiler-Lehman test</head><p>The Wesfeiler-Lehman (1-WL) test <ref type="bibr" target="#b21">[22]</ref> is a popular algorithm for graph isomorphism checking. The classical 1-WL works as follows. At first, all nodes receive a color 1. Each node collects its neighbors' colors into a multiset. Then, 1-WL will update each node's color so that two nodes get the same new color if and only if their current colors are the same and they have identical multisets of neighbor colors. Repeat this process until the number of colors does not increase between two iterations. Then, 1-WL will return that two graphs are non-isomorphic if their node colors are different at some iteration, or fail to determine whether they are non-isomorphic. See <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26]</ref> for more details.</p><p>1-WL essentially encodes the rooted subtrees around each node at different heights into its color representations. <ref type="figure" target="#fig_0">Figure 1</ref> middle shows the rooted subtrees around v 1 and v 2 . Two nodes will have the same color at iteration h if and only if their height-h rooted subtrees are the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Nested Graph Neural Network</head><p>In this section, we introduce our Nested Graph Neural Network (NGNN) framework and theoretically demonstrate its higher representation power than message passing GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Limitations of the message passing GNNs</head><p>Most existing GNNs follow the message passing framework <ref type="bibr" target="#b17">[18]</ref>: given a graph G, each node's hidden state h t+1 v is updated based on its previous state h t v and the messages m t+1 v from its neighbors</p><formula xml:id="formula_1">h t+1 v = U t (h t v , m t+1 v ), where m t+1 v = u?N (v|G) M t (h t v , h t u , e vu ).<label>(1)</label></formula><p>Here M t , U t are the message and update functions at time stamp t, e vu is the feature of edge (v, u), and N (v|G) is the set of v's neighbors in graph G. The initial hidden states h 0 v are given by the raw node features x v . After T time stamps (iterations), the final node representations h T v are summarized into a whole-graph representation with a readout (pooling) function R (e.g., mean or sum):</p><formula xml:id="formula_2">h G = R({h T v |v ? G}).<label>(2)</label></formula><p>Such a message passing (or neighbor aggregation) scheme iteratively aggregates neighbor information into a center node's hidden state, making it encode a local rooted subtree around the node. The final node representations will contain both the local structure and feature information around nodes, enabling node-level tasks such as node classification. After a pooling layer, these node representations can be further summarized into a graph representation, enabling graph-level tasks. When there is no edge feature and the node features are from a countable space, it is shown that message passing GNNs are at most as powerful as the 1-WL test for discriminating non-isomorphic graphs <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>For an h-layer message passing GNN, it will give two nodes the same final representation if they have identical height-h rooted subtrees (i.e., both the structures and the features on the corresponding nodes/edges are the same). If two graphs have a lot of identical (or similar) rooted subtrees, they will also have similar graph representations after pooling. This insight is crucial for the success of modern GNNs in graph classification, because it aligns with the inductive bias that two graphs are similar if they have many common substructures. Such insight has also been used in designing the WL subtree kernel <ref type="bibr" target="#b6">[7]</ref>, a state-of-the-art graph classification method before GNNs.</p><p>However, message passing GNNs have several limitations. Firstly, rooted subtree is only one specific substructure. It is not general enough to represent arbitrary subgraphs, especially those with cycles due to the natural restriction of tree structure. Secondly, using rooted subtree as the elementary substructure results in a discriminating power bounded by the 1-WL test. For example, all n-node r-regular graphs cannot be discriminated by message passing GNNs. Thirdly, standard message passing GNNs do not allow using root-node-specific structural features (such as the distance between a node and the root node) to improve the quality of the learned root node's representation. We might need to break through such limitations in order to design more powerful GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The NGNN framework</head><p>To address the above limitations, we propose the Nested Graph Neural Network (NGNN) framework. NGNN no longer aims to encode a rooted subtree around each node. Instead, in NGNN, each node's final representation encodes the general local subgraph information around it more than a subtree, so that two graphs sharing a lot of identical or similar rooted subgraphs will have similar representations. To make a node's final representation encode a rooted subgraph, we need to compute a subgraph representation. To achieve this, we resort to an arbitrary GNN, which we call the base GNN of NGNN. For example, the base GNN can be simply a message passing GNN, which performs message passing within each rooted subgraph to learn an intermediate representation for every node of the subgraph, and then uses a pooling layer to summarize a subgraph representation from the intermediate node representations. This subgraph representation is used as the final representation of the root node in the original graph. Take root node w as an example. We first perform T rounds of message passing within node w's rooted subgraph G h w . Let v be any node appearing in G h w . We have node v's hidden state and message specific to rooted subgraph G h w at time stamp t + 1. Note that when node v attends different nodes' rooted subgraphs, its hidden states and messages will also be different. This is in contrast to standard GNNs where a node's hidden state and message at time t is the same regardless of which root node it contributes to. For example, h t+1 v and m t+1 v in Eq. 1 do not depend on any particular rooted subgraph.</p><formula xml:id="formula_3">h t+1 v,G h w = U t (h t v,G h w , m t+1 v,G h w ), where m t+1 v,G h w = u?N (v|G h w ) M t (h t v,G h w , h t u,G h w , e vu ).</formula><p>After T rounds of message passing, we apply a subgraph pooling layer to summarize a subgraph representation</p><formula xml:id="formula_4">h G h w from the intermediate node representations {h T v,G h w |v ? G h w }. h w : = h G h w = R 0 ({h T v,G h w |v ? G h w }),<label>(4)</label></formula><p>where R 0 is the subgraph pooling layer. This subgraph representation h G h w will be used as root node w's final representation h w in the original graph. Note that the base GNNs are simultaneously applied to all nodes' rooted subgraphs to return a final node representation for every node in the original graph, and all the base GNNs share the same parameters. With such node representations, NGNN uses an outer GNN to further process and aggregate them into a graph representation of the whole graph. For simplicity, we let the outer GNN be simply a graph pooling layer denoted by R 1 :</p><formula xml:id="formula_5">h G := R 1 ({h w |w ? G}).<label>(5)</label></formula><p>The Nested GNN framework can be understood as a two-level GNN, or a GNN of GNNs-the inner subgraph-level GNNs (base GNNs) are used to learn node representations from their rooted subgraphs, while the outer graph-level GNN is used to return a whole-graph representation from the inner GNNs' outputs. The inner GNNs all share the same parameters which are trained end-to-end with the outer GNN. <ref type="figure">Figure 2</ref> depicts the implementation of the NGNN framework described above.</p><p>Compared to message passing GNNs, NGNN changes the "receptive field" of each node from a rooted subtree to a rooted subgraph, in order to capture better local substructure information. The rooted subgraph is read by a base GNN to learn a subgraph representation. Finally, the outer GNN reads the subgraph representations output by the base GNNs to return a graph representation.</p><p>Note that, when we apply the base GNN to a rooted subgraph, this rooted subgraph is extracted (copied) out of the original graph and treated as a completely independent graph from the other rooted subgraphs and the original graph. This allows the same node to have different representations within different rooted subgraphs. For example, in <ref type="figure">Figure 2</ref>, the same node B appears in four different rooted subgraphs. Sometimes it is the root node, while other times it is a 1-hop neighbor of the root node. NGNN enables learning different representations for the same node when it appears in different rooted subgraphs, in contrast to standard GNNs where a node only has one single representation at one time stamp (Eq. 1). Similarly, NGNN also enables using different initial features for the same node when it appears in different rooted subgraphs. This allows us to customize a node's initial features based on its structural role within a rooted subgraph, as opposed to using the same initial features for a node across all rooted subgraphs. For example, we can optionally augment node B's initial features with the distance between node B and the root-when node B is the root node, we give it an additional feature 0; and when B is a k-hop neighbor of the root, we give it an additional feature k. Such feature augmentation may help better capture a node's structural role within a rooted subgraph. It is an exclusive advantage of NGNN and is not possible in standard GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The representation power of NGNN</head><p>We theoretically characterize the additional expressive power of NGNN (using message passing GNNs as base GNNs) as opposed to standard message passing GNNs. We focus on the ability to discriminate regular graphs because they form an important category of graphs which standard GNNs cannot represent well. Using 1-WL or message passing GNNs, any two n-sized r-regular graphs will have the same representation, unless discriminative node features are available. In contrast, we prove that NGNN can distinguish almost all pairs of n-sized r-regular graphs regardless of node features.</p><p>Definition 2. If the message passing (Eq. 3) and the two-level graph pooling (Eqs. 4,5) are all injective given input from a countable space, then the NGNN is called proper.</p><p>A proper NGNN always exists due to the representation power of fully-connected neural networks used for message passing and Deep Set for graph pooling <ref type="bibr" target="#b27">[28]</ref>. For all pairs of graphs that 1-WL can discriminate, there always exists a proper NGNN that can also discriminate them, because two graphs discriminated by 1-WL means they must have different multisets of rooted subtrees at some height h, while a rooted subtree is always included in a rooted subgraph with the same height.</p><p>Now we present our main theorem.  <ref type="formula" target="#formula_1">(1)</ref>) such pairs of graphs.</p><p>We include the proof in Appendix A. Theorem 1 has three implications. Firstly, since NGNN can discriminate almost all r-regular graphs where 1-WL always fails, it is strictly more powerful than 1-WL and message passing GNNs. Secondly, it implies that NGNN does not need to extract subgraphs with a too large height (about 1 2 log n log (r?1) ) to be more powerful. Moreover, NGNN is already powerful with very few layers, i.e., an arbitrarily small constant times log n log (r?1) (as few as 1 layer). This benefit comes from the subgraph pooling (Eq. 4), freeing us from using deep base GNNs. We further conduct a simulation experiment in Appendix D to verify Theorem 1 by testing how well NGNN discriminates r-regular graphs in practice. The results match almost perfectly with our theory.</p><p>Although NGNN is strictly more powerful than 1-WL and 2-WL (1-WL and 2-WL have the same discriminating power <ref type="bibr" target="#b19">[20]</ref>), it is unclear whether NGNN is more powerful than 3-WL. Our early-stage analysis shows both NGNN and 3-WL cannot discriminate strongly regular graphs with the same parameters <ref type="bibr" target="#b28">[29]</ref>. We leave the exact comparison between NGNN and 3-WL to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussion</head><p>Base GNN. NGNN is a general plug-and-play framework to increase the power of a base GNN. For the base GNN, we are not restricted to message passing GNNs as described in Section 3.2. For example, we can also use GNNs approximating the power of higher-dimensional WL tests, such as 1-2-3-GNN <ref type="bibr" target="#b18">[19]</ref> and PPGN/Ring-GNN <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>, as the base GNN. In fact, one limitation of these high-order GNNs is their O(n 3 ) complexity. Using the NGNN framework we can greatly alleviate this by applying the higher-order GNN to multiple small rooted subgraphs instead of the whole graph. Suppose a rooted subgraph has at most c nodes, then by applying a high-order GNN to all n rooted subgraphs, we can reduce the time complexity from O(n 3 ) to O(nc 3 ).</p><p>Complexity. We compare the time complexity of NGNN (using message passing GNNs as base GNNs) with a standard message passing GNN. Suppose the graph has n nodes with a maximum degree d, and the maximum number of nodes in a rooted subgraph is c. Each message passing iteration in a standard message passing GNN takes O(nd) operations. In NGNN, we need to perform message passing over all n nodes' rooted subgraphs, which takes O(n ? cd). We will keep c small (which can be achieved by using a small h) to improve NGNN's scalability. Additionally, a small c enables the base GNN to focus on learning local subgraph patterns.</p><p>In Appendix B, we discuss some other design choices of NGNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related work</head><p>Understanding GNN's representation power is a fundamental problem in GNN research. Xu et al. <ref type="bibr" target="#b26">[27]</ref> and Morris et al. <ref type="bibr" target="#b18">[19]</ref> first proved that the discriminating power of message passing GNNs is bounded by the 1-WL test, namely they cannot discriminate two non-isomorphic graphs that 1-WL fails to discriminate (such as r-regular graphs). Since then, there is increasing effort in enhancing GNN's discriminating power beyond 1-WL <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b24">25]</ref>. Many GNNs have been proposed to mimic higher-dimensional WL tests, such as 1-2-3-GNN <ref type="bibr" target="#b18">[19]</ref>, Ring-GNN <ref type="bibr" target="#b20">[21]</ref> and PPGN <ref type="bibr" target="#b19">[20]</ref>. However, these models generally require learning the representations of all node tuples of certain cardinality (e.g., node pairs, node triples and so on), thus cannot leverage the sparsity of graph structure and are difficult to scale to large graphs. Some works study the universality of GNNs for approximating any invariant or equivariant functions over graphs <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref>. However, reaching universality would require polynomial(n)-order tensors, which hold more theoretical value than practical applicability. Dasoulas et al. <ref type="bibr" target="#b37">[38]</ref> propose to augment nodes of identical attributes with different colors, which requires exhausting all the coloring choices to reach universality. Similarly, Relational Pooling (RP) <ref type="bibr" target="#b29">[30]</ref> uses the ensemble of permutation-aware functions over graphs to reach universality, which requires exhausting all n! permutations to achieve its theoretical power. Its local version Local Relational Pooling (LRP) <ref type="bibr" target="#b38">[39]</ref> applies RP over subgraphs around nodes, which is similar to our work yet still requires exhausting node permutations in local subgraphs and even more loses RP's theoretical power. In contrast, NGNN maintains a controllable cost by only applying a message passing GNN to local subgraphs, and is guaranteed to be more powerful than 1-WL.</p><p>Because of the high cost of mimicking high-dimensional WL tests, several works have been proposed to increase GNN's representation power within the message passing framework. Observing that different neighbors are indistinguishable during neighbor aggregation, some works propose to add one-hot node index features or random features to GNNs <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>. These methods work well when nodes naturally have distinct identities irrespective of the graph structure. However, although making GNNs more discriminative, they also lose some of GNNs' generalization ability by not being able to guarantee nodes with identical neighborhoods to have the same embedding; the resulting models are also no longer permutation invariant. Repeating random initialization helps with avoiding such an issue but gets much slower convergence <ref type="bibr" target="#b41">[42]</ref>. An exception is structural message-passing (SMP) <ref type="bibr" target="#b42">[43]</ref>, which propagates one-hot node index features to learn a global n ? d feature matrix for each node. The feature matrix is further pooled to learn a permutation-invariant node representation.</p><p>On the contrary, some works propose to use structural features to augment GNNs without hurting the generalization ability of GNNs. SEAL <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref>, IGMC <ref type="bibr" target="#b45">[46]</ref> and DE <ref type="bibr" target="#b23">[24]</ref> use distance-based features, where a distance vector w.r.t. the target node set to predict is calculated for each node as its additional features. Our NGNN framework is naturally compatible with such distance-based features due to its independent rooted subgraph processing. GSN <ref type="bibr" target="#b30">[31]</ref> uses the count of certain substructures to augment node/edge features, which also surpasses 1-WL theoretically. However, GSN needs a properly defined substructure set to incorporate domain-specific inductive biases, while NGNN aims to learn arbitrary substructures around nodes without the need to predefine a substructure set.</p><p>Concurrent to our work, You et al. <ref type="bibr" target="#b31">[32]</ref> propose Identity-aware GNN (ID-GNN). ID-GNN uses different weight parameters between each root node and its context nodes during message passing. It also extracts a rooted subgraph around each node, and thus can be viewed as a special case of NGNN with: 1) the number of message passing layers equivalent to the subgraph height, 2) directly using the root node's intermediate representation as its final representation without subgraph pooling, and 3) augmenting initial node features with 0/1 "identity". However, the extra power of ID-GNN only comes from the "identity" feature, while the power of NGNN comes from the subgraph poolingwithout using any node features, NGNN is still provably more discriminative than 1-WL. Another similar work to ours is natural graph network (NGN) <ref type="bibr" target="#b46">[47]</ref>. NGN argues that graph convolution weights need not be shared among all nodes but only (locally) isomorphic nodes. If we view our distance-based node features as refining the graph convolution weights so that nodes within a center node's neighborhood are no longer treated symmetrically, then our NGNN reduces to an NGN.</p><p>The idea of independently performing message passing within k-hop neighborhood is also explored in k-hop GNN <ref type="bibr" target="#b47">[48]</ref> and MixHop <ref type="bibr" target="#b48">[49]</ref>. However, MixHop directly concatenates the aggregation results of neighbors at different hops as the root representation, which ignores the connections between other nodes in the rooted subgraph. k-hop GNN sequentially performs message passing for k-hop, k ? 1-hop, ..., and 0-hop node (the update of (i?1)-hop nodes depend on the updated states of i-hop nodes), while NGNN simultaneously performs message passing for all nodes in the subgraph thus is more parallelizable. Both MixHop and k-hop GNN directly use the root node's representation as its final node representation. In contrast, NGNN uses a subgraph pooling to summarize all node representations within the subgraph as the final root representation, which distinguishes NGNN from other k-hop models. As Theorem 1 shows, the subgraph pooling enables using a much smaller number of message passing layers l (as small as 1) than the depth k of the subgraph, while MixHop and k-hop GNN always require l ? k. MixHop and k-hop GNN also do not have the strong theoretical power of NGNN to discriminate r-regular graphs. Like SEAL and k-hop GNN, G-Meta <ref type="bibr" target="#b49">[50]</ref> is another work extracting subgraphs around nodes/links. It focuses specifically on a meta-learning setting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we study the effectiveness of the NGNN framework for graph classification and regression tasks. In particular, we want to answer the following questions: We implement the NGNN framework based on the PyTorch Geometric library <ref type="bibr" target="#b50">[51]</ref>. Our code is available at https://github.com/muhanzhang/NestedGNN.</p><formula xml:id="formula_6">Q1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>To answer Q1, we use a simulation dataset of r-regular graphs and the EXP dataset <ref type="bibr" target="#b41">[42]</ref> containing 600 pairs of 1-WL-indistinguishable but non-isomorphic graphs. To answer Q2, we use the QM9 dataset <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref> and the TU datasets <ref type="bibr" target="#b53">[54]</ref>. QM9 contains 130K small molecules. The task here is to perform regression on twelve targets representing energetic, electronic, geometric, and thermodynamic properties, based on the graph structure and node/edge features. TU contains five graph classification datasets including D&amp;D <ref type="bibr" target="#b54">[55]</ref>, MUTAG <ref type="bibr" target="#b55">[56]</ref>, PROTEINS <ref type="bibr" target="#b54">[55]</ref>, PTC_MR <ref type="bibr" target="#b56">[57]</ref>, and ENZYMES <ref type="bibr" target="#b57">[58]</ref>. We used the datasets provided by PyTorch Geometric <ref type="bibr" target="#b50">[51]</ref>, where for QM9 we performed unit conversions to match the units used by <ref type="bibr" target="#b18">[19]</ref>. The evaluation metric is Mean Absolute Error (MAE) for QM9 and Accuracy (%) for TU. To answer Q3, we use two Open Graph Benchmark (OGB) datasets <ref type="bibr" target="#b58">[59]</ref>, ogbg-molhiv and ogbg-molpcba. The ogbg-molhiv dataset contains 41K small molecules, the task of which is to classify whether a molecule inhibits HIV virus or not. ROC-AUC is used for evaluation. The ogbg-molpcba dataset contains 438K molecules with 128 classification tasks. The evaluation metric is Average Precision (AP) averaged over all the tasks. We include the statistics for QM9 and OGB datasets in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Models</head><p>QM9. We use 1-GNN, 1-2-GNN, 1-3-GNN, and 1-2-3-GNN from <ref type="bibr" target="#b18">[19]</ref> as both the baselines and the base GNNs of NGNN. Among them, 1-GNN is a standard message passing GNN with 1-WL power. 1-2-GNN is a GNN mimicking 2-WL, where message passing happens among 2-tuples of nodes. 1-3-GNN and 1-2-3-GNN mimic 3-WL, where message passing happens among 3-tuples of nodes. 1-2-GNN and 1-3-GNN use features computed by 1-GNN as initial node features, and 1-2-3-GNN uses the concatenated features from 1-2-GNN and 1-3-GNN. We additionally include numbers provided by <ref type="bibr" target="#b52">[53]</ref> and Deep LRP <ref type="bibr" target="#b38">[39]</ref> as baselines. Note that we omit more recent methods <ref type="bibr" target="#b59">[60]</ref><ref type="bibr" target="#b60">[61]</ref><ref type="bibr" target="#b61">[62]</ref> using advanced physical representations calculated from angles, atom coordinates, and quantum mechanics, which may obscure the comparison of models' pure graph representation power. For NGNN, we uniformly use height-3 rooted subgraphs. For a fair comparison, the base GNNs in NGNN use exactly the same hyperparameters as when they are used alone, except for 1-GNN where we increase the number of message passing layers from 3 to 5 to make the number of layers larger than the subgraph height, similar to <ref type="bibr" target="#b62">[63]</ref>. For subgraph pooling and graph pooling layers, we uniformly use mean pooling. All other settings follow <ref type="bibr" target="#b18">[19]</ref>.</p><p>TU. We use four widely adopted GNNs as the baselines and the base GNNs of NGNN: GCN <ref type="bibr" target="#b11">[12]</ref>, GraphSAGE <ref type="bibr" target="#b63">[64]</ref>, GIN <ref type="bibr" target="#b26">[27]</ref>, and GAT <ref type="bibr" target="#b14">[15]</ref>. Since TU datasets suffer from inconsistent evaluation standards <ref type="bibr" target="#b64">[65]</ref>, we uniformly use the 10-fold cross validation framework provided by PyTorch Geomtric <ref type="bibr" target="#b65">[66]</ref> for all the models to ensure a fair comparison. For GNNs, we search the number of message passing layers in {2, 3, 4, 5}. For NGNNs, we similarly search the subgraph height h in {2, 3, 4, 5}, so that both NGNNs and GNNs can have equal-depth local receptive fields. For NGNNs, we always use h + 1 message passing layers instead of searching it together with h, because that will make NGNNs have more hyperparameters to tune. All models have 32 hidden dimensions, and are trained for 100 epochs with a batch size of 128. For each fold, we record the test accuracy with the hyperparameters chosen based on the best validation performance of this fold. Finally, we report the average test accuracy across all the 10 folds.</p><p>OGB. We use GNNs achieving top places on the OGB graph classification leaderboard 3 (at the time of submission) as the baselines, including GCN <ref type="bibr" target="#b11">[12]</ref>, GIN <ref type="bibr" target="#b26">[27]</ref>, DeeperGCN <ref type="bibr" target="#b66">[67]</ref>, Deep LRP <ref type="bibr" target="#b38">[39]</ref>, PNA <ref type="bibr" target="#b67">[68]</ref>, DGN <ref type="bibr" target="#b32">[33]</ref>, GINE <ref type="bibr" target="#b68">[69]</ref>, and PHC-GNN <ref type="bibr" target="#b69">[70]</ref>. Note that those high-order GNNs <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b24">25]</ref> are not included here, because despite being theoretically more discriminative, these GNNs are not among the GNNs with the best empirical performance on modern large-scale graph benchmarks, and their O(n 3 ) complexity also raises a scalability issue. For NGNN, we use GIN as the base GNN (although GIN is not among the strongest baselines here). Some baselines additionally use the virtual node technique <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b70">71]</ref>, which are marked by "*". For NGNN, we search the subgraph height h in {3, 4, 5}, and the number of layers in {4, 5, 6}. We train the NGNN models for 100 and 150 epochs for ogbg-molhiv and ogbg-molpcba, respectively, and report the validation and test scores at the best validation epoch. We also find that our models are subject to high performance variance across epochs, likely due to the increased expressiveness. Thus, we save a model checkpoint every 10 epochs, and additionally report the ensemble performance by averaging the predictions from all checkpoints. The final hyperparameter choices and more details about the experimental settings are included in Appendix C. All results are averaged over 10 independent runs.</p><p>In the following, we uniformly use "Nested GNN" to denote an NGNN model using "GNN" as the base GNN. For example, Nested GIN denotes an NGNN model using GIN <ref type="bibr" target="#b26">[27]</ref> as the base GNN. For the NGNN models in QM9, TU and OGB datasets, we augment the initial features of a node with Distance Encoding (DE) <ref type="bibr" target="#b23">[24]</ref>, which uses the (generalized) distance between a node and the root as its additional feature, due to DE's successful applications in link-level tasks <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b45">46]</ref>. Note that such feature augmentation is not applicable to the baseline models as discussed in Section 3.</p><p>2. An ablation study on the effects of the DE features is included in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results and discussion</head><p>To answer Q1, we first run a simulation to test NGNN's power for discriminating r-regular graphs.</p><p>The results are presented in Appendix D. They match almost perfectly with Theorem 1, demonstrating that a practical NGNN can fulfil its theoretical power for discriminating r-regular graphs. We also test NGNN's expressive power using the EXP dataset provided by <ref type="bibr" target="#b41">[42]</ref>, which contains 600 carefully constructed 1-WL indistinguishable but non-isomorphic graph pairs. Each pair of graphs have different labels, thus a standard message passing GNN cannot predict them both correctly, resulting in an expected classification accuracy of only 50%. We exactly follow the experimental settings and copy the baseline results in <ref type="bibr" target="#b41">[42]</ref>. In <ref type="table" target="#tab_2">Table 2</ref>, our Nested GIN model achieves a 99.9% classification accuracy, which outperforms all the baselines and distinguishes almost all the 1-WL indistinguishable graph pairs. These results verified that NGNN's expressive power is indeed beyond 1-WL and message passing GNNs.</p><p>To answer Q2, we adopt the QM9 and TU datasets. We show the QM9 results in <ref type="table" target="#tab_3">Table 3</ref>. If the Nested version of a base GNN achieves a better result than the base GNN itself, we color that cell with light green. As we can see, NGNN brings performance gains to all base GNNs on most targets, sometimes by large margins. We also show the results on TU in <ref type="table" target="#tab_4">Table 4</ref>. NGNNs also show improvement over their base GNNs in most cases. These results indicate that NGNN is a general framework for improving a GNN's power. We further compute the maximum reduction of MAE for QM9 and maximum improvement of accuracy for TU before and after applying NGNN. NGNN reduces the MAE by up to 7.9 times for QM9, and increases the accuracy by up to 14.3% for TU. These results answer Q2, indicating that NGNN can bring steady and significant improvement to base GNNs.</p><p>To answer Q3, we compare Nested GIN with leading methods on the OGB leaderboard. The results are shown in <ref type="table" target="#tab_5">Table 5</ref>. Nested GIN achieves highly competitive performance with these leading GNN models, albeit using a relatively weak base GNN (GIN). Compared to GIN alone, Nested GIN shows clear performance gains. It achieves test scores up to 79.86 and 30.07 on ogbg-molhiv and   In summary, our experiments have firmly shown that NGNN is a theoretically sound method which brings consistent gains to its base GNNs in a plug-and-play way. Furthermore, NGNN still maintains a controllable time complexity compared to other more powerful GNNs.</p><p>Finally, we point out one memory limitation of the current NGNN implementation. Currently, NGNN does not scale to graph datasets with a large average node number (such as REDDIT-BINARY) or datasets with a large average node degree (such as ogbg-ppa) due to copying a rooted subgraph for each node to the GPU memory. Reducing batch size or subgraph height helps, but at the same time leads to performance degradation. One may wonder why materializing all the subgraphs into GPU memory is necessary. The reason is that we want to batch-process all the subgraphs simultaneously. Otherwise, we have to sequentially extract subgraphs on the fly, which results in a much higher latency. We leave the exploration of memory efficient NGNN to the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We have proposed Nested Graph Neural Network (NGNN), a general framework for improving GNN's representation power. NGNN learns node representations encoding rooted subgraphs instead of rooted subtrees. Theoretically, we prove NGNN can discriminate almost all r-regular graphs where 1-WL always fails. Empirically, NGNN consistently improves the performance of various base GNNs across different datasets without incurring the O(n 3 ) complexity like other more powerful GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proof of Theorem 1</head><p>The proof is inspired by the previous theoretical characterization on the power of distance features <ref type="bibr" target="#b23">[24]</ref>. Basically, performing height-k subgraph extraction around a center node is essentially equivalent to injecting distance features that indicate whether the distance between a node and the center node is less than k + 1. In the following part, we will explicitly show how these distance features make NGNN more powerful than the 1-WL test. Let us first introduce the outline of the proof. Consider two n-node r-regular graphs G (1) = (V (1) , E <ref type="bibr" target="#b0">(1)</ref> ) and G (2) = (V <ref type="bibr" target="#b1">(2)</ref> , E <ref type="bibr" target="#b1">(2)</ref> ) and we pick two nodes, each from one graph, denoted by v 1 and v 2 . By performing certain-height (at most ( 1 2 + ) log n log(r?1)height) rooted subgraph extraction around these two nodes, due to the implicit distance features, we may prove that the nodes on the boundary of the obtained two subgraphs will obtain special node representations. These special node representations will be propagated within the subgraphs. After some steps of propagation, we can prove that NGNN by leveraging the subgraph pooling (Eq. 4) can distinguish these two subgraphs. This tells that NGNN may generate different node representations for v 1 and v 2 respectively. Then, a union bound can be used to transform such difference in node representations into the difference in the representations of G <ref type="bibr" target="#b0">(1)</ref> and G <ref type="bibr" target="#b1">(2)</ref> . Note that the proof will assume that there are no node/edge attributes that can be leveraged. Additional node/edge attributes may only improve the possibility to distinguish these two graphs.</p><p>The first lemma is to analyze the difference between the structures of the rooted subgraphs around two nodes over two n-node r-regular graphs. Before introducing that, we need to define a notion termed edge configuration. For a node v in graph G, let Q k v,G denote the set of nodes in G that are exactly k-hop neighbors of v, i.e., the shortest path distance between v and any node u ? Q k v,G is k. Then, we know the height-k rooted subgraph over G around the center node v is the subgraph induced by the node set ? k i=0 Q i v,G .</p><formula xml:id="formula_7">Definition 3. The edge configuration between Q k v,G and Q k+1 v,G is a list C k v,G = (a 1,k v,G , a 2,k v,G , ...) where a i,k v,G denotes the number of nodes in Q k+1 v,G of which each has exactly i edges from Q k v,G .</formula><p>When we say two edge configurations C k v1,G <ref type="bibr" target="#b0">(1)</ref> (between Q k v1,G <ref type="bibr" target="#b0">(1)</ref> and Q k+1 v1,G <ref type="bibr" target="#b0">(1)</ref> ), C k v2,G <ref type="bibr" target="#b1">(2)</ref> (between Q k v2,G <ref type="bibr" target="#b1">(2)</ref> and Q k+1 v2,G <ref type="bibr" target="#b1">(2)</ref> ) are equal, we mean that these two lists are component-wise equal to each other. Obviously, we should also have |Q k+1 v1,G <ref type="bibr" target="#b0">(1)</ref>  <ref type="bibr" target="#b1">(2)</ref> . Now, we are ready to propose the first lemma. Lemma 1. For two graphs G (1) = (V <ref type="bibr" target="#b0">(1)</ref> , E <ref type="bibr" target="#b0">(1)</ref> ) and G (2) = (V <ref type="bibr" target="#b1">(2)</ref> , E <ref type="bibr" target="#b1">(2)</ref> ) that are uniformly independently sampled from all n-node r-regular graphs, where 3 ? r &lt; ? 2 log n, we pick any two nodes, each from one graph, denoted by v 1 and v 2 respectively. Then, there is at least one i ? ( 1 2 log n log(r?1? ) , ( 1 2 + ) log n log(r?1? ) ) with probability 1 ? o(n ?1 ) such that C i v1,G (1) = C i v2,G <ref type="bibr" target="#b1">(2)</ref> . Moreover, with at least the same probability, for all i ? ( 1 2 log n log(r?1? ) , ( 2 3 ? ) log n log(r?1) ), the number of edges between Q i vj ,G (j) and Q i+1 vj ,</p><formula xml:id="formula_8">| = |Q k+1 v2,G (2) | if C k v1,G (1) = C k v2,G</formula><formula xml:id="formula_9">G (j) are at least (r ? 1 ? )|Q i vj ,G (j) | for j ? {1, 2}.</formula><p>Proof. This lemma can be obtained by following the steps 1-3 in the proof of Theorem 3.3 in <ref type="bibr" target="#b23">[24]</ref>. Now, we set K = ( 1 2 + ) log n log(r?1? ) . We focus on the two extracted subgraphs G K v1 and G K v2 . We first prove a lemma that shows with a certain number of layers, a proper NGNN will generate different representations for G K v1 and G K v2 , i.e., h v1 and h v2 in Eq. 4. Lemma 2. For two graphs G (1) = (V <ref type="bibr" target="#b0">(1)</ref> , E <ref type="bibr" target="#b0">(1)</ref> ) and G (2) = (V <ref type="bibr" target="#b1">(2)</ref> , E <ref type="bibr" target="#b1">(2)</ref> ) that are uniformly independently sampled from all n-node r-regular graphs, where 3 ? r &lt; ? 2 log n, we pick any two nodes, each from one graph, denoted by v 1 and v 2 respectively, and do ( 1 2 + ) log n log(r?1? )height rooted subgraph extraction around v 1 and v 2 . With at most log n log(r?1? ) many layers, a proper message passing GNN (with injective U t , M t and subgraph pooling) will generate different representations for the extracted two subgraphs with probability at least 1 ? o(n ?1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Design choices of NGNN</head><p>In this section, we discuss some other design choices of NGNN.</p><p>High-order NGNN. NGNN is a two-level GNN (a GNN of GNNs), where a base GNN is used to learn a final node representation from a rooted subgraph and an outer GNN (graph pooling) is used to learn a graph representation from the base GNNs' outputs. This design thus involves one level of nesting, which we call first-order NGNN. To extend the framework, we propose high-order NGNN, where we make the base GNN itself an NGNN. That is, we perform the subgraph representation learning tasks each using a first-order NGNN, where we treat each subgraph the same as the graph in the original NGNN. This way, we arrive at a second-order NGNN with two levels of nesting (a GNN of NGNNs, or a GNN of GNNs of GNNs). Repeating this construction, we can in principle construct an arbitrary-order NGNN. It is interesting to investigate whether high-order NGNNs can further enhance the representation power and the practical performance of a base GNN. We leave the exploration of such architectures to future work.</p><p>Pooling functions R 0 and R 1 . To summarize node representations into a subgraph/graph representation, we need a readout (pooling) function. Popular choices include sum, mean, max, as well as more complex ones such as selecting top-K nodes <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b71">72]</ref> and hierarchical approaches <ref type="bibr" target="#b16">[17]</ref>. In this paper, we find mean pooling works very well, which directly takes the mean of node representations as the subgraph/graph representation. We also find another pooling function to be sometimes useful for subgraph pooling, called center pooling (CP). CP directly uses the root node's representation to represent the entire subgraph. The success of CP relies on using more layers of message passing than the height of the rooted subgraph, so that even the intermediate representation of the center root node alone can have sufficient information about the entire subgraph. This is feasible for rooted subgraphs with a small height. Note that when using a number of message passing layers smaller than the subgraph height, NGNN with CP will reduce to a standard message passing GNN.</p><p>Subgraph height h and base GNN layers l. NGNN is flexible in terms of choosing the subgraph height h and the number of message passing layers l in the base GNN. Theorem 1 provides a guide for choosing h and l when discriminating r-regular graphs. In practice, we find using h = 3 and l = 4 generally performs well across various tasks. Using a small h will restrict the receptive field, causing NGNN to learn too local features. Using a too large h might cause each rooted subgraph to include the entire graph. For the number of message passing layers l, we find that using l ? h performs better. This can be explained by that using a large l makes each node in a rooted subgraph to more sufficiently absorb the whole-subgraph information thus learning a better intermediate node representation reflecting its structural position within the subgraph. Please refer to <ref type="bibr" target="#b62">[63]</ref> for more motivations for using deeper message passing layers than the subgraph height.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C More details about the experimental settings</head><p>The experiments were run on a Linux server with 64GB memory, two NVIDIA RTX 2080S (8GB) GPUs and an INTEL i9-9900 8-core CPU. For ogbg-molhiv, the final NGNN architecture used a rooted subgraph height h = 4 and number of GIN layers l = 6. Mean pooling is used in both the subgraph and graph pooling. The final NGNN architecture for ogbg-molpcba used a rooted subgraph height h = 3 and the number of GIN layers l = 4. Center pooling (CP) is used in the subgraph pooling and mean pooling is used in the graph pooling. Although we searched h and l, we found the final performance is not very sensitive to these hyperparameters as long as h is between 3 and 5 and l &gt; h. For the DE features, we use shortest path distance and resistance distance <ref type="bibr" target="#b72">[73]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Simulation experiments to verify Theorem 1</head><p>We conduct a simulation over random regular graphs to validate Lemma 2 (how well NGNN distinguishes nodes of regular graphs) and Theorem 1 (how well NGNN distinguishes regular graphs). The results are shown in <ref type="figure" target="#fig_1">Figure 3</ref>, which match our theory almost perfectly. Basically, we sample 100 n-node 3-regular graphs uniformly at random, and then apply an untrained NGNN to these graphs to see how often NGNN can distinguish the nodes and graphs at different rooted subgraph height h and node number n. The required h at different n matches almost perfectly with the lower bound in Lemma 2. More details are contained in the caption of <ref type="figure" target="#fig_1">Figure 3</ref>. The left graph shows the node-level (with only subgraph pooling) simulation results. The right graph shows the graph-level (with both subgraph and graph pooling) simulation results. We uniformly sample 100 n-node 3-regular graphs with n ranging from 10 to 1280. We let the rooted subgraph height h range from 1 to 10. We apply an untrained Nested GIN with one message passing layer to these graphs (with a uniform 1 as node features). In the left figure, we compare the final node representations (after subgraph pooling) from all graphs output by the Nested GIN. If the difference between two node representations ||hu ? hv||2 is greater than machine accuracy, they are regarded as indistinguishable. The shade of each scatter point's color reflects the portion of indistinguishable node pairs at certain (n, h). The darker, the more indistinguishable node pairs. In the right graph, we compare the final graph representations (after graph pooling) output by the Nested GIN. The blue and red dashed lines show the theoretical upper and lower bounds for h to discriminate almost all nodes in n-node 3-regular graphs, respectively. As we can see, the node-level simulation results perfectly match the theory (Lemma 2)-when h is larger than 0.5 log(n)/ log(r ? 1), almost all nodes from r-regular graphs are distinguishable by NGNN. When h is even larger than log(n)/ log(r ? 1), the nodes can hardly be distinguished because each subgraph contains the entire regular graph. The graph-level simulation results show that even using a very small h NGNN can still discriminate almost all r-regular graphs-h in practice even does not need to be always chosen beyond 0.5 log(n)/ log(r ? 1). This is because although most nodes from two r-regular graphs cannot be distinguished when h ? 0.5 log(n)/ log(r ? 1), the graph pooling can still distinguish the two graphs as long as there exists one single node from one graph holding a representation different from any node representation from the other graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Ablation study on DE</head><p>In this paper, we choose Distance Encoding (DE) <ref type="bibr" target="#b23">[24]</ref> to augment the initial node features of NGNN, due to its good theoretical properties for improving the expressive power of message passing GNNs as well as its superb empirical performance on link prediction tasks <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b45">46]</ref>. DE encodes the distance between a node and the root node into a vector through an embedding layer. The distance embedding is concatenated with the raw features of a node as its new features (in this rooted subgraph) input to the base GNN. Note that when this node appears in another rooted subgraph, it may have a different distance to that root node, thus resulting in different DE features in different subgraphs. Only the NGNN framework can leverage such a subgraph-specific feature augmentation-a standard GNN treats a node always the same no matter which node's rooted subgraph/subtree it is in.  In this section, we do ablation experiments to study the effect of the DE features. We choose QM9 as the testbed. The base GNNs are the same as in <ref type="table" target="#tab_3">Table 3</ref>. For each base GNN, we compare it with its Nested GNN version without DE features (no DE) and its Nested GNN version with DE features (with DE). The results are shown in <ref type="table" target="#tab_6">Table 6</ref>.</p><p>In <ref type="table" target="#tab_6">Table 6</ref>, we color the cell with light green if the NGNN (no DE) is better than the base GNN, and mark the cell with green if the NGNN (with DE) is additionally better than the NGNN (no DE). From the results, we can first observe that NGNNs (no DE) generally outperform the base GNNs, validating that even without any feature augmentation the NGNN framework still enhances the performance of base GNNs. Furthermore, we can observe that if NGNN improves over the base GNN, adding DE features could further enlarge the performance improvement by achieving the smallest MAEs among the three (i.e., base GNN, NGNN (no DE) and NGNN (with DE)). This demonstrates the usefulness of augmenting NGNN with DE features. Note that adding such DE features can be done simultaneously with the rooted subgraph extraction process, which only adds a negligible amount of time. Thus, augmenting NGNN with DE features is almost a free yet powerful operation to further enhance NGNN's power, which motivates us to make it a default choice of NGNN.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Definition 1 .</head><label>1</label><figDesc>(Rooted subgraph) Given a graph G and a node v, the height-h rooted subgraph G h v of v is the subgraph induced from G by the nodes within h hops of v (including h-hop nodes).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 3 )v</head><label>3</label><figDesc>Here M t , U t are the message and update functions of the base GNN at time stamp t, N (v|G h w ) denotes the set of v's neighbors within w's rooted subgraph G h w , and h t+1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Theorem 1 .</head><label>1</label><figDesc>Consider all pairs of n-sized r-regular graphs, where 3 ? r &lt; (2 log n) 1/2 . For any small constant &gt; 0, there exists a proper NGNN using at most ( 1 2 + ) log n log(r?1? ) -height rooted subgraphs and log n log(r?1? ) -layer message passing, which distinguishes almost all (1 ? o</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Simulation to verify Theorem 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>-3-GNN 0.476 0.27 0.00337 0.00351 0.0048 22.9 0.00019 0.0427 0.111 0.0419 0.0469 0.0944 Nested 1-2-3-GNN (no DE) 0.449 0.306 0.00282 0.00286 0.0041 22.0 0.00023 0.220 0.218 0.268 0.205 0.0975 Nested 1-2-3-GNN (with DE) 0.433 0.265 0.00279 0.00276 0.0039 20.1 0.00015 0.205 0.200 0.249 0.253 0.0811</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics and evaluation metrics of the QM9 and OGB datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="5">#Graphs Avg. #nodes Avg. #edges Split ratio #Tasks</cell><cell>Task type</cell><cell>Metric</cell></row><row><cell>QM9</cell><cell>129,433</cell><cell>18.0</cell><cell>18.6</cell><cell>80/10/10</cell><cell>12</cell><cell>Regression</cell><cell>MAE</cell></row><row><cell>ogbl-molhiv</cell><cell>41,127</cell><cell>25.5</cell><cell>27.5</cell><cell>80/10/10</cell><cell>1</cell><cell cols="2">Classification ROC-AUC</cell></row><row><cell cols="2">ogbl-molpcba 437,929</cell><cell>26.0</cell><cell>28.1</cell><cell>80/10/10</cell><cell>128</cell><cell>Classification</cell><cell>AP</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Can NGNN reach its theoretical power to discriminate 1-WL-indistinguishable graphs? Q2 How often and how much does NGNN improve the performance of a base GNN? Q3 How does NGNN perform in comparison to state-of-the-art GNN methods in open benchmarks? Q4 How much extra computation time does NGNN incur?</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results (%) on EXP.</figDesc><table><row><cell>Method</cell><cell>Test Accuracy</cell></row><row><cell>GCN-RNI [42]</cell><cell>98.0?1.85</cell></row><row><cell>PPGN [20]</cell><cell>50.0?0.00</cell></row><row><cell>1-2-3-GNN [19]</cell><cell>50.0?0.00</cell></row><row><cell>3-GCN [42]</cell><cell>99.7?0.004</cell></row><row><cell>Nested GIN</cell><cell>99.9?0.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>MAE results on QM9 (smaller the better). A colored cell means NGNN is better than the base GNN. Method (Ne. for Nested)DTNN MPNN Deep LRP 1-GNN 1-2-GNN 1-3-GNN 1-2-3-GNN Ne. 1-GNN Ne. 1-2-GNN Ne. 1-3-GNN Ne. 1-2-3-GNN Max. reduction</figDesc><table><row><cell>Target</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>?</cell><cell>0.244</cell><cell>0.358</cell><cell>0.364</cell><cell>0.493</cell><cell>0.493</cell><cell>0.473</cell><cell>0.476</cell><cell>0.428</cell><cell>0.437</cell><cell>0.436</cell><cell>0.433</cell><cell>1.2?</cell></row><row><cell>?</cell><cell>0.95</cell><cell>0.89</cell><cell>0.298</cell><cell>0.78</cell><cell>0.27</cell><cell>0.46</cell><cell>0.27</cell><cell>0.29</cell><cell>0.278</cell><cell>0.261</cell><cell>0.265</cell><cell>2.7?</cell></row><row><cell cols="7">?HOMO 0.00388 0.00541 0.00254 0.00321 0.00331 0.00328</cell><cell>0.00337</cell><cell>0.00265</cell><cell>0.00275</cell><cell>0.00265</cell><cell>0.00279</cell><cell>1.2?</cell></row><row><cell cols="7">?LUMO 0.00512 0.00623 0.00277 0.00355 0.00350 0.00354</cell><cell>0.00351</cell><cell>0.00297</cell><cell>0.00271</cell><cell>0.00269</cell><cell>0.00276</cell><cell>1.3?</cell></row><row><cell>??</cell><cell cols="2">0.0112 0.0066</cell><cell>0.00353</cell><cell>0.0049</cell><cell>0.0047</cell><cell>0.0046</cell><cell>0.0048</cell><cell>0.0038</cell><cell>0.0039</cell><cell>0.0039</cell><cell>0.0039</cell><cell>1.8?</cell></row><row><cell>R 2</cell><cell>17.0</cell><cell>28.5</cell><cell>19.3</cell><cell>34.1</cell><cell>21.5</cell><cell>25.8</cell><cell>22.9</cell><cell>20.5</cell><cell>20.4</cell><cell>20.2</cell><cell>20.1</cell><cell>1.7?</cell></row><row><cell cols="7">ZPVE 0.00172 0.00216 0.00055 0.00124 0.00018 0.00064</cell><cell>0.00019</cell><cell>0.00020</cell><cell>0.00017</cell><cell>0.00017</cell><cell>0.00015</cell><cell>6.2?</cell></row><row><cell>U0</cell><cell>2.43</cell><cell>2.05</cell><cell>0.413</cell><cell>2.32</cell><cell>0.0357</cell><cell>0.6855</cell><cell>0.0427</cell><cell>0.295</cell><cell>0.252</cell><cell>0.291</cell><cell>0.205</cell><cell>7.9?</cell></row><row><cell>U</cell><cell>2.43</cell><cell>2.00</cell><cell>0.413</cell><cell>2.08</cell><cell>0.107</cell><cell>0.686</cell><cell>0.111</cell><cell>0.361</cell><cell>0.265</cell><cell>0.278</cell><cell>0.200</cell><cell>5.8?</cell></row><row><cell>H</cell><cell>2.43</cell><cell>2.02</cell><cell>0.413</cell><cell>2.23</cell><cell>0.070</cell><cell>0.794</cell><cell>0.0419</cell><cell>0.305</cell><cell>0.241</cell><cell>0.267</cell><cell>0.249</cell><cell>7.3?</cell></row><row><cell>G</cell><cell>2.43</cell><cell>2.02</cell><cell>0.413</cell><cell>1.94</cell><cell>0.140</cell><cell>0.587</cell><cell>0.0469</cell><cell>0.489</cell><cell>0.272</cell><cell>0.287</cell><cell>0.253</cell><cell>4.0?</cell></row><row><cell>Cv</cell><cell>0.27</cell><cell>0.42</cell><cell>0.129</cell><cell>0.27</cell><cell>0.0989</cell><cell>0.158</cell><cell>0.0944</cell><cell>0.174</cell><cell>0.0891</cell><cell>0.0879</cell><cell>0.0811</cell><cell>1.8?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Accuracy results (%) on TU datasets.</figDesc><table><row><cell></cell><cell>D&amp;D</cell><cell cols="4">MUTAG PROTEINS PTC_MR ENZYMES</cell></row><row><cell>#Graphs</cell><cell>1178</cell><cell>188</cell><cell>1113</cell><cell>344</cell><cell>600</cell></row><row><cell>Avg. #nodes</cell><cell>284.32</cell><cell>17.93</cell><cell>39.06</cell><cell>14.29</cell><cell>32.63</cell></row><row><cell>GCN</cell><cell cols="2">71.6?2.8 73.4?10.8</cell><cell>71.7?4.7</cell><cell>56.4?7.1</cell><cell>27.3?5.5</cell></row><row><cell>GraphSAGE</cell><cell cols="2">71.6?3.0 74.0?8.8</cell><cell>71.2?5.2</cell><cell>57.0?5.5</cell><cell>30.7?6.3</cell></row><row><cell>GIN</cell><cell cols="2">70.5?3.9 84.5?8.9</cell><cell>70.6?4.3</cell><cell>51.2?9.2</cell><cell>38.3?6.4</cell></row><row><cell>GAT</cell><cell cols="2">71.0?4.4 73.9?10.7</cell><cell>72.0?3.3</cell><cell>57.0?7.3</cell><cell>30.2?4.2</cell></row><row><cell>Nested GCN</cell><cell cols="2">76.3?3.8 82.9?11.1</cell><cell>73.3?4.0</cell><cell>57.3?7.7</cell><cell>31.2?6.7</cell></row><row><cell cols="3">Nested GraphSAGE 77.4?4.2 83.9?10.7</cell><cell>74.2?3.7</cell><cell>57.0?5.9</cell><cell>30.7?6.3</cell></row><row><cell>Nested GIN</cell><cell cols="2">77.8?3.9 87.9?8.2</cell><cell>73.9?5.1</cell><cell>54.1?7.7</cell><cell>29.0?8.0</cell></row><row><cell>Nested GAT</cell><cell cols="2">76.0?4.4 81.9?10.2</cell><cell>73.7?4.8</cell><cell>56.7?8.1</cell><cell>29.5?5.7</cell></row><row><cell>Max. improvement</cell><cell>10.4%</cell><cell>13.4%</cell><cell>4.7%</cell><cell>5.7%</cell><cell>14.3%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Results (%) on OGB datasets (* virtual node). respectively, which outperform all the baselines. In particular, for the challenging ogbg-molpcba, our Nested GIN can achieve 30.07 and 28.32 test AP with and without ensemble, respectively, outperforming the plain GIN model (with 27.03 test AP) significantly. These results demonstrate the great empirical performance and potential of NGNN even compared to heavily tuned open leaderboard models, despite using only GIN as the base GNN. To answer Q4, we report the training time per epoch for GIN and Nested GIN on OGB datasets. On ogbg-molhiv, GIN takes 54s per epoch, while Nested GIN takes 183s. On ogbg-molpcba, GIN takes 10min per epoch, while Nested GIN takes 20min. This verifies that NGNN has comparable time complexity with message passing GNNs. The extra complexity comes from independently learning better node representations from rooted subgraphs, which is a trade-off for the higher expressivity.</figDesc><table><row><cell></cell><cell cols="2">ogbg-molhiv (AUC)</cell><cell cols="2">ogbg-molpcba (AP)</cell></row><row><cell>Method</cell><cell>Validation</cell><cell>Test</cell><cell>Validation</cell><cell>Test</cell></row><row><cell>CCN*</cell><cell>83.84?0.91</cell><cell>75.99?1.19</cell><cell cols="2">24.95?0.42 24.24?0.34</cell></row><row><cell>GIN*</cell><cell>84.79?0.68</cell><cell>77.07?1.49</cell><cell cols="2">27.98?0.25 27.03?0.23</cell></row><row><cell>Deep LRP</cell><cell>82.09?1.16</cell><cell>77.19?1.40</cell><cell>-</cell><cell>-</cell></row><row><cell>DeeperGCN*</cell><cell>-</cell><cell>-</cell><cell cols="2">29.20?0.25 27.81?0.38</cell></row><row><cell>HIMP</cell><cell>-</cell><cell>78.80?0.82</cell><cell>-</cell><cell>-</cell></row><row><cell>PNA</cell><cell>85.19?0.99</cell><cell>79.05?1.32</cell><cell>-</cell><cell>-</cell></row><row><cell>DGN</cell><cell cols="2">84.70?0.47 79.70?0.97 -</cell><cell>-</cell><cell></cell></row><row><cell>GINE*</cell><cell>-</cell><cell>-</cell><cell cols="2">30.65?0.30 29.17?0.15</cell></row><row><cell>PHC-GNN</cell><cell>82.17?0.89</cell><cell>79.34?1.16</cell><cell cols="2">30.68?0.25 29.47?0.26</cell></row><row><cell>Nested GIN*</cell><cell>83.17?1.99</cell><cell>78.34?1.86</cell><cell cols="2">29.15?0.35 28.32?0.41</cell></row><row><cell cols="2">Nested GIN* (ens) 80.80?2.78</cell><cell>79.86?1.05</cell><cell cols="2">30.59?0.56 30.07?0.37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Ablation study on QM9 comparing Nested GNNs with and without DE features.</figDesc><table><row><cell>Method</cell><cell>?</cell><cell>?</cell><cell>?HOMO</cell><cell>?LUMO</cell><cell>??</cell><cell>R 2</cell><cell>ZPVE</cell><cell>U0</cell><cell>U</cell><cell>H</cell><cell>G</cell><cell>Cv</cell></row><row><cell>1-GNN</cell><cell cols="7">0.493 0.78 0.00321 0.00355 0.0049 34.1 0.00124</cell><cell>2.32</cell><cell>2.08</cell><cell>2.23</cell><cell>1.94</cell><cell>0.27</cell></row><row><cell>Nested 1-GNN (no DE)</cell><cell cols="7">0.466 0.38 0.00292 0.00294 0.0042 24.0 0.00040</cell><cell>1.09</cell><cell>1.76</cell><cell>1.04</cell><cell>1.19</cell><cell>0.111</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://ogb.stanford.edu/docs/leader_graphprop/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledge</head><p>The authors greatly thank the actionable suggestions from the reviewers to improve the manuscript. Li is partly supported by the 2021 JP Morgan Faculty Award and the National Science Foundation (NSF) award HDR-2117997.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Proof. According to Lemma 1, we know that with probability 1 ? o(n ?1 ), there exists at least one i ? ( 1 2 log n log(r?1? ) , ( 1 2 + ) log n log(r?1? ) ) such that C k v1,G <ref type="bibr" target="#b0">(1)</ref> = C k v2,G <ref type="bibr" target="#b1">(2)</ref> . So there exists at least one k ? K that make C k v1,G <ref type="bibr" target="#b0">(1)</ref> = C k v2,G <ref type="bibr" target="#b1">(2)</ref> (thus the difference in edge configurations appears in G K v1 and G K v2 ) and we pick the largest k. Now let us consider running a message passing GNN over the two subgraphs G K vj , j ? {1, 2}. All nodes are initialized with the same node features. The nodes of these two subgraphs can be categorized into Q i vj ,G (j) (0 ? i ? K), for j ? {1, 2} respectively. Next, let us consider the node representations in these categories during the message passing procedure. We have the following observations.</p><p>1. Note that all the nodes other than those in Q K vj ,G (j) have degree r in both subgraphs. Therefore, in the t-th iteration, the nodes in ? K?t i=0 Q i vj ,G (j) for j ? {1, 2} will share the same node representation. We call this node representation as default representation. Note that if we do not perform rooted subgraph extraction, then all nodes in all r-regular graph hold default representation.</p><p>2. Node representations that are different from default representations will first appear among the nodes in Q K vj ,G (j) after the first iteration. This is because there are at least (r ? 1 ? )|Q K vj ,G (j) | edges between Q K vj ,G (j) and Q K+1 vj ,G (j) before performing subgraph extraction (due to Lemma 1) and all these edges will not appear in the extracted subgraphs. Then, almost all nodes in Q K vj ,G (j) hold only degree one (and thus do not have degree r to keep default representations) within the corresponding extracted subgraphs. We uniformly call the node representations that are different from the default ones as new representations. New representations may be mutually different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Those new different node representations will propagate to nodes in</head><p>and so on and so forth via iterative message passing. Moreover, during such propagation procedure, after t ? 2 iterations, new representations will at least make almost all nodes in Q i vj ,G (j) hold representations different form almost all nodes in Q i+1 vj ,G (j) for i = K ? t + 1, K ? t + 2, ..., K ? 1, which can be easily obtained by doing induction from t = t 1 to t = t 1 + 1.</p><p>Observing the above three points, We may compare the above propagating procedure between G K v1 and G K v2 . Suppose in the first K ? k steps of message passing, the set of node representations (both the default ones and the new ones) can keep the same between the two extracted subgraphs. If this is not true, we have already proven the results. As they hold different edge configurations in C k v1,G (1) = C k v2,G <ref type="bibr" target="#b1">(2)</ref> , when the new node representations propagate from Q k+1 vj ,G (j) to Q k vj ,G (j) , it will definitely induce different sets of new node representations between Q k v1,G <ref type="bibr" target="#b0">(1)</ref> and Q k v2,G <ref type="bibr" target="#b1">(2)</ref> . Currently, node representations are kept the same between Q i v1,G <ref type="bibr" target="#b0">(1)</ref> and Q i v2,G <ref type="bibr" target="#b1">(2)</ref> for i = [0, k ? 1] as they are all default node representations. Though ? K i=k+1 Q i vj ,G (j) also hold new node representations, they are different from those in Q k vj ,G (j) for j ? {1, 2}. At this point, if an injective subgraph pooling operation is adopted, then the obtained representations of G K v1 and G K v2 , i.e., h v1 and h v2 , are different.</p><p>Based on Lemma 2, using a union bound by comparing a node representation of G <ref type="bibr" target="#b0">(1)</ref> with all node representaitons of G <ref type="bibr" target="#b1">(2)</ref> , we may achieve the final conclusion. Specifically, we consider a node of G <ref type="bibr" target="#b0">(1)</ref> , say v 1 , and another arbitrary node of G <ref type="bibr" target="#b1">(2)</ref> , say v 2 . Using Lemma 2, we know with probability 1 ? o(n ?1 ), h v1 is different from h v2 . Then, using the union bound, with probability 1 ? o(1), we have h v1 / ? {h v2 |v 2 ? V (G (2) )}. Therefore, if the final graph pooling (Eq. 5) is injective, we may guarantee that NGNN can generate different representations for G <ref type="bibr" target="#b0">(1)</ref> and G <ref type="bibr" target="#b1">(2)</ref> .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Convolution kernels on discrete structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Haussler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<pubPlace>Citeseer</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The graphlet spectrum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Shortest-path kernels on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th IEEE International Conference on Data Mining</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Propagation kernels: efficient graph kernels from propagated information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="209" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">Jan</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al?n</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3837" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Discriminative embeddings of latent variable models for structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<meeting>The 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2702" to="2711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4438" to="4445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Provably powerful graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heli</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadar</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2156" to="2167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the equivalence between graph isomorphism testing and function approximation with gnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soledad</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15894" to="15902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A reduction of a graph to a canonical form and an algebra arising during this reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lehman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="12" to="16" />
		</imprint>
	</monogr>
	<note type="report_type">Nauchno-Technicheskaya Informatsia</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vichy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nicol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1201" to="1242" />
			<date type="published" when="2010-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Distance encoding-design provably more powerful gnns for structural representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.00142</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Weisfeiler and leman go sparse: Towards scalable higher-order graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Mutzel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman neural machine for link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="575" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Strongly regular graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Andries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><forename type="middle">H</forename><surname>Brouwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haemers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spectra of Graphs</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="115" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Relational pooling for graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balasubramaniam</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinayak</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4663" to="4673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Bouritsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09252</idno>
		<title level="m">Improving graph neural network expressivity via subgraph isomorphism counting</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gomes-Selman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.10320</idno>
		<title level="m">Identity-aware graph neural networks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saro</forename><surname>Passaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>L?tourneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02863</idno>
		<title level="m">Directional graph networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heli</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ben-Hamu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.09902</idno>
		<title level="m">Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On the universality of invariant networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haggai</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nimrod</forename><surname>Segol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4363" to="4371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Universal invariant and equivariant graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Keriven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Peyr?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04943</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Characterizing the expressive power of invariant and equivariant graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wa?ss</forename><surname>Azizian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lelarge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15646</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Coloring graph neural networks for node disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Dasoulas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic Dos</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Scaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aladin</forename><surname>Virmaux</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.06058</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Can graph neural networks count substructures?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soledad</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03199</idno>
		<title level="m">What graph neural networks cannot learn: depth vs width</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryoma</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03155</idno>
		<title level="m">Random features strengthen graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">The surprising power of graph neural networks with random node initialization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Abboud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?smaililkan</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Grohe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01179</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Vignac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
		<title level="m">Building powerful and equivariant graph neural networks with structural message-passing. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5165" to="5175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Revisiting graph neural networks for link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinglong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.16103</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Inductive matrix completion based on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ByxxgCEYDS" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><surname>Pim De Haan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08349</idno>
		<title level="m">Natural graph networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">and Michalis Vazirgiannis. k-hop graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Dasoulas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="195" to="205" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">Ver</forename><surname>Steeg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Graph meta learning via local subgraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<title level="m">Fast graph representation learning with pytorch geometric</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Quantum chemistry structures and properties of 134 kilo molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghunathan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pavlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Dral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O Anatole Von</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Moleculenet: a benchmark for molecular machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenqin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caleb</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aneesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Petra Mutzel, and Marion Neumann. Benchmark data sets for graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<ptr target="http://graphkernels.cs.tu-dortmund.de" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Distinguishing enzyme structures from non-enzymes without alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew J</forename><surname>Dobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of molecular biology</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kumar Debnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>De Compadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gargi</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">J</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corwin</forename><surname>Shusterman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hansch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medicinal chemistry</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="786" to="797" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Statistical evaluation of the predictive toxicology challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwin</forename><surname>Hannu Toivonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Helma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1183" to="1193" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Brenda, the enzyme database: updates and major new developments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ida</forename><surname>Schomburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antje</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ebeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Gremse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Heldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregor</forename><surname>Huhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Schomburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">suppl_1</biblScope>
			<biblScope unit="page" from="431" to="433" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Truong-Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Hy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kondor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04015</idno>
		<title level="m">Cormorant: Covariant molecular neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Directional message passing for molecular graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janek</forename><surname>Gro?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03123</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Orbnet: Deep learning for quantum chemistry using symmetry-adapted atomic-orbital features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoran</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Welborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animashree</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Frederick R Manby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">124111</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinglong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Malevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.01380</idno>
		<title level="m">Deep graph neural networks with shallow subgraph samplers</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">A fair comparison of graph neural networks for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Errica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Podda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Bacciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Micheli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.09893</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepergcn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07739</idno>
		<title level="m">All you need to train deeper gcns</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Principal neighbourhood aggregation for graph nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05718</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?my</forename><surname>Brossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriel</forename><surname>Frigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dehaene</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.15069</idno>
		<title level="m">Graph convolutions that can finally model local structure</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Bertolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>No?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arn?</forename><surname>Clevert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16584</idno>
		<title level="m">Parameterized hypercomplex graph neural networks for graph classification</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Graph warp module: an auxiliary module for boosting the power of graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhiko</forename><surname>Ishiguro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01020</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05178</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Graph u-nets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Resistance distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Randi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Chemistry</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="95" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
