<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Ordinal Regression using Optimal Transport Loss and Unimodal Output Probabilities</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-11-19">November 19, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Shaham</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Outcome Research and Evaluation</orgName>
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igal</forename><surname>Zaidman</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Bar Ilan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Svirsky</surname></persName>
						</author>
						<title level="a" type="main">Deep Ordinal Regression using Optimal Transport Loss and Unimodal Output Probabilities</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-11-19">November 19, 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It is often desired that ordinal regression models yield unimodal predictions. However, in many recent works this characteristic is either absent, or implemented using soft targets, which do not guarantee unimodal outputs at inference. In addition, we argue that the standard maximum likelihood objective is not suitable for ordinal regression problems, and that optimal transport is better suited for this task, as it naturally captures the order of the classes. In this work, we propose a framework for deep ordinal regression, based on unimodal output distribution and optimal transport loss. Inspired by the well-known Proportional Odds model, we propose to modify its design by using an architectural mechanism which guarantees that the model output distribution will be unimodal. We empirically analyze the different components of our proposed approach and demonstrate their contribution to the performance of the model. Experimental results on eight real-world datasets demonstrate that our proposed approach consistently performs on par with and often better than several recently proposed deep learning approaches for deep ordinal regression with unimodal output probabilities, while having guarantee on the output unimodality. In addition, we demonstrate that proposed approach is less overconfident than current baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Ordinal regression is an area of supervised machine learning, where the goal is to predict the value of a discrete dependent variable, whose set of (symbolic) possible values is ordered. Despite often overshadowed by more common applications like classification and regression, ordinal regression covers a wide range of important applications, such as prediction of failure times, ranking, age estimation and many more.</p><p>Many practitioners often treat ordinal regression problems as classification or regression problems (for example, this was indeed the case with many submissions to Kaggle's Diabetic Retinopathy competition 1 in 2015). While having common characteristics with both classification and regression, ordinal regression can arguably be viewed a mid-point between the two. An ordinal model is of course similar to a classification model, as both predict a discrete value ("label") out of a finite set of possible ones. However, the existence of an order on set of labels, when available, can potentially lead to an improved performance, comparing to a standard classifier, which does not assume such order. This typically occurs via distinguishing between the severity of prediction mistakes: while in classification typically "all mistakes are created equal", in ordinal regression different mistakes may be associated with different severity (for example, predicting "moderately-sized" when the ground truth value is "big" may be less severe than a "tiny" prediction. In regression problems, the dependent variable naturally does take values from an ordered set. However, this set is typically a continuum. Moreover, regression performance may be sensitive to monotonic transformations of the dependent variable, while such sensitivity does not take place in ordinal regression problems. Hence one may expect that typical ordinal regression algorithms have potential to outperform classification or regression approaches, when the range of the dependent variable is finite and ordered. In section 5 we will provide examples to the superiority of our proposed approach over classification and regression in benchmark tasks.</p><p>The arguably most fundamental ordinal regression model is the Proportional Odds Model (POM), a generalized linear model, similar in spirit to logistic regression, however where the logits are defined for cumulative probabilities. POM is typically trained via maximum likelihood (as is also the case for several recently proposed deep ordinal regression approaches, which will be reviewed in section 2). We argue that likelihood is a sub-optimal measure of quality for ordinal regression setup, as it only considers the probability mass the model assigns to the true class, ignoring the remaining mass. This implicitly assumes that "all mistakes are equal", which, as discussed above, is not the case for ordinal regression. Hence we seek for an alternative measure of quality which may be more appropriate for ordinal regression. We argue that the optimal transport divergence might be a better fit. In addition, this divergence turns out to be particularly appealing, as it obtains a simple, differentiable form in the case that one of the distributions is Dirac, which is indeed the case in ordinal regression; this will be explained in Section 3.</p><p>Another potential source of sub-optimality of POM (and of several recently-proposed approaches for deep ordinal regression) is the often-reasonable requirement that a probabilistic model for ordinal regression will output unimodal probabilities (i.e., that when moving in either direction from the most probable class, the probabilities predicted by the model will decay in a monotonic fashion. Although there are domains in which unimodality is not necessarily a desirable property, such as in movie rankings (where people may have either positive or negative definitive opinions about a particular movie), in many other real world domains it is a natural requirement, for example when predicting age of a person or a grade of a tumor, as it may be counter-intuitive to trust a model prediction which says that a predicted tumor grade is either 1 or 4, but not 2 or 3. However, despite often being a desired characteristic, unimodality is unfortunately not always fulfilled. While this was identified by several recent works for deep ordinal regression, unimodality is often encouraged (but not enforced) via soft targets. In the next section, we will argue that this is a sub-optimal means to achieve unimodality. To the contrary, we propose a novel mechanism to enforce unimodality of the output distribution, implemented via architectural design, and demonstrate that it does not hurt the level of performance.</p><p>Experimentally, we analyze the contribution of the optimal transport objective and our proposed unimodality mechanism to the performance of the model, and provide results on eight real world image benchmark datasets which demonstrate that our proposed approach consistently performs on par with and often better than several recently proposed approaches for deep ordinal regression, while having a unimodality guarantee. In addition, we demonstrate that the predictions made by our proposed approach tend to be consistently less overconfident than those of the competing methods, manifested in greater uncertainty in cases of wrong predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Being a traditional area of machine learning and statistics, there exists a large corpus of literature on ordinal regression. In this section we focus on approaches based on deep architectures. Several such approaches were proposed in the recent years. A common approach seems to be to turn the ordinal regression problem into a multi-label classification problem, for example <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6]</ref>. We argue that the multi-label approach has two major problematic aspects: first, the output probabilities are not always guaranteed to be consistent, in the sense of increasing cumulative distribution (i.e., we would like to predict Pr(y ? 1) ? Pr(y ? 2) ? . . . ? Pr(y ? k). Second, even if the output probabilities are consistent, as is the case in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b3">4]</ref> for example, the predicted class probabilities are not necessarily unimodal, i.e., there is no guarantee for existence of j ? {1, . . . , k} such that</p><formula xml:id="formula_0">Pr(Y = 1) ? . . . ? Pr(Y = j) ? . . . ? Pr(Y = k)</formula><p>. This is the case in several recent works, e.g., <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b24">25]</ref>. <ref type="bibr" target="#b0">[1]</ref> proposed an elegant mechanism to obtain unimodal output probabilities, based on either the Poisson or the Binomial distributions, which are both unimodal. In both cases their model outputs a scalar (? in case of the Poisson, p in the case of the binomial), which is then mapped to a probability mass function that uses (after normalization) as the model output probabilities. While being a convenient, architectural-based solution for the unimodality issue, their approach is inherently limited in its ability to express the level of uncertainty of the model's prediction: since a single parameter determines both the location of the mode, and the decay of the probabilities, the model cannot output a highly flat or highly peaked probability vector; in addition, instances of the same predicted class ought to have similar output probabilities. Inspired by their approach, we utilize the normal distribution, in which one parameter determines the location while another determines the decay. In section 5 we will demonstrate that this greater flexibility yields an improvement in performance. <ref type="bibr" target="#b1">[2]</ref> propose a constrained optimization approach to achieve unimodality. However, this comes at a cost of a somewhat cumbersome optimization process. More importantly, even if unimodality is indeed achieved for the train data, there are no guarantees that this will also be the case for unseen test data.</p><p>Several works propose to handle the unimodality requirement via soft targets, for example <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b16">17]</ref>. Despite the fact that usage of soft targets to obtain unimodality is sub-optimal, as it does not guarantee unimodal outputs at inference (and not even at train time), it often led to improved performance, comparing to <ref type="bibr" target="#b0">[1]</ref>, where unimodality is guaranteed. In section 5 we will demonstrate that the proposed approach enables one to enjoy both worlds, and have a unimodality guarantee while not hurting the quality of predictions.</p><p>Several works use cross entropy as a training objective, while using one-hot (or binary) targets, see, for example <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b3">4]</ref>. As pointed out in several papers, and will also be demonstrated in section 3, in the case of one-hot targets, the cross entropy term equals the negative log of the probability assigned by the model to the true class, making it invariant to the distribution of the remaining probability mass. While a reasonable thing in a standard classification setting, this ignores the order of the classes, making it a suboptimal choice for ordinal regression setting. To overcome this limitation of cross entropy <ref type="bibr" target="#b13">[14]</ref>, followed by <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18]</ref> use optimal transport loss, which is a natural way to incorporate the order of the classes into the loss term. In this sense, it is similar to the approach we take in this manuscript.</p><p>To summarize this section, we identify the following requirements for an appropriate ordinal regression model:</p><p>? Unimodality of the model's output distribution.</p><p>? It is advantageous to enforce the unimodality via the design of the model, rather than via soft targets.</p><p>? A model utilizing one-hot targets should not be trained using cross entropy objective (or maximum likelihood in general).</p><p>? The decay of the output probabilities should reflect the uncertainty of the model in its predictions.</p><p>These requirements naturally lead us to our proposed approach in section 4. However, before we specify it, we begin with a brief review of of the proportional odds model and optimal transport divergence. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>We begin this section with a description of the proportional odds model from a latent variable perspective. We then briefly review optimal transport as a divergence between two probability distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Proportional Odds Model</head><p>Let (X, Y ) ? X ?Y be random variables, having joint probability P XY , where X = R d and Y = {1, . . . , k}, where 1, . . . , k are considered as symbols. Let be an order relation defined on Y such that 1 . . . k. The proportional odds model is parametrized by ? ? R k?1 , ? ? R d and applies to data</p><formula xml:id="formula_1">{(x i , y i )} n i=1 , sampled i.i.d from P XY .</formula><p>Let be a logistic random variable (thus having a sigmoid cumulative distribution function F (x) = 1 1+exp(?x) ), and let Z be a random variable defined as Z = ? T X + . The entries of ? use to define the cumulative conditional probabilities via</p><formula xml:id="formula_2">Pr(Y j|X = x) = Pr(Z ? ? j ) = F (? j ? ? T x).<label>(1)</label></formula><p>Similarly to logistic regression, this yields linear log-odds (logits), however, defined with respect to cumulative terms</p><formula xml:id="formula_3">? j ? log Pr(Y j|X = x) Pr(Y j|X = x) = ? j ? ? T x.</formula><p>It is convenient to interpret equation <ref type="formula" target="#formula_2">(1)</ref> by viewing ? T x as a factor that shifts the standard logistic density function, while the ? j terms are thresholds, with respect to which the cumulative probabilities are defined. This is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>. Let (x, y) be a realization of (X, Y ). The likelihood assigned by the model to (x, y) is</p><formula xml:id="formula_4">L(?, ?; (x, y)) = Pr(Y = y|X = x; ?, ?) =F(? y ? ? T x) ? F(? y?1 ? ? T x),<label>(2)</label></formula><p>considering ? 0 = ?? and ? k = ?. The model is typically trained in a standard fashion by maximizing the log-likelihood function on the training data. </p><formula xml:id="formula_5">= k) = Pr(Z ? ? k )?Pr(Z ? ? k?1 ).</formula><p>Therefore, the above plot shows an example where the output probabilities are such that</p><formula xml:id="formula_6">Pr(Y = 1) &gt; Pr(Y = 2) &lt; Pr(Y = 3) &gt; Pr(Y = 4) (i.e.</formula><p>, the output probabilities are bimodal and not unimodal).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3:</head><p>The likelihood function of POM is invariant to the way the predicted probability mass of the incorrect classes is assigned. In the above example the correct class is 3, and the two instances have the same likelihood, despite the fact that in the bottom case, the probability mass assigns to neighboring class 2 is larger, making the bottom case more appropriate than the top one.</p><p>Despite its popularity, the POM suffers from two main issues: First, the model's output probabilities are not necessarily unimodal. This is depicted in <ref type="figure" target="#fig_1">Figure 2</ref>. Second, the likelihood function (2) depends only on the probability the model assigns to the correct class y, and is invariant to the way the remaining probability mass is assigned by the model. This ignores the order on the label set, and hence does not use important information that might be used to improve prediction quality, as depicted in <ref type="figure">Figure 3</ref>.</p><p>It is important to mention that as cross entropy term is essentially equivalent to model's negative loglikelihood function, this invariance to the partition of the remaining mass over the incorrect classes is common to all models trained via cross entropy minimization, as long as the target labels are one-hot.</p><p>In section 4 we will show how our method overcomes these two limitations of the POM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Optimal Transport</head><p>Let (M, d) be a finite metric space, and let p, q be probability mass functions defined on M. Optimal transport, also denoted as the 1-Wasserstein distance and the Earth Mover Distance, between p and q is</p><formula xml:id="formula_7">OT (p, q) = inf ??? M ?M c(x, y)d?(x, y),<label>(3)</label></formula><p>where ? is the set of all joint probabilities on M ? M , having marginals p and q, and the metric c specifies the costs of moving probability mass between every two elements of M . This amounts to the optimal transportation of probability mass that transforms p into q and vice versa. In the general case, the distance can be found by solving a linear program, and several relaxations have been proposed to accelerate its computations while preserving its geometrical properties, see, for example, <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b6">7]</ref>. However, in the case where p is a Dirac point mass (i.e., having a a one-hot probability mass function), solving equation <ref type="formula" target="#formula_7">(3)</ref> becomes trivial and becomes where j is the correct class, and k is the total number of classes, as is also depicted in <ref type="figure" target="#fig_2">Figure 4</ref>. Letting q denote a model's output probabilities and p denote a one-hot target, equation <ref type="formula" target="#formula_8">(4)</ref> is of course differential with respect to the model outputs q and therefore can be used as a loss term for gradient-based optimization. The cost metric c can incorporate domain knowledge in order to quantify the semantic distance between every two elements of M . Since in our case M = {1, . . . , k} is an ordered space, a natural possibility is to define</p><formula xml:id="formula_8">OT (p, q) = k i=1 q i c(i, j),<label>(4)</label></formula><formula xml:id="formula_9">c(i, j) = |i ? j| m ,</formula><p>for some m ? 1, i.e., ,mapping the symbolic class labels to consecutive integers, and computing powers of absolute differences. When m = 1, the optimal transport can also be computed as the 1 distance between the cumulative mass functions 2 , CMF(p) ? CMF(q) 1 , (see <ref type="bibr" target="#b15">[16]</ref>, for example). This is equivalent to the computation in equation <ref type="formula" target="#formula_8">(4)</ref>, and also generalizes to arbitrary targets (i.e., not necessarily Dirac).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Proposed Approach</head><p>In this section we describe our proposed mechanism for architectural-based generation of unimodal output probability distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Rational</head><p>The fact that the unimodality is obtained directly via architectural design has a major advantage over using soft targets for training, since the output probabilities are guaranteed to be unimodal for every input instance, as is also the case for the mechanism proposed by <ref type="bibr" target="#b0">[1]</ref>. However, unlike <ref type="bibr" target="#b0">[1]</ref>, our proposed approach employs the normal distribution, depending separately on a location parameter and a scale parameter, so that the location of the mode is detached from the decay of the probability mass, which yields a more flexible design than the single-parameter distributions used by <ref type="bibr" target="#b0">[1]</ref>, in which the single parameter determines both the mode and the decay. We will demonstrate in section 5 that this greater flexibility is helpful in expressing prediction uncertainty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Unimodal Output Probabilities Generation</head><p>Inspired by the POM, we utilize thresholds to define bins, so that the total mass inside each bin is the output probability of the corresponding class. However, observe that the lack of unimodality of POM can be fixed by letting the bins be of equal length and remain fixed during training. Therefore, instead of learning the thresholds, during training a map x ? (?, ?) is learned, where ? is a location parameter, and ? is a scale parameter, which define a N (?, ? 2 ) distribution, using which the output probabilities are computed.</p><p>Formally, we divide the range [?1, 1] to k equal bins, where k is the number of classes, defined by ?1 = ? 0 , ? 1 , ..., ? k = 1, so that ? i ? ? i?1 = 2 k . The (un-normalized) probabilities are given b?</p><formula xml:id="formula_10">p i (x) = ? ?(x),?(x) (? i ) ? ? ?(x),?(x) (? i?1 ),<label>(5)</label></formula><p>where ? ?,? (?) is the N (?, ? 2 ) cumulative distribution function, and we have emphasized that ?, ? are in fact functions of the input instance x. Since the bins cover [-1,1] and not the entire real line, we normalize the probabilities to obtain proper model predictions via</p><formula xml:id="formula_11">Pr (Y = i|x) = p i (x) ?p i (x) k j=1p j (x) .<label>(6)</label></formula><p>To compensate for the fact that the probability generating mechanism depends on less parameters than POM (2 for the former, d + k ? 1 for the latter), the map x ? (?, ?) is expressed via a deep network, which is therefore able to represent a complex nonlinear relation. Our proposed mechanism for generation of unimodal output probabilities is depicted in <ref type="figure" target="#fig_3">Figure 5</ref>.</p><p>The following lemma, proved in Appendix A, establishes that the model output probabilities are indeed unimodal. We remark that all arguments made here with regard to the normal distribution also hold for other unimodal distributions, which are symmetric around ?, such as Logistic(?, ?) and Cauchy(?, ?), which both have slower decay patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Training procedure</head><p>To summarize this section, given an instance x, our model outputs parameters ? ? (x), ? ? (x) of a normal distribution, where ? collectively denotes the weights of the network. ? ? (x), ? ? (x) determine the un-normalized categorical probabilitiesp i (x), i = 1, . . . , k via equation <ref type="bibr" target="#b4">(5)</ref>, which are then normalized to p i (x), i = 1, . . . , k using equation <ref type="bibr" target="#b5">(6)</ref>. Following equation <ref type="formula" target="#formula_8">(4)</ref>, the objective equation used to train the model is</p><formula xml:id="formula_12">l(?; x) = k i=1 p i (x)c(i, j),</formula><p>where j ? 1, . . . , k is the correct class of x. In our experiments we used c(i, j) = |i ? j|.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>To analyze the components of the proposed approach, we begin this section with small-scale ablation studies on the Abalone dataset. We then report experimental results on eight real world benchmark image datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablation Studies</head><p>Here we use a modified version of the Abalone dataset <ref type="bibr" target="#b2">3</ref> . The dependent variable counts the number of rings inside the abalone shell, corresponding to its age, which should be predicted from 8 numeric features. We slightly re-arranged the range of the dependent variable using a hand-crafted monotonic transform, resulting in a a partition of the data to eight classes, with number of instances per class ranging between 391 and 689.</p><p>The data file appears in supplementary material. <ref type="table">Table 1</ref> shows the performance of our proposed approach, along with the following baselines:</p><p>? A neural network regression model, trained using squared error loss. The predicted class is obtained by rounding the model's (linear) output.</p><p>? A neural network classification model, trained using cross entropy loss and one-hot targets.</p><p>? A standard POM. As POM is a generalized linear model, we expect that it may be outperformed by the other baselines, which are all non-linear.</p><p>In addition, to evaluate the contribution of the optimal transport objective and the unimodal probabilities mechanism, we also train two "hybrid" models:</p><p>? A neural network classification model, trained using optimal transport loss and one-hot targets (classification OT)</p><p>? A neural network model, with our proposed mechanism for unimodal output probabilities, trained using cross entropy loss, and one-hot targets (Unimodal CE)</p><p>All neural network models shared the same architecture, except for the output layer, and were trained using identical batch sizes and learning rate policies.  <ref type="table">Table 1</ref>: Performance of various methods on the Abalone dataset, in terms of mean ? standard deviation over 5 independent trials. <ref type="table">Table 1</ref> demonstrates a few interesting properties of the proposed approach. First, by analyzing the performance of the baseline models, we observe that the proposed approach outperforms standard regression and classification, in terns of both Mean Absolute Error (MAE), Spearman correlation and Quadratic Weighted Kappa (QWK). All three methods (the proposed, classification and regression) outperform POM on this dataset, perhaps due to the latter being a linear model, unlike the other ones. By looking at the performance of the hybrid models, we see that optimal transport objective, missing in Unimodal CE, and the proposed unimodal probabilities generation mechanism, missing in Classifier OT, both contribute to the performance. In addition, we observe that the optimal transport objective does not manifest its full advantage with softmax-generated probabilities, perhaps as softmax often yield a peaked probability mass function. Similarly, cross entropy seem to work better with standard softmax, comparing to our proposed unimodality mechanism. We observed these phenomena also in several other experiments, which are not described here. Analysis of uncertainty <ref type="bibr" target="#b1">[2]</ref> claims that methods that learn a variance parameter may simply push it to zero, hence this parameter need to be manually set. We show that in our case, the learned map x ? (?, ?) does not yield a vanishing standard deviation parameter. Rather, the standard deviation corresponds to the uncertainty of the model in its predictions. To see that, we plot in <ref type="figure" target="#fig_5">Figure 6</ref> the histograms of the probability of the mode of the predicted distribution (i.e., histograms of max{p 1 , . . . , p k }), as a function of whether or not the mode predicts the ground truth class or not. As can be seen, the modes are higher in cases of correct predictions, than in cases of incorrect predictions. Put another way, the prediction confidence is lower in wrong predictions, comparing to correct predictions. The higher the mode is, the more peaked the output probability function is, and the predictions indeed tend to be more accurate. We will observe a similar phenomenon in our experiments on real world datasets, reported next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results on Real World Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Datasets</head><p>We evaluate our method on eight real world benchmark image datasets, involving various ordinal regression tasks: age-detection (Adience <ref type="bibr" target="#b8">[9]</ref>, FG-Net <ref type="bibr" target="#b11">[12]</ref>, AAF <ref type="bibr" target="#b4">[5]</ref>, AFAD-LITE <ref type="bibr" target="#b22">[23]</ref>, WIKI <ref type="bibr" target="#b26">[27]</ref>), bio-medical image classification (Retina-MNIST <ref type="bibr" target="#b29">[30]</ref>), historical image dating (HCI <ref type="bibr" target="#b23">[24]</ref>) and image aesthetics estimation (EVA <ref type="bibr" target="#b14">[15]</ref>). A more detailed description of the datasets appears in Appendix B. Some examples from the Adience, HCI and Retina-MNIST datasets are shown in <ref type="figure" target="#fig_6">Figures 7, 8 and 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Methods Compared</head><p>We compare our proposed approach to four recently proposed approaches for deep ordinal regression, with unimodal output probabilities:</p><p>? DLDL <ref type="bibr" target="#b12">[13]</ref>, an approach utilizing soft labels, generated using squared exponentially decaying distributions, trained using Kullback-Leibler divergence minimization (equivalent to cross entropy minimization).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Method MAE OOA Spearman QWK % Unimodal Entropy ratio HCI <ref type="bibr" target="#b0">[1]</ref> .62 ? .04 .85 ? .02 .71 ? .03 .75 ? .02 1 ? 0 .82 ? .1 <ref type="bibr" target="#b17">[18]</ref> .57 ? .05 .86 ? .02 .68 ? .04 .74 ? .03 .4 ? .04 .81 ? .1 <ref type="bibr" target="#b12">[13]</ref> .71 ? .04 .87 ? .02 .67 ? .03 .7 ? .03 .99 ? .02 1.28 ? .13 <ref type="bibr" target="#b7">[8]</ref> . Adience <ref type="bibr" target="#b0">[1]</ref> .53 ? .08 .94 ? .02 .9 ? .02 .91 ? .03 1 ? 0 1 ? .01 <ref type="bibr" target="#b17">[18]</ref> .48 ? .06 .94 ? .02 .88 ? .03 .9 ? .03 .53 ? .05 1.08 ? .01 <ref type="bibr" target="#b12">[13]</ref> .5 ? .08 .94 ? .02 .88 ? .02 .9 ? .03 .6 ? .05 1. ? .01 <ref type="bibr" target="#b7">[8]</ref> .  <ref type="bibr" target="#b17">[18]</ref> .39 ? .01 .98 ? .01 .82 ? .01 .85 ? .01 .7 ? .03 1.08 ? .01 <ref type="bibr" target="#b12">[13]</ref> .4 ? .01 .98 ? .01 .82 ? .01 .85 ? .01 .9 ? .03 1. ? .01 <ref type="bibr" target="#b7">[8]</ref> .  <ref type="bibr" target="#b17">[18]</ref> .5 ? .01 .92 ? .01 .67 ? .01 .69 ? .01 .96 ? .01 1.11 ? .01 <ref type="bibr" target="#b12">[13]</ref> .5 ? .01 .93 ? .01 .67 ? .01 .69 ? .01 1 ? .01 1 ? .01 <ref type="bibr" target="#b7">[8]</ref> .  <ref type="bibr" target="#b17">[18]</ref> .61 ? .02 .92 ? .01 .56 ? .03 .56 ? .03 .68 ? .02 1.04 ? .01 <ref type="bibr" target="#b12">[13]</ref> .62 ? .03 .92 ? .01 .55 ? .03 .54 ? .03 .91 ? .02 1. ? .01 <ref type="bibr" target="#b7">[8]</ref> . .68 ? .01 .92 ? .01 .68 ? .01 .68 ? .01 1 ? 0 1. ? .01 <ref type="bibr" target="#b17">[18]</ref> .42 ? .01 95. ? .01 .69 ? .01 .7 ? .01 .95 ? .01 1.05 ? .01 <ref type="bibr" target="#b12">[13]</ref> .44 ? .01 .94 ? .01 .68 ? .01 .7 ? .01 .97 ? .01 1. ? .01 <ref type="bibr" target="#b7">[8]</ref> .44 ? .01 .94 ? .01 .69 ? .01 .7 ? .01 .99 ? .01 1.03 ? .01 Proposed .43 ? .01 .95 ? .01 .69 ? .01 .71 ? .01 1 ? 0 1.01 ? 0.01 <ref type="table">Table 2</ref>: Performance of various methods on real world datasets, in a mean ? std format.</p><p>? SORD <ref type="bibr" target="#b7">[8]</ref>, an approach utilizing soft labels, generated using linear exponentially decaying distributions, trained using Kullback-Leibler divergence minimization. . <ref type="figure">Figure 8</ref>: Examples from the HCI dataset. Decades categories are indicated above.</p><p>.</p><p>? <ref type="bibr" target="#b0">[1]</ref>, where architectural-based unimodal output probabilities are generated using binomial distribution (single-learned parameter), trained using optimal transport loss.</p><p>? <ref type="bibr" target="#b17">[18]</ref>, an approach utilizing soft labels, created as a mixture of Dirac, uniform and linear exponentially decaying distributions, trained using optimal transport loss.</p><p>In order to perform a fair comparison, we implemented all methods, using the same image transformations, backbone CNN and training procedures, so that the methods differ only in their output layer architectures and loss functions. We performed 5 independent trials, using the same train-validation-test splits for all methods. Additional technical details can be found in Section C. For reproducibility, our GitHub repository https: //github.com/jsvir/uniord contains code reproducing the results reported in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Evaluation Metrics</head><p>We report several commonly-used evaluation metrics for ordinal regression tasts: MAE, One-Off Accuracy (OOA), Spearman correlation, QWK, as well as the percentage of test examples with unimodal predicted output probabilities. In addition, viewing the output probabilities as a multinomial distribution, we evaluate the level of overconfidence by comparing the ratio of the average entropy in incorrect predictions, to the average entropy in correct predictions. A higher ratio is therefore desirable and indicates a lower level of overconfidence. <ref type="table">Table 2</ref> shows the test results of each method on the eight benchmark datasets. As can be seen, the proposed approach performs at least on-par and often better than the compared baselines, in a fairly consistent manner, across the various datasets and evaluation metrics. In addition, observe that only the proposed approach and the method of <ref type="bibr" target="#b0">[1]</ref> output unimodal probabilities, both via architectural design, while the other baselines, <ref type="figure">Figure 9</ref>: Examples from the Retina mnist dataset. Diabetic Retinopathy classes are indicated above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Results</head><p>trained using soft targets, do not always output unimodal probabilities. Moreover, on six of the eight datasets, the proposed approach outperforms all baselines also in terms of overconfidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this manuscript we presented an approach for deep ordinal regression, inspired by the proportional odds model, utilizing an architectural mechanism for generation of unimodal output probabilities, and trained using optimal transport objective. We empirically analyzed the components of the proposed approach, and demonstrated that they both contribute to the performance of the model. We demonstrated that while performing on-par with or better than other recently proposed approaches for ordinal regression, our proposed method enjoys the benefits of guaranteed unimodal output probabilities, and of less overconfidence about its incorrect predictions. <ref type="figure" target="#fig_0">Figure 10</ref>: Examples from the AFAD-LITE dataset. Age classes are indicated above. <ref type="figure" target="#fig_0">Figure 11</ref>: Examples from the FG-Net dataset. Age classes are indicated above.</p><p>AAF (All-Age-Faces) dataset is already pre-processed and contains 13,322 face images (mostly Asian), distributed across all ages (from 2 to 80). We partitioned the dataset into 6 classes and augmentations were the same as in the Adience experiment. WIKI dataset contains 62,328 images of celebrities from wikipedia with ages ranging form 1 to 100. Images with wrong timestamps are removed, and the dataset is partitioned into 6 classed. Augmentations were same as in the Adience experiment. <ref type="figure" target="#fig_0">Figures 10, 11</ref>   <ref type="table" target="#tab_9">Table 4</ref> shows the technical details for the experiments on the real world benchmark datasets reported in this manuscript. The Adam optimizer was used in all experiments, with the default ? = (0.9, 0.999). The means and standard deviations reported in table 2 are based on 10 repetitions of each experiment, differing in weights initialization and random train-test splits, except for Adience, for which we repeated the experiment five <ref type="figure" target="#fig_0">Figure 12</ref>: Examples from the EVA dataset. Aesthetics classes are indicated above. <ref type="figure" target="#fig_0">Figure 13</ref>: Examples from the AAF dataset. Age classes are indicated above.   <ref type="figure" target="#fig_0">Figure 14</ref>: Examples from the WIKI dataset. Age classes are indicated above.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The proportional odds model. xi is a realization of X. The standard logistic density is shifted by ? T xi. The thresholds ?j define the bins which determine the probability predicted by the model to each class. For example, the green area defines the probability Pr(Y = 3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>POM does not always output unimodal probabilities. Recall that Pr(Y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Optimal transport between model output probability mass function and a Dirac (one-hot) probability mass, using c(i, j) = |i ? j| m cost.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Generation of unimodal output probabilities for k = 3 classes. An input x is mapped to a (?, ?) pair, which define a normal distribution N (?(x), ?(x) 2 ) over the real line. The output probabilities are proportional to the mass in the bins, which are of equal length. The green area equals to the un-normalized probabilityp2(x), corresponding to Pr(Y = 2|X = x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Lemma 4 . 1 .</head><label>41</label><figDesc>Let x ? R d be an input to the model, which is mapped to ? = ?(x), ? = ?(x), and let p 1 , . . . p k be the model output probabilities, generated via equation<ref type="bibr" target="#b5">(6)</ref>. Then p 1 , . . . p k define a unimodal multinomial random variable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Histograms showing the probability of the mode of the output probabilities for Abalone test instances with correct and incorrect predictions .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Examples from the Adience dataset. Age category is indicated above each image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>? .02 0.75 ? .02 0.76 ? .02 Classification 1.12 ? .02 0.72 ? .02 0.73 ? .01 POM 1.43 ? .01 0.63 ? .01 0.73 ? .01 Proposed 0.98 ? .03 0.78 ? .01 0.79 ? .02 Classification OT 1.12 ? .02 0.69 ? .01 0.75 ? .01 Unimodal CE 1.70 ? .54 0.63 ? .11 0.67 ? .07</figDesc><table><row><cell>Method</cell><cell>MAE</cell><cell>QWK</cell><cell>Spearman</cell></row><row><cell>Regression</cell><cell>1.01</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>57 ? .04 .86 ? .02 .69 ? .03 .75 ? .02 .96 ? .01 .71 ? .06 Proposed .54 ? .03 .89 ? .01 .7 ? .03</figDesc><table><row><cell>.77 ? .02</cell><cell>1 ? 0</cell><cell>.95 ? 0.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>47 ? .07 .94 ? .02 .89 ? .01 .91 ? .03 Proposed .35 ? .03 .98 ? .01 .84 ? .02 .87 ? .01 .97 ? .01 .83 ? .01 .85 ? .05 1 ? 0 1. ? .01</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>.99 ? .01</cell><cell>1.06 ? .03</cell></row><row><cell></cell><cell cols="2">Proposed .45 ? .05 .95 ? .01</cell><cell>.9 ? .02</cell><cell>.92 ? .02</cell><cell>1 ? 0</cell><cell>1.19 ? .16</cell></row><row><cell></cell><cell>[1]</cell><cell>.78 ? .02 .8 ? .01</cell><cell>.6 ? .01</cell><cell>.55 ? .02</cell><cell>1 ? 0</cell><cell>1.02 ? 0.011</cell></row><row><cell></cell><cell>[18]</cell><cell>.69 ? .02 .82 ? .01</cell><cell>.6 ? .02</cell><cell>.58 ? .02</cell><cell>.69 ? .04</cell><cell>1.05 ? .02</cell></row><row><cell>Retina MNIST</cell><cell>[13]</cell><cell>.76 ? .09 .8 ? .04</cell><cell cols="2">.59 ? .06 .54 ? .09</cell><cell>.94 ? .07</cell><cell>1. ? .01</cell></row><row><cell></cell><cell>[8]</cell><cell>.77 ? .06 .79 ? .3</cell><cell cols="2">.57 ? .05 .56 ? .05</cell><cell>.88 ? .03</cell><cell>1.13 ? .01</cell></row><row><cell></cell><cell cols="3">Proposed .68 ? .01 .83 ? .01 .62 ? .01</cell><cell>.6 ? .01</cell><cell>1 ? 0</cell><cell>2.08 ? .08</cell></row><row><cell></cell><cell>[1]</cell><cell cols="2">.46 ? .01 .94 ? .03 .75 ? .04</cell><cell>.8 ? .03</cell><cell>1 ? 0</cell><cell>1.01 ? .01</cell></row><row><cell></cell><cell>[18]</cell><cell cols="3">.36 ? .05 .96 ? .01 .82 ? .06 .83 ? .05</cell><cell>.2 ? .04</cell><cell>1.27 ? .1</cell></row><row><cell>FG-NET</cell><cell>[13]</cell><cell cols="3">.46 ? .05 .94 ? .02 .75 ? .05 .77 ? .04</cell><cell>.09 ? .03</cell><cell>1. ? .1</cell></row><row><cell></cell><cell>[8]</cell><cell>.38 ? .05 .95 ? .02</cell><cell>.8 ? .04</cell><cell>.83 ? .04</cell><cell>.98 ? .01</cell><cell>1.13 ? .04</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>.03</cell><cell>1 ? 0</cell><cell>1.58 ? 0.16</cell></row><row><cell></cell><cell>[1]</cell><cell>.44 ?</cell><cell></cell><cell></cell><cell></cell></row><row><cell>AAF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>39 ? .01 .98 ? .01 .82 ? .02 .85 ? .01 1 ? .01 1.05 ? .01 Proposed .38 ? .01 .98 ? .01 .83 ? .01 .86 ?</figDesc><table><row><cell></cell><cell>.01</cell><cell>1 ? 0</cell><cell>1.2 ? 0.01</cell></row><row><cell>[1]</cell><cell>.51 ? .01 .91 ? .01 .67 ? .01 .69 ? .01</cell><cell>1 ? 0</cell><cell>2.18 ? .05</cell></row><row><cell>AFAD-LITE</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>5 ? .01 .92 ? .01 .67 ? .01 .69 ? .01 1 ? .01 1.1 ? .01 Proposed .49 ? .01 .93 ? .01 .68 ?</figDesc><table><row><cell></cell><cell></cell><cell>.01</cell><cell>.7 ? .01</cell><cell>1 ? 0</cell><cell>2.26 ? 0.23</cell></row><row><cell>[1]</cell><cell>.63 ? .02 .92 ? .01</cell><cell>.6 ? .03</cell><cell>.6 ? .02</cell><cell>1 ? 0</cell><cell>1. ? .01</cell></row><row><cell>EVA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>59 ? .03 .93 ? .01 .57 ? .03 .55 ? .03 .99 ? .01 1.01 ? .01 Proposed .58 ? .02 .94 ? .01 .58 ? .03 .56 ? .03</figDesc><table><row><cell>1 ? 0</cell><cell>1.05 ? 0.02</cell></row><row><cell>[1]</cell><cell></cell></row><row><cell>WIKI</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>,<ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14</ref> show examples from the AFAD-LITE, FG-Net, EVA, AAF and WIKI datasets.</figDesc><table><row><cell>Dataset</cell><cell>Task</cell><cell cols="3">Train images Test Images Classes</cell></row><row><cell>Adience</cell><cell>age estimation</cell><cell cols="2">pre-defined splits</cell><cell>8</cell></row><row><cell>HCI</cell><cell>image dating</cell><cell>1,276</cell><cell>50</cell><cell>5</cell></row><row><cell>FG-Net</cell><cell>age estimation</cell><cell>902</cell><cell>100</cell><cell>8</cell></row><row><cell cols="2">RetinaMNIST DR classification</cell><cell>1200</cell><cell>400</cell><cell>5</cell></row><row><cell>AFAD-LITE</cell><cell>age estimation</cell><cell>37980</cell><cell>11869</cell><cell>4</cell></row><row><cell>AAF</cell><cell>age estimation</cell><cell>9058</cell><cell>2665</cell><cell>6</cell></row><row><cell>EVA</cell><cell>aesthetics estimation</cell><cell>2940</cell><cell>611</cell><cell>5</cell></row><row><cell>WIKI</cell><cell>age estimation</cell><cell>38660</cell><cell>12082</cell><cell>6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Benchmark datasets characteristics</figDesc><table><row><cell>C Technical details</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Technical details of the experiments times, using the same train-test splits as the creators of the dataset 4 .</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This holds when the classes are ordered.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://archive.ics.uci.edu/ml/datasets/abalone</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/GilLevi/AgeGenderDeepLearning/tree/master/Folds/train val txt files per fold</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Proof of Lemma 4.1</p><p>Proof. Let p i , p i+1 be the output probabilities of two adjacent classes, and let ?1 = ? 0 , . . . , ? k = 1 be the thresholds. We will show that (i) if ? ? ? i?1 then p i ? p i+1 . Symmetrical argument will then imply that</p><p>, whenever the latter exists. Similarly, this would imply that p i &gt; p i?1 . Together, (i) and (ii) will imply the statement of the lemma.</p><p>Denote by f the density of the N (?, ? 2 ) distribution. To prove (i) observe thatp i &gt; 2f (?i)</p><p>To prove (ii), divide the i'th bin to two sub-bins B i,1 , B i,2 , of lengths a = ? ? ? i?1 and b = ? i ? ?, respectively. Similarly, divide the i + 1'th bin to two bins B i+1,1 , B i+1,2 lengths b and a. Then from (i)</p><p>In addition, observe that</p><p>Adding up equations <ref type="formula">(7)</ref> and <ref type="formula">(8)</ref>, we obtainp i &gt;p i+1 , which gives p i &gt; p i+1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Datasets</head><p>Tabel 3 contains information on the benchmark datasets used for our experiments. HCI (Historical Color Image) dataset contains 1326 images, partitioned to 5 classes, corresponding to decades from 1930s to 1970s, and the task is to associate each images with the decade it was taken at. Random affine, radom horizontal / vertical flips and random crops of 224 transformations are applied during the training. The images are normalized in each color channel with mean and standard deviation of 0.5. The dataset was randomly split to the train/test as described in the <ref type="table">Table 4</ref>. Adience: During the training the images are resized to (256,256) and random crop of size 224 and random horizontal flip are applied as augmentations. FG-Net: We partitioned the dataset to 8 classes, corresponding to decades. Augmentations were same as in the Adience experiment.</p><p>RetinaMNIST dataset has 5 classes and we apply random affine, horizontal and vertical flips as augmentations during the training. The size of the images is <ref type="bibr" target="#b27">(28,</ref><ref type="bibr" target="#b27">28)</ref> as it provided by the dataset contributors. The train/test splits are proved by the contributors and were used as-is. AFAD-LITE is a subset of the full AFAD dataset, which contains images of 22 continuous ages (for 15 to 40), in the amount of 60K. We partitioned the dataset into 4 classes and augmentations were the same as in the Adience experiment. EVA (Explainable Visual Aesthetics) dataset containes 4070 images aesthetically ranked from 0 to 10 by multiple voters. We calculate the average score for each image and partition the data into 5 classes in accordance with the average score. Augmentations were same as in the Adience experiment.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unimodal probability distributions for deep ordinal classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="411" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soufiane</forename><surname>Belharbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><surname>Ben Ayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Mccaffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Granger</surname></persName>
		</author>
		<title level="m">Non-parametric uni-modality constraints for deep ordinal classification. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">1911</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Oskarsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark O&amp;apos;</forename><surname>Connor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15864</idno>
		<title level="m">Deep ordinal regression with label diversity</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rank consistent ordinal regression for neural networks with application to age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vahid</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Raschka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="325" to="331" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploiting effective facial patches for robust gender recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tsinghua Science and Technology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="333" to="345" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A neural network approach to ordinal regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianluca</forename><surname>Pollastri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1279" to="1284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2292" to="2300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Soft labels for ordinal regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Marathe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4738" to="4747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Age and gender estimation of unfiltered faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Eidinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Enbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2170" to="2179" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Interpolating between optimal transport and mmd using sinkhorn divergences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Feydy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>S?journ?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois-Xavier</forename><surname>Vialard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Shun-Ichi Amari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Trouv?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peyr?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2681" to="2690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kayhan</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2002" to="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Interestingness prediction by robust learning to rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="488" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep label distribution learning with label ambiguity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bin-Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Wei</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2825" to="2838" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Squared earth mover&apos;s distance-based loss for training deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Ping</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Samaras</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05916</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Eva: An explainable visual aesthetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Valenzise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?d?ric</forename><surname>Dufaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Workshop on Aesthetic and Technical Quality Assessment of Multimedia and Media Analytics for Societal Trends</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The earth mover&apos;s distance is the mallows distance: Some insights from statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizaveta</forename><surname>Levina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bickel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Eighth IEEE International Conference on Computer Vision. ICCV</title>
		<meeting>Eighth IEEE International Conference on Computer Vision. ICCV</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="251" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unimodal regularized neuron stick-breaking for ordinal classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangfang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingsheng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unimodal-uniform constrained wasserstein training for medical diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukai</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Site</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ordinal regression with neuron stick-breaking for medical diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bv K Vijaya</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep ordinal regression based on data relationship for small datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><surname>Wai-Kin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><forename type="middle">Keong</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2372" to="2378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A constrained deep neural network for ordinal regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><surname>Wai Kin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><forename type="middle">Keong</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="831" to="839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Probabilistic deep ordinal regression based on gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams Wai Kin</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5301" to="5309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ordinal regression with multiple output cnn for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxing</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4920" to="4928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dating historical color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Palermo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="499" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mean-variance loss for deep age estimation from a face</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5285" to="5294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Computational optimal transport: With applications to data science. Foundations and Trends? in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Peyr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="355" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep expectation of real and apparent age from a single image without facial landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Rasmus Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">2-4</biblScope>
			<biblScope unit="page" from="144" to="157" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Data-driven prognostics with predictive uncertainty estimation using ensemble of deep ordinal regression models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Vishnu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pankaj</forename><surname>Malhotra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lovekesh</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Shroff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.09795</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cumulative link models for deep ordinal classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>V?ctor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">Antonio</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?sar</forename><surname>Guti?rrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Herv?s-Mart?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiancheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilian</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14795</idno>
		<title level="m">Medmnist v2: A large-scale lightweight benchmark for 2d and 3d biomedical image classification</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
