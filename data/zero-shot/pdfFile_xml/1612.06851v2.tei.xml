<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beyond Skip Connections: Top-Down Modulation for Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Beyond Skip Connections: Top-Down Modulation for Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, we have seen tremendous progress in the field of object detection. Most of the recent improvements have been achieved by targeting deeper feedforward networks. However, many hard object categories such as bottle, remote, etc. require representation of fine details and not just coarse, semantic representations. But most of these fine details are lost in the early convolutional layers. What we need is a way to incorporate finer details from lower layers into the detection architecture. Skip connections have been proposed to combine high-level and low-level features, but we argue that selecting the right features from low-level requires top-down contextual information. Inspired by the human visual pathway, in this paper we propose top-down modulations as a way to incorporate fine details into the detection framework. Our approach supplements the standard bottom-up, feedforward ConvNet with a top-down modulation (TDM) network, connected using lateral connections. These connections are responsible for the modulation of lower layer filters, and the top-down network handles the selection and integration of contextual information and lowlevel features. The proposed TDM architecture provides a significant boost on the COCO benchmark, achieving 28.6 AP for VGG16 and 35.2 AP for ResNet101 networks. Using InceptionResNetv2, our TDM model achieves 37.3 AP, which is the best single-model performance to-date on the COCO testdev benchmark, without any bells and whistles.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional neural networks (ConvNets) have revolutionized the field of object detection <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b48">49]</ref>. Most standard ConvNets are bottom-up, feedforward architectures constructed using repeated convolutional layers (with non-linearities) and pooling operations <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref>. These convolutional layers learn invariances and the spatial pooling increases the receptive field of subsequent layers; thus resulting in a coarse, highly semantic representation at the final layer.</p><p>However, consider the images shown in <ref type="figure">Figure 1</ref>. Detecting and recognizing an object like the bottle in the left <ref type="bibr">Figure 1</ref>. Detecting objects such as the bottle or remote shown above requires low-level finer details as well as high-level contextual information. In this paper, we propose a top-down modulation (TDM) network, which can be used with any bottom-up, feedforward ConvNet. We show that the features learnt by our approach lead to significantly improved object detection. image or remote in the right image requires extraction of very fine details such as the vertical and horizontal parallel edges. But these are exactly the type of edges ConvNets try to gain invariance against in early convolutional layers. One can argue that ConvNets can learn not to ignore such edges when in context of other objects like table. However, objects such as table do not emerge until very late in feedforward architecture. So, how can we incorporate these fine details in object detection?</p><p>A popular solution is to use variants of 'skip' connections <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b52">53]</ref>, that capture these finer details from lower convolutional layers with local receptive fields. But simply incorporating high-dimensional skip features into detection does not yield significant improvements due to overfitting caused by curse of dimensionality. What we need is a selection/attention mechanism that selects the relevant features from lower convolutional layers.</p><p>We believe the answer lies in the process of top-down modulation. In the human visual pathway, once receptive field properties are tuned using feedforward processing, top-down modulations are evoked by feedback and horizontal connections <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30]</ref>. These connections modulate representations at multiple levels <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55]</ref> and are responsible for their selective combination <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25]</ref>. We argue that the use of skip connections is a special case of this process, where the modulation is relegated to the final classifier, which directly tries to influence lower layer features and/or learn how to combine them.</p><p>In this paper, we propose to incorporate the top-down modulation process in the ConvNet itself. Our approach supplements the standard bottom-up, feedforward ConvNet with a top-down network, connected using lateral connections. These connections are responsible for the modulation and selection of the lower layer filters, and the top-down network handles the integration of features.</p><p>Specifically, after a bottom-up ConvNet pass, the final high-level semantic features are transmitted back by the topdown network. Bottom-up features at intermediate depths, after lateral processing, are combined with the top-down features, and this combination is further transmitted down by the top-down network. Capacity of the new representation is determined by lateral and top-down connections, and optionally, the top-down connections can increase the spatial resolution of features. These final, possibly high-res, top-down features inherently have a combination of local and larger receptive fields.</p><p>The proposed Top-Down Modulation (TDM) network is trained end-to-end and can be readily applied to any base ConvNet architecture (e.g., VGG <ref type="bibr" target="#b45">[46]</ref>, ResNet <ref type="bibr" target="#b23">[24]</ref>, Inception-Resnet <ref type="bibr" target="#b46">[47]</ref> etc.). To demonstrate its effectiveness, we use the proposed network in the standard Faster R-CNN detection method <ref type="bibr" target="#b40">[41]</ref> and evaluate on the challenging COCO benchmark <ref type="bibr" target="#b32">[33]</ref>. We report a consistent and significant boost in performance on all metrics across network architectures. TDM network increases the performance of vanilla Faster R-CNN with: (a) VGG16 from 23.3 AP to textbf28.6 AP, (b) ResNet101 from 31.5 AP to 35.2 AP, and (c) InceptionResNetv2 from 34.7 AP to 37.2 AP. These are the best performances reported to-date for these architectures without any bells and whistles (e.g., multi-scale features, iterative box-refinement). Furthermore, we see drastic improvements in small objects (e.g., +4.5 AP) and in objects where selection of fine details using top-down context is important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>After the resurgence of ConvNets for image classification <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b27">28]</ref>, they have been successfully adopted for a variety of computer vision tasks such as object detection <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b48">49]</ref>, semantic segmentation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>, instance segmentation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b37">38]</ref>, pose estimation <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref>, depth estimation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b51">52]</ref>, edge detection <ref type="bibr" target="#b52">[53]</ref>, optical flow predictions <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b39">40]</ref> etc. However, by construction, final ConvNet features lack the finer details that are captured by lower convolutional layers. These finer details are considered necessary for a variety of recognition tasks, such as accurate object localization and segmentation.</p><p>To counter this, 'skip' connections have been widely used with ConvNets. Though the specifics of methods vary widely, the underlying principle is same: using or combining finer features from lower layers and coarse seman-tic features for higher layers. For example, <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b42">43]</ref> combine features from multiple layers for the final classifier; while <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b42">43]</ref> use subsampled features from finer scales, <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22]</ref> upsample the features to the finest scale and use their combination. Instead of combining features, <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b52">53]</ref> do independent predictions at multiple layers and average the results. In our proposed framework, such upsampling, subsampling and fusion operations can be easily controlled by the lateral and top-down connections.</p><p>The proposed TDM network is conceptually similar to the strategies explored in other contemporary works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42]</ref>. All methods, including ours, propose architectures that go beyond the standard skip-connection paradigm and/or follow a coarse-to-fine strategy when using features from multiple levels of the bottom-up feature hierarchy. However, different methods focus on different tasks which guide their architectural design and training methodology.</p><p>Conv-deconv <ref type="bibr" target="#b35">[36]</ref> and encoder-decoder style networks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b41">42]</ref> have been used for image segmentation to utilize finer features via lateral connections. These connections generally use the 'unpool' operation <ref type="bibr" target="#b55">[56]</ref>, which merely inverts the spatial pooling operation. Moreover, there is no modulation of bottom-up network. In comparison, our formulation is more generic and is responsible for the flow of high-level context features <ref type="bibr" target="#b36">[37]</ref>.</p><p>Pinheiro et al. <ref type="bibr" target="#b38">[39]</ref> focus on refining class-agnostic object proposals by first selecting proposals purely based on bottom-up feedforward features <ref type="bibr" target="#b37">[38]</ref>, and then post-hoc learning how to refine each proposal independently using top-down and lateral connections (due to the computational complexity, only a few proposals can be selected to be refined). We argue that this use of top-down and lateral connections for refinement is sub-optimal for detection because it relies on the proposals selected based on feedforward features, which are insufficient to represent small and difficult objects. This training methodology also limits the ability to update the feedforward network through lateral connections. In contrast to this, we propose to learn better features for recognition tasks in an end-to-end trained system, and these features are used for both proposal generation and object detection. Similar to the idea of coarseto-fine refinement, Ranjan and Black <ref type="bibr" target="#b39">[40]</ref> propose a coarseto-fine spatial pyramid network, which computes a low resolution residual optical flow and iteratively improves predictions with finer pyramid levels. This is akin to only using a specialized top-down network, which is suited for low-level tasks (like optical flow) but not for recognition tasks. Moreover, such purely coarse-to-fine networks cannot utilize models pre-trained on large-scale datasets <ref type="bibr" target="#b7">[8]</ref>, which is important for recognition tasks <ref type="bibr" target="#b16">[17]</ref>. Therefore, our approach learns representation using bottom-up (fineto-coarse), top-down (coarse-to-fine) and lateral networks simultaneously, and can use different pre-trained modules.  The proposed top-down network is closest to the recent work of Lin et al. <ref type="bibr" target="#b31">[32]</ref>, developed concurrently to ours, on feature pyramid network for object detection. Lin et al. <ref type="bibr" target="#b31">[32]</ref> use bottom-up, top-down and lateral connections to learn a feature pyramid, and require multiple proposal generators and region classifiers on each level of the pyramid. In comparison, the proposed top-down modulation focuses on using these connections to learn a single final feature map that is used by a single proposal generator and region classifier.</p><p>There is strong evidence of such top-down context, feedback and lateral processing in the human visual pathway <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55]</ref>; wherein, the topdown signals are responsible for modulating low-level features <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55]</ref> as well as act as attentional mechanism for selection of features <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25]</ref>. In this paper, we propose a computation model that captures some of these intuitions and incorporates them in a standard ConvNets, giving substantial performance improvements.</p><p>Our top-down framework is also related to the process of contextual feedback <ref type="bibr" target="#b1">[2]</ref>. To incorporate top-down feedback loop in ConvNets, contemporary works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b43">44]</ref>, have used 'unrolled' networks (trained stage-wise). The proposed top-down network with lateral connections explores a complementary paradigm and can be readily combined with them. Contextual features have also been used for Con-vNets based object detectors; e.g., using other objects <ref type="bibr" target="#b19">[20]</ref> or regions <ref type="bibr" target="#b17">[18]</ref> as context. We believe the proposed topdown path can naturally transmit these contextual features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Top-Down Modulation (TDM)</head><p>Our goal is to incorporate top-down modulation into current object detection frameworks. The key idea is to select/attend to fine details from lower level feature maps based on top-down contextual features and select top-down contextual features based on the fine low-level details. We formalize this by proposing a simple top-down modulation (TDM) network as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Proposed Architecture</head><p>An overview of the proposed framework is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. The standard bottom-up network is represented by blocks of layers, where each block C i has multiple operations. The TDM network hinges on two key components: a lateral module L, and a top-down module T (see  <ref type="figure">Figure 3</ref>. The basic building blocks of Top-Down Modulation Network (detailed Section 3.1).</p><formula xml:id="formula_0">C i H x W x k i x i C H x W x l i x i L L i T j,i H x W x t j x j T 2H x 2W x t i x i T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>this new representation.</head><p>Architecture details. The top-down and lateral modules described above are essentially small ConvNets, which can vary from a single or a hierarchy of convolutional layers to more involved blocks with Residual <ref type="bibr" target="#b23">[24]</ref> or Inception <ref type="bibr" target="#b46">[47]</ref> units. In this paper, we limit our study by using modules with a single convolutional layer and non-linearity to analyze the impact of top-down modulation.</p><p>A detailed example of lateral and top-down modules is illustrated in <ref type="figure">Figure 4</ref>. The lateral module L i is a 3 ? 3 convolutional layer with ReLU non-linearity, which trans-</p><formula xml:id="formula_1">forms an (H i ? W i ? k i ) input x C i , to (H i ? W i ? l i ) lateral feature x L i . The top-down module T j,i is also a 3?3 convo- lutional layer with ReLU, that combines this lateral feature with (H i ? W i ? t j ) top-down feature x T j , to produce an in- termediate output (H i ? W i ? t i )</formula><p>. If the resolution of next lateral feature x L i?1 is higher than the previous lateral feature (e.g., H i?1 = 2 ? H i ), then T j,i also upsamples the intermediate output to produce (</p><formula xml:id="formula_2">H i?1 ? W i?1 ? t i ) top-down feature x T i .</formula><p>In <ref type="figure" target="#fig_1">Figure 2</ref>, we denote a i = t j + l i for simplicity. The final T out i module can additionally have a 1 ? 1 convolutional layer with ReLU to output a (H i ? W i ? k out ) feature, which is used by the detection system.</p><p>Varying l i , t i and k out controls the capacity of the topdown modulation system and dimension of the output features. These hyperparameters are governed by the base network design, detection system and hardware constraints (discussed in Section 4.3). Notice that the upsampling step is optional and depends on the content and arrangement of C blocks (e.g., no upsampling by T 4 in <ref type="figure" target="#fig_1">Figure 2</ref>). Also, the first top-down module (T 5 in the illustration) only operates on will begin by adding (L 4 , T <ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4</ref> ) and use T out 4 to get features for object detection. After training (L 4 , T 5,4 ) modules, we will add the next pair (L 3 , T 4,3 ) and use a new T out 3 module to get features for detection; and we will repeat this process. With each new pair, the entire network, top-down and bottom-up along with lateral connections, is trained end-toend. Implementation details of this training methodology depends on the base network architecture, and will be discussed in Section 4.3.</p><p>To better understand the impact of the proposed TDM network, we conduct extensive experimental evaluation; and provide ablation analysis of various design decisions. We describe our approach in detail (including preliminaries and implementation details) in Section 4 and present our results in Section 5. We also report ablation analysis in Section 6. We would like to highlight that the proposed framework leads to substantial performance gains across different base network architectures, indicating its wide applicability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Approach Details</head><p>In this section, we describe the preliminaries and provide implementation details of our top-down modulation (TDM) network under various settings. We first give a a brief overview of the object detection system and the Con-vNet architectures used throughout this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Preliminaries: Faster R-CNN</head><p>We use the Faster R-CNN <ref type="bibr" target="#b40">[41]</ref> framework as our base object detection system. Faster R-CNN consists of two core modules: 1) ROI Proposal Network (RPN), which takes an image as input and proposes rectangular regions of interests (ROIs); and 2) ROI Classifier Network (RCN), which is a Fast R-CNN <ref type="bibr" target="#b15">[16]</ref> detector that classifies these proposed regions and learns to refine ROI coordinates. Given an image, Faster R-CNN first uses a ConvNet to extract features that are shared by both RPN and RCN. RPN uses these features to propose candidate ROIs, which are then classified by RCN. The RCN network projects each ROI onto the shared feature map and performs the 'ROI Pooling' <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23]</ref> operation to extract a fixed length representation. Finally, this feature is used for classification and box regression. See <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b40">41]</ref> for details. Due to lack of support for recent ConvNet architectures <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b46">47]</ref> in the Faster R-CNN framework, we use our implementation in Tensorflow <ref type="bibr" target="#b0">[1]</ref>. We follow the design choices outlined in <ref type="bibr" target="#b25">[26]</ref>. In Section 5, we will provide performance numbers using both the released code <ref type="bibr" target="#b40">[41]</ref> as well as our implementation (which tends to generally perform better). We use the end-to-end training paradigm for Faster R-CNN for all experiments <ref type="bibr" target="#b40">[41]</ref>. Unless specified otherwise, all methods start with models that were pre-trained on ImageNet classification <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Preliminaries: Base Network Architectures</head><p>In this paper, we use three standard ConvNet architectures: VGG16 <ref type="bibr" target="#b45">[46]</ref>, ResNet101 <ref type="bibr" target="#b23">[24]</ref> and InceptionRes-Netv2 <ref type="bibr" target="#b46">[47]</ref>. We briefly explain how they are incorporated in the Faster R-CNN framework (see <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b40">41]</ref> for details), and give a quick overview of these architectures with reference to the bottom-up blocks C from Section 3.1.</p><p>We use the term 'Base Network' to refer to the part of ConvNet that is shared by both RPN and RCN; and 'Classifier Network' to refer to the part that is used as RCN. For VGG16 <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b45">46]</ref>, ConvNet till conv5_3 is used as the base network, and the following two fc layers are used as the classifier network. Similarly, for ResNet101 <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b40">41]</ref>, base network is the ConvNet till conv4_x, and classifier network is the conv5_x block (with 3 residual units or 9 convolutional layers). For InceptionResNet101v2 <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b46">47]</ref>, ConvNet till the 'Block 20x' is used as the base network, and the remaining layers ('Mixed 7a' and 'Block 9x', with a total of 11 inception-resnet units or 48 convolutional layers) are used as the classifier network. Following <ref type="bibr" target="#b25">[26]</ref>, we change the pooling stride of the penultimate convolutional block in ResNet101 and InceptionResNetv2 to 1 to maintain spatial resolution, and use atrous <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b33">34]</ref> convolution to recover the original field-of-view. Properties of bottom-up blocks C i , including number of layers, the output feature resolution and feature dimension etc. are given in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Top-Down Modulation</head><p>To add the proposed TDM network to the ConvNet architectures described above, we need to decide the extent of top-down modulation, the frequency of lateral connections InceptionResNetv2 To build a TDM network, we start with a standard bottom-up model trained on the detection task, and add (T i,j , L j ) progressively. The capacity for different T, L, and T out modules is given in <ref type="table" target="#tab_1">Table 2</ref>. For the VGG16 network, we add top-down and lateral modules all the way to the conv1_x feature. Notice that the input feature dimension to RPN and RCN networks changes from 512 (for conv5_x) to 256 (for T out 4 ), therefore we initialize the fc layers in RPN and RCN randomly <ref type="bibr" target="#b18">[19]</ref>. However, since t out is same for the last three T out modules, we re-use the RPN and RCN layers for these modules.</p><formula xml:id="formula_3">T i,j L j t i,j l j tout T B 20 ,</formula><p>For the ResNet101 and InceptionResNetv2, we add topdown and lateral modules for till conv1 and 'Mixed 5b' respectively. Similar to VGG16, their base networks are initialized with a model pre-trained on the detection task. However, as opposed to VGG16, where the RCN has just 2 <ref type="table">Table 3</ref>. Object detection results on the COCO benchmark. Different methods use different networks for region proposal generation (ROINet) and for region classification (ClsNet). Results for the top block (except Faster R-CNN ) were directly obtained from their respective publications <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b43">44]</ref>. Faster R-CNN was reproduced by us. Middle block shows our implementation of Faster R-CNN framework, which we use as our primary baseline. Bottom block presents the main results of TDM network, with current state-of-the-art single-model performance highlighted. To counter this, we ensure that all T out output feature dimensions (t out ) are same, so that we can be readily use pre-trained RPN and RCN. This is implemented using an additional 1 ? 1 convolutional layer wherever (t i,j + l j ) differs from t out (e.g., all T out modules in ResNet101, and the final T out module in InceptionResNetv2). We would like to highlight an issue with training of RPN at high-resolution feature maps. RPN is a fully convolutional module of Faster R-CNN, that generates an intermediate 512 dimension representation which is of the same resolution as input; and losses are computed at all pixel locations. This is efficient for coarse features (e.g., last row in <ref type="table" target="#tab_0">Table 1</ref>), but the training becomes prohibitively slow for finer resolution features. To counter this, we apply RPN at a stride which ensures that computation remains exactly the same (e.g., using stride of 8 for T out 1 in VGG16). Because of 'ROI Pooling' operations, RCN module still efficiently utilizes the finer resolution features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>In this section, we evaluate our method on the task of object detection, and demonstrate consistent and significant improvement in performance when using features from the proposed TDM network.</p><p>Dataset and metrics. All experiments and analysis in this paper are performed on the COCO dataset <ref type="bibr" target="#b32">[33]</ref>. All models were trained on 40k train and 32k val images (which we refer to as 'trainval * ' set). All ablation evaluations were performed on 8k val images ('minival * ' set) held out from the val set. We also report quantitative results on the standard testdev2015 split. For quantitative evaluation, we use the COCO evaluation metric of mean average precision (AP 1 ).</p><p>Experimental Setup. We conduct experiments with three standard ConvNet architectures: VGG16 <ref type="bibr" target="#b45">[46]</ref>, ResNet101 <ref type="bibr" target="#b23">[24]</ref> and InceptionResNetv2 <ref type="bibr" target="#b46">[47]</ref>. All models ('Baseline' Faster R-CNN and ours) were trained with SGD for 1.5M mini-batch iterations, with batch size of 256 and 128 ROIs for RPN and RCN respectively. We start with an initial learning rate of 0.001 and decay it by 0.1 at 800k and 900k iterations.</p><p>Baselines. Our primary baseline is using vanilla VGG16, ResNet101 and InceptionResNetv2 features in the Faster R-CNN framework. However, due to lack of implementations supporting all three ConvNets, we opted to re-implement Faster R-CNN in Tensorflow <ref type="bibr" target="#b0">[1]</ref>. The baseline numbers reported in <ref type="table">Table 3</ref>(middle) are using our implementation and training schedule and are generally higher than the ones reported in <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b43">44]</ref>. Faster R-CNN was reproduced by us using the official implementation <ref type="bibr" target="#b40">[41]</ref>. All other results in <ref type="table">Table 3</ref>(top) were obtained from the original papers.</p><p>We also compare against models which use a single region proposal generator and a single region classifier network. In particular, we compare with SharpMask <ref type="bibr" target="#b38">[39]</ref>, because of its refinement modules with top-down and lateral connections, and <ref type="bibr" target="#b43">[44]</ref> because they also augment the standard VGG16 network with top-down information. Note that different methods use different networks and train/test splits (see <ref type="table">Table 3</ref>), making it difficult to do a comprehensive comparison. Therefore, for discussion, we will directly compare against our Faster R-CNN baseline ( <ref type="table">Table 3</ref>(middle)), and highlight that the improvements obtained by our approach are much bigger than the boosts by other methods. <ref type="bibr" target="#b0">1</ref>   <ref type="table">bowl  zebra  boat  sink  parkingmeter  surfboard  mouse  keyboard  frisbee  handbag  bird  skateboard  bicycle  airplane  baseballbat  clock  suitcase  stopsign  truck  tie  knife  tennisracket  cow  person  cup  skis  baseballglove  trafficlight  bottle  fork  sheep  kite  remote  wineglass</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">COCO Results</head><p>In <ref type="table">Table 3</ref>(bottom), we report results of the proposed TDM network on the testdev2015 split of the COCO dataset. We see that the TDM network leads to a 5.3 AP point boost over the vanilla VGG16 network (28.6 AP vs. 23.3 AP), indicating that TDM learns much better features for object detection. Note that even though our algorithm is trained on less data (trainval * ), we outperform all methods with VGG16 architecture. For ResNet101, we improve the performance by 3.7 points to 35.2 AP. InceptionRes-Netv2 <ref type="bibr" target="#b46">[47]</ref> architecture was the cornerstone of the winning entry to COCO 2016 detection challenge <ref type="bibr" target="#b25">[26]</ref>. The best single model performance used by this entry achieves 34.7 AP on the testdev split ( <ref type="bibr" target="#b25">[26]</ref>). Using InceptionResNetv2 as base, our TDM network achieves 37.3 AP, which is currently the state-of-the-art single model performance on the testdev split without any bells and whistles (e.g., multiscale, iterative box refinement, etc.). In fact, the TDM network outperforms the baselines (with same base networks) on almost all AP and AR metrics. Similarly, in <ref type="table">Table 4</ref>, we observe that TDM achieves similar boosts across all network architectures on the minival * split as well. <ref type="figure">Figure 5</ref> shows change in AP from Faster R-CNN base-line to the TDM network (for the testdev split). When using VGG16, for all but one category, TDM features improve the performance on object detection. In fact, more than 50% of the categories improve by 5 AP points or more, highlighting that the features are good for small and big objects alike. Similar trends hold for ResNet101 and InceptionResNetv2.</p><p>Improved localization. In <ref type="table">Table 3</ref>, we also notice that for VGG16, our method performs exceptionally well on the AP 75 metric, improving the baseline Faster R-CNN by 8.9 AP 75 points, which is much higher than the 3.5 point AP <ref type="bibr" target="#b49">50</ref> boost. We believe that using contextual features to select and integrate low-level finer details is they key reason for this improvement. Similarly for ResNet101 and Inception-ResNetv2, we see 4.8 AP 75 and 3.1 AP 75 boost respectively.</p><p>Improvement for small objects. In <ref type="table">Table 3</ref>, for VGG16, ResNet101 and InceptionResNetv2), we see 4.8, 3 and 3.6 point boost respectively for small objects (AP S ) highlighting the effectiveness of features with TDM. Moreover, small objects are often on top of the list in <ref type="figure">Figure 5</ref> (e.g., sportsball +13 AP point, mouse +10 AP for VGG16). This is in line with other studies <ref type="bibr" target="#b19">[20]</ref>, which show that context is particularly helpful for some objects. Similar trends hold for the minival * split as well: 5.6, 7.4 and 8.5 AP S boost for VGG16, ResNet101 and InceptionResNetv2 respectively.</p><p>Qualitative Results. In <ref type="figure" target="#fig_7">Figure 6</ref>, we display qualitative results of the Top-down Modulation Network. Notice that the network can find small objects like remote (second row, last column; fourth row, first column) and sportsball (second row, third and fourth column). Also notice the detection results in the presence of heavy clutter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Design and Ablation Analysis</head><p>In this section, we perform control and ablative experiments to study the importance of top-down and lateral modules in the proposed TDM network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">How low should the Top-Down Modulation go?</head><p>In Section 4.3, we discussed the principles that we follow to add top-down and lateral modules. We connected these modules till the lowest layer choosing design decisions that hardware constraints would permit. However, is that overkill? Is there an optimal layer, after which this modulation does not help or starts hurting? To answer these questions, we limit the layer till which the modulation process happens, and use those features for Faster R-CNN.</p><p>Recall that the TDN network is built progressively, i.e., we add one pair of lateral and top-down module at a time. So for this control study, we simply let each subsequent pair train for the entire learning schedule, treating the T out as the final output. We report the results on minival * set in <ref type="table">Table 4</ref>. <ref type="table">Table 4</ref>. Ablation analysis on the COCO benchmark using the Faster R-CNN detection framework. All methods are trained on trainval * and evaluated on minival * set (Section 5). Methods are grouped based on their base network, best results are highlighted in each group.  As we can see, adding more top-down modulation helps in general. However, for VGG16, we see that the performance saturates at T out 2 , and adding modules till T out 1 do not seem to help much. Deciding the endpoint criteria for top-down modulation is an interesting future direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">No lateral modules</head><p>To analyze the importance of lateral modules, and to control for the extra parameters added by the TDM network <ref type="table" target="#tab_1">(Table 2)</ref>, we train additional baselines with variants of VGG16 + TDM network. In particular, we remove the lateral modules and use convolutional and upsampling operations from the top-down modules T to train 'deeper' variants of VGG16 as baseline. To control for the extra parameters from lateral modules, we also increase the parameters in the convolutional layers. Note that for this baseline, we follow training methodology and design decisions used for training TDM networks. <ref type="table" target="#tab_6">Table 5</ref>(left), even though using more depth increases the performance slightly, the performance boost due to lateral modules is much higher. This highlights the importance of dividing the capacity of TDM network amongst lateral and top-down modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">No top-down modules</head><p>Next we want to study the importance of the top-down path introduced by our TDM network. We believe that this path is responsible for transmitting contextual features and for selection of relevant finer details. Removing the topdown path exposes the 'skip'-connections from bottom-up features, which can be used for object detection. We follow the strategy from <ref type="bibr" target="#b3">[4]</ref>, where they ROI-pool features from different layers, L2-normalize and concatenate these features and finally scale them back to the original conv5_3 magnitude and dimension.</p><p>We tried many variants of the Skip-pooling baseline, and report the best results in <ref type="table">Table 4</ref> (Skip-pool). We see that performance for small objects (AP S ) increases slightly, but overall the AP does not change much. This highlights the importance of using high-level contextual features in the top-down path for the selection of low-level features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Impact of Pre-training</head><p>Finally, we study the impact of using a model pre-trained on the detection task to initialize our base networks and ResNet101/InceptionResNetv2's RPN and RCN networks vs. using only an image classification <ref type="bibr" target="#b7">[8]</ref> pre-trained model. In <ref type="table" target="#tab_6">Table 5</ref>(right), we see that initialization does not impact the performance by a huge margin. However, pre-training on the detection task is consistently better than using the classification initialization.  dataset, we demonstrate the effectiveness and importance of features from the TDM network. We show empirically that the proposed representation benefits all objects, big and small, and is helpful for accurate localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>Even though we focused on the object detection, we believe these top-down modulated features will be helpful in a wide variety of computer vision tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>k 4 37 x 63 x k 5 75 x 125 x a 4 75 x 125 x a 3 150 x 250 x a 2 Described in Figure 3</head><label>45323</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The illustration shows an example of Top-Down Modulation (TDM) Network, which is integrated with the bottom-up network with lateral connections. Ci are bottom-up, feedforward feature blocks, Li are the lateral modules which transform low level features for the top-down contextual pathway. Finally, Tj,i, which represent flow of top-down information from index j to i. Individual components are explained in Figure 3 and 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>The TDM network starts from the last layer of bottom-up feedforward network. For example, in the case of VGG16, the input to the first layer of the TDM network is the conv5_3 output. Every layer of TDM network also gets the bottom-up features as inputs via lateral connections. Thus, the TDM network learns to: (a) transmit high-level contextual features that guide the learning and selection of relevant low-level features, and (b) use the bottom-up features to select the contextual information to transmit. The output of the proposed network captures both pertinent finer details and high-level information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 3). Each lateral module L i takes in a bottom-up feature x C i (output of C i ) and produces the corresponding lateral feature x L i . These lateral features x L i and top-down features x T j are combined, and optionally upsampled, by the T j,i module to produce the top-down features x T i . These modules, T j,i and L i , control the capacity of the modulation network by changing their output feature dimensions. The feature from the last top-down module T out i is used for the task of object detection. For example, in Figure 2, instead of x C 5 , we use T out 2 as input to ROI proposal and ROI classifier networks of the Faster R-CNN [41] detection system (discussed in Section 4.1). During training, gradient updates from the object detector backpropagate via top-down and lateral modules to the C i blocks. The lateral modules L ? learn how to transform low-level features and the topdown modules T ? learn what semantic or context information to preserve in the top-down feature transmission as well as the selection of relevant low-level lateral features. Ultimately, the bottom-up features are modulated to adapt for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>x C 5 (Figure 4 .</head><label>54</label><figDesc>the final output of the bottom-up network).Training methodology. Integrating top-down modulation framework into a bottom-up ConvNet is only meaningful when the latter can represent high-level concepts in higher layers. Thus, we typically start with a pre-trained bottomup network (see Section 6.4 for discussion). Starting with this pre-trained network, we find that progressively building the top-down network performs better in general. Therefore, we add one new pair of lateral and top-down modules at a time. For example, for the illustration inFigure 2, we An example with details of top-down modules and lateral connections. Please see Section 3.1 for details of the architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>car sportsball InceptionResNetv2 Figure 5 .</head><label>InceptionResNetv25</label><figDesc>Improvement in AP over Faster R-CNN baseline. Base Networks: (left) VGG16, (middle) ResNet101, and (right) Incep-tionResNetv2. Improved performance for almost all categories emphasize the effectiveness of Top-Down Modulation for object detection. (best viewed digitally)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>This paper introduces the Top-Down Modulation (TDM) network, which leverages top-down contextual features and lateral connections to bottom-up features for object detection. The TDM network uses top-down context to select low-level finer details, and learns to integrate them together. Through extensive experiments on the challenging COCO</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative results of the proposed TDM network on randomly selected images from the minival * set (best viewed digitally).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Base networks architecture details for VGG16, ResNet101 and InceptionResNetv2. Legend: Ci: bottom-up block id, N: number of convolutional filters, NR: number of residual units, NI: number of inception-resnet units, dim x C i : Resolution and dimensions of the output feature. Refer to<ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref> for details</figDesc><table><row><cell></cell><cell></cell><cell>VGG16</cell><cell></cell><cell></cell><cell></cell><cell cols="2">ResNet101</cell><cell></cell><cell></cell><cell cols="3">InceptionResNetv2</cell><cell></cell></row><row><cell>name</cell><cell>C i</cell><cell>N</cell><cell>dim x C i</cell><cell>name</cell><cell>C i</cell><cell>NB</cell><cell>N</cell><cell>dim x C i</cell><cell>name</cell><cell>C i</cell><cell>NI</cell><cell>N</cell><cell>dim x C i</cell></row><row><cell cols="2">conv1 x C 1</cell><cell>2</cell><cell>(300, 500, 64)</cell><cell>conv1</cell><cell>C 1</cell><cell>1</cell><cell>1</cell><cell>(300, 500, 64)</cell><cell>conv x</cell><cell>Cx</cell><cell>-</cell><cell>5</cell><cell>(71, 246, 192)</cell></row><row><cell cols="2">conv2 x C 2</cell><cell cols="2">2 (150, 250, 128)</cell><cell cols="2">conv2 x C 2</cell><cell>3</cell><cell>9</cell><cell>(150, 250, 256)</cell><cell>Mixed 5b</cell><cell>M 5</cell><cell>1</cell><cell>7</cell><cell>(35, 122, 320)</cell></row><row><cell cols="2">conv3 x C 3</cell><cell>3</cell><cell>(75, 125, 256)</cell><cell cols="2">conv3 x C 3</cell><cell cols="2">4 12</cell><cell>(75, 125, 512)</cell><cell>Block 10x</cell><cell>B 10</cell><cell>10</cell><cell>70</cell><cell>(35, 122, 320)</cell></row><row><cell cols="2">conv4 x C 4</cell><cell>3</cell><cell>(37, 63, 512)</cell><cell cols="2">conv4 x C 4</cell><cell cols="2">23 69</cell><cell>(75, 125, 1024)</cell><cell>Mixed 6a</cell><cell>M 6a</cell><cell>1</cell><cell>3</cell><cell>(33, 120, 1088)</cell></row><row><cell cols="2">conv5 x C 5</cell><cell>3</cell><cell>(37, 63, 512)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Block 20x</cell><cell>B 20</cell><cell cols="2">20 100</cell><cell>(33, 120, 1088)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Top-Down Modulation network design for VGG16, ResNet101 and InceptionResNetv2. Notice that tout VGG16 is much smaller than 512, thus requiring fewer parameters in RPN and RCN modules. Also note that it is important to keep tout</figDesc><table><row><cell cols="10">fixed for ResNet101 and InceptionResNetv2 in order to utilize pre-</cell></row><row><cell cols="5">trained RPN and RCN modules</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>VGG16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ResNet101</cell><cell></cell><cell></cell></row><row><cell>T i,j</cell><cell>L j</cell><cell>t i,j</cell><cell>l j</cell><cell>tout</cell><cell>T i,j</cell><cell>L j</cell><cell>t i,j</cell><cell>l j</cell><cell>tout</cell></row><row><cell>T 5,4</cell><cell>L 4</cell><cell cols="3">128 128 256</cell><cell>T 4,3</cell><cell>L 3</cell><cell cols="3">128 128 1024</cell></row><row><cell>T 4,3</cell><cell>L 3</cell><cell>64</cell><cell cols="2">64 128</cell><cell>T 3,2</cell><cell>L 2</cell><cell cols="3">128 128 1024</cell></row><row><cell>T 3,2</cell><cell>L 2</cell><cell>64</cell><cell cols="2">64 128</cell><cell>T 2,1</cell><cell>L 1</cell><cell>32</cell><cell cols="2">32 1024</cell></row><row><cell>T 2,1</cell><cell>L 1</cell><cell>64</cell><cell cols="2">64 128</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>AP AP 50 AP 75 AP S AP M AP L AR 1 AR 10 AR 100 AR S AR M AR L</figDesc><table><row><cell>Method</cell><cell>train</cell><cell>test</cell><cell>ROINet</cell><cell>ClsNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Faster R-CNN [41]</cell><cell>train</cell><cell>val</cell><cell>VGG16</cell><cell>VGG16</cell><cell>21.2 41.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Faster R-CNN [24]</cell><cell>train</cell><cell>val</cell><cell>ResNet101</cell><cell>ResNet101</cell><cell>27.2 48.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SharpMask [39]</cell><cell>train</cell><cell>testdev</cell><cell>ResNet50</cell><cell>VGG16</cell><cell>25.2 43.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Faster R-CNN [41]</cell><cell>trainval</cell><cell>testdev</cell><cell>VGG16</cell><cell>VGG16</cell><cell cols="11">24.5 46.0 23.7 8.2 26.4 36.9 24.0 34.8 35.5 13.4 39.2 54.3</cell></row><row><cell>[44]</cell><cell>trainval</cell><cell>testdev</cell><cell>VGG16++</cell><cell>VGG16++</cell><cell cols="11">27.5 49.2 27.8 8.9 29.5 41.5 25.5 37.4 38.3 14.6 42.5 57.4</cell></row><row><cell>Faster R-CNN</cell><cell cols="2">trainval *  testdev</cell><cell>VGG16</cell><cell>VGG16</cell><cell cols="11">23.3 44.7 21.5 9.4 27.1 32.0 22.7 36.8 39.4 18.3 44.0 56.2</cell></row><row><cell>Faster R-CNN</cell><cell cols="2">trainval *  testdev</cell><cell>ResNet101</cell><cell>ResNet101</cell><cell cols="11">31.5 52.8 33.3 13.6 35.4 44.5 28.0 43.6 45.8 22.7 51.2 64.1</cell></row><row><cell>Faster R-CNN</cell><cell cols="2">trainval *  testdev</cell><cell>IRNv2</cell><cell>IRNv2</cell><cell cols="11">34.7 55.5 36.7 13.5 38.1 52.0 29.8 46.2 48.9 23.2 54.3 70.8</cell></row><row><cell>TDM [ours]</cell><cell cols="2">trainval *  testdev</cell><cell>VGG16 + TDM</cell><cell>VGG16 + TDM</cell><cell cols="11">28.6 48.1 30.4 14.2 31.8 36.9 26.2 42.2 44.2 23.7 48.3 59.3</cell></row><row><cell>TDM [ours]</cell><cell cols="2">trainval *  testdev</cell><cell cols="13">ResNet101 + TDM ResNet101 + TDM 35.2 55.3 38.1 16.6 38.4 47.9 30.4 47.8 50.3 27.8 54.9 67.6</cell></row><row><cell>TDM [ours]</cell><cell cols="2">trainval *  testdev</cell><cell>IRNv2 + TDM</cell><cell>IRNv2 + TDM</cell><cell cols="11">37.3 57.8 39.8 17.1 40.3 52.1 31.6 49.3 51.9 28.1 56.6 71.1</cell></row><row><cell cols="5">fc layers, ResNet101 and InceptionResNetv2 models have</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">an RCN with 9 and 48 convolutional layers respectively.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">This makes training RCN from random initialization diffi-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>cult.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>COCO<ref type="bibr" target="#b32">[33]</ref> AP averages over classes, recall, and IoU levels.</figDesc><table><row><cell>VGG16</cell><cell></cell><cell>ResNet101</cell><cell></cell><cell></cell><cell></cell></row><row><cell>hairdrier book cake toothbrush hotdog apple orange spoon banana carrot handbag baseballbat broccoli knife scissors backpack bear suitcase boat bench sink parkingmeter pottedplant cellphone toaster dog skis firehydrant umbrella diningtable chair surfboard bed sandwich oven bird baseballglove snowboard vase microwave cat tv pizza tennisracket donut tie bowl laptop bicycle truck fork teddybear remote couch toilet keyboard motorcycle cup bottle person skateboard clock stopsign frisbee airplane wineglass horse trafficlight sheep giraffe train cow car refrigerator elephant kite zebra mouse bus sportsball</cell><cell>toaster hairdrier cellphone bear dog orange banana bench teddybear bird pottedplant book broccoli cake hotdog sandwich boat parkingmeter donut apple cat backpack oven bed toilet handbag carrot toothbrush clock bowl train airplane remote truck chair vase zebra giraffe horse bicycle motorcycle scissors sink bus kite baseballbat snowboard couch sportsball baseballglove keyboard stopsign fork skateboard suitcase laptop wineglass firehydrant frisbee elephant trafficlight surfboard refrigerator mouse tie cow bottle car cup pizza knife tennisracket person tv umbrella skis diningtable sheep microwave spoon</cell><cell></cell><cell>hairdrier toaster toilet cat broccoli sandwich apple train teddybear dog pizza microwave bear bench refrigerator cake orange tv horse banana scissors giraffe firehydrant carrot laptop cellphone motorcycle bus backpack book spoon hotdog oven bed snowboard umbrella toothbrush couch chair elephant donut vase pottedplant diningtable</cell><cell></cell><cell></cell></row><row><cell>0 Change in AP ? 5 10</cell><cell>15</cell><cell>?4 Change in AP ? 0 4</cell><cell>8</cell><cell>?4 Change in AP ? 0 4</cell><cell>8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>AP AP 50 AP 75 AP S AP M AP L AR 1 AR 10 AR 100 AR S AR M AR L</figDesc><table><row><cell>Method</cell><cell>Net</cell><cell>Features from:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline</cell><cell>VGG16</cell><cell>C5</cell><cell>25.5 46.7 24.6 6.1 23.3 37.0 23.9 38.2</cell><cell>40.7</cell><cell>14.1 39.5 55.3</cell></row><row><cell>Skip-pool</cell><cell>VGG16</cell><cell>C2, C3, C4, C5</cell><cell>25.3 46.3 25.9 9.1 24.0 36.0 24.6 40.0</cell><cell>42.4</cell><cell>18.6 41.8 54.1</cell></row><row><cell>TDM [ours]</cell><cell>VGG16 + TDM</cell><cell>T out 4</cell><cell>26.2 45.7 27.2 9.4 25.1 34.8 25.0 40.7</cell><cell>43.0</cell><cell>18.7 43.1 54.7</cell></row><row><cell>TDM [ours]</cell><cell>VGG16 + TDM</cell><cell>T out 3</cell><cell>28.8 48.6 30.7 11.0 27.1 37.3 26.5 42.7</cell><cell>45.0</cell><cell>21.1 44.2 56.4</cell></row><row><cell>TDM [ours]</cell><cell>VGG16 + TDM</cell><cell>T out 2</cell><cell>29.9 50.3 31.6 11.4 28.1 38.6 27.3 43.7</cell><cell>46.0</cell><cell>22.8 44.7 57.1</cell></row><row><cell>TDM [ours]</cell><cell>VGG16 + TDM</cell><cell>T out 1</cell><cell>29.8 49.9 31.7 11.7 28.0 39.3 27.1 43.5</cell><cell>45.9</cell><cell>23.9 45.4 56.8</cell></row><row><cell>Baseline</cell><cell>ResNet101</cell><cell>C5</cell><cell>32.1 53.2 33.8 9.4 29.7 45.7 28.3 44.3</cell><cell>46.7</cell><cell>19.3 46.3 60.9</cell></row><row><cell>TDM [ours]</cell><cell>ResNet101 + TDM</cell><cell>T out 3</cell><cell>34.4 54.4 37.1 10.9 31.8 48.2 30.1 47.5</cell><cell>49.8</cell><cell>21.7 49.1 64.0</cell></row><row><cell>TDM [ours]</cell><cell>ResNet101 + TDM</cell><cell>T out 2</cell><cell>35.3 55.1 38.3 11.2 33.0 48.2 30.7 48.0</cell><cell>50.5</cell><cell>22.5 50.1 63.6</cell></row><row><cell>TDM [ours]</cell><cell>ResNet101 + TDM</cell><cell>T out 1</cell><cell>35.7 56.0 38.5 16.8 39.2 49.0 30.9 48.5</cell><cell>50.9</cell><cell>28.1 55.6 68.5</cell></row><row><cell>Baseline</cell><cell>IRNv2</cell><cell>B20</cell><cell>35.7 56.5 38.0 8.9 32.0 52.5 30.8 47.8</cell><cell>50.3</cell><cell>19.6 49.9 66.9</cell></row><row><cell>TDM [ours]</cell><cell>IRNv2 + TDM</cell><cell>T out 6a</cell><cell>37.3 57.9 39.5 11.4 33.3 53.3 32.8 49.1</cell><cell>51.5</cell><cell>22.7 50.6 67.5</cell></row><row><cell>TDM [ours]</cell><cell>IRNv2 + TDM</cell><cell>T out 5a</cell><cell>38.1 58.6 40.7 17.4 41.1 54.7 32.4 50.1</cell><cell>52.6</cell><cell>28.9 57.2 72.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>(left) Importance of lateral modules: We use operations from the top-down module to increase depth of the VGG16 network; ? Ti,j represents modified top-down module to account for more parameters. (right) Impact of Pre-training.</figDesc><table><row><cell>i, j</cell><cell>? T i,j</cell><cell>(T i,j , L j )</cell><cell></cell><cell cols="2">Pre-trained on:</cell></row><row><cell>5, 4</cell><cell>24.8</cell><cell>26.2</cell><cell></cell><cell cols="2">COCO ImageNet</cell></row><row><cell>4, 3</cell><cell>25.1</cell><cell>28.8</cell><cell>T4,3</cell><cell>34.4</cell><cell>34.0</cell></row><row><cell>3, 2</cell><cell>26.5</cell><cell>29.9</cell><cell>T3,2</cell><cell>35.3</cell><cell>34.1</cell></row><row><cell>2, 1</cell><cell>21.4</cell><cell>29.6</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">Large-scale machine learning on heterogeneous distributed systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A hidden ambiguity of the term feedback in its use as an explanatory mechanism for psychophysical visual phenomena. Feedforward and Feedback Processes in Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bachmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00561</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Insideoutside net: Detecting objects in context with skip pooling and recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.06550</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Top-down attentional guidance based on implicit learning of visual covariation. Psychological Science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>H?usser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Haz?rba?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06852</idno>
		<title level="m">Flownet: Learning optical flow with convolutional networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unrolling loopy top-down semantic feedback in convolutional deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Veijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR Workshops</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Top-down modulation: bridging selective attention and working memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gazzaley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Nobre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Trends in cognitive sciences</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Object detection via a multiregion and semantic segmentation-aware cnn model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Brain states: top-down influences in sensory processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sigman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Contextual action recognition with RCNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Exploring person context and local scene context for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08177</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The neural mechanisms of top-down attentional control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Hopfinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Buonocore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Mangun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The ventral visual pathway: an expanded neural framework for the processing of object quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Saleem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">I</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Ungerleider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mishkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The distinct modes of vision offered by feedforward and recurrent processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Lamme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Roelfsema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in neurosciences</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Feedforward, horizontal, and feedback processing in the visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Lamme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Super</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Spekreijse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current opinion in neurobiology</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Iterative instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08498</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<title level="m">Parsenet: Looking wider to see better</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Network model of top-down influences on local gain and contextual interactions in visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pi?ch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">N</forename><surname>Reeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08695</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00850</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pedestrian detection with unsupervised multi-stage feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Contextual priming and feedback for Faster R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Training regionbased object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07261</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Designing deep networks for surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Top-down modulation of visual feature processing: the role of the inferior frontal junction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Zanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Rubens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bollinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gazzaley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Causal role of the prefrontal cortex in top-down modulation of visual processing and working memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Zanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Rubens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thangavel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gazzaley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
