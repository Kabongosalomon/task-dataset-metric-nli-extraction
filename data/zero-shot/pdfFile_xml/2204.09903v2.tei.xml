<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beyond the Prototype: Divide-and-conquer Proxies for Few-shot Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunbo</forename><surname>Lang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binfei</forename><surname>Tu</surname></persName>
							<email>binfeitu@mail.nwpu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Gong Cheng ?</roleName><forename type="first">Junwei</forename><surname>Han</surname></persName>
							<email>jhan@nwpu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Beyond the Prototype: Divide-and-conquer Proxies for Few-shot Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract xml:lang="it">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot segmentation, which aims to segment unseen-class objects given only a handful of densely labeled samples, has received widespread attention from the community. Existing approaches typically follow the prototype learning paradigm to perform meta-inference, which fails to fully exploit the underlying information from support image-mask pairs, resulting in various segmentation failures, e.g., incomplete objects, ambiguous boundaries, and distractor activation. To this end, we propose a simple yet versatile framework in the spirit of divide-and-conquer. Specifically, a novel self-reasoning scheme is first implemented on the annotated support image, and then the coarse segmentation mask is divided into multiple regions with different properties. Leveraging effective masked average pooling operations, a series of support-induced proxies are thus derived, each playing a specific role in conquering the above challenges. Moreover, we devise a unique parallel decoder structure that integrates proxies with similar attributes to boost the discrimination power. Our proposed approach, named divideand-conquer proxies (DCP), allows for the development of appropriate and reliable information as a guide at the "episode" level, not just about the object cues themselves. Extensive experiments on PASCAL-5 i and COCO-20 i demonstrate the superiority of DCP over conventional prototype-based approaches (up to 5 ?10% on average), which also establishes a new state-of-the-art. Code is available at github.com/chunbolang/DCP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Benefiting from the superiority of deep convolutional neural networks, various computer vision tasks have made tremendous progress over the past few years, including image classification <ref type="bibr" target="#b5">[He et al., 2016]</ref>, object detection <ref type="bibr" target="#b11">[Ren et al., 2015]</ref>, and semantic segmentation <ref type="bibr">[Long et al., 2015]</ref>, to name a <ref type="figure">Figure 1</ref>: Comparison of conventional FSS approaches (left part) and our DCP approach (right part). Conventional approaches typically leverage support features and corresponding masks to generate single or multiple foreground prototypes for guiding query image segmentation, while DCP utilizes the visual reasoning results of support images to derive many different "proxies" from a broader episodic perspective (divide), each of which plays a specific role <ref type="bibr">(conquer)</ref>. Overall, such operations could provide pertinent and reliable guidance, including foreground/background cues, ambiguous boundaries, and distractor objects, rather than merely object-related information.</p><p>few. However, such an effective technique also has its inherent limitation: the strong demand for a considerable number of annotated samples from well-established datasets to achieve satisfactory performance <ref type="bibr" target="#b2">[Deng et al., 2009]</ref>. For the tasks such as semantic segmentation that performs dense predictions on given images, the acquisition of training sets with sufficient data costs even more, substantially hindering the development of deep learning systems. Few-shot learning (FSL) has emerged as a promising research direction to address this issue, which intends to learn a generic model that can recognize new concepts with very limited information available <ref type="bibr" target="#b12">[Vinyals et al., 2016]</ref>  <ref type="bibr" target="#b7">[Lang et al., 2022]</ref>.</p><p>In this paper, we undertake the few-shot segmentation (FSS) task that can be regarded as a natural application of FSL technology in the field of image segmentation <ref type="bibr" target="#b11">[Shaban et al., 2017]</ref>. Specifically, FSS models aim to retrieve the foreground regions of a specific category in the query image while only providing a handful of support images with corresponding annotated masks. Prevalent FSS approaches <ref type="bibr" target="#b13">[Zhang et al., 2019;</ref><ref type="bibr" target="#b12">Tian et al., 2022]</ref> follow the prototype learning paradigm to perform meta-reasoning, where single or multiple prototypes are generated by the average pooling operation <ref type="bibr" target="#b14">[Zhang et al., 2020]</ref>, furnishing query object inference with essential foreground cues. However, their segmentation capabilities could be fragile when confronted with daunting few-shot tasks with factors such as blurred object boundaries and irrelevant object interference. We argue that one possible reason for these failures lies in the underexplored information from support image-mask pairs, and it is far from enough to rely solely on squeezed object-related features to guide segmentation. Taking the 1-shot segmentation task in <ref type="figure">Figure 1</ref> as an example, conventional approaches tend to confuse several distractor objects (e.g., cat and sofa) and yield inaccurate segmentation boundaries (e.g., dog's paw and head), revealing the limitations of the scheme based on such descriptors.</p><p>To alleviate the abovementioned problems, we propose a simple yet versatile framework in the spirit of divide-andconquer (refer to the right part of <ref type="figure">Figure 1</ref>), where the descriptors for guidance are derived from a broader episodic perspective, rather than just the objects themselves. Concretely, we first implement a novel self-reasoning scheme on the annotated support image, and then divide the coarse segmentation mask into multiple regions with different properties. Using effective masked average pooling operations, a series of support-induced proxies are thus generated, each playing a specific role in conquering the typical challenges of daunting FSS tasks. Furthermore, we devise a unique parallel decoder structure in which proxies with similar attributes are integrated to boost the discrimination power. Compared with conventional prototype-based approaches, our proposed framework, named divide-and-conquer proxies (DCP), allows for the development of appropriate and reliable information as a guide at the "episode" level, not just about the object cues.</p><p>Our primary contributions can be summarized as follows:</p><p>? We present a simple yet versatile FSS framework in the spirit of divide-and-conquer, where the descriptors for guidance are derived from a broader episodic perspective to tackle the typical challenges of daunting tasks.</p><p>? Compared with conventional prototype-based approaches, the proposed framework allows for the development of appropriate and reliable information, rather than merely the object cues themselves, providing a fresh insight for future works.</p><p>? Extensive experiments on two standard benchmarks demonstrate the superiority of our framework over the baseline approach (up to 5?10% on average), establishing new state-of-the-arts in few-shot literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Semantic Segmentation. Semantic segmentation is a fundamental and essential computer vision task, the goal of which is to classify each pixel in the image according to a predefined set of semantic categories. Most advanced approaches follow fully convolutional networks (FCNs) <ref type="bibr">[Long et al., 2015]</ref> to perform dense predictions, vastly boosting the segmentation performance . On top of that, the rapid development of this field has also brought some impressive techniques, such as dilated convolutions <ref type="bibr" target="#b0">[Chen et al., 2017]</ref> and attention mechanism <ref type="bibr" target="#b6">[Huang et al., 2019]</ref>. In this work, we adopt the Atrous Spatial Pyramid Pooling (ASPP) module <ref type="bibr" target="#b0">[Chen et al., 2017]</ref> based on dilated convolution to build the decoder, enjoying a large receptive field and multi-scale feature aggregation. Few-shot Learning. Few-shot learning (FSL) is proposed to tackle the generalization problem of conventional deep networks, where meta-knowledge is shared across tasks to facilitate the recognition of new concepts with very little support information. Representative FSL approaches can be grouped into three categories: optimization-based method <ref type="bibr" target="#b4">[Finn et al., 2017]</ref>, hallucination-based method <ref type="bibr" target="#b1">[Chen et al., 2019]</ref>, and metric-based method <ref type="bibr" target="#b12">[Vinyals et al., 2016]</ref>. Our work closely relates to the prototypical network (PN) <ref type="bibr" target="#b12">[Snell et al., 2017]</ref> assigned to the third category. PN directly leverages the feature representations (i.e., prototypes) computed through global average pooling operations to perform non-parametric nearest neighbor classification. Our DCP, on the other hand, utilizes foreground/background prototypes for dense feature comparison <ref type="bibr" target="#b13">[Zhang et al., 2019]</ref>, providing essential information for subsequent parametric query image segmentation. Few-shot Segmentation. Few-shot segmentation (FSS) has emerged as a promising research direction to tackle the dense prediction problem in a low-data regime. Existing approaches typically employ a two-branch structure, in which the annotation information travels from the support branch to the query branch <ref type="bibr" target="#b13">[Zhang et al., 2019]</ref>  <ref type="bibr" target="#b12">[Siam et al., 2019]</ref>. Generally speaking, how to achieve more effective information interaction between the two branches is the primary concern in this field. OSLSM <ref type="bibr" target="#b11">[Shaban et al., 2017]</ref>, the pioneering work of FSS, proposed to generate classifier weights for query image segmentation in the conditioning (support) branch. Whereafter, the prototype learning paradigm <ref type="bibr" target="#b12">[Snell et al., 2017]</ref> was introduced into state-of-the-art approaches for better information exchange. SG-One <ref type="bibr" target="#b14">[Zhang et al., 2020]</ref> computed the affinity scores between the prototype and query features, yielding the spatial similarity map as a guide. CANet <ref type="bibr" target="#b13">[Zhang et al., 2019]</ref> proposed to conduct a novel dense comparison operation based on the prototype, which exhibited decent segmentation performance under few-shot settings. However, these methods are limited to the utilization of features of foreground objects (i.e., prototype) to facilitate segmentation and fail to provide pertinent and reliable guidance from a broader episodic perspective. Our method draws on the idea of prototype learning while proposing a set of support-induced proxies that go beyond it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Setup</head><p>Few-shot segmentation (FSS) aims to segment the objects of a specific category in the query image using only a few labeled samples. To improve the generalization ability for unseen classes, existing approaches widely adopt the metalearning strategy for training, also known as episodic learning <ref type="bibr" target="#b12">[Vinyals et al., 2016]</ref>. Specifically, given a training set D train and a testing set D test that are disjoint with regard to object classes, a series of episodes are sampled from them to simulate the few-shot scenario. For a K-shot segmentation task, each episode is composed of 1) a support set</p><formula xml:id="formula_0">S = {(I k s , M k s,c )} k=1,...,K c?C episode , where I k s is the k-th support im- age, M k</formula><p>s,c is the corresponding mask for class c, and C episode represents the class set of the given episode; and 2) a query set Q = {I q , M q,c } where I q is the query image and M q,c is the ground-truth mask available during training while unknown during testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head><p>In this section, we propose a simple yet effective few-shot segmentation framework, termed divide-and-conquer proxies (DCP). The unique feature of our approach, as also the source of improvements, lies in the spirit of divide-and-conquer. The divide module generates a set of proxies according to the prediction and ground-truth mask of the support image; the conquer module utilizes these distinguishing proxies to provide essential segmentation cues for the query branch, as depicted in <ref type="figure" target="#fig_0">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Divide</head><p>Self-reasoning Scheme. Following the paradigm of advanced FSS approaches <ref type="bibr" target="#b13">[Zhang et al., 2019]</ref>, we freeze the backbone network to boost generalization capabilities. Taking ResNet <ref type="bibr" target="#b5">[He et al., 2016]</ref> for example, the intermediate feature maps after each convolutional block of the support branch are represented as {F b s } B b=1 * . We use the high-level features generated by the last block, i.e., block4, to perform the self-reasoning scheme:</p><formula xml:id="formula_1">M aux s =f seg (F 4 s ),</formula><p>(1) where f seg (?) is a lightweight segmentation decoder with residual connections, including three convolutions with 256 filters, one convolution with 2 filters, and an arg max(?) operation for producing coarse prediction mask? M aux s ?{0, 1} H?W . H and W denote the height and weight respectively, and their product H?W is the minimum resolution of all feature maps. Unlike the widely adopted prototypeguided segmentation framework, such a mask-agnostic selfreasoning scheme is more efficient and can also provide reliable auxiliary information. Proxy Acquisition. Given the coarse predictionM aux s and down-sampled ground-truth mask M s of the support image, we first evaluate their differences to derive the corresponding mask (i.e., valid region) M m?P1 of each proxy where P 1 ={?, ?, ?, ?} denotes the index set of proxies:</p><formula xml:id="formula_2">? ? ? ? ? ? ? M (x,y) ? = 1[M aux;(x,y) s = M (x,y) s = 1] M ? = M s ?M ? M (x,y) ? = 1[M aux;(x,y) s = M (x,y) s = 0] M ? = G?M s ?M ? ,<label>(2)</label></formula><p>* B denotes the total number of convolutional blocks contained in the backbone network, which is equal to 4 for ResNet. where (x, y) indexes the spatial location of the mask, 1(?) is an indicator function that outputs 1 if the condition is satisfied or 0 otherwise, and G?R H?W represents an all 1's matrix with the same shape as M m . M ? and M ? aggregate the main and auxiliary foreground information respectively, generally corresponding to the body and boundary regions of objects; while M ? and M ? gather the primary and secondary background information respectively, which typically correspond to the regions of the generalized background and distractor objects. As described above, these masks are generated according to the segmentation results of the model, therefore the main (primary) and auxiliary (secondary) status may change when encountering complex tasks. The extreme case is M ? =M s , but even so, the guidance information is still complete. To better understand the effect of each mask, we present several examples in <ref type="figure" target="#fig_1">Figure 3</ref>.</p><p>Then, we conduct masked average pooling operations <ref type="bibr" target="#b14">[Zhang et al., 2020]</ref> on the support features F s ?R C?H?W and masks M m computed above to encode the set of proxies {p m }:</p><formula xml:id="formula_3">p m = x,y F (x,y) s ?M (x,y) m x,y M (x,y) m ,<label>(3)</label></formula><p>where p m ?R C represents a specific proxy in the set, playing some roles in conquering the typical challenges of FSS tasks. Prototype Acquisition. Similar to the acquisition method of proxies <ref type="figure" target="#fig_1">(Eq. (3)</ref>), the set of prototypes {p n } n?P2 are also generated using the masked average pooling operation where P 2 ={f, b} represents the index set of prototypes:</p><formula xml:id="formula_4">p n = x,y F (x,y) s ? 1[M (x,y) s =j] x,y 1[M (x,y) s =j] ,<label>(4)</label></formula><p>where j=1 if the foreground prototype p f is evaluated and j=0 if the background one p b is calculated. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Conquer</head><p>Feature Matching. Dense feature comparison is a common approach in the FSS literature that compares the query feature maps F q with the global feature vector (i.e., prototype) at all spatial locations. In view of its effectiveness, we follow the same technical route and extend it to a dual-prototype setting (see <ref type="figure" target="#fig_1">Figure 3(b)</ref>), which can be defined as:</p><formula xml:id="formula_5">F tmp q;n = (F q , ? H?W (p n )) ,<label>(5)</label></formula><p>where ? h?w (?) expands the given vector to a spatial size h?w and represents the concatenation operation along channel dimensions. The superscript "tmp" denotes temporary, and the subscript n?P 2 indicates the index of the prototype. Feature Activation. Given the proxies {p m } m?P1 and prototypes {p n } n?P2 computed by Eqs. <ref type="figure" target="#fig_1">(3-4)</ref>, we evaluate the cosine similarity between each of them and query features at each spatial location respectively:</p><formula xml:id="formula_6">A (x,y) l = p T l ? F (x,y) q p l ? F (x,y) q , l ? P 1 ? P 2 ,<label>(6)</label></formula><p>where A l ?R H?W denotes the activation map of the l-th item. <ref type="figure" target="#fig_2">Figure 4</ref> presents some examples to illustrate the region of interest for each feature vector p l . Actually, there is an alternative approach to propagate information about feature vectors {p l } l?P1?P2 to the query branch, that is, to perform the same operation as Eq. <ref type="formula" target="#formula_5">(5)</ref>. Such a scheme, however, introduces additional computational overhead and performance degradation, as discussed in ? 5.3. Parallel Decoder Structure. Most previous works abandon the prototype p b due to the complexity of background regions in support images. We argue that specifically designing the network to capture foreground and background features separately could have a positive effect on performance, whereas a simple fusion strategy would be counterproductive. To this end, a unique parallel decoder structure (PDS) is devised in which proxies/prototypes with similar attributes are integrated to boost the discrimination power:</p><formula xml:id="formula_7">F refined q;n = F tmp q;n , A n , {A m } m?Ptmp ,<label>(7)</label></formula><p>where P tmp is a temporary set of vector indexes, representing {?, ?} when foreground features F refined q;f are evaluated and {?, ?} when background ones F refined q;b are computed. These two refined query features are then fed into the corresponding decoder network respectively, and the output single-channel probability maps are concatenated to generate the final segmentation result.  <ref type="table">Table 1</ref>: Comparison with state-of-the-arts on PASCAL-5 i in mIoU under 1-shot and 5-shot settings. "P." means PASCAL. RED/BLUE represents the 1 st /2 nd best performance. Superscript " ?" indicates that PFENet is served as the baseline.  <ref type="bibr" target="#b11">[Nguyen and Todorovic, 2019]</ref>   <ref type="table">Table 2</ref>: Comparison with state-of-the-arts on COCO-20 i in mIoU under 1-shot and 5-shot settings. "C." means COCO. The methods with superscript " ?" use the ResNet101 backbone while other approaches use the ResNet50. "#Params." indicates the number of learnable parameters.  <ref type="table">Table 3</ref>: Comparison with state-ofthe-arts on PASCAL-5 i in FB-IoU under 1-shot and 5-shot settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training Loss</head><p>To guarantee the richness of the information provided by proxies while preventing extreme situations ( ? 4.1), we introduce additional constraints on the prediction results of selfreasoning schemes for both support and query branches. The total loss for each episode can be written as:</p><formula xml:id="formula_8">L total =L main + ? 1 L aux1 + ? 2 L aux2 ,<label>(8)</label></formula><p>where L represents the binary cross-entropy (BCE) loss function. ? 1 and ? 2 , the balancing coefficients, are set to 1.0 in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>Datasets. We evaluate the proposed approach on two standard FSS benchmarks: PASCAL-5 i [Shaban et al., 2017] and COCO-20 i <ref type="bibr" target="#b11">[Nguyen and Todorovic, 2019]</ref>. The former is created from PASCAL VOC 2012 <ref type="bibr" target="#b3">[Everingham et al., 2010]</ref> with additional mask annotations from SDS <ref type="bibr" target="#b4">[Hariharan et al., 2011]</ref>, consisting of 20 semantic categories evenly divided into 4 folds: {5 i } 3 i=0 , while the latter, built from MS COCO <ref type="bibr" target="#b9">[Lin et al., 2014]</ref>, is composed of 80 semantic categories divided into 4 folds: {20 i } 3 i=0 . Models are trained on 3 folds and tested on the remaining one in a cross-validation manner. We randomly sample 1, 000 episodes from the unseen fold i for evaluation. Evaluation Metrics. Following previous FSS approaches <ref type="bibr" target="#b8">[Li et al., 2021;</ref><ref type="bibr" target="#b12">Tian et al., 2022]</ref>, we adopt mean intersectionover-union (mIoU) and foreground-background IoU (FB-IoU) as our evaluation metrics, in which the mIoU metric is primarily used since it better reflects the overall performance of different categories. Implementation Details. Two different backbone networks (i.e., <ref type="bibr">VGG16 [Simonyan and Zisserman, 2014]</ref> and ResNet50 <ref type="bibr" target="#b5">[He et al., 2016]</ref>) are adopted for a fair comparison with exist-ing FSS methods. Following <ref type="bibr" target="#b12">[Tian et al., 2022]</ref>, we pre-train these backbone networks on ILSVRC <ref type="bibr" target="#b11">[Russakovsky et al., 2015]</ref> and freeze them during training to boost the generalization capability. The setup of pre-processing technology is the same as <ref type="bibr" target="#b12">[Tian et al., 2022]</ref>. The SGD optimizer is utilized with a learning rate of 0.005 for 200 epochs on PASCAL-5 i and 50 epochs on COCO-20 i . All experiments are performed on NVIDIA RTX 2080Ti GPUs with the PyTorch framework. We report the average results of 5-runs with different random seeds to reduce the influence of selected support-query image pairs on performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison with State-of-the-arts</head><p>We evaluate the proposed DCP framework with state-of-theart methods on two standard FSS datasets. <ref type="table">Table 1</ref> presents the 1-shot and 5-shot results assessed under the mIoU metric on PASCAL-5 i . It can be observed that our models outperform existing approaches by a sizeable margin, especially under the 5-shot setting, indicating that more appropriate and reliable information is supplied for query image segmentation. <ref type="table">Table 2</ref> summarizes the performance of different methods with ResNet50 backbone on COCO-20 i , in which the proposed approach sets new state-of-the-arts with a small number of learnable parameters. Specifically, our DCP achieves 2.89% (1-shot) and 3.78% (5-shot) of mIoU improvements over the previous best approach (i.e., PFENet), respectively. The FB-IoU evaluation metric is also included for further comparison, as shown in <ref type="table">Table 3</ref>. Once again, the proposed DCP substantially surpasses recent methods.</p><p>For qualitative analysis, we first visualize the segmentation results of DCP and baseline approach, as illustrated in <ref type="figure">Figure 5</ref>. It can be found that the false positives caused by irrelevant objects are significantly reduced (see the first two rows). Meanwhile, more accurate segmentation boundaries are obtained, benefiting from the superiority of p ? (see the last row). Note that for more details of the baseline approach,   <ref type="table">Table 5</ref>: Ablation studies on fusion strategies. "FLOPs" indicates the computational overhead. "Speed" denotes the evaluation of average frame-per-second (FPS) under the 1-shot setting.</p><p>please refer to <ref type="table" target="#tab_4">Table 4</ref> in the ablation studies ( ? 5.3). Then we randomly sample 2, 000 episodes from 20 categories of PAS-CAL VOC 2012 to draw the confusion matrix, as shown in <ref type="figure">Figure 6</ref>. The proposed DCP effectively reduces the semantic confusion between object categories and exhibits better segmentation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Studies</head><p>The following ablation studies are performed with VGG16 backbone on PASCAL-5 i unless otherwise stated.</p><p>Proxy &amp; Prototype. As discussed in ? 4.2, the derived proxies, along with foreground/background prototypes, tend to propagate pertinent information for the query branch in the form of activation maps. In <ref type="table" target="#tab_4">Table 4</ref>, we evaluate the impact of each of them on segmentation performance under 1-shot settings. Overall, the following observations could be made: (i) By introducing the foreground activation map M f , the performance of baseline approach is greatly improved from 58.52% to 59.64% +1.12 , demonstrating the importance of foreground cues for guidance. Furthermore, we argue that such a simple yet powerful variant could serve as a new baseline approach for future FSS works. (ii) The background information provided by mask annotations needs to be used with caution since the performance might even deteriorate without PDS (see the third row). (iii) Our proposed proxies show a superior capability compared to the prototypes in facilitating query image segmentation (60.12% vs. 60.63% mIoU), which is the so-called "beyond the prototype". (iv) There is a complementary relationship between the prototype and proxy, the combination of which could further boost performance. (v) Surprisingly, background-related proxies could be better integrated with prototypes (see rows 6-7). One possible reason for this phenomenon is that the interference of irrelevant objects may have a greater influence on the FSS model among many factors, while M ? can effectively reduce false positives. Parallel Decoder Structure. After obtaining the activation maps (Eq. (6)), we first attempt to leverage a simple fusion scheme to guide the query branch, i.e., concatenating F tmp q;f and {A l } l?P1?P2 and feeding the refined features to a single decoder network. Unfortunately, such a fusion scheme does not offer performance gains, which we attribute to the information confusion between the expanded foreground prototype ? H?W (p f ) and background-related activation maps <ref type="figure">Figure 5</ref>: Semantic segmentation results on unseen categories under 1-shot setting. "AM" denotes activation map. From left to right, each column represents support images, query images (blue), baseline AMs, baseline results (red), our AMs, and our results (yellow), respectively. Best viewed in color. <ref type="figure">Figure 6</ref>: Confusion matrices of baseline approach and our DCP.</p><p>As can be seen, our DCP effectively reduces semantic confusion between object categories. Best viewed in color.</p><p>(M ? and M ? ). To this end, we devise a unique parallel decoder structure (PDS) where proxies/prototypes with similar attributes are integrated, yielding a superior result compared to the previous scheme (see rows 3-4 of <ref type="table" target="#tab_4">Table 4</ref>). Efficiency. We also evaluate the efficiency of different fusion strategies between derived proxies {p m } m?P1 and query features F q , as presented in <ref type="table">Table 5</ref>. Please note that the first row represents the previously discussed scheme (see Eq. <ref type="formula" target="#formula_7">(7)</ref>), while the second row denotes the scheme that concatenates query features with the expanded proxies, similar to Eq. (6).</p><p>The experiment results indicate that the first fusion strategy exhibits remarkable segmentation performance with fewer parameters (0.26M) and faster speed (16+ FPS), demonstrating the effectiveness of activation maps for guidance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we have proposed an efficient framework with the spirit of divide-and-conquer, which performs a selfreasoning scheme to derive support-induced proxies for tackling the typical challenges of FSS tasks. Moreover, a novel parallel decoder structure was introduced to further boost the discrimination power. Qualitative and quantitative results show that the proposed proxies could provide appropriate and reliable information to facilitate query image segmentation, surpassing conventional prototype-based approaches by a considerable margin, i.e., the so-called "beyond prototype".</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>DCP for 1-shot semantic segmentation. (a) Overall pipeline of the proposed approach. (b) Implementation details of the conquer module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of the masks for generating proxies. From top to bottom, each row represents (a) raw (support) images, (b) ground-truth masks, (c) prediction masks, and (d) masks for generating various proxies, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of activation maps generated by proxies and prototypes. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>20 0 C.-20 1 C.-20 2 C.-20 3 Mean C.-20 0 C.-20 1 C.-20 2 C.-20 3 Mean FWB ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Shaban et al., 2017]  33.60 55.30 40.90 33.50 40.80 35.90 58.10 42.70 39.10 43.90  FWB [Nguyen and<ref type="bibr" target="#b11">Todorovic, 2019]</ref> 47.04 59.64 52.61 48.27 51.90 50.87 62.86 56.48 50.09 55.08 PANet [Wang et al., 2019] 42.30 58.00 51.10 41.20 48.10 51.80 64.60 59.80 46.50 55.70 PFENet [Tian et al., 2022] 56.90 68.20 54.40 52.40 58.00 59.00 69.10 54.80 52.90 59.00 MMNet [Wu et al., 2021] 57.10 67.20 56.60 52.30 58.30 56.60 66.70 53.60 56.50 58.30 DCP (ours) 59.67 68.67 63.78 53.11 61.31 64.25 70.66 67.38 61.08 65.84 ResNet50 CANet [Zhang et al., 2019] 52.50 65.90 51.30 51.90 55.40 55.50 67.80 51.90 53.20 57.10 SCL ? [Zhang et al., 2021] 63.00 70.00 56.50 57.70 61.80 64.50 70.90 57.30 58.70 62.90 SAGNN [Xie et al., 2021] 64.70 69.60 57.00 57.20 62.10 64.90 70.00 57.00 59.30 62.80 MMNet [Wu et al., 2021] 62.70 70.20 57.30 57.00 61.80 62.20 71.50 57.50 62.40 63.40 CWT [Lu et al., 2021] 56.30 62.00 59.90 47.20 56.40 61.30 68.50 68.50 56.60 63.70 DCP (ours) 63.81 70.54 61.16 55.69 62.80 67.19 73.15 66.39 64.48 67.80</figDesc><table><row><cell>Backbone Method</cell><cell>P.-5 0</cell><cell>P.-5 1</cell><cell>1-shot P.-5 2</cell><cell>P.-5 3 Mean P.-5 0</cell><cell>P.-5 1</cell><cell>5-shot P.-5 2</cell><cell>P.-5 3 Mean</cell></row><row><cell>OSLSM [</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VGG16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>M b M ? M ? M ? M ?</figDesc><table><row><cell cols="2">Prototype M f 58.52 Proxy PDS mIoU FB-IoU 72.46</cell></row><row><cell>59.64</cell><cell>73.04</cell></row><row><cell>59.05</cell><cell>72.86</cell></row><row><cell>60.12</cell><cell>73.20</cell></row><row><cell>60.63</cell><cell>73.85</cell></row><row><cell>60.42</cell><cell>73.56</cell></row><row><cell>60.76</cell><cell>74.11</cell></row><row><cell>61.31</cell><cell>74.87</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies on each component.</figDesc><table><row><cell cols="4">Fusion Strategy mIoU FB-IoU #Params. FLOPs</cell><cell>Speed</cell></row><row><cell>Activation Map 61.31</cell><cell>74.87</cell><cell>0.26M</cell><cell cols="2">0.24G 16.1FPS</cell></row><row><cell>Tile &amp; Concat. 60.50</cell><cell>74.56</cell><cell>0.52M</cell><cell cols="2">0.47G 14.9FPS</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by the National Natural Science Foundation of China under Grants 62136007 and U20B2065, in part by the Shaanxi Science Foundation for Distinguished Young Scholars under Grant 2021JC-16, and in part by the Fundamental Research Funds for the Central Universities.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image deformation metanetworks for one-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8680" to="8689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Mach. Learn</title>
		<meeting>IEEE Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
	<note>Proc. IEEE Int. Conf. Comput. Vis.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning what not to segment: A new perspective on few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive prototype learning and allocation for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8334" to="8343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
	<note>Proc. Eur. Conf. Comput. Vis.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simpler is better: Few-shot semantic segmentation with classifier weight transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8741" to="8750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todorovic ; Khoi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Todorovic ; Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Adv. Neural Inform. Process. Syst</title>
		<editor>Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Li Fei-Fei</editor>
		<meeting>Conf. Adv. Neural Inform. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="211" to="252" />
		</imprint>
	</monogr>
	<note>One-shot learning for semantic segmentation</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lillicrap, koray kavukcuoglu, and Daan Wierstra. Matching networks for one shot learning</title>
		<idno type="arXiv">arXiv:1409.1556</idno>
	</analytic>
	<monogr>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<editor>Wang et al., 2019] Kaixin Wang, Jun Hao Liew, Yingtian Zou, Daquan Zhou, and Jiashi Feng</editor>
		<meeting><address><addrLine>Charles Blundell, Timothy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="517" to="526" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proc. IEEE Int. Conf. Comput. Vis.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Canet: Class-agnostic segmentation networks with iterative refinement and attentive few-shot learning</title>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5217" to="5226" />
		</imprint>
	</monogr>
	<note>Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sg-one: Similarity guidance network for oneshot semantic segmentation</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3855" to="3865" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-guided and cross-guided learning for few-shot segmentation</title>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8312" to="8321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
