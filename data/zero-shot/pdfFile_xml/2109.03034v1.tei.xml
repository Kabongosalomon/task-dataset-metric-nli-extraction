<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generate &amp; Rank: A Multi-task Framework for Math Word Problems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhao</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichun</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
							<email>qun.liu@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huawei</forename><surname>Noah&amp;apos;s Ark Lab</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huawei</forename><surname>Hisilicon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Generate &amp; Rank: A Multi-task Framework for Math Word Problems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Math word problem (MWP) is a challenging and critical task in natural language processing. Many recent studies formalize MWP as a generation task and have adopted sequenceto-sequence models to transform problem descriptions to mathematical expressions. However, mathematical expressions are prone to minor mistakes while the generation objective does not explicitly handle such mistakes. To address this limitation, we devise a new ranking task for MWP and propose Generate &amp; Rank, a multi-task framework based on a generative pre-trained language model. By joint training with generation and ranking, the model learns from its own mistakes and is able to distinguish between correct and incorrect expressions. Meanwhile, we perform tree-based disturbance specially designed for MWP and an online update to boost the ranker. We demonstrate the effectiveness of our proposed method on the benchmark and the results show that our method consistently outperforms baselines in all datasets. Particularly, in the classical Math23k, our method is 7% (78.4% ? 85.4%) higher than the state-of-theart 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Solving math word problems (MWP) <ref type="bibr" target="#b2">(Bobrow, 1964)</ref> is an important and fundamental task in natural language processing (NLP), which requires to provide a solution expression given a mathematical problem description, as illustrated in <ref type="table" target="#tab_0">Table 1</ref>. Many recent studies formalize MWP as a generation task and commonly adopt LSTM-based sequence-tosequence (Seq2Seq) models <ref type="bibr" target="#b33">(Wang et al., 2017</ref><ref type="bibr" target="#b31">(Wang et al., , 2018b</ref><ref type="bibr" target="#b34">Xie and Sun, 2019)</ref>, where problem texts are source sequences, mathematical expressions are target sequences and the model learns the mapping ? This work is done when Jianhao Shen is an intern at Huawei Noah's Ark Lab *Corresponding author 1 Code will be available soon.</p><p>Original MWP Problem A project is completed in 25 days by 12</p><p>workers. If it takes 20 days to complete, how many workers will it take? Solution 25 * 12 / 20</p><p>Number-mapped MWP Problem A project is completed in NUM0 days by NUM1 workers. If it takes NUM2 days to complete, how many workers will it take? Solution NUM0 * NUM1 / NUM2 from source texts to target expressions. These studies have proposed numerous advanced techniques to improve the MWP solver, but their performance is still unsatisfactory yet.</p><p>We argue that it is not sufficient to model MWP as only a generation task, because there is a significant difference between mathematical expressions and natural language sequences: one minor mistake in a mathematical expression will change the whole semantic thus lead to a wrong answer, whereas natural language is more robust to such minor mistakes. The objective function of the generation task is to maximize generation likelihood on ground-truth expressions, which does not have an explicit strategy to make the model learn to distinguish between ground-truth and expressions that have minor mistakes. In addition, previous works <ref type="bibr" target="#b15">(Liu et al., 2019a;</ref><ref type="bibr" target="#b34">Xie and Sun, 2019;</ref><ref type="bibr" target="#b36">Zhang et al., 2020)</ref> find that the performance of generation models degrades fast as the expression gets longer.</p><p>To handle the above problems, we propose Generate &amp; Rank, a multi-task framework for MWP, which introduces a new ranker to explicitly distinguish between correct and incorrect expressions. Specifically, our framework includes two modules: a generator and a ranker. The former is designed to generate candidate expressions given a problem text and the latter aims to rank the candidate expressions. They are built based on an encoderdecoder model and are jointly trained with generation loss and ranking loss. In this work, we build our model based on BART , a widely used pre-trained language model that achieves SOTA performance on various sequenceto-sequence tasks <ref type="bibr" target="#b0">(Ahmad et al., 2021;</ref>. During multi-task training, expressions produced by the generator are used to construct an expression bank and train the ranker, in which way the model can learn from its own mistakes. To construct more informative candidates for the ranker, we specially design tree-based disturbance for MWP. We also introduce an online update mechanism to generate a new set of candidate expressions at each training epoch. The overall training procedure is in an iterative manner, in which the ranker and generator continue to enhance each other.</p><p>To evaluate the effectiveness of the proposed model, we conduct extensive experiments on the datasets of Math23K <ref type="bibr" target="#b33">(Wang et al., 2017)</ref> and MAWPS <ref type="bibr" target="#b10">(Koncel-Kedziorski et al., 2016)</ref>. The results show that our model outperforms typical baselines. Particularly, we obtain an improvement of 7% in the Math23K dataset that is extensively studied. Moreover, we do ablation study and model analysis, which shows that (1) joint training improves the performance of the generator and ranker over separate training; (2) both strategies of constructing candidate expressions and online updating are important to the success of the ranker. We also find that with the ranker, our model achieves a large improvement in generation of long expressions.</p><p>The contributions of our work are two-fold: (1) We propose Generate &amp; Rank, a new multi-task framework to train a pre-trained language model for math word problem solving. To construct informative candidate expressions for the ranker, we propose two effective generation methods and also introduce an online update strategy. (2) Experiments show that our proposed model consistently outperforms the state-of-the-art models and achieves a significant improvement on the Math23K dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Math Word Problem</head><p>A math word problem P is a sequence of word tokens and numeric values, which typically describes a partial quantitative state of a world and some updates or relationships among quantities, then asks a question about an unknown quantity. The solution S to the question is a mathematical expression that consists of math operators and numbers. In solving a math word problem, we usually do not care about the specific number of a quantity, so the numbers in problems and solution expressions are mapped to special tokens NUM#i according to their orders in the problem text. <ref type="table" target="#tab_0">Table 1</ref> gives an example of an original math word problem and the corresponding number-mapped problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">BART</head><p>BART is a widely-used pre-trained language model. It follows a standard encoder-decoder structure using Transformer layers <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref> and is pre-trained with text denoising tasks. The pretrained BART can be fine-tuned for tasks of sequence classification and generation.</p><p>Transformer-based Encoder-Decoder. BART uses an encoder-decoder structure that is the mainstream architecture for sequence-to-sequence tasks. The encoder adopts the bidirectional selfattention to map an input sequence of tokens P = (x 1 , x 2 , . . . , x n ) to a sequence of continuous representations R = (r 1 , r 2 , . . . , r n ). The BART encoder is composed of multiple Transformer layers, each consists of a multi-head self-attention (MHA) module and a fully connected feed-forward (FFN) module. We denote the mapping function of the BART encoder as follows:</p><p>(r 1 , r 2 , . . . , r n ) = BART Enc (x 1 , x 2 , . . . , x n )</p><p>(1) The BART decoder also consists of multiple Transformer layers. Besides MHA and FFN modules, the decoder layer adds another multi-head attention over the output of the encoder. The decoder takes in one token s i at a time, and gives an output state based on the output of the encoder and previous tokens in the decoder input. This output state is then fed into a linear transformation followed by a softmax function to get the predicted next-token probabilities. This one-step decoding process is denoted as follows:</p><formula xml:id="formula_0">P ( * ) = softmax(d i W + b) (2) d i = BART Dec (R; s 0 , s 1 , . . . , s i?1 ), (3)</formula><p>where s 0 is a special [bos] token indicating the start of decoding, and R is the output of encoder.</p><p>BART Pre-training. BART is pre-trained by the tasks of recovering a corrupted document to orig- Decoder Score Decoder Encoder inal one. The input to BART is corrupted in two ways: (1) a number of text spans are replaced with a single [MASK] token; (2) sentences in a document are shuffled in a random order. The objective of BART pre-training is to minimize the cross-entropy loss between the decoder's generation probabilities and the ground-truth of original document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-task Training Expression Online Updating</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>We propose Generate &amp; Rank, a BART-based multitask framework for math word problems. Our model consists of a generator and a ranker, which share a BART model and are jointly trained with a generating task and ranking task. The objective of generating is to generate expressions given a math word problem. We also add a ranking task so that the model can select a correct expression from a set of candidates. We construct an expression bank to provide training examples for the ranker. <ref type="figure" target="#fig_0">Figure  1</ref> shows our proposed framework and we introduce details for each task and the whole framework in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-task Training</head><p>Task #1: Generating. We first formulate the math word problem as a sequence-to-sequence task, in which BART is trained to generate solution expressions given a math word problem. Following the fine-tuning strategy of BART , we take problem text, a sequence of tokens P = (x 1 , x 2 , . . . , x n ), as input to BART encoder, and minimize negative log-likelihood of the solution expression S = (s 1 , s 2 , . . . , s m ),</p><formula xml:id="formula_1">J GEN = 1 |D| (P,S)?D ? log Pr(S|P ),<label>(4)</label></formula><p>where the conditional probability is decomposed in an auto-regressive way as:</p><formula xml:id="formula_2">Pr(S|P ) = m i=1 Pr(s i |P, S j&lt;i ) (5) Pr( * |P, S j&lt;i ) = softmax(d i W + b) (6) d i = BART Dec (R; S j&lt;i ) (7) R = BART Enc (P ).<label>(8)</label></formula><p>Additionally, we add two special tokens s 1 =[bos] and s m =[eos] to indicate the start and end symbols of decoding sequences.</p><p>Task #2: Ranking. Through generating, we obtain many candidate solution expressions. To decide which expression is a correct solution to the problem, we propose a ranking task which is essentially a task of sequence pair classification. Given pairs of problems and candidate expressions, the ranker chooses the expression with highest ranking score as the final solution to the problem. Specifically, we add an MLP classifier on top of the final layer hidden state of the last decoder token. The last decoder token is always a special [eos] token and its corresponding hidden state can attend to all token representations of problem text and expression. Same as the generation task, we feed the problem text into the encoder and expression into the decoder, obtaining sequence representations. The last decoder representation is then taken as input to the classifier for ranking score prediction:</p><formula xml:id="formula_3">Pr(?|P, S) = softmax(d m+1 ) (9) d m+1 = tanh(d m+1 W 1 + b 1 )W 2 + b 2 (10) d m+1 = BART Dec (R; S),<label>(11)</label></formula><p>where R is the output of the encoder, S is the expression token sequence, d m+1 is the decoder representation of the last token, and W 1|2 and b 1|2 are trainable parameters. The training objective of the ranker is cross-entropy between classifier output and correct labels, Optimization Objective. We train the model on the joint loss of two tasks together:</p><formula xml:id="formula_4">J RANK = ? 1 |D + ? D ? | (P,S)?D + log Pr(1|P, S) + (P,S)?D ? log Pr(0|P, S)</formula><formula xml:id="formula_5">J = J GEN + J RANK .<label>(13)</label></formula><p>and the two modules share BART parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Expression Bank</head><p>By definition, any expression that does not equal the ground-truth can serve as a negative example, but we cannot use all of them due to limited computational resources. To train the ranker efficiently, we use two different strategies, namely modelbased generation and tree-based disturbance, to construct an expression bank for ranker training.</p><p>Model-based Generation. The first strategy is to produce new expressions with the generator. Specifically, given a problem, we use beam search with the generator to produce top-K expressions. Each expression is labeled as positive or negative depending on whether its calculation result equals the result of ground-truth.</p><p>Tree-based Disturbance. Our second way to construct new expressions is adding disturbance to ground-truth expressions. We design four kinds of disturbances which are illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>. The ground-truth expression is first transformed to an abstract syntax tree (AST) <ref type="bibr" target="#b15">(Liu et al., 2019a)</ref>. Then we disturb tree nodes or sub-structures to produce new expressions in four ways: a) Expand. A leaf node is expanded into a sub-tree with a new operation and a number. b) Edit. A node is randomly changed to another while keeping the expression valid (i.e., a number node will be changed to another number, and an operator node to another operator). c) Delete. Delete a leaf node and replace its father with its sibling node. d) Swap. Swap the left and right children of an operation node.</p><p>We use the above methods to construct the expression bank. Since new expressions may also be correct (for example, swapping two operands of addition or multiplication), we compare the numerical results of newly obtained expressions with that of the ground-truth, and add them to positive or negative samples depending on the comparison. Then both positive and negative pairs are sampled from this expression bank for the multi-task training. In order to make the model learn with more informative examples, we do an online update for expression bank, which means that we use new expressions obtained by model-based generation and tree-based disturbance at each training epoch. Get new expressions {S i } by adding treebased disturbance to S 10: Repeat lines 6-18 to reconstruct expression bank 23: end for</p><formula xml:id="formula_6">/ + + NUM1 / (NUM2 + NUM3) ( NUM1 + NUM3 ) / (NUM2 + NUM3) (a) Expand NUM1 / (NUM2 -NUM3) (b) Edit NUM1 / NUM3<label>(</label></formula><formula xml:id="formula_7">{S i } ? {S i } ? {S i } 11: forS ? {S i } do</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Procedure</head><p>The training procedure includes multi-task training and expression online updating. We first finetune the pre-trained BART for the generation task (J GEN in Eq. 4). After that, we use the fine-tuned BART and tree-based disturbance to generate expressions as the training samples for the ranker. Then we do the joint training of generation and ranking. This process is performed in an iterative manner and the two modules (i.e., generator and ranker) continue to enhance each other. Meanwhile, training examples for ranking are updated after each epoch. We summarize the overall training procedure in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Inference</head><p>We perform a two-stage model inference, namely generation and ranking. Specifically, given a new problem text sequence P , we first pass it to the encoder to get the problem representation R. Then we perform the beam search to generate top-K expressions. These generated expressions are used as candidate solutions for the ranker. All expressions are passed to the ranker and that with the highest score is selected as the final result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Datasets. We conduct the experiments on two commonly-used datasets: Math23K <ref type="bibr" target="#b33">(Wang et al., 2017)</ref> and MAWPS <ref type="bibr" target="#b10">(Koncel-Kedziorski et al., 2016)</ref>. Math23K is a large-scale Chinese dataset that contains 23,162 math word problems and their corresponding expression solutions. MAWPS is a English dataset containing 2,373 problems. All the problems are one-unknown-variable linear problems and can be solved with a single expression.</p><p>Baselines. We compare our model with the following baselines including the state-of-the-art models: DNS <ref type="bibr" target="#b33">(Wang et al., 2017)</ref> uses a vanilla Seq2Seq model to generate expressions. Math-EN <ref type="bibr" target="#b31">(Wang et al., 2018b)</ref> uses the equation normalization to avoid equation duplication problem. T-RNN <ref type="bibr" target="#b32">(Wang et al., 2019b)</ref> applies recursive neural networks to model the tree structures of expressions. S-Aligned (Chiang and Chen, 2019) tracks the semantic meanings of operands with a stack during decoding. Group-ATT <ref type="bibr" target="#b13">(Li et al., 2019)</ref> leverages the attention mechanism to enrich problem representation. Both AST-Dec <ref type="bibr" target="#b15">(Liu et al., 2019a)</ref> and GTS (Xie and Sun, 2019) develop a tree-based decoder to generate expressions. Graph2Tree <ref type="bibr" target="#b36">(Zhang et al., 2020)</ref> proposes to build a quantity cell graph and a comparison graph to better capture the quantity relationships of the problem. Multi-E/D <ref type="bibr" target="#b25">(Shen and Jin, 2020</ref>) is an ensemble model which combines multiple encoders and decoders. Implementation Details. We use the PyTorch 2 implementations and pre-trained language models provided by the Transformers library 3 . Since the Math23K dataset is a Chinese dataset and officially released BART is only for English, we switch to mBART25 , which is a multilingual BART for 25 languages including Chinese. For the MAWPS dataset, we also use mBART25. We optimize our model with AdamW <ref type="bibr" target="#b18">(Loshchilov and Hutter, 2019)</ref>. The training hyperparameters are set as follows. We set the batch size to 128, the learning rate to 5e-5 and the warm-up ratio to 0.1. The weight decay is set to 0.01. The number of epochs M for fine-tuning and multi-task training are set to 50. We set beam size K to 10 in beam search and expression bank size to 20 unless otherwise stated. All experiments are carried out on NVIDIA Tesla V100. We use 8 GPUs for training and 1 for testing. For our proposed framework, the training time is 1.5 hours for one epoch and testing time is 15 minutes for the whole test set.</p><p>Evaluation Metric. Both MAWPS and Math23K are evaluated with a metric of "solution accuracy", that is, the expression is considered as correct if it induces the same number as the ground-truth. For the Math23K dataset, some baselines are evaluated using the public available test set while others use the results of 5-fold cross-validation. We report our results on both settings. For the MAWPS dataset, models are evaluated with 5-fold cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Analysis</head><p>Evaluation results of our model and baselines are summarized in <ref type="table" target="#tab_3">Table 2</ref>. We observe that: (1) direct fine-tuning of mBART already outperforms the state-of-the-art models on Math23K, which shows the powerful generation ability of mBART. (2) on MAWPS, mBART outperforms most Seq2Seq baselines but is worse than GTS and Graph2Tree. These two models leverage tree structure of expressions during decoding which is critical for math word problem solving. We believe that pre-trained language models would achieve a better performance if combined with structure information, and we leave it as a future work 4 . (3) Generate &amp; Rank framework further improves mBART and achieves new state-of-the-art results. In particular, Generate &amp; Rank outperforms mBART baselines by more than 4% in all the evaluation settings and also outperforms the previous best models by 7% on Math23K ? , 7.4% on 5-fold crossvalidation Math23K ? . The improvement over pretrained mBART demonstrates the effectiveness of <ref type="bibr">4</ref> One may think that the sequence decoder might not always generate valid expressions. However, we check all expressions generated by mBART and find that 99.9% are valid. our multi-task training framework.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study and Model Analysis</head><p>To better understand our model, we further conduct ablation study on Math23K to show how the proposed components affect performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Effect of Joint Training</head><p>To investigate the effect of joint training, we introduce the baseline of two-stage training (i.e., w/o Joint), which means we first train the generator, then train the ranker, and the modules are trained independently. We also study the effect of joint training on generation and perform comparison between mBART and our generator (i.e., w/o Ranker). The results are listed in <ref type="table" target="#tab_5">Table 3</ref>. We can see that the joint training brings 2.2% improvement compared with the two-stage training and 2.6% for the generator compared with the mBART trained alone, suggesting that the joint training of generator and ranker benefits each other. Besides, the joint training is more space efficient since we only need to save one unified model rather than two.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Effect of Expression Bank Strategy</head><p>We investigate the effect of different strategies to construct the expression bank. Here we choose a random sampling strategy as our baseline, where the set of expressions that appeared in the training data is sampled as the expression bank. We evaluate different strategies with and without online updating and summarize the results in <ref type="table">Table 4</ref>  <ref type="table">Table 4</ref>: Accuracy for different expression bank strategies. The expression bank size is 20 for all settings.</p><p>We can see that our strategies outperform the random sampling strategy. Since the ground-truth can not be accessed during model inference, we cannot use the tree-based disturbance to generate candidate expressions as in the training phase. This discrepancy between training and inference leads to poor performance if we only use tree-based disturbance to construct the expression bank. However, combining the tree-based disturbance and modelbased generation strategies, we can obtain better results than the only model-based generation, which gives evidence that the tree-based disturbance contains some informative examples that the generator does not cover and it is possible to improve the performance based on the human knowledge of math expression.</p><p>We can also see that strategies have a performance drop without online updating. We conjecture that without online updating the ranker may tend to memorize existing negative expressions thus generalize poorly on new problems. As for strategies with model-based generation, there is another possible reason: the generator keeps updating during multi-task training, so the previously generated expressions are no longer good samples of the current model, and newly generated expressions are more informative. To summarize, both strategies of constructing the expressions bank and online updating play an important role in the success of the ranker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Impact of Expression Bank Size</head><p>We further analyze the impact of expression bank size on the ranker and results are shown in <ref type="figure">Figure 3</ref>. If the model-based generation is used, performance reaches the best at expression bank size 20. This suggests that the expression bank size should not be too small nor too large. One possible reason may be that the generated expressions cannot cover  possible mistakes when the expression bank is too small, and when the expression bank is too large, low-quality expressions may be generated and hinder ranker training. Tree-based disturbance has a similar trend and the best bank size is 10. <ref type="figure">Figure 3</ref>: Accuracy with different expression bank sizes from 5 to 30.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Model Analysis</head><p>In <ref type="table" target="#tab_8">Table 5</ref>, we list how the model accuracy changes with respect to the number of operations in expressions. We do not discuss the case of 6 operators since it has too few examples and high variance.</p><p>For expressions less than 6 operators, all models perform worse when the expression gets longer. This is as expected since longer expressions require more steps of reasoning and have less data to train. In addition, we also observe that Generate &amp; Rank training has larger improvement over finetuned mBART on longer expressions. This implies that our model is more suitable to handle complex problems and expressions. Following <ref type="bibr" target="#b15">Liu et al. (2019a)</ref>, we also examine the performance of our model in different domains. The domain of each problem is defined by whether it contains any keywords of this domain and we use the same keyword list as <ref type="bibr" target="#b15">Liu et al. (2019a)</ref>. <ref type="table" target="#tab_10">Table 6</ref> shows the results. We observe the similar pattern that the fine-tuned mBART has limitations in geometry which requires external knowledge such as formulas for the circumference and area of a circle. Interestingly, our proposed model mainly improves on these domains. This suggests that the ranking task may be a better choice to learn and use mathematical knowledge than generating.  Rule-based methods. Early approaches on math word problems mainly craft rules and templates for pattern matching <ref type="bibr" target="#b2">(Bobrow, 1964;</ref><ref type="bibr" target="#b26">Slagle, 1965;</ref><ref type="bibr" target="#b6">Fletcher, 1985;</ref><ref type="bibr" target="#b1">Bakman, 2007)</ref>. These methods rely heavily on manual design and can only solve a limited scope of problems.</p><p>Parsing-based methods. Later on, researchers use statistical methods to solve MWP and achieve a great performance improvement. One line of research focuses on semantic parsing, which leverages traditional machine learning techniques to identify entities, quantities, and operators from the problem text.  proposes three types of classifiers to identify different elements of problems. ARIS <ref type="bibr" target="#b7">(Hosseini et al., 2014)</ref> splits the problem into fragments and updates a logic template named state by verb categorization. Other works <ref type="bibr" target="#b27">(Sundaram and Khemani, 2015;</ref><ref type="bibr" target="#b19">Mitra and Baral, 2016;</ref><ref type="bibr" target="#b14">Liang et al., 2016</ref>) follow a similar process with different templates and annotations.</p><p>Two-stage methods. Another research line first obtains an expression template then maps numbers to the template slots.  train a classifier to select from a set of pre-defined templates.  propose to construct candidate expressions in a bottom-up manner and train a global scoring function to guide the beam search process. ALGES <ref type="bibr" target="#b9">(Koncel-Kedziorski et al., 2015)</ref> converts the process of searching valid expressions to an integer linear programming problem and adopts a different scoring function. Unit-Dep <ref type="bibr" target="#b23">(Roy and Roth, 2017)</ref> proposes Unit Dependency Graph to enhance the scoring function.</p><p>Deep learning methods. Recently, deep learning models have become prevailing methods for math word problems. DNS <ref type="bibr" target="#b33">(Wang et al., 2017)</ref> is the first to apply vanilla RNN-based models to MWP. Math-EN <ref type="bibr" target="#b31">(Wang et al., 2018b)</ref> introduces equation normalization and compares three Seq2Seq models on MWP solving. Group-ATT <ref type="bibr" target="#b13">(Li et al., 2019)</ref> uses multi-head attention to capture different aspects of features. Some works also leverage tree structures and graph information to improve performance <ref type="bibr" target="#b32">(Wang et al., 2019b;</ref><ref type="bibr" target="#b4">Chiang and Chen, 2019;</ref><ref type="bibr" target="#b15">Liu et al., 2019a;</ref><ref type="bibr" target="#b34">Xie and Sun, 2019;</ref><ref type="bibr" target="#b36">Zhang et al., 2020)</ref>. <ref type="bibr" target="#b25">Shen and Jin (2020)</ref> propose a model of multi-encoders and multi-decoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Pre-trained Language Model</head><p>Pre-trained language models have obtained stateof-the-art results in many NLP benchmarks <ref type="bibr" target="#b30">(Wang et al., 2018a</ref><ref type="bibr" target="#b29">(Wang et al., , 2019a</ref>. These models are usually based on Transformer layers <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref> and trained on large corpus with self-supervised tasks. According to their architectures, pre-trained language models can be categorized into three types: encoder-only, decoder-only and encoderdecoder models. BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> is an encoder-only model which firstly proposes masked token prediction and next sentence prediction to train a language representation model. Following this, many other models are proposed like RoBERTa <ref type="bibr" target="#b17">(Liu et al., 2019b)</ref> and SpanBERT <ref type="bibr" target="#b8">(Joshi et al., 2020)</ref>. Decoder-only models are typically auto-regressive models trained to estimate the probability distribution of a text corpus, including GPT2 <ref type="bibr" target="#b20">(Radford et al., 2019)</ref>, GPT3 <ref type="bibr">(Brown et al., 2020)</ref> and XLNet <ref type="bibr" target="#b35">(Yang et al., 2019)</ref>. Encoderdecoder models like BART  and T5 <ref type="bibr" target="#b21">(Raffel et al., 2020)</ref> use the encoder-decoder architecture and are trained on sequence-to-sequence tasks such as text denoising and translation.</p><p>We propose Generate &amp; Rank, a new multi-task framework for math word problems. Specifically, our model has a generator and a ranker which enhance each other with joint training. We also use tree-based disturbance and online update to further improve the performance. The experimental results on the benchmark show that our work consistently outperforms baselines in all datasets. In future work, we will explore the generation and ranking framework to other tasks like summarization and translation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Our proposed Generate &amp; Rank framework for BART-based MWP solver. The model consists of a generator and a ranker. They share BART encoder and decoder, and are jointly trained with generating loss and ranking loss. We construct an expression bank for training the ranker with expressions produced by the generator and ones obtained by tree-based disturbance. The expression bank is updated every epoch so that the model can constantly learn from new informative examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(12) where D + and D ? are sets of positive and negative examples, respectively. We introduce how to generate negative examples in the next section.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Overview of tree-based disturbance. Algorithm 1 Training Algorithm Input: MWP Dataset D = {(P, S)} Parameter: Pre-trained BART encoder and decoder parameters ? e and ? d , random initialized ranker ? v , beam size K, epoch number M 1: // Fine-tune the generator 2: for epoch = 1 to M do 3: Fine-tuning BART encoder ? e and decoder ? d on D with generation loss Eq. (4). 4: end for 5: // Construct expression bank 6: D + ? D, D ? ? {} 7: for (P, S) ? D do 8: Generate top-K expressions {S i } for problem P with beam search 9:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>12:if result ofS equals result of S then 13:D + ? D + ? {(P,S)} for 19: // Joint training 20: for epoch = 1 to M do 21: Train ? e , ? d , ? v w.r.t. the joint loss Eq.(13) on D + and D ? 22:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>An example of MWP, where numbers are usually mapped to special tokens, such as Num0/1/2.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Solution accuracy on MAWPS and Math23K.</figDesc><table><row><cell>? refers to the result of test set and  ? denotes the result</cell></row><row><cell>of 5-fold cross-validation. "-" means that the results</cell></row><row><cell>are not reported in the original papers.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Effect of joint training.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Accuracy for increasing length of expressions. #Op is the number of operations in expressions. Pro denotes proportion of expressions with different lengths.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Accuracy for different problem domains. Pro denotes the proportion of each domain in the test data. Note that the sum of proportion is not 100% since there are problems not belonging to any specified domain.</figDesc><table><row><cell>5 Related Work</cell></row><row><cell>5.1 Math Word Problem</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://pytorch.org/ 3 https://github.com/huggingface/ transformers</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This paper is partially supported by National Key Research and Development Program of China with Grant No. 2018AAA0101900/2018AAA0101902 as well as the National Natural Science Foundation of China (NSFC Grant No. 62106008 and No.  61772039).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unified pre-training for program understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saikat</forename><surname>Wasi Uddin Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baishakhi</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
		<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Robust Understanding of Word Problems with Extraneous Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yefim</forename><surname>Bakman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:math/0701393</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Natural Language Input for a Computer Problem Solving System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bobrow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<title level="m">Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners</title>
		<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner; Alec Radford</addrLine></address></meeting>
		<imprint>
			<publisher>Sam Mc-Candlish</publisher>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantically-Aligned Equation Generation for Solving and Reasoning Math Word Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1272</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2656" to="2668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding and solving arithmetic word problems: A computer simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fletcher</surname></persName>
		</author>
		<idno type="DOI">10.3758/BF03207654</idno>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods, Instruments, &amp; Computers</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="565" to="571" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to Solve Arithmetic Word Problems with Verb Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad Javad</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1058</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="523" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<title level="m">SpanBERT: Improving Pre-training by Representing and Predicting Spans. Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Parsing Algebraic Word Problems into Equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siena</forename><surname>Dumas Ang</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00160</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="585" to="597" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">MAWPS: A Math Word Problem Repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aida</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1136</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1152" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to Automatically Solve Algebra Word Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-1026</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="271" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">BART: Denoising Sequence-to-Sequence Pretraining for Natural Language Generation, Translation, and Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeling Intra-Relation in Math Word Problems with Different Functional Multi-Head Attentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jierui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><forename type="middle">Tian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxiang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1619</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6162" to="6167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A tag-based statistical English math word problem solver with understanding, reasoning and explanation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Chun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Yi</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Tsung</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Min</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen-Yu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keh-Yih</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI&apos;16</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI&apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4254" to="4255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tree-structured Decoding for Solving Math Word Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyv</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1241</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2370" to="2379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multilingual Denoising Pre-training for Neural Machine Translation. Transactions of the Association for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="726" to="742" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning To Use Formulas To Solve Simple Arithmetic Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2144" to="2153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploring the Limits of Transfer Learning with a Unified Textto-Text Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Solving General Arithmetic Word Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1743" to="1752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unit Dependency Graph and Its Application to Arithmetic Word Problem Solving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Vieira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00118</idno>
		<title level="m">Reasoning about Quantities in Natural Language. Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Solving Math Word Problems with Multi-Encoders and Multi-Decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheqing</forename><surname>Jin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.262</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2924" to="2934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Experiments with a deductive question-answering program</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Slagle</surname></persName>
		</author>
		<idno type="DOI">10.1145/365691.365960</idno>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="792" to="798" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Natural Language Processing for Solving Simple Word Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sowmya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khemani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Natural Language Processing</title>
		<meeting>the 12th International Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="394" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiser</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3261" to="3275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-5446</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Translating a Math Word Problem to a Expression Tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1132</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1064" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Template-Based Math Word Problem Solvers with Recursive Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianli</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><forename type="middle">Tian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33017144</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7144" to="7151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep neural solver for math word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1088</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="845" to="854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A Goal-Driven Tree-Structured Neural Model for Math Word Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019/736</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5299" to="5305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Graph-to-Tree Learning for Solving Math Word Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy Ka-Wei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.362</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3928" to="3937" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
