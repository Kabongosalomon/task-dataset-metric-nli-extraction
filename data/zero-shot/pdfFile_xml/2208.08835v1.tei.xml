<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Differentiable Architecture Search with Random Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyang</forename><surname>Zhang</surname></persName>
							<email>xuanyang91.zhang@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonggang</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongtao</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
							<email>sunjian@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Differentiable Architecture Search with Random Features</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Differentiable architecture search (DARTS) has significantly promoted the development of NAS techniques because of its high search efficiency and effectiveness but suffers from performance collapse. In this paper, we make efforts to alleviate the performance collapse problem for DARTS from two aspects. First, we investigate the expressive power of the supernet in DARTS and then derive a new setup of DARTS paradigm with only training BatchNorm. Second, we theoretically find that random features dilute the auxiliary connection role of skip-connection in supernet optimization and enable search algorithm focus on fairer operation selection, thereby solving the performance collapse problem. We instantiate DARTS and PC-DARTS with random features to build an improved version for each named RF-DARTS and RF-PCDARTS respectively. Experimental results show that RF-DARTS obtains 94.36% test accuracy on CIFAR-10 (which is the nearest optimal result in NAS-Bench-201), and achieves the newest state-of-the-art top-1 test error of 24.0% on ImageNet when transferring from CIFAR-10. Moreover, RF-DARTS performs robustly across three datasets (CIFAR-10, CIFAR-100, and SVHN) and four search spaces (S1-S4). Besides, RF-PCDARTS achieves even better results on ImageNet, that is, 23.9% top-1 and 7.1% top-5 test error, surpassing representative methods like single-path, trainingfree, and partial-channel paradigms directly searched on Im-ageNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Differentiable architecture search (DARTS) <ref type="bibr" target="#b28">(Liu, Simonyan, and Yang 2019)</ref> has demonstrated both higher search efficiency and better search efficacy than early pioneering neural architecture search (NAS) <ref type="bibr" target="#b51">(Zoph and Le 2016;</ref><ref type="bibr" target="#b0">Baker et al. 2016;</ref> attempts in the image classification task. In the past few years, many following works further improve DARTS by introducing additional modules, such as Gumbel-softmax <ref type="bibr" target="#b13">(Dong and Yang 2019b)</ref>, early stop criterion , auxiliary skip-connection <ref type="bibr" target="#b9">(Chu et al. 2021)</ref>, etc. We have witnessed tremendous improvements in the image recognition task, but it is getting farther away from exploring how DARTS works. Newly, in this * These authors contributed equally. This work is supported by Science and Technology Innovation 2030-New Generation Artificial Intelligence (2020AAA0104401). work, we intend to demystify DARTS by disassembling key modules rather than make it more complex.</p><p>We overview the vanilla DARTS paradigm, and summary three key modules, namely dataset, evaluation metric, and supernet as follows:</p><p>? Dataset. DARTS <ref type="bibr" target="#b28">(Liu, Simonyan, and Yang 2019)</ref> searches on proxy dataset and then transfers to target dataset due to huge requirements for GPU memory. PC-DARTS ) proves that proxy datasets inhibit the effectiveness of DARTS and directly searching on target dataset obtains more promising architectures. UnNAS ) and RLNAS <ref type="bibr" target="#b47">(Zhang et al. 2021c</ref>) ablate the role of labels in DARTS, and further conclude that ground truth labels are not necessary for DARTS. ? Evaluation metric. DARTS <ref type="bibr" target="#b28">(Liu, Simonyan, and Yang 2019)</ref> introduces architecture parameters to reflect the strengths of the candidate operations. PT-DARTS ) suspects the effectiveness of architecture parameters and shows that the magnitude of architecture parameters does not necessarily indicate how much the operation contributes to the supernet's performance. FreeNAS <ref type="bibr" target="#b46">(Zhang et al. 2021b</ref>) and TE-NAS <ref type="bibr" target="#b6">(Chen, Gong, and Wang 2021)</ref> further put forward training-free evaluation metrics to predict the performance of candidate architectures. ? Supernet. DARTS encodes all candidate architectures in search space into the supernet. The search cell of supernet will change as the search space changes. R-DARTS <ref type="bibr" target="#b42">(Zela et al. 2020)</ref> proposes four challenging search spaces S1-S4 where DARTS obtains inferior performance than the Random-search baseline. R-DARTS attributes the failure of vanilla DARTS to the dominant skip-connections. Thus R-DARTS concludes that the topology of supernet has great influence on the efficacy of DARTS. P-DARTS  finds that the depth gap between search and evaluation architecture prevents DARTS from achieving better search results.</p><p>In this paper, we take a further step to investigate the supernet in DARTS from two respects. First, we ablate the expressive power of supernet in DARTS. Specifically, each convolution operation (like separable convolutions) consists of two learnable modules: convolution layers and Batch-(a) CIFAR-10 (b) CIFAR-100 (c) ImageNet16-120 <ref type="figure">Figure 1</ref>: The correlation between the DARTS supernet performance (blue histograms) and the searched architecture performance (orange histograms) in NAS-Bench-201 <ref type="bibr" target="#b14">(Dong and Yang 2020)</ref> search space (best viewed in color). We directly searched on target datasets CIFAR-10, CIFAR-100, and ImageNet16-120 in three runs and plot average results. Histograms on three datasets reflect a consistent phenomenon. Supernet performance and the expressive power are positively correlated. However, supernet with higher performance can not search for better architectures, and supernet optimized with only BN has the best search effect.   <ref type="bibr" target="#b28">(Liu, Simonyan, and Yang 2019;</ref><ref type="bibr" target="#b8">Chen et al. 2019;</ref><ref type="bibr" target="#b37">Xu et al. 2020</ref>) adopt option B by default, which disables BN affine weights and only trains convolution weights during the supernet training. On the contrary, options A, C, and D are still unexplored, thus it is a mystery what will happen when supernet is equipped with different expressive power, that is, trained with options of A, C, and D. Hence, we make a sanity check between supernet performance and searched architecture performance across the above four combinations. As shown in <ref type="figure">Fig. 1</ref>, the relative ranking of supernet performance is A?B&gt;C&gt;D, which is consistent with the ranking of supernet's expressive power. However, the relative ranking of searched architecture performance is C D&gt;A?B, which sounds count-intuitive. This result implies that the performance of supernet is not such significant and scaling random features with BN affine weights is just perfect for differentiable architecture search. Therefore, we propose a new extension of DARTS with random features driven by the surprising results of only training BN. Second, we explore the working mechanism of random features by considering skip-connection roles in DARTS. Skip-connection in DARTS plays two roles <ref type="bibr" target="#b9">(Chu et al. 2021)</ref>: 1) as a shortcut to help the optimization of supernet, and 2) a candidate operation for architecture search. Random features dilute the role of auxiliary connection that skip-connection plays in supernet training and enable DARTS to focus on fairer operation selection, thus implicitly solving performance collapse of DARTS <ref type="bibr" target="#b42">(Zela et al. 2020;</ref><ref type="bibr" target="#b9">Chu et al. 2021</ref>). Based on the versatility of supernet optimization, we arm popular DARTS <ref type="bibr" target="#b28">(Liu, Simonyan, and Yang 2019)</ref> and PC-DARTS  with random features to build more effective algorithms RF-DARTS and RF-PCDARTS. On CIFAR-10, RF-DARTS obtains 94.36% test accuracy that is the nearest optimal results (94.37%) in NAS-Bench-201. RF-DARTS achieves the state-of-the-art 24.0% top-1 test error on ImageNet when transferred from CIFAR-10 in DARTS search space. RF-DARTS also performs robustly on CIFAR-10, CIFAR-100, and SVHN across S1-S4. RF-PCDARTS directly searches on ImageNet and achieves 23.9% top-1 test error, which surpasses representative methods from single-path, training-free, and partial channel paradigms. Overall, comprehensive results reveal that the expressive power of DARTS supernet is over-powerful, and random features is just perfect for DARTS. We hope these fundamental analyses and results will inspire new understandings for NAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Differentiable architecture search. With the promising search efficiency, the variants of differentiable architecture search (DARTS) <ref type="bibr" target="#b28">(Liu, Simonyan, and Yang 2019)</ref> have achieved remarkable performance improvement in various computer vision tasks, like image classification <ref type="bibr" target="#b32">Wan et al. 2020)</ref>, object detection <ref type="bibr" target="#b36">(Xu et al. 2019)</ref>, and image segmentation . However, DARTS achieves an efficient search with the cost of several optimization gaps <ref type="bibr" target="#b37">Xu et al. 2020;</ref><ref type="bibr" target="#b38">Yang et al. 2020</ref><ref type="bibr" target="#b39">Yang et al. , 2021</ref><ref type="bibr" target="#b35">Xie et al. 2019;</ref><ref type="bibr" target="#b13">Dong and Yang 2019b;</ref><ref type="bibr" target="#b50">Zhou et al. 2021;</ref><ref type="bibr" target="#b24">Liang et al. 2019;</ref><ref type="bibr" target="#b3">Cai, Zhu, and Han 2019;</ref><ref type="bibr" target="#b41">Yu and Peng 2020)</ref> between the search and retraining stage. Nevertheless, except for the above optimization gaps, DARTS suffers from the severe performance collapse <ref type="bibr" target="#b9">(Chu et al. 2021</ref><ref type="bibr" target="#b10">(Chu et al. , 2020</ref><ref type="bibr" target="#b1">Bi et al. 2019;</ref><ref type="bibr" target="#b42">Zela et al. 2020;</ref><ref type="bibr" target="#b7">Chen and Hsieh 2020;</ref><ref type="bibr" target="#b24">Liang et al. 2019)</ref>, which hinders a wide range of applications of DARTS algorithms. DARTS- <ref type="bibr" target="#b9">(Chu et al. 2021)</ref> finds that the skip-connection has the advantage for stabilizing the supernet training, and thereby supernet has the tendentiousness for choosing skip-connection compared with other operations. Therefore, they introduce an auxiliary skip-connection to decouple its role for stabilizing the gradient flow and role as a candidate operation. Compared to it, RF-DARTS provides a straightforward method -directly training supernet with random features, to prevent the performance collapse brought by skip-connection.</p><p>Random features. Learning with random features <ref type="bibr" target="#b2">(Block 1962;</ref><ref type="bibr" target="#b30">Rahimi and Recht 2007;</ref><ref type="bibr" target="#b40">Yehudai and Shamir 2019;</ref><ref type="bibr" target="#b16">Ghorbani et al. 2019)</ref>, which fixes the neural network's weights at initialization, has been developed a long time and has considerable expressive power <ref type="bibr" target="#b16">(Ghorbani et al. 2019;</ref><ref type="bibr" target="#b40">Yehudai and Shamir 2019)</ref> for building networks. Recently, Frankle et al. <ref type="bibr" target="#b15">(Frankle, Schwab, and Morcos 2021)</ref> investigated the performance achieved by random features when training only the affine parameters of Batch-Norm (BN) <ref type="bibr" target="#b21">(Ioffe and Szegedy 2015)</ref>. Nevertheless, there are few works to explore the expressive power of random features for neural architecture search. Our work is similar to the recent work BN-NAS , which follows the single path one-shot (SPOS) NAS <ref type="bibr" target="#b18">(Guo et al. 2020)</ref> paradigm and uses the scale of affine parameters as the performance indicator. Our work differs from BN-NAS from three aspects. Firstly, although both RF-DARTS and BN-NAS train supernet with only BN, their motivations are quite different. This paper aims to alleviate the problem of performance collapse in DARTS, while BN-NAS aims to improve search efficiency. Secondly, training only BN is first proposed in <ref type="bibr" target="#b15">(Frankle, Schwab, and Morcos 2021)</ref> and then it is used as a general tool. Both RF-DARTS and BN-NAS follows Frankle et al. <ref type="bibr" target="#b15">(Frankle, Schwab, and Morcos 2021)</ref>. We note that training only BN is not claimed as our contribution but training only BN can alleviate skip-connect dominance and further avoid performance collapse are our main contributions. At last, RF-DARTS belonging to gradient-based NAS is quite different from BN-NAS (a one-shot method). Overall, we argue that RF-DARTS is not simply an extension of BN-NAS and both analysis and findings are rather unusual and valuable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">RF-DARTS: DARTS with Random Features</head><p>The preliminary about DARTS <ref type="bibr" target="#b28">(Liu, Simonyan, and Yang 2019)</ref> is described in the appendix. Based on the DARTS, we further introduce our method RF-DARTS. The supernet of DARTS usually contains two learnable modules -convolution (Conv) and BatchNorm (BN) layer. However, DARTS and its variants only consider optimizing the weights of Conv but freezing the learnable affine weights of BN. To understand the expressive power of the above two learnable modules in the weight-sharing supernet, we extend DARTS by introducing the learnable affine weights of BN w bn to the search stage of supernet:</p><formula xml:id="formula_0">o(x, w conv , w bn , ?) = o?O exp(? o ) o ?O exp(? o ) o(x, w o conv , w o bn ).</formula><p>(1)</p><p>Through the analyses of Sec. 4, we find that it is advantageous for search promising architecture when we optimize the weights of the Conv and BN in an inverse way as DARTS. Consequently, we propose the DARTS with random features (RF-DARTS), which only updates the weights of BN but freezes the weights of the Conv at initialization. Hence, the optimization object of RF-DARTS can be formulated as follows:</p><formula xml:id="formula_1">min ? L val (w init conv , w * bn (?), ?),<label>(2)</label></formula><formula xml:id="formula_2">s.t. w * bn (?) = argmin wbn L train (w init conv , w bn , ?),<label>(3)</label></formula><p>where w init conv is kept at initialization and only w bn and ? are optimized. To solve the above objective, we alternatively optimize the architecture parameters ? and the weights of BN w bn in a similar way as DARTS. Furthermore, to accelerate the search procedure, we simply utilize the first-order approximation of gradient by setting ? = 0 when calculating gradient ? ? L val (w conv , w bn ? ?? wbn , ?).</p><p>Feasibility of RF-DARTS. The basic assumption of RF-DARTS is that scaling and shifting random features can reflect the representation capability to some extent. Even though it is very difficult to obtain theoretical analysis, previous works <ref type="bibr" target="#b30">(Rahimi and Recht 2007;</ref><ref type="bibr" target="#b15">Frankle, Schwab, and Morcos 2021)</ref> have empirically demonstrated this assumption. When comes to our framework, the gradients w.r.t. architecture parameters is "feasible" as long as the basic assumption is correct, because large architecture coefficient represents subnetwork with lower loss. Though it is also feasible for conventional DARTS to jointly optimize the conv weights and ?, however, due to convergence speed varies for these two weights, DARTS supernet tends to occur gradient vanishing, which pushes ? converge to skip-connect. Instead, our method traines only BN, thus does not suffer from the vanishing gradient issue. We provide comprehensive analysis ans discussions in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis and Discussions</head><p>As verified by Frankle et al. <ref type="bibr" target="#b15">(Frankle, Schwab, and Morcos 2021)</ref>, random features which only train BN (we refer it as random features in below for simplification) have good enough expressive power for image classification tasks. In this section, we mainly focus on verifying that the expressive power of random features is just perfect for differentiable architecture search. Firstly, we provide analysis for exploring the failure of DARTS when we train all the weights of supernet in Sec. 4.1. Secondly, we provide theoretical analyses to explain how random features solve the failure of DARTS in Sec. 4.2. Finally, we conduct experiments to verify the above analyses in Sec. 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The devil of skip-connection as an auxiliary</head><p>connection in the supernet of DARTS DARTS suffers from the performance collapse mainly due to the instability optimization when the search space includes skip-connection. For example, Amended-DARTS <ref type="bibr" target="#b1">(Bi et al. 2019)</ref> finds that the normal cell will collapse to choose only skip-connection operation in every edge after a very long time search, like 200 epochs. Similarly, DARTS- <ref type="bibr" target="#b9">(Chu et al. 2021</ref>) also thinks that DARTS tenders to select the skip-connection in the final architecture, since skipconnection can help the back-propagation of the gradient when the supernet is very deep. To verify it, they introduce an extra trainable coefficient ? ? [0, 1] on all skipconnections of ResNet-50 <ref type="bibr" target="#b20">(He et al. 2016</ref>) and observe that the coefficient will converge to 1 no matter the initialization (see <ref type="figure" target="#fig_2">Fig. 3a</ref>). Theoretically, given a deep residue model with L residue blocks, the output X l+1 ? R N ?C is calculated as:</p><formula xml:id="formula_3">X l+1 = f l (X l , W l ) + ?X l ,<label>(4)</label></formula><p>where X l ? R N ?C is the input feature and f l is the nonlinear function with weights W l ? R C?C . They investigate the gradient of X l w.r.t. the loss L, which can calculate as (1 is the identity matrix):</p><formula xml:id="formula_4">?L ?X l = ( ?f l ?X l + ? ? 1) ? ?L ?X l+1 .<label>(5)</label></formula><p>With the above gradient formulation, the deep model prefers to push the coefficient ? to 1 for better gradient backpropagation. Thereby, the skip-connection not only serves as a candidate operation but also serves as an auxiliary connection for stabilizing training in the supernet of DARTS. With the above two roles of skip-connection, it is hard to determine the best sub-network since the supernet will tend to choose skip-connection as the candidate operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The power of random features for diluting skip-connection's role of auxiliary connection</head><p>To resolve the search performance collapse of DARTS, it is vital to remove the unfair advantages of skip-connection. DARTS-introduces an auxiliary skip-connection to distinguish the two roles that skip-connection plays in supernet training, thus reserving only its role as candidate operation.</p><p>In this paper, we focus on using random features to resolve the search performance collapse of DARTS. Through the analysis below, we find the gradient vanishing problem of the deep model will not occur when we use random features. In this way, there is no need for skip-connection to solve the gradient vanishing problem, but only as of the specific Conv operation with identity kernel. Thereby, random features can help dilute the role of auxiliary connection that skip-connection plays and thus make a fairer competition with all operations. The analysis is shown below.</p><p>To understand how random features solve the gradient vanishing problem, we investigate the variance of the gradient in each layer of the plain network. Following the derivation of <ref type="bibr" target="#b19">(He et al. 2015)</ref>, we assume each layer with one Conv layer and the activation function g, then the output X l+1 ? R C?C is:</p><formula xml:id="formula_5">X l+1 = g(Y l ), Y l = W l X l ,<label>(6)</label></formula><p>where X l ? R k 2 C?1 represents k ? k pixels in C input channels, and W ? R C?k 2 C is the Conv kernal with spatial size k. Then gradient of X l w.r.t loss L and its variance can be represented as (?X and ?Y donate gradients ?L ?X and ?L ?Y ):</p><formula xml:id="formula_6">?X l = W l ?Y l , (7) V ar[?X l ] = n ? V ar[W l ] ? V ar[?Y l ],<label>(8)</label></formula><p>where n = k 2 C and</p><formula xml:id="formula_7">?Y l = g (Y l )?X l+1 . With the as- sumption of g is ReLU (thus g (Y l ) is zero or one with equal probabilities), we have V ar[?Y l ] = 1 2 V ar[?X l+1 ].</formula><p>Putting together the following layers for layer o, we have:</p><formula xml:id="formula_8">V ar[?X o ] = V ar[?X L+1 ]( L l=o 1 2 nV ar[W l ]). (9)</formula><p>Kaiming initialization <ref type="bibr" target="#b19">(He et al. 2015)</ref> gives a sufficient initialization condition ( 1 2 nV ar[W l ] = 1, ?l) to make sure the variance of gradient at initialization not exponentially large/small, even when the model is extremely deep. Nevertheless, when we train the weights W of deep models using stochastic gradient descent, variances of weights W will change along with the training procedure, which thereby still cannot avoid the gradients vanishing problem for deep models.</p><p>On the contrary, by utilizing random features which freezes all Conv weights of the conv network at initialization, the variances of gradient will not change, and the gradient vanishing problem is also avoided. However, to make the conv network having the learning ability, we need to introduce the BN layer to the network. Therefore, the variances of the gradient are still changing along with the changing of affine weights of BN. Despite all this, we find that by fixing the weights of Conv and only updating the affine weights, it is still much easy to keep the variances of gradients in a normal range compared with the standard training setting. The derivation is shown as below.</p><p>We further include the BN in each layer, then X l+1 = g(Y l ) and:</p><formula xml:id="formula_9">Y l = Transform(Normalize(Conv(X l , W l )), {? bn l , ? bn l }) (10) = W l X l ? ? l ? 2 l + ? ? bn l + ? bn l ,<label>(11)</label></formula><p>where ? l ? R C?1 and ? 2 l ? R C?1 are the means and the variances of W l X l respectively, and ? bn l ? R C?1 and ? bn l ? R C?1 are the learnable affine weights of BN. To simplify the analysis, we only consider the transformation function of BN and ignore the normalization function of BN by letting ? l = 0 and ? 2 l = 1 as the static value. Thereby, the gradient of X l w.r.t. loss L can be denoted as:</p><formula xml:id="formula_10">?X l = W l ? 2 l + ? ? bn l ? ?Y l W l ? ? bn l ? ?Y l .<label>(12)</label></formula><p>Then the variance of the gradient ?X l will be:</p><formula xml:id="formula_11">V ar[?X l ] = n ? V ar[W l ? ? bn l ] ? V ar[?Y l ].<label>(13)</label></formula><p>Thereby, to make the gradient not vanish, we consider the similar condition 1 2 n?V ar[W l ? ? bn l ? l ] = 1 2 n?V ar[W l ?? bn l ] = 1 like the kaiming initalization. Since we have initialized the weight W l using xavier <ref type="bibr" target="#b17">(Glorot and Bengio 2010)</ref> or kaiming initialization <ref type="bibr" target="#b19">(He et al. 2015)</ref>, we only need to make the W l ? ? bn l have variance as W l to keep every layer with a normal range of gradient. Therefore, it is easy and enough to only update the weights of BN to avoid the gradient vanishing problem of deep model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Verification of random features's power through experiments</head><p>Here, we conduct an intuitive experiment to verify that the deep model with random features will not occur the performance degradation problem since it has solved gradient vanishing problem (Sec. 4.2). Instead of leveraging the fixed statistic population in the above theoretical analysis, we introduce running mean and running variance in BN layers to optimize models as the practical setting. We train 18-layer and 50-layer plain ResNets (without skip-connection in the building blocks) on CIFAR-10 with the standard training setting (Conv+BN) and the only BN training setting (Only BN). As shown in <ref type="figure" target="#fig_1">Fig. 2a</ref>, both the training and test error rate of 50-layer plain ResNet is higher than the 18-layer one. On the contrary, the deeper network has a lower training error rate and test error rate ( <ref type="figure" target="#fig_1">Fig. 2b</ref>) when we use random features.</p><p>With the above observations, we can conclude that random features can avoid the performance degradation problem of the deep model directly.   Skip-connection introduced by ResNet has been the essential technique for solving the performance degradation problem. With the utilization of random features, we avoid the performance degradation problem. In this way, the supernet will have no preference of choosing skip-connection. Thereby, random features can dilute the role of auxiliary connection that skip-connection plays. We follow the setting of DARTS-for training ResNet-50 with CIFAR-10 to verify it. We visualize changes of ? for skip-connection of ResNet-50 with different initialization . As shown in <ref type="figure" target="#fig_2">Fig. 3a</ref>, when we train all the trainable weights including the weights of Conv and BN, ? converges to 1 for all initialization settings. On the contrary, we find that ? does not have an evident tendency when we train only the weights of BN. As shown in <ref type="figure" target="#fig_2">Fig. 3b</ref>, except the begin epochs ? changing frequently, ? keeps constant in the end 100 epochs. Finally, we observe the evidence of random features solving the gradient vanishing problem. We calculate V ar(W l ? ? bn l ? l ) and V ar(W init l ) on plain ResNet-50. Then, we plot the <ref type="figure" target="#fig_3">Fig. 4</ref>. When we train only the weights of BN, the mean of relative scale is close to 1. In this way, the gradients vanishing problem will not happen. However, the relative scale is close to 0 when we train all trainable weights, which makes the gradient vanish for the extremely deep model.</p><formula xml:id="formula_12">relative variance ? = V ar[W l ? ? bn l ? l ]/V ar[W init l ] in</formula><p>The above phenomenons demonstrate that random features can avoid the gradient vanishing problem, and skipconnection is not necessary for the training of supernet with random features. Thereby, we can diminish the role of skipconnection for avoiding the gradient vanishing of supernet and make supernet focusing on fairer operation selection.</p><p>Based on the above theoretical analyses, we further evaluate DARTS with random features across three popular search spaces. In NAS-Bench-201 (Dong and Yang 2020) (Sec. 5.1), we compare RF-DARTS with the concurrent work BN-NAS, visualize the architecture parameters in RF-DARTS and further make comparisons with the state-of-the-art methods. In DARTS search space (Liu, Simonyan, and Yang 2019) (Sec. 5.2), RF-DARTS searches on CIFAR-10 and then searched architectures are transferred to CIFAR-100 and ImageNet. RF-PCDARTS directly searches on ImageNet. In RobustDARTS S1-S4 search spaces <ref type="bibr" target="#b42">(Zela et al. 2020)</ref>, we verify the robustness of RF-DARTS. RF-DARTS and RF-PCDARTS follow the default training setting as DARTS and PC-DARTS respectively. These results in RobustDARTS S1-S4 are reported in the Appendix. More implementation details and visualization of searched architectures can also be found in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">NAS-Bench-201 results</head><p>Comparison with BN-NAS. BN-NAS  has introduced a similar supernet optimization paradigm of only training BatchNorm as RF-DARTS. BN indicator for predicting performance is the key contribution in BN-NAS, and only training BN aims to obtain the bonus of 20% search efficiency improvement. Instead, RF-DARTS intends to investigate the expressive power of supernet and alleviate the problem of performance collapse. Except for the different motivations of these works, we further compare the search performance of RF-DARTS and BN-NAS in NAS-Bench-201. As described in BN-NAS ), BN-indicator for an operation is dependent on the Batch-Norm (BN) layer, but there is no BN in none-parametric operations,like skip-connection, avg-pooling, and none. To</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Configurations CIFAR-10 (%)   Visualization of architecture parameters. As discussed in Sec. 4, we attribute the success of RF-DARTS to alleviate the issue of skip-connection dominance. To further support our analysis in the practical search phase, we visualize the evolved architecture parameters of best searched architecture on CIFAR-10. As <ref type="figure" target="#fig_4">Fig. 5</ref> shows, after searching 50 epochs, the evolved architecture parameters converge to architecture {edge 0?1: conv-3x3, edge 0?2: conv-3x3, edge 1?2: conv-3x3, edge 0?3: skip-connection, edge 1?3: conv-3x3, edge 2?3: conv-3x3}. There is only one skip-connection in the searched architecture rather than six skip-connection as DARTS 1st and DARTS 2nd . As for the performance, RF-DARTS boosts test accuracy from 54.30% to 94.36% (+40.06%) on CIFAR-10, 15.61% to 73.51% (+57.90%) on CIFAR-100 and 16.32% to 46.34% (+30.02%) on ImageNet16-120. Thus we further conclude that RF-DARTS dilutes role of skip-connection in supernet optimization and improves the search efficacy by a large margin by making a fairer comparison.</p><p>Comparison with state-of-the-art methods. We further verify the efficacy of RF-DARTS with the comparison of state-of-the-art DARTS variants in NAS-Bench-201. RF-DARTS searches on CIFAR-10 with three runs and then we look up the ground truth performance (both validation accuracy and test accuracy) of searched architectures across CIFAR-10, CIFAR-100 and ImageNet16-120. We report results with the mean?std format in Tab. 3. RF-DARTS almost achieves the optimal performance. More specifically, for test accuracy, RF-DARTS outperforms the newest stateof-the-art method DARTS-by 0.47%, 1.41% and 0.98% on CIFAR-10, CIFAR-100, and ImageNet16-120 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">DARTS search space results</head><p>Results searched on CIFAR-10. In DARTS search space, RF-DARTS searches architectures on CIFAR-10 and then evaluates the searched architectures on CIFAR-10, CIFAR-100, and ImageNet. As shown in Tab. 2, RF-DARTS obtains test error 2.60%, 16.50%, and 24.0% on these three datasets respectively. Compared with vanilla DARTS 2nd ,    RF-DARTS achieves 0.16%, 0.96%, and 2.9% improvements across three datasets. As for the comparison with the state-of-the-art method PDARTS, except for the inferior performance in CIFAR-10, RF-DARTS can also obtain 0.13% and 0.4% improvements on the other two datasets. Besides, RF-DARTS obtains comparable test error compared with DARTS-on CIFAR-10, but RF-DARTS has a much stronger transferring ability. To the best of our knowledge, 24.0% test error on ImageNet is the newest state-of-the-art result with 600M FLOPs constrain when transferring architectures from CIFAR-10.</p><p>Results searched on ImageNet. We verify the search efficacy of RF-PCDARTS on ImageNet. To overcome the challenge of huge GPU memory requirements in vanilla DARTS, there are total three popular paradigms that can directly searched on ImageNet in DARTS search space, namely the partial-channel paradigm, the single-path paradigm and the training-free paradigm. We select two representative methods from each paradigm as the baselines: 1) PC-DARTS and DARTS+ belong the partial-channel paradigm; 2) SPOS and RLNAS belong the single-path paradigm; 3) FreeNAS and TE-NAS follow the training-free paradigm. Following the setting of PC-DARTS, RF-PCDARTS directly searches architectures on ImageNet. As Tab. 4 shows, considering top-1 and top-5 test error comprehensively, RF-PCDARTS consistently exceeds six powerful benchmarks across three paradigms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>To alleviate performance collapse in DARTS, we challenge the conventional supernet optimization paradigm and demystify DARTS from a new perspective of expressive power in supernet. We surprisingly find that only training Batch-Norm achieves higher search performance, which surpasses DARTS with other three learnable module combinations by a large margin and thus we propose RF-DARTS. We further theoretically analyze the variance of gradient in RF-DARTS and conclude that random features can alleviate the problem of performance collapse by diminishing the role of skip-connection. Comprehensive experiments across various datasets and search spaces consistently demonstrate the effectiveness and robustness of RF-DARTS. </p><formula xml:id="formula_13">o(x, w conv , ?) = o?O exp(? o ) o ?O exp(? o ) o(x, w o conv ),<label>(14)</label></formula><p>where parameter ? ? R |O| represents the operations mixing weight, and w conv is the weight of convolution layer for each operation (DARTS disables the affine weights of Batch-Norm (BN), thus only optimizes the weights of the convolution layer). Thereby, for determining a neural architecture with high validation performance, DARTS aims to optimize the architecture parameters ? and the convolution weights w conv of supernet jointly, and finally uses the optimized parameters ? to derive discrete architectures. Formally, DARTS seek to find the architecture parameters ? * with minimal validation loss L val (w * conv , ? * ) where the architecture weights w * conv are optimized by minimizing the training loss L train (w conv , ? * ). This objective can be represented as the below bi-level optimization formulation: min</p><formula xml:id="formula_14">? L val (w * conv (?), ?),<label>(15)</label></formula><formula xml:id="formula_15">s.t. w * conv (?) = argmin wconv L train (w conv , ?).<label>(16)</label></formula><p>However, it is tough to solve the above nested optimization problem directly due to the prohibitive gradient of ? w.r.t. L val . Therefore, DARTS proposes to approximate the gradient using only one single training step as below:</p><formula xml:id="formula_16">? ? L val (w * conv (?), ?)) ? (17) ? ? L val (w conv ? ?? wconv L train (w conv , ?), ?)).<label>(18)</label></formula><p>In this way, DARTS iteratively optimize the weights w conv and parameters ? jointly through gradient descent. DARTS search space. The main body of DARTS <ref type="bibr" target="#b28">(Liu, Simonyan, and Yang 2019)</ref> supernet consists of three parts: 1) a stem layer, 2) eight stacked search cells, and 3) a global average pooling layer and a classifier layer. Specifically, DARTS supernet includes two search cell types, namely normal cell and reduction cell. The reduction cells are located at 1/3 and 2/3 of the supernet depth, and the other search cells are called normal cells. There are six nodes and fourteen edges in both normal cells and reduction cells. Each edge has eight candidate operations: (1) none, (2) 3?3 average pooling, (3) 3?3 max pooling, (4) skip-connection, (5) 3?3 SepConv, (6) 5?5 SepConv, (7) 3?3 DilConv, (8) 5?5 DilConv. With the assumption of the shared normal cell and reduction cell topology, there are total 10 18 candidate architectures in the DARTS search space. In search phase, there are 8 search cells both on CIFAR and ImageNet.</p><p>In evaluation phase, the number of cell is increased from 8 to 20 on CIFAR, and is increased to 14 on ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Datasets and training settings</head><p>CIFAR. CIFAR <ref type="bibr" target="#b22">(Krizhevsky, Hinton et al. 2009</ref>) consists of CIFAR-10 and CIFAR-100. Both CIFAR-10 and CIFAR-100 contains 50K training images and 10K test images. CIFAR-10 has 10 image categories and CIFAR-100 has 100 image categories. The image resolution in CIFAR-10 and CIFAR-100 is 32?32. In the search stage, the original training dataset is splitted into training dataset and validation dataset with equal size. The new training dataset is used to train supernet weights and validation dataset is used to optimize architecture parameters. We use similar search training setting in both NAS-Bench-201 and DARTS search space as vanilla DARTS. The number of training epochs is 50. We use SGD optimizer with a cosine learning rate scheduler initialized with 0.025, a momentum of 0.9, a weight decay of 1e-3 (14e-4 in NAS-Bench-201) and a gradient clip of 5. We also use an Adam optimizer with a constant learning rate 3e-4, a beta of (0.5, 0.999), and a weight decay of 1e-3. There is no evaluation stage in NAS-Bench-201 because of the providing ground truth accuracy. For the evaluation stage in DARTS search space, we retrain searched architectures 600 epochs on both CIFAR-10 and CIFAR-100. Besides, the depth of searched architecture is increased from 8 to 20 and the number of initial channel is increased from 16 to 36.</p><p>Other training settings keep the same as the ones of supernet weight optimization in the search stage.</p><p>ImageNet. ImageNet-1K <ref type="bibr" target="#b11">(Deng et al. 2009</ref>) consists of 1.28M training images and 50K validation images. The image resolution keeps a defaut setting with 224?224. We follow the search and evaluation training settings provided in PC-DARTS . The depth of DARTS supernet is also 8 cells. However, with limited GPU memory, the DARTS supernet use three convolution layer with stride 2 to down-sample feature resolution from 224?224 to 28?28. In the search phase, data subsets 10% and 2.5% of the images from each class are randomly sampled from training dataset. The former (10% of the training images) is used to train supernet weights and the latter subset (5% of the training images) is used to optimize architecture parameters.</p><p>We train supernet with 50 epochs. For the first 35 epochs, we only train BN affine weights. Then, we jointly optimize BN affine weights and architecture parameters in a iterative way. For BN affine weights optimization, we use SGD optimizer with a cosine learning rate scheduler initialized with 0.5, batch size 1024, a momentum of 0.9, a weight decay of 1e-3 and a gradient clip of 5. As for architecture parameters, we use an Adam optimizer with a constant learning rate 6e-3, a beta of (0.5, 0.999), and a weight decay of 1e-3. After search, we build the searched architecture with 14 cells and 48 initial channels. We evaluate the architecture with 250 training epochs and a SGD optimizer with a momentum of 0.9, an initial learning rate of 0.5 (decayed down to zero linearly), and a weight decay of 3e-5. Label smoothing with confidence 0.9 and an auxiliary loss tower are adopted during training. We warm-up learning rate for the first 5 epochs.  <ref type="table">Table 6</ref>: Test error on CIFAR-10, CIFAR-100 and SVHN across RobustDARTS S1-S4. Both RF-DARTS and three benchmarks search directly on target datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">A.3 Robustness verification across DARTS S1-S4</head><p>R-DARTS <ref type="bibr" target="#b42">(Zela et al. 2020)</ref> proposes four search spaces S1-S4 where vanilla DARTS fails. We evaluate the robustness of RF-DARTS in S1-S4 across CIFAR-10, CIFAR-100, and SVHN. In this section, RF-DARTS directly searches architectures on target datasets. To prove the effectiveness of RF-DARTS, we choose DARTS, R-DARTS(DP/L2), and DARTS(ES/ADA) as benchmarks. Tab. 6 shows comparison results between RF-DARTS and three benchmarks. RF-DARTS consistently outperforms vanilla DARTS by a large margin across four search spaces and three datasets. For the other two stronger elaborate benchmarks R-DARTS(DP/L2) and DARTS(ES/ADA), RF-DARTS still obtain superior performance in most cases. These results demonstrate that RF-DARTS performs substantially robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">A.4 Visualization of searched architectures</head><p>Here we visualize the searched cell architectures: RF-DARTS searched on CIFAR-10 ( <ref type="figure">Figure 6</ref>), RF PCDARTS searched on ImageNet-1K <ref type="figure">(Figure 7)</ref>, and RF-DARTS searched on CIFAR-10, CIFAR-100, and SVHN across Ro-bustDARTS S1-S4 <ref type="figure">(Figure 8 to Figure 19</ref>). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Comparison the training and test error rate of two training settings on CIFAR-10 dataset with 18-layer and 50layer "plain" ResNet (without skip-connection in the building blocks).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of evolved ? in two training settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of relative variance ? on plain ResNet-50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Evolved architecture parameters on CIFAR-10. There are total 6 edges in each search cell. Edge i?j to represent the operation from source node i to target node j.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>reduction cell Figure 8: RF-DARTS (S1) searched on CIFAR-10 reduction cell Figure 12: RF-DARTS (S1) searched on CIFAR-100 Figure 14: RF-DARTS (S3) searched on CIFAR-100 reduction cell Figure 19: RF-DARTS (S4) searched on SVHN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Enumerate four combinations of learnable modules in supernet. We keep the composition of supernet unchanged and just change the optimizable weights. means updating weights with the optimizer. means freezing weights at the initialization.</figDesc><table /><note>Norm (BN) (Ioffe and Szegedy 2015) layers. A private BN layer follows each convolution layer. To study the expres- sive power in isolation, we thoroughly traverse four com- binations (named A, B, C, and D for simple) of learnable modules as shown in Tab. 1. Existing DARTS variants</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison with BN-NAS searched on CIFAR-10.</figDesc><table><row><cell>make BN-NAS generalize to NAS-Bench-201, we introduce</cell></row><row><cell>additional a BN layer after each none-parametric operation.</cell></row><row><cell>We train BN-NAS with convolution and BN or only with BN</cell></row><row><cell>layer with 10 epochs, 50 epochs, and 250 epochs and use</cell></row><row><cell>evolution search with BN indicator. We conduct each exper-</cell></row><row><cell>iment in 3 runs and report the best accuracy in Tab. 2. BN-</cell></row><row><cell>NAS converges to search cell with six none operations in 3</cell></row><row><cell>out of 6 configurations and obtains inferior performance less</cell></row><row><cell>than 93% in other three configurations, while RF-DARTS</cell></row></table><note>achieves the nearest optimal test accuracy 94.36%. These comparison results imply that RF-DARTS surpasses BN- NAS by a large margin and it is hard to extend BN-NAS to search space with none-parametric operations.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Search performance on NAS-Bench-201 across CIFAR-10, CIFAR-100 and ImageNet16-120.</figDesc><table><row><cell>Method</cell><cell>CIFAR-10</cell><cell>Test error (%) CIFAR-100</cell><cell>ImageNet</cell><cell>Params (M)</cell><cell>FLOPs (M)</cell><cell>Search Method</cell></row><row><cell>NASNet-A (Zoph and Le 2016)</cell><cell>2.65</cell><cell>17.81</cell><cell>26.0/8.4</cell><cell>3.3</cell><cell>564</cell><cell>RL</cell></row><row><cell>PNAS (Liu et al. 2018)</cell><cell>3.41?0.09</cell><cell>17.63</cell><cell>25.8/8.1</cell><cell>3.2</cell><cell>588</cell><cell>SMBO</cell></row><row><cell>AmoebaNet-A (Real et al. 2019)</cell><cell>3.34?0.06</cell><cell>-</cell><cell>25.5/8.0</cell><cell>3.2</cell><cell>555</cell><cell>EA</cell></row><row><cell>ENAS (Pham et al. 2018)</cell><cell>2.89</cell><cell>18.91</cell><cell>-</cell><cell>4.6</cell><cell>-</cell><cell>RL</cell></row><row><cell>EN 2 AS (Zhang et al. 2020b)</cell><cell>2.61?0.06</cell><cell>16.45</cell><cell>26.7/8.9</cell><cell>3.1</cell><cell>506</cell><cell>EA</cell></row><row><cell>RandomNAS (Li and Talwalkar 2020)</cell><cell>2.85?0.08</cell><cell>17.63</cell><cell>27.1</cell><cell>4.3</cell><cell>613</cell><cell>random</cell></row><row><cell>NSAS (Zhang et al. 2020a)</cell><cell>2.59?0.06</cell><cell>17.56</cell><cell>25.5/8.2</cell><cell>3.1</cell><cell>506</cell><cell>random</cell></row><row><cell>PARSEC (Casale, Gordon, and Fusi 2019)</cell><cell>2.86?0.06</cell><cell>-</cell><cell>26.3</cell><cell>3.6</cell><cell>509</cell><cell>gradient</cell></row><row><cell>SNAS (Xie et al. 2019)</cell><cell>2.85?0.02</cell><cell>20.09</cell><cell>27.3/9.2</cell><cell>2.8</cell><cell>474</cell><cell>gradient</cell></row><row><cell>SETN (Dong and Yang 2019a)</cell><cell>2.69</cell><cell>17.25</cell><cell>25.7/8.0</cell><cell>4.6</cell><cell>610</cell><cell>gradient</cell></row><row><cell>MdeNAS (Zheng et al. 2019)</cell><cell>2.55</cell><cell>17.61</cell><cell>25.5/7.9</cell><cell>3.6</cell><cell>506</cell><cell>gradient</cell></row><row><cell>GDAS (Dong and Yang 2019b)</cell><cell>2.93</cell><cell>18.38</cell><cell>26.0/8.5</cell><cell>3.4</cell><cell>545</cell><cell>gradient</cell></row><row><cell>PDARTS (Chen et al. 2019)</cell><cell>2.50</cell><cell>16.63</cell><cell>24.4/7.4</cell><cell>3.4</cell><cell>557</cell><cell>gradient</cell></row><row><cell>PC-DARTS (Xu et al. 2020)</cell><cell>2.57?0.07</cell><cell>17.11</cell><cell>25.1/7.8</cell><cell>3.6</cell><cell>586</cell><cell>gradient</cell></row><row><cell>DARTS 2nd (Liu, Simonyan, and Yang 2019)</cell><cell>2.76?0.09</cell><cell>17.54</cell><cell>26.9/8.7</cell><cell>3.4</cell><cell>574</cell><cell>gradient</cell></row><row><cell>DARTS- ? (Chu et al. 2021)</cell><cell>2.58</cell><cell>16.80</cell><cell>25.4/7.9</cell><cell>3.5</cell><cell>547</cell><cell>gradient</cell></row><row><cell>RF-DARTS (ours)</cell><cell>2.60</cell><cell>16.50</cell><cell>24.0/7.2</cell><cell>4.6</cell><cell>593</cell><cell>gradient</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison results with state-of-the-art weight-sharing NAS approaches in DARTS search space. These evaluated architectures are searched on CIFAR-10 and transferred to CIFAR-100 and ImageNet. ? we retrain the architecture reported in DARTS-<ref type="bibr" target="#b9">(Chu et al. 2021</ref>).</figDesc><table><row><cell>Paradigm</cell><cell>Method</cell><cell>Test error (%)</cell><cell>Params (M)</cell><cell>FLOPs (M)</cell></row><row><cell>single-path</cell><cell>SPOS (Guo et al. 2020) RLNAS (Zhang et al. 2021c)</cell><cell>25.5/7.9 24.1/7.1</cell><cell>4.6 5.5</cell><cell>512 597</cell></row><row><cell>training-free</cell><cell>FreeNAS (Zhang et al. 2021b) TE-NAS (Chen, Gong, and Wang 2021)</cell><cell>25.1/7.8 24.5/7.5</cell><cell>3.6 5.4</cell><cell>592 591</cell></row><row><cell></cell><cell>PC-DARTS (Xu et al. 2020)</cell><cell>24.2/7.3</cell><cell>5.3</cell><cell>597</cell></row><row><cell>partial-channel</cell><cell>DARTS+ (Liang et al. 2019)</cell><cell>23.9/7.4</cell><cell>5.1</cell><cell>582</cell></row><row><cell></cell><cell>RF-PCDARTS</cell><cell>23.9/7.1</cell><cell>5.4</cell><cell>600</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Comparison results with popular NAS paradigms</cell></row><row><cell>directly searched on ImageNet (mobile setting) in DARTS</cell></row><row><cell>search space.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Preliminary about DARTS DARTS searches for the shared cells for a network with L layers, where each cell is a directed acyclic graph (DAG) with N nodes. Given the pre-defined candidate operations set O with candidate operation o(?) (e.g., skip-connection, convolution), DARTS needs to determine the operation selection o ? O for each edge between every two nodes of</figDesc><table><row><cell>7 A Appendix</cell></row><row><cell>7.1 A.1 the shared cell. Rather than directly making the categori-</cell></row><row><cell>cal choice of a certain operation like ENAS (Pham et al.</cell></row><row><cell>2018), DARTS relaxes the search space to the continuous</cell></row><row><cell>one through a softmax over all operations in the candidate</cell></row><row><cell>set O:</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02167</idno>
		<title level="m">Designing neural network architectures using reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Stabilizing DARTS with Amended Gradient Estimation on Architectural Parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The perceptron: A model for brain functioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-D</forename><surname>Block</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reviews of Modern Physics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">123</biblScope>
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">P</forename><surname>Casale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fusi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05116</idno>
		<title level="m">Probabilistic neural architecture search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BN-NAS: Neural Architecture Search With Batch Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural Architecture Search on ImageNet in Four GPU Hours: A Theoretically Inspired Perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stabilizing Differentiable Architecture Search via Perturbation-based Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Progressive Differentiable Architecture Search: Bridging the Depth Gap Between Search and Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">DARTS-: Robustly Stepping out of Performance Collapse Without Indicators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fair DARTS: Eliminating Unfair Advantages in Differentiable Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">One-shot neural architecture search via self-evaluated template network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Searching for a Robust Neural Architecture in Four GPU Hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Training BatchNorm and Only BatchNorm: On the Expressive Power of Random Features in CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Linearized two-layers neural networks in high dimension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Misiakiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montanari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<editor>AIS-TATS</editor>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Single Path One-Shot Neural Architecture Search with Uniform Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Random search and reproducibility for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in artificial intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06035</idno>
		<title level="m">Darts+: Improved differentiable architecture search with early stopping</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Are labels necessary for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient Neural Architecture Search via Parameter Sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Random Features for Large-Scale Kernel Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<title level="m">FBNetV2: Differentiable Neural Architecture Search for Spatial and Channel Dimensions</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rethinking Architecture Selection in Differentiable NAS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">FB-Net: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">SNAS: stochastic neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Auto-FPN: Automatic Network Architecture Adaptation for Object Detection Beyond Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">PC-DARTS: Partial Channel Connections for Memory-Efficient Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">ISTA-NAS: Efficient and Consistent Neural Architecture Search by Sparse Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards Improving the Consistency, Efficiency, and Flexibility of Differentiable Neural Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On the Power and Limitations of Random Features for Understanding Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yehudai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10724</idno>
		<title level="m">Cyclic differentiable architecture search</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Understanding and Robustifying Differentiable Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Marrakchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Differentiable Neural Architecture Search in Equivalent Space with Exploration Enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">One-Shot Neural Architecture Search via Novelty Driven Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Abbasnejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Haffari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10784</idno>
		<title level="m">iDARTS: Differentiable Architecture Search with Stochastic Implicit Gradients</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Haffari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.11542</idno>
		<title level="m">Differentiable Architecture Search Without Training Nor Labels: A Pruning Perspective</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Neural architecture search with random labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multinomial distribution learning for effective neural architecture search</title>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">EC-DARTS: Inducing Equalized and Consistent Optimization Into DARTS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
