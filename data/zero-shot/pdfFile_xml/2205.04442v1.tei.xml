<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MixAugment &amp; Mixup: Augmentation Methods for Facial Expression Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Psaroudakis</surname></persName>
							<email>andreaspsaroudakis@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">National Technical University of Athens</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
							<email>d.kollias@qmul.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">Queen Mary University of London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MixAugment &amp; Mixup: Augmentation Methods for Facial Expression Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic Facial Expression Recognition (FER) has attracted increasing attention in the last 20 years since facial expressions play a central role in human communication. Most FER methodologies utilize Deep Neural Networks (DNNs) that are powerful tools when it comes to data analysis. However, despite their power, these networks are prone to overfitting, as they often tend to memorize the training data. What is more, there are not currently a lot of in-the-wild (i.e. in unconstrained environment) large databases for FER. To alleviate this issue, a number of data augmentation techniques have been proposed. Data augmentation is a way to increase the diversity of available data by applying constrained transformations on the original data. One such technique, which has positively contributed to various classification tasks, is Mixup. According to this, a DNN is trained on convex combinations of pairs of examples and their corresponding labels.</p><p>In this paper, we examine the effectiveness of Mixup for in-the-wild FER in which data have large variations in head poses, illumination conditions, backgrounds and contexts. We then propose a new data augmentation strategy which is based on Mixup, called MixAugment. According to this, the network is trained concurrently on a combination of virtual examples and real examples; all these examples contribute to the overall loss function. We conduct an extensive experimental study that proves the effectiveness of MixAugment over Mixup and various state-of-the-art methods. We further investigate the combination of dropout with Mixup and MixAugment, as well as the combination of other data augmentation techniques with MixAugment.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The human emotion constitutes a conscious subjective experience that can be expressed in various ways. During the past decade, with the rapid development in the field of Artificial Intelligence, scientists have conducted numerous studies to develop systems and robots that will be capable of perceiving automatically people's feelings and behaviors <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29]</ref>. An ultimate goal is the creation of digital assistants that will display a human-centered character and interact with users in the most natural way possible. It is a very complex and demanding task, since expression recognition in real world conditions is not easy and straightforward to do <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>Over the last decade, Deep Neural Networks have emerged as a method to solve any computer vision task <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b56">57]</ref>. DNNs in order to work and generalise well, need to be trained on large and diverse databases <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b37">38]</ref>. Nevertheless, in multiple applications, the collection of new data and their corresponding annotation is not always an easy or possible task to do (eg it is a quite time consuming and costly process). In the FER domain, RAFD-DB <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>, Affect-Net <ref type="bibr" target="#b43">[44]</ref> and Aff-Wild2 <ref type="bibr">[15, 21-24, 27, 30, 32, 34, 36, 37, 58]</ref> are the most widely used in-the-wild databases. Additionally, despite DNNs' considerable power, the networks are prone to overfitting. This means that they often tend to memorize the input data or learn the noise and not the real data distribution, thus failing to generalize successfully when faced with data that are (considerably) different to the input ones.</p><p>One possible solution would be to expand the training set by adding new samples (although as we previously mentioned that is not always feasible due to inavailability of existing large in-the-wild datasets). Another way of extending the training set is by adding artificial samples that have been produced using 3D methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b52">53]</ref> or Generative Adversarial Networks (GANs) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b59">60]</ref>. However, in this case, the generated samples must be realistic to the human eye, which still remains a very challenging task under investigation. In various application fields other problems may arise as well (eg for creating human faces the identity of the human should be preserved).</p><p>Another approach is to use data augmentation techniques, i.e., methods that produce new samples, by utilizing those that are already available and exist in the training set. Data augmentation is a way to increase the diver-sity of available data by applying constrained transformations on the original data. A fairly recent technique of this kind, which has positively contributed to various tasks, is Mixup <ref type="bibr" target="#b58">[59]</ref>. According to that, a DNN is trained on convex combinations of pairs of examples and their corresponding labels. By doing so, the distribution of the available data is extended and the generalization ability of the network improves. This principle has already been applied in some particular fields but has hardly ever been tried out in human affect estimation problems, especially in "in-the-wild" conditions with variations in head poses, illumination conditions, backgrounds and contexts.</p><p>In this paper, we examine the effectiveness of Mixup for 7 basic expression classification (categorical model <ref type="bibr" target="#b5">[6]</ref>) by utilizing the Real-world Affective Faces Database (RAF-DB) <ref type="bibr" target="#b42">[43]</ref>, a large-scale facial expression database with around 30K great-diverse facial images downloaded from the Internet. We further propose a new data augmentation strategy that is based on Mixup, which we call MixAugment; according to this a DNN is trained on a combina- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Mixup <ref type="bibr" target="#b58">[59]</ref> constitutes a simple but powerful data augmentation routine that has already been applied in various tasks in Computer Vision, Natural Language Processing (NLP) and the audio domain. Some indicative examples pertain to medical image segmentation <ref type="bibr" target="#b4">[5]</ref>, sentence classification <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b48">49]</ref>, audio tagging <ref type="bibr" target="#b54">[55]</ref>, audio scene classification <ref type="bibr" target="#b55">[56]</ref> and image classification <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>Regarding expression recognition, Mixup has been tried out only in very limited scenarios. In particular, this data augmentation technique was applied for the first time in speech expression recognition (SER) data to alleviate the issue of small existing datasets in the field. In <ref type="bibr" target="#b38">[39]</ref> a framework that combined Mixup with a Generative Adversarial Network was proposed so as to improve the generation of synthetic samples. Specifically, they utilized this routine to train a GAN for synthetic expression feature generation and also for learning expression feature representation. To prove the effectiveness of the proposed framework, they showed results for SER on synthetic feature vectors, augmentation of the training data with synthetic features and encoded features in compressed representation. The results indicated that the proposed network can successfully learn compressed expression representations and can also produce synthetic samples that enhance performance in within-corpus and cross-corpus evaluation.</p><p>Apart from the lack of many large in-the-wild datasets in the field of SER, another problem is affiliated with the common difference between the training and test data distributions. SER systems can achieve high accuracy when these two sets are identically distributed, but this assumption is often violated in practice and the systems' performance declines against unforeseen data shifts. In <ref type="bibr" target="#b40">[41]</ref>, the authors proved that the use of Mixup enhances the robustness to noise and adversarial examples in DNN Architectures. As a result, the generalization ability of the models improves and the DNNs perform better against unseen realtime situations. Moreover, the evaluations on the widely used IEMOCAP and MSP-IMPROV datasets showed that Mixup is a better augmentation technique for SER compared to the popular speed perturbation <ref type="bibr" target="#b39">[40]</ref>.</p><p>Jia and Zheng <ref type="bibr" target="#b13">[14]</ref> tried to solve the problems of naturalness, robustness, fidelity and expression recognition accuracy in the process of expression speech synthesis. For that purpose, they designed an expression speech synthesis method based on multi-channel time-frequency generative adversarial networks (MC-TFD GANs) and Mixup. The comparative experiments were carried out on the IEMO-CAP corpus. The results showed that the mean opinion score (MOS) and the unweighted accuracy (UA) of the speech generated by the synthesis method were improved by 4% and 2.7%, respectively. The proposed method was superior in subjective evaluation and objective experiments, proving that the speech produced by this model had higher reliability, better fluency and emotional expression ability.</p><p>In terms of expression recognition from facial images,a published work in which Mixup is utilized, is from <ref type="bibr" target="#b45">[46]</ref>. In this paper, the researchers made use of Mixup to improve the generalization of a proposed DNN, named eXnet. The model was trained and evaluated on FER-2013, CK+, and RAF-DB benchmark datasets. The experimental results showed that the model trained with Mixup technique witnessed an increase in accuracy of about 1%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Mixup</head><p>Mixup <ref type="bibr" target="#b58">[59]</ref> is a simple and data-agnostic data augmentation routine that trains a DNN on convex combinations of pairs of examples and their labels. In other words, Mixup constructs virtual training examples (x,?) as follows:</p><formula xml:id="formula_0">x = ?x i + (1 ? ?)x j y = ?y i + (1 ? ?)y j<label>(1)</label></formula><p>where x i and x j are two random raw inputs (i.e., images), y i and y j ? {0, 1} 7 are their corresponding one-hot label encodings and ? ? B(?, ?) ? [0, 1] (i.e., Beta distribution) for ? ? (0, ?).</p><p>Therefore, Mixup extends the training distribution by incorporating the prior knowledge that linear interpolations of feature vectors should lead to linear interpolations of the associated targets. By doing so, it regularizes the DNN (while training) to favor linear behavior in-between training examples. The implementation of Mixup training is straightforward, and introduces a minimal computation overhead.</p><p>In the studied case, the training samples are aligned facial images and the labels are one-hot encoding vectors corresponding to one of the 7 basic expressions. When training a DNN with the Mixup technique, the mixup loss function is the categorical cross entropy (CCE) of the virtual (v) samples defined as:</p><formula xml:id="formula_1">L v CCE = ? y,? [?? ? log?]<label>(2)</label></formula><p>where? is the predicted probability of the samplex;x and y are given in Eq. 1.</p><p>An example of Mixup implementation on facial images is illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>. An image that corresponds to a "happy" facial expression is linearly mixed with another one that demonstrates a "sad" expression, in a 60:40 ratio. The resulting image depicts a human face, that combines facial characteristics from the two initial images. Its label, which is written above the constructed image, states that this virtual sample belongs to class "happy" by 60% and to class "sad" by 40%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">MixAugment</head><p>In the typical Mixup data augmentation routine, randomly selected pairs of images are linearly interpolated and then fed into the DNN for training. However, "in-the-wild" facial databases contain a lot of images with large variations in head poses, gazes and angles. As a result, when mixing randomly selected images, it is possible for two images with different head poses to be combined. An indicative example is illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>, where a "happy" facial expression is mixed with a "sad" reaction, in a 50:50 ratio. As one can see, the resulting image does not resemble a real human face. Such cases may hinder training and learning of DNNs. To cope with this problem, we propose a simple approach name MixAugment. According to this, during each training iteration, the DNN is trained concurrently on both real (r) and virtual (v) examples. Specifically, in each training iteration, the DNN is fed with both x i and x j , and the generated imagex = ?x i + (1 ? ?)x j (of Eq. 1). In this scenario, the loss function is:</p><formula xml:id="formula_2">L total = L v CCE + L ri CCE + L rj CCE = E[?? ? log? ? y i ? log? i ? y j ? log? j ] = E ? [?y i + (1 ? ?)y j ] ? log? ? y i ? log? i ? y j ? log? j = E[?yi ? log (? i? ? ) ? y j ? log (? j? 1?? )]<label>(3)</label></formula><p>where? is the predicted probability of the samplex;x and y are given in Eq. 1; y i and y j are the labels of two (random) images (mentioned in Eq. 1) and? i and? j are their corresponding predicted probabilities (the indices in the expectations are omitted for simplicity).</p><p>As can be seen in Eq. 3 we merge the mixup loss with the classification loss to enhance the classification ability on both raw samples and mixup samples. This is different from the original design of Mixup <ref type="bibr" target="#b58">[59]</ref> where the authors replaced the classification loss with the mixup loss.  <ref type="figure" target="#fig_3">Figure 3</ref>. It is worth mentioning that the two sets are imbalanced, with the expression "happy" having by far the largest number of samples and the class "fearful" being the least popular in both cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance Metric</head><p>For the evaluation of our models we make use of three different performance metrics: i) Accuracy, ii) Average Accuracy (mean diagonal of the normalized confusion matrix) and iii) macro F1-score (harmonic mean of Precision and Recall). Accuracy is defined as the percentage of correct predictions among the total number of predictions. It is the most common evaluation metric, however not preferred in imbalanced classification problems. Our dataset is imbalanced, therefore, to have a superior insight, we should take into account some additional metrics. The Average Accuracy (aka macro Recall) and macro F1-score are useful, since they give equal importance to each class, in contrast to Accuracy which gives equal importance to each sample, thus favoring majority classes. During the training phase, we monitor these metrics, and if no improvement is observed over the test set, we apply early stopping and keep the best configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Pre-processing</head><p>Data pre-processing consists of the steps required for facilitating extraction of meaningful features from the data. In a typical expression recognition problem with facial images, the usual steps are face detection and alignment, image resizing and image normalization. We experimented with using two different face detectors to extract bounding boxes around each face and detect 68 facial landmarks. In the first version of the database (the public release), a face detector from the dlib library has been used, while in the other case the detector is the RetinaFace <ref type="bibr" target="#b2">[3]</ref>. The alignment step is the same for both versions. Out of all 68 located landmarks, we focus on 5 -corresponding to the location of the left eye, right eye, nose and mouth in a prototypical frontal face -as rigid, anchor points. Then, for every image, the respective 5 facial landmarks are extracted and affinity transformations between the coordinates of these 5 landmarks and the coordinates of the 5 landmarks of the frontal face are computed; these transformations are imposed to the whole new frame for the alignment to be performed. All resulting images are resized to 100?100?3 or 112?112?3. Finally, all cropped and aligned images' pixel intensity values are normalized to the range [0, 1]. <ref type="table" target="#tab_0">Table 1</ref> demonstrates all implementation details pertained to the training session. Where dropout <ref type="bibr" target="#b47">[48]</ref> was applied, its value was 0.5. In the following, to not clutter the presented results, we present results only for the publiclyreleased dataset version (1st version); the same conclusions have been drawn when utilizing the other version (2nd).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Training Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Results</head><p>Utilize Mixup vs Vanilla Case We start our experiments by training a ResNet50 <ref type="bibr" target="#b10">[11]</ref>, pretrained on Ima-geNet, for 100 epochs, when applying and not applying dropout (vanilla case). We also train the exact same  <ref type="table">Table 2</ref> illustrates the results of these experiments. Let us not that large values of the hyperparameter (? ? {4, 8}) in Mixup lead to underfitting and not good network performance. In <ref type="table">Table 2</ref>, it can be seen that the best configuration (i.e., when the model trained with Mixup achieves its higher performance in all 3 studied metrics) is when ? = 0.1 and no dropout has been used. In this case the model trained with Mixup outperforms by at least 0.3% in all studied evaluation metrics the model trained without Mixup. In terms of the use of dropout, <ref type="table">Table 2</ref> shows that its addition sometimes contributes positively, whereas some other times seems to contribute negatively.</p><p>Utilize MixAugment vs Vanilla Case Similar as before, we use the same model (ResNet50 pretrained on ImageNet) with the same training parameters and compare its performance when the proposed MixAugment is and is not used. <ref type="table" target="#tab_1">Table 3</ref> illustrates that performance comparison for ? ? {0.1, 0.2, 0.6, 1} and when dropout is and is not applied. Similarly as in the Mixup case, we noticed that for large values of the hyperparameter (? ? {4, 8}) Mixup leads to underfitting and and not good network performance. In <ref type="table" target="#tab_1">Table 3</ref>, it can be seen that the best configuration is when ? = 0.1 and no dropout has been used. In this case, the model trained with MixAugment outperforms by at least 1.7% in all studied evaluation metrics the model trained without MixAugment. In terms of the use of dropout, <ref type="table" target="#tab_1">Table  3</ref> shows that its addition sometimes contributes positively, whereas some other times seems to contribute negatively. Finally, one can observe in <ref type="table" target="#tab_1">Tables 2 and 3</ref>, that in both cases (when Mixup or MixAugment have been used), best results across all metrics have been obtained when ? = 0.1 and no dropout has been used. We can deduct that optimal results cannot be achieved when dropout is used in addition to Mixup or MixAugment. Next, in <ref type="table" target="#tab_2">Table 4</ref> we present the model's confidence for the correct and wrong predictions under two settings: i) when the model is trained with the proposed MixAugment and ii) when it is trained without MixAugment (this is the vanilla case). As illustrated in <ref type="table" target="#tab_2">Table 4</ref>, our proposed technique helps the network make right decisions with higher confidence and wrong decisions with less assuredness, which is obviously desirable. In addition, it would be interesting to examine the performance of MixAugment separately on each one of the 7 basic expression categories across various metrics. For that purpose, in <ref type="table" target="#tab_3">Table 5</ref>, we present the Precision, Recall and F1-score for each class when the model is trained with and without MixAugment. It can be seen that when the network is trained with MixAugment, in the vast majority of the categories, there is a substantial rise in all the aforementioned metrics. It is worth mentioning that the highest growth is observed for the minority class "fearful". Particularly, Pre- cision, Recall and F1-score increase by approximately 10%, 5% and 6% respectively when MixAugment is used. This is a really promising result, since there are many classification tasks in which the classes with the smallest number of samples are of paramount importance (e.g. medical classification problems).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Utilize MixAugment vs Utilize Mixup vs Vanilla Case</head><p>To summarise the main presented results and to illustrate the difference in ResNet50's performance when the model is trained with the proposed MixAugment, when it is trained with Mixup and when it is trained without any of these, we have created <ref type="table" target="#tab_4">Table 6</ref>. It can be observed that Mixup improves the model's performance and MixAugment further improves its performance. Compared to Mixup, our technique further improves all three evaluation metrics for at least 1%. Finally, it is notable to mention that when Mix-Augment is used in network training, the convergence is faster compared to the cases when Mixup is used or when neither of the two is used. Finally, let us mention a final experiment that we conducted. When training the model (ResNet50) with the proposed MixAugment we further performed flipping, which resulted in further boosting the model's performance by around 1% in each studied metric (Accuracy, F1-score and Average Accuracy).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Utilize MixAugment with other DNNs</head><p>We further used our proposed MixAugment when training other widely used DNNs, such as VGG16 <ref type="bibr" target="#b46">[47]</ref>, DenseNet121 <ref type="bibr" target="#b12">[13]</ref> and Ef-ficientNet <ref type="bibr" target="#b51">[52]</ref>. We noticed the same observations as before (i.e., as in the case of ResNet50 described previously). In more detail, the performance of these networks trained with MixAugment outperformed -in all 3 studied metricsthe performance of the networks trained with Mixup, which outperformed -over all metrics-the performance of the corresponding vanilla networks.</p><p>Utilize MixAugment vs State-of-the-Art In the previous cases, our model (ResNet50) was only pre-trained on Ima-geNet. It is known that if the model is further pre-trained on a similar task to the studied one (which is FER), then its performance further increases. Therefore we first pre-trained ResNet50 on AffectNet and then trained it with MixAugment and flipping. In <ref type="table" target="#tab_5">Table 7</ref> we compare its performance to the performance of various state-of-the-art methods. It can be observed that our approach outperforms all state-ofthe-art methods in the accuracy metric and shows a slightly worse performance than two state-of-the-art methods (Face-BehaviorNet (Residual) <ref type="bibr" target="#b23">[24]</ref>) and VGGFACE <ref type="bibr" target="#b18">[19]</ref>) in the average accuracy metric. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy Aver. Acc.</p><p>RAN <ref type="bibr" target="#b53">[54]</ref> 86,90 -Ad-Corre <ref type="bibr" target="#b6">[7]</ref> 86,96 -mSVM + DLP-CNN <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref> -74,20 MT-ArcRes <ref type="bibr" target="#b33">[34]</ref> -75,00 MT-ArcVGG <ref type="bibr" target="#b33">[34]</ref> -76,00 FaceBehaviorNet (VGG) <ref type="bibr" target="#b22">[23]</ref> -71,00 FaceBehaviorNet (Residual) <ref type="bibr" target="#b23">[24]</ref> -78,00 VGGFACE <ref type="bibr" target="#b18">[19]</ref> -77,50 pre-train, MixAugment + Flipping 87,54 77,30</p><p>The FaceBehaviorNet (Residual) <ref type="bibr" target="#b23">[24]</ref> is a multi-task learning network that has been trained on over 5M of images. The VGGFACE <ref type="bibr" target="#b18">[19]</ref> is a network that has been trained on an augmented training set consisting of the training set of RAF-DB plus 13,000 other synthetic/generated images (more than the training size of RAF-DB); therefore our method is expected to perform worse than such methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In this paper, at first, we examine the effectiveness of Mixup for in-the-wild Facial Expression Recognition. Mixup is a data augmentation technique in which a DNN is trained on convex combinations of pairs of examples and their corresponding labels. Taking into account that in in-the-wild FER people display high variations in head poses, illumination conditions, backgrounds and contexts, we have proposed a variation of Mixup, called MixAugment in which the network is trained on a combination of virtual examples (generated by Mixup) and real examples; in our approach both examples contribute to the overall loss function. We have conducted a large experimental study that includes: performance comparison between models trained with Mixup, MixAugment or without any of the two versus state-of-the-art methods; ablation studies; testing the combination of Mixup or MixAugment and dropout; testing the combination of MixAugment and other data augmentation techniques such as flipping. The experimental study proves that models perform better when our proposed MixAugment is used during training.</p><p>In our future work, we aim to extend and apply the proposed MixAugment technique to other small and large-scale "in-the-wild" datasets, as well as to other affect recognition tasks, such as valence-arousal estimation and action unit detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>tion of virtual examples and real examples. The overall loss function of DNN training consists of the loss of the real examples and the loss of the virtual examples. Finally we examine the effect of dropout [48] when used in combination with Mixup and our proposed MixAugment. Useful conclusions are drawn from the experimental study and the foundations are laid for future extensions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Construction of virtual example with the Mixup technique on samples taken from RAF-DB</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Example of mixing images with different head poses</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Class distribution of the RAF-DB training and test sets the crowdsourcing annotation, each image has been independently labeled by about 40 annotators. Images in this database are of great variability in subjects' age, gender and ethnicity, head poses, lighting conditions, occlusions, (e.g. glasses, facial hair, self-occlusion). RAF-DB includes two subsets: (i) single-label subset, which consists of images annotated in terms of the seven basic expressions (surprise, fear, disgust, happiness, sadness, anger and neutral); (ii) multi-label subset, which consists of images annotated in terms of twelve compound expressions. In our experiments, we use the single-label subset. The database has been split into a training set (consisting of around 12,200 images) and a test set (consisting of around 3,100 images) where the size of training set is four times larger than the size of the test set; expressions in both sets have a near-identical distribution, as illustrated in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Training parameters with their corresponding values</figDesc><table><row><cell>Parameters</cell><cell></cell><cell>Values</cell><cell></cell></row><row><cell>Image size</cell><cell cols="3">100 ? 100 ? 3 (1st version) 112 ? 112 ? 3 (2nd version)</cell></row><row><cell>Batch size</cell><cell></cell><cell>32</cell><cell></cell></row><row><cell>Loss function</cell><cell cols="3">Categorical cross entropy</cell></row><row><cell>Optimizer</cell><cell></cell><cell>Adam</cell><cell></cell></row><row><cell>Learning rate</cell><cell></cell><cell>10 ?3 , 10 ?4</cell><cell></cell></row><row><cell>Dropout rate</cell><cell></cell><cell>0.5</cell><cell></cell></row><row><cell>Number of epochs</cell><cell></cell><cell>100</cell><cell></cell></row><row><cell cols="4">Table 2. ResNet50 trained with Mixup and without Mixup (i.e.,</cell></row><row><cell>vanilla case)</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Mixup Dropout Accuracy F1-score Aver. Acc.</cell></row><row><cell>No</cell><cell>83,31 83,21</cell><cell>75,25 75,16</cell><cell>73,46 74,07</cell></row><row><cell>? = 0.1</cell><cell>84,06 83,25</cell><cell>75,51 74,88</cell><cell>74,38 73,60</cell></row><row><cell>? = 0.2</cell><cell>82,33 83,12</cell><cell>73,95 74,62</cell><cell>73,38 74,14</cell></row><row><cell>? = 0.6</cell><cell>83,15 83,47</cell><cell>74,81 75,23</cell><cell>72,73 74,29</cell></row><row><cell>? = 1</cell><cell>82,55 82,67</cell><cell>74,01 74,69</cell><cell>73,29 73,58</cell></row><row><cell cols="4">model (same weight initialization) with Mixup, for ? ?</cell></row><row><cell>{0.1, 0.2, 0.6, 1}.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>ResNet50 trained with the proposed MixAugment and without MixAugment (i.e., vanilla case)</figDesc><table><row><cell cols="4">MixAugment Dropout Accuracy F1-score Aver. Acc.</cell></row><row><cell>No</cell><cell>83,31 83,21</cell><cell>75,25 75,16</cell><cell>73,46 74,07</cell></row><row><cell>? = 0.1</cell><cell>85,04 83,96</cell><cell>77,30 76,03</cell><cell>75,32 73,77</cell></row><row><cell>? = 0.2</cell><cell>84,19 84,39</cell><cell>76,57 76,64</cell><cell>74,74 75,13</cell></row><row><cell>? = 0.6</cell><cell>84,13 84,26</cell><cell>75,04 76,46</cell><cell>73,38 74,38</cell></row><row><cell>? = 1</cell><cell>83,74 83,51</cell><cell>75,43 75,58</cell><cell>73,87 74,36</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Prediction confidence with MixAugment and without MixAugment (i.e. vanilla case)</figDesc><table><row><cell>Confidence</cell><cell>mean</cell><cell>median</cell></row><row><cell>Type of predictions</cell><cell cols="2">correct wrong correct wrong</cell></row><row><cell cols="3">No MixAugment (Vanilla) 96,37 92,66 98,82 99,69</cell></row><row><cell>MixAugment</cell><cell cols="2">98,77 84,24 100,0 90,75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Comparison between ResNet50 trained with the proposed MixAugment and without MixAugment (i.e., vanilla case) for each one of the 7 classes</figDesc><table><row><cell>Class</cell><cell cols="2">Precision No MixAugment (Vanilla) MixAugment</cell><cell cols="2">Recall No MixAugment (Vanilla) MixAugment</cell><cell cols="2">F1-score No MixAugment (Vanilla) MixAugment</cell><cell>Samples</cell></row><row><cell>surprised</cell><cell>83,49</cell><cell>? 85,14</cell><cell>79.94</cell><cell>? 83,59</cell><cell>81,68</cell><cell>? 84,36</cell><cell>329</cell></row><row><cell>fearful</cell><cell>69,09</cell><cell>? 78,85</cell><cell>51,35</cell><cell>? 55,41</cell><cell>58,91</cell><cell>? 65,08</cell><cell>74</cell></row><row><cell>disgusted</cell><cell>62,60</cell><cell>? 65,89</cell><cell>51,25</cell><cell>? 53,12</cell><cell>56,36</cell><cell>? 58,82</cell><cell>160</cell></row><row><cell>happy</cell><cell>92,33</cell><cell>? 92,73</cell><cell>93,50</cell><cell>? 92,57</cell><cell>92,91</cell><cell>? 92,65</cell><cell>1185</cell></row><row><cell>sad</cell><cell>81,01</cell><cell>? 83,37</cell><cell>80,33</cell><cell>? 82,85</cell><cell>80,67</cell><cell>? 83,11</cell><cell>478</cell></row><row><cell>angry</cell><cell>79,47</cell><cell>? 75,08</cell><cell>74,07</cell><cell>? 71,60</cell><cell>76,68</cell><cell>? 75,08</cell><cell>162</cell></row><row><cell>neutral</cell><cell>78,30</cell><cell>? 77,73</cell><cell>85,44</cell><cell>? 86,76</cell><cell>81,72</cell><cell>? 82,00</cell><cell>680</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>ResNet50 trained with MixAugment, with Mixup and without any of the two</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell><cell>F1-score</cell><cell>Aver. Acc.</cell></row><row><cell>Vanilla</cell><cell>83,31</cell><cell>75,25</cell><cell>73,46</cell></row><row><cell>Mixup</cell><cell>84,06</cell><cell>75,51</cell><cell>74,38</cell></row><row><cell>MixAugment</cell><cell>85,04</cell><cell>77,30</cell><cell>75,32</cell></row><row><cell>MixAugment + Flipping</cell><cell>86,06</cell><cell>78,24</cell><cell>76,28</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Performance comparison between state-of-the-art methods and ResNet50 trained with MixAugment</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">. Experimental Studies 4.1. Database All the experiments are carried out utilizing the Realworld Affective Faces Database (RAF-DB), a large-scale facial expression database with around 30K great-diverse facial images downloaded from the Internet. Based on</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bringing portraits to life</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadar</forename><surname>Averbuch-Elor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">196</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Mixtext: Linguistically-informed interpolation of hidden space for semi-supervised text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12239</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Retinaface: Single-shot multilevel face localisation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Ververas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5203" to="5212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exprgan: Facial expression editing with controllable expression intensity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename><surname>Sricharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improving data augmentation for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Eaton-Rosen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Bragman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M Jorge</forename><surname>Cardoso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Facial action coding system (facs). A human face</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Ekman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ad-corre: Adaptive correlation-based loss for facial expression recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Pourramezan Fard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="26756" to="26768" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Perspective-aware manipulation of portrait photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ohad</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Finkelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">128</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Augmenting data with mixup for sentence classification: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.08941</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cmc-cov19d: Contrastive mixup classification for covid-19 diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junlin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuejie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiya</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="454" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Emotion speech synthesis method based on multi-channel time-frequency domain generative adversarial networks (mc-tfd gans) and mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjun</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arabian Journal for Science and Engineering</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Abaw: Valence-arousal estimation, expression recognition, action unit detection &amp; multi-task learning challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.10659,2022.1</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mia-cov19d: Covid-19 detection through 3-d chest ct image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasios</forename><surname>Arsenos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levon</forename><surname>Soukissian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Kollias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="537" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep transparent prediction through latent representation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bouas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vlaxos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brillakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilianna</forename><surname>Seferis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levon</forename><surname>Kollia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Sukissian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wingate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kollias</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.07044</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Photorealistic facial synthesis in the dimensional affect space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</title>
		<meeting>the European Conference on Computer Vision (ECCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep neural network augmentation: Generating faces for affect analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Ververas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1455" to="1484" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Interweaving deep learning and semantic techniques for emotion analysis in human-machine interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Marandianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaryllis</forename><surname>Raouzaiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas-Georgios</forename><surname>Stafylopatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Workshop on Semantic and Social Media Adaptation and Personalization (SMAP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recognition of affect in the wild using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mihalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoying</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1972" to="1979" />
		</imprint>
	</monogr>
	<note>2017 IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Analysing affective behavior in the first abaw 2020 competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schulc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hajiyev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG)</title>
		<imprint>
			<biblScope unit="page" from="794" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Face behavior a la carte: Expressions, affect and action units in a single network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktoriia</forename><surname>Sharmanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11111</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Distribution matching for heterogeneous multitask learning: a large-scale face study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktoriia</forename><surname>Sharmanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03790</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On line emotion detection using retrainable deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Athanasios Tagaris, and Andreas Stafylopatis</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Computational Intelligence (SSCI)</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Andreas Stafylopatis, Stefanos Kollias, and Georgios Tagaris</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Athanasios</forename><surname>Tagaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complex &amp; Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="131" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Deep neural architectures for prediction in healthcare</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiotis</forename><surname>Tzirakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mihalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Athanasios</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoying</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Transparent adaptation in deep medical image diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vlaxos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilianna</forename><surname>Seferis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levon</forename><surname>Kollia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Sukissian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wingate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kollias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on the Foundations of Trustworthy AI Integrating Learning, Optimization and Reasoning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="251" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adaptation and contextualization of deep neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Athanasios</forename><surname>Tagaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Georgios Leontidis, Andreas Stafylopatis, and Stefanos Kollias</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Computational Intelligence (SSCI)</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Aff-wild2: Extending the aff-wild database for affect recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.07770</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A multicomponent cnn-rnn approach for dimensional emotion recognition in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.01452</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A multi-task learning &amp; generation framework: Valence-arousal, action units &amp; primary expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.07771</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Training deep neural networks with different datasets in-the-wild: The emotion recognition paradigm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04855</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Va-stargan: Continuous affect generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advanced Concepts for Intelligent Vision Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="227" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15792</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Analysing affective behavior in the second abaw2 competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3652" to="3660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exploiting multi-cnn features in cnn-rnn based dimensional emotion recognition on the omg in-the-wild dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stefanos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Augmenting generative adversarial networks for speech emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddique</forename><surname>Latif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Asim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajib</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Khalifa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Jurdak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bj?rn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schuller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08447</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Direct modelling of speech emotion from raw speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddique</forename><surname>Latif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajib</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Khalifa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Jurdak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Epps</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03833</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Deep architecture enhancing robustness to noise, adversarial attacks, and cross-corpus setting for speech emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddique</forename><surname>Latif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajib</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Khalifa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Jurdak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bj?rn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schuller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08453</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Reliable crowdsourcing and deep locality-preserving learning for unconstrained facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junping</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2852" to="2861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Affectnet: A database for facial expression, valence, and arousal computing in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behzad</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Ganimation: Anatomically-aware facial animation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Pumarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Agudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aleix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Sanfeliu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Exnet: An efficient approach for emotion recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Muhammad Naveed Riaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyi</forename><surname>Sohail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1087</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02394</idno>
		<title level="m">Mixup-transformer: dynamic data augmentation for nlp tasks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Assessment of parkinson&apos;s disease based on deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Athanasios</forename><surname>Tagaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stafylopatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Engineering Applications of Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="391" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Machine learning for neurodegenerative disorder diagnosis-survey of practices and launch of benchmark dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Athanasios</forename><surname>Tagaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stafylopatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Georgios Tagaris, and Stefanos Kollias</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">1850011</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Face2face: Real-time face capture and reenactment of rgb videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2387" to="2395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Region attention networks for pose and occlusion robust facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="4057" to="4069" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Sample mixed-based data augmentation for domestic audio tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kele</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifan</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaimin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuqiang</forename><surname>Kong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.03883</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Mixup-based acoustic scene classification using multi-channel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kele</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lilun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengxing</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuwen</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific Rim conference on multimedia</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="14" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Niro Siriwardena, and Stefanos Kollias. Machine learning for predictive modelling of ambulance calls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Wingate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">482</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Aff-wild: Valence and arousal &apos;in-the-wild&apos;challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mihalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Athanasios</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoying</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kotsia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1980" to="1987" />
		</imprint>
	</monogr>
	<note>2017 IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Photorealistic facial expression synthesis by the conditional difference adversarial autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertram</forename><forename type="middle">Emil</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="370" to="376" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
