<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sicilian Translator: A Recipe for Low-Resource NMT Arba Sicula</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-10-05">5 Oct 2021 September 27, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eryk</forename><surname>Wdowiak</surname></persName>
							<email>eryk@wdowiak.me</email>
						</author>
						<title level="a" type="main">Sicilian Translator: A Recipe for Low-Resource NMT Arba Sicula</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-10-05">5 Oct 2021 September 27, 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With 17,000 pairs of Sicilian-English translated sentences, Arba Sicula developed the first neural machine translator for the Sicilian language. Using small subword vocabularies, we trained small Transformer models with high dropout parameters and achieved BLEU scores in the upper 20s. Then we supplemented our dataset with backtranslation and multilingual translation and pushed our scores into the mid 30s. We also attribute our success to incorporating theoretical information in our dataset. Prior to training, we biased the subword vocabulary towards the desinences one finds in a textbook. And we included textbook exercises in our dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With patience and dedication to a clear long-term vision, you can create amazing things.</p><p>So we've been steadily assembling a corpus of parallel text to create a machine translator for the Sicilian language. It now translates simple sentences fairly well. With a little more work, we will soon have a good-quality translator.</p><p>Sicilian is a good case study for several reasons. First, the language has been continuously recorded since the Sicilian School of Poets joined the imperial court of Frederick II in the 13th century.</p><p>And in our times, Arba Sicula has spent the past 40 years translating Sicilian literature into English (among its numerous activities to promote the Sicilian language). In the course of their work with the many dialects of Sicilian, they also established a "Standard Sicilian," which is what has enabled us to create a high-quality corpus of Sicilian-English parallel text.</p><p>High-quality parallel text is the necessary ingredient in any neural machine translation project. And recent advances in the field have made it possible to develop neural machine translators with limited amounts of parallel text.</p><p>With just 16,945 translated sentence pairs containing 266,514 Sicilian words and 269,153 English words, our Tradutturi Sicilianu achieved a BLEU score of 25.1 on English-to-Sicilian translation and 29.1 on Sicilian-to-English.</p><p>That's a good result for a small amount of parallel text. And you can always add more parallel text. Augmenting our dataset with backtranslations and multilingual translation further increased our BLEU scores to 35.0 on English-to-Sicilian and to 36.8 on Sicilian-to-English.</p><p>The traditional recommendation for languages without any parallel text has been to create a rules-based translator, using a framework like Apertium. But rules are difficult to write and, after a certain point, writing more rules won't improve translation quality very much.</p><p>But you can always find ways to create more parallel text. With patience and dedication, you can assemble tens of thousands of sentence pairs. Then assemble thousands more to further improve translation quality.</p><p>So the next section describes our data sources. The section on subword splitting explains how we prepare sentences for translation. Then our "recipe" describes our method of training a translator on little parallel text. Finally, the section on multilingual translation explains how adding Italian-English translations to our dataset enables translation with Italian and further improves translation quality. And the last section concludes.</p><p>When we first set out to create a machine translator for the Sicilian language, we thought that the limited number of parallel texts, the diversity of the Sicilian language and the diverse ways that the Sicilian language has been written would make it impossible to use statistical methods to create a machine translator.</p><p>Just a few years ago, <ref type="bibr">Koehn and Knowles (2017)</ref> calculated learning curves for English-to-Spanish translation. At 377,000 words, the BLEU scores were 1.6 for neural machine translation, 16.4 for statistical machine translation and 21.8 for statistical with a big language model. Recent advances in the field of neural machine translation allowed us to obtain better scores with half the number of words. But first we had to collect some Sicilian text.</p><p>Repositories of open-source parallel text, like the OPUS project, do not have any Sicilian language resources. There are no government documents, Wikipedia articles or movie subtitles that we can use as a source of parallel text. But good resources can be found elsewhere.</p><p>To seed this project, Arthur Dieli kindly provided 34 translations of Giuseppe Pitr?'s Sicilian Folk Tales and lots of encouragement. And Arba Sicula, which has been translating Sicilian literature into English for over 40 years, contributed its bilingual journal of Sicilian history, language, literature, art, folklore and cuisine.</p><p>Just as importantly, Arba Sicula developed a standard form of the language, providing the consistency we need in a sea of orthographic and dialectical diversity.</p><p>Most of our data comes from Arba Sicula articles. Some parallel text comes from Dr. Dieli's translations of Pitr?'s Folk Tales. And some comes from translations of the homework exercises in the Mparamu lu sicilianu <ref type="bibr" target="#b2">(Cipolla, 2013)</ref> and Introduction to Sicilian Grammar <ref type="bibr" target="#b1">(Bonner, 2001)</ref> textbooks.</p><p>Although it only makes up a small portion of the dataset, adding the textbook examples yielded large improvements in translation quality on a test set drawn only from Arba Sicula articles. Just as a grammar book helps a human learn in a systematic way, it also seems to help a machine learn in a systematic way.</p><p>Another (ironic) source of parallel text is monolingual text. Efforts to create neural machine translators for other low-resource languages often involve the back-translation method developed by <ref type="bibr" target="#b9">Sennrich et al. (2015)</ref>, in which monolingual, target-side text is used to supplement the available parallel text.</p><p>We may make more use this method in the future. So far we have not used it much because assembling Sicilian monolingual text requires almost as much time as assembling parallel text. Nonetheless, we also have some leftover unmatched text, which we can use for back-translation.</p><p>For example, to develop our English-to-Sicilian model, we could automatically translate Sicilian text into English to create a "synthetic dataset" of real Sicilian sentences and synthetic English sentences. Then we would train a new English-to-Sicilian model on the combination of the parallel and synthetic data.</p><p>And in general, you can always find ways to assemble more parallel text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Subword Splitting</head><p>In a recent case study, <ref type="bibr" target="#b8">Sennrich and Zhang (2019)</ref> develop a set of best practices for low-resource neural machine translation and show that those best practices can achieve better translation quality than phrase-based statistical machine translation in a 100,000 word dataset derived from the 2014 German-English IWSLT.</p><p>In their best practices, they suggest using a smaller neural network with fewer layers, smaller batch sizes and a larger the dropout parameter. And their largest improvements in translation quality (as measured by BLEU score) came from the application of a byte-pair encoding that reduced the vocabulary from 14,000 words to 2000 words. The best neural model that they developed with that 100,000 word dataset scored 16.6 on German-to-English translation, while their phrase-based statistical model scored 15.9. For comparison, just two years earlier, with a 377,000 word English-to-Spanish dataset, <ref type="bibr">Koehn and Knowles (2017)</ref> only obtained a BLEU score of 1.6 with a neural model, but 16.4 with a phrase-based statistical model.</p><p>Although the languages are different, the comparison seems valid because the better results required far less parallel text and because both pairs of researchers used recurrent neural networks. The difference was the algorithm that <ref type="bibr" target="#b10">Sennrich et al. (2016)</ref> developed to replace the model's fixed vocabulary with a vocabulary of "subwords."</p><p>For example, the English present tense only has two formsspeak and speaks -while the Sicilian present tense has sixparru, parri, parra, parramu, parrati and parranu. But upon splitting them into subwords, parr+ matches speak+, while the Sicilian verb endings (+u, +i, +a, +amu, +ati and +anu) match the English pronouns.</p><p>So in theory, subword splitting should allow us represent many different word forms with a much smaller vocabulary and should allow the translator to learn rare words and unknown words. For example, even if "jo manciu" ("I eat") does not appear at all in the dataset, but forms like "jo parru" ("I speak") and "iddu mancia" ("he eats") do appear, then subword splitting should allow the translator to learn "jo manciu" ("I eat").</p><p>In practice, achieving that effect required us to bias the learned subword vocabulary towards the desinences one finds in a textbook. Specifically, we added a unique list of words from the Dieli Dictionary and the inflections of verbs, nouns and adjectives from Chi? d? Palora to the Sicilian data.</p><p>Because each word was only added once, none of them affected the distribution of whole words. But once the words were split, they greatly affected the distribution of subwords, filling it with stems and suffixes. So the subword vocabulary that the machine learns is similar to the theoretical stems and desinences of a textbook.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">A Recipe for Low-Resource NMT</head><p>Even though we only have a little parallel text, we can still develop a reasonably good neural machine translator. We just have to to train a smaller model for the smaller dataset.</p><p>Training a large model on a small dataset is comparable to estimating a regression model with a large number of parameters on a dataset with few observations: It leaves you with too few degrees of freedom. The model thus becomes over-fit and does not make good predictions.</p><p>Reducing the vocabulary with subword-splitting, training a smaller network and setting a high-dropout parameter all reduce over-fitting. And self-attentional neural networks also reduce over-fitting because (compared to recurrent and convolutional networks) they are less complex. They directly model the relationships between words in a pair of sentences.</p><p>This combination of splitting, dropout and self-attention achieved a BLEU score of 25.1 on English-to-Sicilian translation and 29.1 on Sicilian-to-English with only 16,945 lines of parallel training data containing 266,514 Sicilian words and 269,153 English words.</p><p>And because the networks were small, each model took just under six hours to train on CPU. The textbook exercises form a trilingual "bridge," * larger model the strategy proposed by <ref type="bibr" target="#b3">Fan et al. (2020)</ref>.</p><p>? many-to-many model Our success is an implementation of the best practices developed by <ref type="bibr" target="#b8">Sennrich and Zhang (2019)</ref> with the selfattentional Transformer model developed by <ref type="bibr">Vaswani et al. (2017)</ref>. For training, we used the Sockeye toolkit by <ref type="bibr" target="#b4">Hieber et al. (2017)</ref> running on a server with four 2.40 GHz virtual CPUs.</p><p>In their best practices for low-resource NMT, Sennrich and Zhang suggest the byte-pair encoding (i.e. subwordsplitting) developed by <ref type="bibr" target="#b10">(Sennrich et al., 2016)</ref>, a smaller neural network with fewer layers, smaller batch sizes and larger dropout parameters.</p><p>As discussed above, a subword-splitting that reduced the vocabulary to 2000 words yielded their largest improvements in translation quality. But their most successful training also occurred when they set high dropout parameters.</p><p>During training, dropout randomly shuts off a percentage of units (by setting it to zero), which effectively prevents the units from adapting to each other. Each unit therefore becomes more independent of the others because the model is trained as if it had a smaller number of units, thus reducing over-fitting <ref type="bibr" target="#b11">(Srivastava et al., 2014)</ref>.</p><p>Subword-splitting and high dropout parameters helped us achieve better than expected results with a small dataset, but it was the Transformer model that pushed our BLEU scores into the double digits.</p><p>Compared to recurrent neural networks, the self-attention layers in the Transformer model more easily learn the dependencies between words in a sequence because the self-attention layers are less complex.</p><p>Recurrent networks read words sequentially and employ a gating mechanism to identify relationships between separated words in a sequence. By contrast, self-attention examines the links between all the words in the paired sequences and directly models those relationships. It's a simpler approach.</p><p>Combining these three features -subword-splitting, dropout and self-attention -yields a trained model that makes relatively good predictions. And as we assemble more parallel text, translation quality will improve even more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Multilingual Translation</head><p>Our discussion so far has focused on a dataset of Sicilian-English parallel text. This section augments our dataset with parallel text in other languages to enable multilingual translation. It explains how we can train a single model to translate between multiple languages, including some for which there is little or no parallel text.</p><p>In our case, we can obtain Sicilian-English parallel text from the issues of Arba Sicula, but finding Sicilian-Italian parallel text is difficult.</p><p>Nonetheless, we trained a model to translate between Sicilian and Italian without any Sicilian-Italian parallel text at all (i.e. "zero shot" translation) by including Italian-English parallel text in our dataset. Then, to improve translation quality between Sicilian and Italian, we implemented a simple version of the "bridging strategy" proposed by <ref type="bibr" target="#b3">Fan et al. (2020)</ref> and added Sicilian-Italian-English homework exercises to our dataset.</p><p>To enable multilingual translation, we followed <ref type="bibr" target="#b5">Johnson et al. (2016)</ref> and placed a directional token -for example, &lt;2it&gt; ("to Italian") -at the beginning of each source sequence. The directional token enables multilingual translation in an otherwise conventional model.</p><p>It's an example of transfer learning. In our case, as the model learns to translate from Italian to English, it also learns to translate from Sicilian to English. And as the model learns to translate from English to Italian, it also learns how to translate from English to Sicilian.</p><p>More parallel text is available for some languages than others however, so Johnson et al. also studied the effect on translation quality and found that oversampling low-resource language pairs improves their translation quality, but at expense of quality among high-resource pairs. Importantly however, the comparison with bilingual translators holds constant the number of parameters in the model.  show that training a larger model can improve translation quality across the board.</p><p>Our experience was consistent with their findings. As shown in <ref type="table" target="#tab_1">Table 2</ref>, holding model size constant reduced translation quality when we added the Italian-English subset of Farkas' Books data (from the OPUS project) to our dataset. So to push our BLEU scores into the thirties, we trained a larger model.</p><p>More broadly, <ref type="bibr" target="#b3">Fan et al. (2020)</ref> developed the strategies to collect data for and to train a model that can directly translate between 100 languages. Previous efforts had resulted in poor translation quality in non-English directions because the data consisted entirely of translations to and from English.</p><p>To overcome the limitations of English-centric data, Fan et al. strategically selected pairs to mine data for, based on geography and linguistic similarity. Training a model on such a more multilingual dataset yielded very large improvements in translation quality in non-English directions, while matching translation quality in English directions.</p><p>Given such potential to expand the directions in which languages can be translated and to improve the quality with which they can be translated, an important question is what the model learns. Does it learn to represent similar sentences in similar ways regardless of language? Or does it represent similar languages in similar ways?</p><p>Johnson et al. examined two trained trilingual models. In one, they observed similiar representations of translated sentences, while in the second they noticed that the representations of zero-shot translations were very different. <ref type="bibr" target="#b7">Kudugunta et al. (2019)</ref> examined the question in a model trained on 103 languages and found that the representations depend on both the source and target languages and they found that the encoder learns a representation in which linguistically similar languages cluster together.</p><p>In other words, because similar languages learn similar representations, our model would learn Sicilian-English better from Italian-English data than from Polish-English data. And other Romance languages, like Spanish, would also be good languages to consider.</p><p>We can collect some of that parallel text from the resources at OPUS <ref type="bibr" target="#b12">(Tiedemann, 2012)</ref>, an open repository of parallel corpora. Because it contains so many language resources, <ref type="bibr" target="#b14">Zhang et al. (2020)</ref> recently used it to develop the OPUS-100 corpus, an open-source collection of English-centric parallel text for 100 languages.</p><p>Because it's a "rough and ready" massively multilingual dataset, it highlights some of the challenges facing massively multilingual translation. In particular, <ref type="bibr">Zhang et al.</ref> show that a model trained with a vanilla setup exhibits off-target translation issues in zero-shot directions. In the English-centric case, that means the model often translates into the wrong language when not translating to or from English.</p><p>Zhang et al. tackle this challenge by simulating the missing translation directions. They first observe that <ref type="bibr">Sennrich et al.'s (2015)</ref> method of back-translation "converts the zero-shot problem into a zero-resource problem" because it creates synthetic source language text. They then observe that this synthetic source language text simulates the missing translation directions.</p><p>The only obstacle is scalability. In a massively multilingual context, there are thousands of translation directions, which requires prohibitively many back-translations. To overcome this obstacle, Zhang et al. incorporate backtranslation directly into the training process. And their final models exhibit improved translation quality and fewer off-target translation errors.</p><p>So we're excited about the potential for multilingual translation to improve translation quality and to create new translation directions for the Sicilian language. Using the Italian-English subset of Farkas' Books from OPUS, multilingual translation greatly improved the quality of translation between Sicilian and English (as shown in <ref type="table" target="#tab_1">Table 2</ref>).</p><p>And after first enabling zero-shot translation between Sicilian and Italian with the technique proposed by Johnson et al., we further improved translation quality between Sicilian and Italian with Fan et al.'s "bridging strategy."</p><p>In our case with only three languages, we bridged Sicilian, English and Italian by translating 4,660 homework exercises from the Mparamu lu sicilianu <ref type="bibr" target="#b2">(Cipolla, 2013)</ref> and Introduction to Sicilian Grammar <ref type="bibr" target="#b1">(Bonner, 2001)</ref> textbooks. As shown in <ref type="table" target="#tab_1">Table 2</ref>, this technique yielded translation quality between Sicilian and Italian that's almost as good as translation quality between Sicilian and English, for which we have far more parallel text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Our recipe for low-resource neural machine translation -subword-splitting, dropout and self-attention -yields a trained model that makes relatively good predictions. Adding multilingual translation improves translation quality even more. And we improved upon our zero-shot result by bridging the three languages with textbook exercises.</p><p>So come to Napizia where we're developing Sicilian language resources and try our Tradutturi Sicilianu.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : Model Sizes</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>defaults</cell><cell>our models</cell><cell>larger</cell><cell>many-to-many</cell></row><row><cell>layers</cell><cell>6</cell><cell>3</cell><cell>4</cell><cell>4</cell></row><row><cell>embedding size</cell><cell>512</cell><cell>256</cell><cell>384</cell><cell>512</cell></row><row><cell>model size</cell><cell>512</cell><cell>256</cell><cell>384</cell><cell>512</cell></row><row><cell>attention heads</cell><cell>8</cell><cell>4</cell><cell>6</cell><cell>8</cell></row><row><cell>feed forward</cell><cell>2048</cell><cell>1024</cell><cell>1536</cell><cell>2048</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="3">: Datasets and Results</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">word count (in tokens)</cell><cell cols="2">BLEU score</cell></row><row><cell>dataset</cell><cell>subwords</cell><cell>lines</cell><cell cols="3">Sicilian English Italian</cell><cell cols="2">En-Sc Sc-En</cell></row><row><cell>20</cell><cell>2,000</cell><cell>7,721</cell><cell cols="2">121,136 121,892</cell><cell>-</cell><cell>11.4</cell><cell>12.9</cell></row><row><cell>21</cell><cell>2,000</cell><cell>8,660</cell><cell cols="2">146.370 146,437</cell><cell>-</cell><cell>12.9</cell><cell>13.3</cell></row><row><cell>23</cell><cell>3,000</cell><cell>12,095</cell><cell cols="2">171,278 175,174</cell><cell>-</cell><cell>19.6</cell><cell>19.5</cell></row><row><cell>24</cell><cell>3,000</cell><cell>13,060</cell><cell cols="2">178,714 183,736</cell><cell>-</cell><cell>19.6</cell><cell>21.5</cell></row><row><cell>25</cell><cell>3,000</cell><cell>13,392</cell><cell cols="2">185,540 190,538</cell><cell>-</cell><cell>21.1</cell><cell>21.2</cell></row><row><cell>27</cell><cell>3,000</cell><cell>13,839</cell><cell cols="2">190,072 195,372</cell><cell>-</cell><cell>22.4</cell><cell>24.1</cell></row><row><cell>28</cell><cell>3,000</cell><cell>14,494</cell><cell cols="2">196,911 202,652</cell><cell>-</cell><cell>22.5</cell><cell>25.2</cell></row><row><cell>29</cell><cell>3,000</cell><cell>16,591</cell><cell cols="2">258,730 261,474</cell><cell>-</cell><cell>24.6</cell><cell>27.0</cell></row><row><cell>30</cell><cell>3,000</cell><cell>16,945</cell><cell cols="2">266,514 269,153</cell><cell>-</cell><cell>25.1</cell><cell>29.1</cell></row><row><cell>30</cell><cell>5,000</cell><cell>16,829</cell><cell cols="2">261,421 264,242</cell><cell>-</cell><cell>27.7</cell><cell>-</cell></row><row><cell>+back</cell><cell></cell><cell>+3,251</cell><cell>+92,141</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell>30</cell><cell>Sc: 5,000</cell><cell>16,891</cell><cell cols="2">262,582 266,740</cell><cell>-</cell><cell>19.7</cell><cell>26.2</cell></row><row><cell>Books</cell><cell>En: 7,500</cell><cell>32,804</cell><cell>-</cell><cell cols="2">929,043 838,152</cell><cell cols="2">35.1* 34.6*</cell></row><row><cell>+back</cell><cell>It: 5,000</cell><cell>+3,250</cell><cell>+92,146</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell>33</cell><cell></cell><cell>12,357</cell><cell cols="2">237,456 236,568</cell><cell>-</cell><cell></cell></row><row><cell>Books</cell><cell></cell><cell>28,982</cell><cell>-</cell><cell cols="2">836,757 755,196</cell><cell cols="2">35.0* 36.8*</cell></row><row><cell cols="2">+back (En/It)-Sc Sc: 5,000</cell><cell>+3,250</cell><cell>+92,146</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell>+back Sc-It</cell><cell>En: 7,500</cell><cell>+3,250</cell><cell>-</cell><cell>-</cell><cell>+84,657</cell><cell></cell></row><row><cell></cell><cell>It: 5,000</cell><cell>4,660</cell><cell>30,244</cell><cell>35,173</cell><cell>-</cell><cell cols="2">It-Sc Sc-It</cell></row><row><cell>textbook</cell><cell></cell><cell>4,660</cell><cell>30,244</cell><cell>-</cell><cell>29,855</cell><cell cols="2">36.5 ? 30.9 ?</cell></row><row><cell></cell><cell></cell><cell>4,660</cell><cell>-</cell><cell>35,173</cell><cell>29,855</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Arba Sicula, Gaetano Cipolla and Arthur Dieli developed the resources that made this project possible. I would like to thank them for their support and encouragement.</p><p>Prof. Cipolla helped me learn Sicilian and he also helped me develop this recipe for low-resource neural machine translation. We thought about the problem together. He encouraged me to incorporate theoretical information into the model and that's why we got good results.</p><p>Dr. Dieli seeded this project with his vocabulary list and translations of Pitr?'s Folk Tales. He helped me get started. And he and his family gave me a lot of support and encouragement. This project is dedicated to his memory.</p><p>Finally, I would like to thank Arba Sicula for the language resources that we used to develop the dictionary and translator. And I would like to thank the organization and its members for their sponsorship and development of Sicilian language and culture. Their poetry made this project beautiful.</p><p>Grazzi!</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">F</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1907.05019" />
		<title level="m">Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Introduction to Sicilian Grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Bonner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Legas</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learn Sicilian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<pubPlace>Mineola, NY</pubPlace>
		</imprint>
	</monogr>
	<note>Mparamu lu sicilianu. Legas</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Beyond English-Centric Multilingual Machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El-Kishky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Armand</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2010.11125" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Domhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sokolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Post</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1712.05690" />
		<title level="m">Sockeye: A Toolkit for Neural Machine Translation. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Google&apos;s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Vi?gas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1611.04558" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Six Challenges for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Knowles</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1706.03872" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Kudugunta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Caswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1909.02197" />
		<title level="m">Investigating Multilingual NMT Representations at Scale</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Revisiting Low-Resource Neural Machine Translation: A Case Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1905.11901" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improving Neural Machine Translation Models with Monolingual Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.06709" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1508.07909" />
		<title level="m">Neural Machine Translation of Rare Words with Subword Units</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v15/srivastava14a.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">56</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Parallel Data, Tools and Interfaces in OPUS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tiedemann</surname></persName>
		</author>
		<ptr target="https://opus.nlpl.eu/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Language Resources and Evaluation</title>
		<meeting>the 8th International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1706.03762" />
		<title level="m">Attention Is All You Need</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2004.11867" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
