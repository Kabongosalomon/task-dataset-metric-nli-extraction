<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CICERO: A Dataset for Contextualized Commonsense Inference in Dialogues</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghosal</forename><surname>Deepanway</surname></persName>
							<email>deepanwayghosal@mymail.</email>
							<affiliation key="aff0">
								<orgName type="department">DeCLaRe DeCLaRe Lab</orgName>
								<orgName type="institution" key="instit1">Singapore University of Technology and Design</orgName>
								<orgName type="institution" key="instit2">Singapore University of Michigan</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Declare</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeCLaRe DeCLaRe Lab</orgName>
								<orgName type="institution" key="instit1">Singapore University of Technology and Design</orgName>
								<orgName type="institution" key="instit2">Singapore University of Michigan</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Shen</surname></persName>
							<email>shensq@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="department">DeCLaRe DeCLaRe Lab</orgName>
								<orgName type="institution" key="instit1">Singapore University of Technology and Design</orgName>
								<orgName type="institution" key="instit2">Singapore University of Michigan</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Declare</forename><surname>Majumder</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeCLaRe DeCLaRe Lab</orgName>
								<orgName type="institution" key="instit1">Singapore University of Technology and Design</orgName>
								<orgName type="institution" key="instit2">Singapore University of Michigan</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihalcea</forename><surname>Rada</surname></persName>
							<email>mihalcea@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="department">DeCLaRe DeCLaRe Lab</orgName>
								<orgName type="institution" key="instit1">Singapore University of Technology and Design</orgName>
								<orgName type="institution" key="instit2">Singapore University of Michigan</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Poria</forename><surname>Soujanya</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeCLaRe DeCLaRe Lab</orgName>
								<orgName type="institution" key="instit1">Singapore University of Technology and Design</orgName>
								<orgName type="institution" key="instit2">Singapore University of Michigan</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Declare</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeCLaRe DeCLaRe Lab</orgName>
								<orgName type="institution" key="instit1">Singapore University of Technology and Design</orgName>
								<orgName type="institution" key="instit2">Singapore University of Michigan</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CICERO: A Dataset for Contextualized Commonsense Inference in Dialogues</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>CICERO is available at: https://declare-lab.github.io/CICERO</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses the problem of dialogue reasoning with contextualized commonsense inference. We curate CICERO, a dataset of dyadic conversations with five types of utterance-level reasoning-based inferences: cause, subsequent event, prerequisite, motivation, and emotional reaction. The dataset contains 53,105 of such inferences from 5,672 dialogues. We use this dataset to solve relevant generative and discriminative tasks: generation of cause and subsequent event; generation of prerequisite, motivation, and listener's emotional reaction; and selection of plausible alternatives. Our results ascertain the value of such dialogue-centric commonsense knowledge datasets. It is our hope that CI-CERO will open new research avenues into commonsense-based dialogue reasoning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Conversational content on the internet is quickly growing, and such content holds valuable knowledge about how information exchange takes place among speakers. A key step towards understanding such dialogues is gaining the ability to reason with the information shared in the dialogue. To this end, we curate a dataset of dyadic conversations named CICERO (ContextualIzed CommonsEnse InfeRence in dialOgues) , which contains inferences around the utterances in the dialogues. The dataset focuses on five types of reasoning-based inferences for a given utterance in a dialogue: cause, subsequent event, prerequisite, motivation, and emotional reaction.</p><p>Arguably, making such reasoning-based inferences often demands commonsense knowledge, especially when the inference is implicit. <ref type="figure">Fig. 1a</ref> shows such a case where the cause behind the target utterance is not explicit in the context. However, applying the commonsense knowledge worn gloves motivates ? ?????? ? buy new pair of gloves allowed the annotator to infer a probable cause of the utterance. On the other hand, commonsense can be crucial in sifting relevant information from the context. <ref type="figure">Fig. 1b</ref> depicts an instance where the cause behind the target utterance is inferred from the context. This inference can be explained by commonsense knowledge (see <ref type="figure" target="#fig_1">Fig. 3</ref> Thus, it is reasonable to posit that such knowledge could aid to bridge the gap between the input and the target inference.</p><p>ATOMIC <ref type="bibr">(Sap et al., 2019;</ref><ref type="bibr">Hwang et al., 2020</ref>) is one such dataset for commonsense reasoningbased inference, allowing for a large set of inference types. However, ATOMIC is context-free, as it only provides inferences on short phrases, ignoring the broader context around them. Making an inference on an entire utterance, on the other hand, requires understanding the context around it. As per Grice's maxim <ref type="bibr">(Grice, 1975)</ref>, in conversations, the interlocutors provide any piece of information as is needed, and no more. Thus, much of the information required to understand an utterance is likely interspersed along the dialogue, and not necessarily localized in the given utterance. For instance, in the example in <ref type="figure">Figure 1b</ref>, understanding the cause for one of the speakers' desire to go to McDonald's requires the context of the previous utterances. ATOMIC is thus not ideal for commonsense reasoning-based inferences on dialogues, where context is critical for understanding an utterance's implications. We confirm this with our experiments in the subsequent sections ( ?4). <ref type="bibr">GLUCOSE (Mostafazadeh et al., 2020)</ref> exclu-sively curates causal inferences -cause, enable, and result in -from monologues. Thus, it is not ideal for making context-consonant inferences on the dialogues. Also, dialogue-specific dimensions like motivation and reaction are beyond its scope. On the other hand, CIDER <ref type="bibr">(Ghosal et al., 2021a)</ref> does provide a dataset for commonsensebased inference on dialogues, but it is limited to inferences explicitly observable in the dialogues. As such, systems based on CIDER cannot effectively speculate around the dialogue for implicit inference.</p><p>CICERO strives to bring the best of these three datasets by creating a dataset that can enable models to effectively operate on a dialogue by considering the context and speculating when the answer is not apparent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Construction of CICERO</head><p>We create CICERO -a large dataset of English dyadic conversations annotated with five types of inferences with the help of human annotators, who are instructed with a carefully crafted set of guidelines. <ref type="figure">Figure 2</ref>: A dialogue-target pair. The utterances with red border is the target for this dialogue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Annotation Instructions</head><p>The annotators are given a dialogue and a target utterance, as exemplified in <ref type="figure">Fig. 2</ref>. The annotators are then asked to make an inference, posed as a question, about the target utterance. They write a one-sentence answer that is grammatically correct, concise, and consistent with the dialogue. The answer may contain both overt and speculative scenarios. An overt scenario is explicitly or implicitly present in the dialogue context. If such contextual scenarios answer the question, the annotators write them as a well-formed sentence. However, in many cases, the dialogue may not hold the answer, neither explicitly nor implicitly. In such cases, the annotators are asked to speculate plausible scenarios around the dialogue, using commonsense and world knowledge, to devise answers that do not contradict the given dialogue context.</p><p>Given the dialogue-target pair in <ref type="figure">Fig. 2</ref>, at least one of the following five inferences about the target is made by the annotators:</p><p>Q1. What is the event that directly causes (overt) or could cause (speculative) Target?</p><p>The annotators consider if any of the events that are or likely to be antecedent to the target can cause the target. Answer: Linda didn't exercise regularly during the winter. Remark: The annotators provided possible, speculative answers as the dialogue itself does not provide any reason for Linda's weight gain.</p><p>Q2. What subsequent event happens (overt) or could happen (speculative) following the Target? The annotators write about the event that happens or could happen following the target. Additionally, annotators were told that sometimes, such subsequent events of the target are triggered or likely to be triggered by the target. Answer: Linda starts a diet and tries to lose weight.</p><p>Remark: The answer is speculative as the dialogue contains no explicit/implicit subsequent event.</p><p>Q3. What is (overt) or could be (speculative) the prerequisite of Target? Does the target have any direct prerequisite or dependency that has to happen or be fulfilled first? (In most cases, prerequisite is the state/event which has to be satisfied before another event causes target.) The answer is a state/event which enables the happening of the target. In other words, prerequisites are the prior assumptions or background information that the interlocutors agree on about the context. Answer: Linda was slimmer before the winter. Remark: Annotators were required to understand the difference between cause and prerequisite clearly before proceeding with the final annotation. Cause of an event X is the event that directly causes X. Prerequisite of an event X is the condition which has to be satisfied in order for X to happen.</p><p>Q4. What is an emotion or basic human drive that motivates or could motivate Target? Consider the basic human drives, needs (and/or likely emotions) of the speaker of the target. Basic human drives and needs are food, water, clothing, warmth, rest, security, safety, intimate relationships, friends, prestige, feeling of accomplishment, self-fulfillment, creative activities, enjoyment, etc. Do any of these human drives/states of mind/emotional feelings motivate the target? Answer: Not Applicable for this target. Q5. What is the possible emotional reaction of the listener: A (or B)? What could be the possible emotional reaction or responses of the listener with respect to the target? The annotators capture the appropriate emotion of the listener using the emotion terms listed in <ref type="table">Table 1</ref> verbatim or related words <ref type="bibr">(e.g., anxious, confused, interested, etc)</ref>. Answer: The listener encourages Linda to maintain her diet.</p><p>Additional Guidelines. To ensure the quality and diversity of the samples, we also ask the annotators to adhere to the following guidelines:</p><p>? Be creative in speculation.</p><p>Refrain from rephrasing the target and writing low-effort trivial answers. It is recommended to skip a question if rephrasing the target is the only possible answer.</p><p>? Avoid repeating the same answer for distinct questions on the same target. ? The answer must be consistent with the given dialogue. ? It is recommended to base the answer on the most important phrase of the target should it contain multiple phrases.  DREAM (Sun et al., 2019) is a multiple-choice reading-comprehension dataset collected from exams of English as a foreign language. The dataset presents significant challenges as many answers are non-extractive and require commonsense knowledge and multi-sentence reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Selection Process</head><p>We use the following procedure to select a subset of dialogues from the three datasets:</p><p>1. We remove dialogues that are too short or long on either utterance or word level. Dialogues with fewer than five utterances or fewer than six words per utterance on average are removed. Dialogues having more than 15 utterances or more than 275 words in total are also removed.</p><p>2. All three source datasets contain dialogues having near identical utterances. We remove these near duplicate dialogues to ensure topical diversity of CICERO. We use a sentence embedding model based on fine-tuned RoBERTa <ref type="bibr">(Gao et al., 2021)</ref> to extract dense feature vectors of the dialogues. We remove the duplicates assuming that a pair of duplicate dialogues have at least 0.87 cosine similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Target Utterance Selection</head><p>Given a dialogue D, we select the target utterances as follows:</p><p>? We first determine the number of target utterances in D: if D has 1-6 utterances, then we select 2 or 3 targets; if it has 7-12 utterances then we select 3-5 targets; otherwise, we select 4-7 targets if it has more than 12 utterances.</p><p>? We divide D into 2-3 segments having roughly equal number of consecutive utterances. We choose roughly an equal number of the topranking utterances from each segment. We call this set of utterances x 1 . The ranking is performed using a sentence ranking algorithm <ref type="bibr">(Erkan and Radev, 2004;</ref><ref type="bibr">Mihalcea and Tarau, 2004)</ref> with sentence-BERT embeddings (Reimers and Gurevych, 2019a).</p><p>? We also select the longest utterances in D and the utterances that contain phrases such as I'm, I'd, I've, I'll or their expansions. We call this set of utterances x 2 . The sets x 1 and x 2 may not be disjoint. ? Set x 3 consisting of the final utterance of D.</p><p>We choose the inference type for the target utterances from the sets x 1,2,3 as follows:</p><p>? From x 1 ? x 2 :</p><p>-Subsequent Event: 80% of the targets.</p><p>-Both Cause and Prerequisite: 60% of the targets. -Exclusively Cause: 28% of the targets.</p><p>-Exclusively Prerequisite: 12% of the targets.</p><p>? From x 2 : Motivation for all targets.</p><p>? From x 3 : Reaction of listener for all targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Quality Assurance of CICERO</head><p>Dataset quality is ensured with the following steps:</p><p>? Initially, we sample 50 random dialogues and manually annotate all the questions (as in ?2.1) in those. Each annotator is then evaluated on those dialogues, and is selected for the annotation task if 95% of his/her annotations are approved by us.</p><p>? We constantly review and provide feedback to the annotators during the annotation process. Annotators are also instructed to amend their answers.</p><p>? Upon completion of the annotation, we employ three additional annotators who manually check the annotated samples and score their acceptability. These annotators reached a consensus for approving 86% of these samples. The samples not bearing majority agreement were removed from the dataset. The statistics of the annotated dataset is shown in <ref type="table" target="#tab_5">Table 3</ref>. A number of annotated examples from CICERO are also shown in <ref type="table" target="#tab_3">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Features of CICERO</head><p>Following <ref type="table" target="#tab_5">Table 3</ref>, a majority (? 59%) of the inferences in CICERO are causal in nature. Again, roughly 80% of the inferences are speculative and context consonant. CICERO is thus much more versatile in terms of its applications as compared to <ref type="bibr">CIDER (Ghosal et al., 2021a</ref>) that only contains explicit contextual inferences. CICERO also contains varied commonsense knowledgefrom general to physical and social commonsense (see Appendix B for more details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Commonsense Inference on CICERO</head><p>We design generative and multi-choice question answering tasks on CICERO to evaluate dialogue-level commonsense-based reasoning capabilities of language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task 1: CICERO NLG</head><p>The objective is to generate the answer to question q, representing one of the five inference types, for a target utterance u t in a dialogue D. Each inference type has its respective q (illustrated in ?4).</p><p>Task 1.1: Dialogue Causal Inference. Causality pertains to causes and effects of events and situations. We formulate the dialogue causal inference task as generating the cause or subsequent event of an utterance as an answer to a causal question: 1. Cause: Given D, u t , generate the cause c t of u t .</p><p>2. Subsequent Event: Given D, u t , generate the subsequent event e t of u t .</p><p>3. Subsequent Event Clipped (Subsequent EC): Given u t , the dialogue up to u t : D :u t , generate the subsequent event e t of u t .</p><p>We consider two different scenarios for subsequent event, as the event often appear after the target utterance in the dialogue. Hence, subtask 3 is No, I don't think so. My parents have offered to take care of him, and I don't think he'd be happy in the city. A (u 9 ) (u 9 ) (u 9 ): You're probably right. But aren't you afraid of moving to such a big place, especially after living in a small village? B (u 10 ) (u 10 ) (u 10 ): Not really. I think I'll enjoy myself. There's so much to do there; I expect I won't miss the countryside much and I can always come back and visit. A (u 11 ) (u 11 ) (u 11 ): Well, I just hope you'll invite me to stay when you get settled. B (u 12 ) (u 12 ) (u 12 ): Of course I will.</p><formula xml:id="formula_0">A (u 1 ) (u 1 ) (u 1 ): Hi,</formula><p>Targetu 6 u 6 u 6 ; Inference: Cause; Annotation: Being an expensive city, it is quite difficult to find an affordable place to live in London.</p><p>Targetu 10 u 10 u 10 ; Inference: Cause; Annotation: Jinny realizes that a city like London will provide a great quality of life for her.</p><p>Targetu 6 u 6 u 6 ; Inference: Subsequent Event; Annotation: The listener gives an idea to Jenny to find the flat on some online portal for searching flatmates as well plenty of cheaper options.</p><p>Targetu 10 u 10 u 10 ; Inference: Subsequent Event; Annotation: Jenny inquired a social club in London and ask for their membership to utilize her free time.</p><p>Targetu 4 u 4 u 4 ; Inference: Prerequisite; Annotation: Jenny has completed her studies.</p><p>Targetu 12 u 12 u 12 ; Inference: Prerequisite; Annotation: Jenny and the listener are good friends.</p><p>Targetu 6 u 6 u 6 ; Inference: Motivation; Annotation: Jenny is optimistic about having someone as her flatmate to save rent.</p><p>Targetu 12 u 12 u 12 ; Inference: Reaction; Annotation: The listener is happy for Jenny and looks forward to being invited to London by Jenny. more challenging to evaluate a model's ability to reason about unobserved effects. We extend subtasks 1, 2 to incorporate longer chains and formulate the chained generation task. We consider utterances u t in our dataset that has both cause and subsequent event annotated i.e. c t ? u t ? e t . The causal chain is considered as a triplet, and we formulate tasks where a missing segment has to be generated from the rest of the components: 4. Chained Cause: Generate c t from u t and e t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Chained Subsequent Event (Chained SE):</head><p>Generate e t from u t and c t .</p><p>Task 1.2: Prerequisite, Motivation and Reaction Generation. The objective is to generate the prerequisite/motivation/reaction of listener from a given D and u t . The target u t is the final utterance of D for reaction generation. Generating the prerequisite (task 1.2.1) requires an understanding of the dependency of events. Gen-erating the motivation (task 1.2.2) and reaction (task 1.2.3) is about learning basic human drives and emotions. Note that, reaction generation is a different problem from dialogue response generation. Responses follow utterance level distributions which are substantially different from emotional reactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Task 2: CICERO MCQ</head><p>Given dialogue D, target u t , one of the five questions (inference type) q, true answer a t , alternate choices F t = { f t1 , f t2 , f t3 , f t4 }, the CICERO MCQ task aims to select the correct answer a t (see <ref type="figure">Fig. 4</ref>) and additionally any answer among F t which might be correct. The alternate choices F t are created through a combination of automated generation and human supervision as follows:</p><p>? We train a T5 large model on SNLI contradictory pairs <ref type="bibr">(Bowman et al., 2015)</ref>   ate contradictions/counterfactuals from input sentences. We use this model to generate a pool of alternate answers from the true annotated answers. Alternate answers which have an embedding cosine similarity less than 0.9 with the true answer (from all-mpnet-base-v2 in Reimers and Gurevych (2019b)) and are contradictory w.r.t the true answer (from roberta-large-mnli) are kept, and the rest are discarded. The filtered set is termed N.</p><p>? We use the adversarial filtering (AF) algorithm <ref type="bibr">(Zellers et al., 2018)</ref> to select the four alternate answers F t from N. For multi-choice QA tasks, AF is an effective method to detect easily identifiable alternate answers and replace them with more difficult candidates by detecting and reducing stylistic artifacts. The algorithm is as follows:</p><p>(i) We start with annotated true answer a t and any four choicesF t from N for all instances in our dataset to createD. We randomly splitD int? D train (80%) andD test (20%) according to dialogue IDs.</p><p>(ii) A multi-choice QA model (discriminator) is trained onD train that scores all five choices for all instances inD test . The highest scoring choice is considered as the predicted answer. For a particular test instance, choices inF t that have lower scores than a t are replaced with other high scoring choices in N ?F t . Answers inF t which are being replaced are removed from N.</p><p>(iii)F t now consists of relatively more difficult choices. A new random splitD train andD test is created, and we go back to step (ii). The algorithm is terminated when the accuracy in successiveD test reaches a convergence. The final alternate choice set is termed as F t .</p><p>The AF algorithm ensures a robust final dataset D irrespective of the final train, validation, and test split. We use a new roberta-large model to initialize the discriminator and train for 3 epochs before scoring and replacement in step (ii). 14 iterations were required for convergence in D test .</p><p>? Annotators perform manual checking on the final AF selected choices F t . They mark each of the alternate choices in F t in D to be speculatively correct or incorrect given the context. Hence, instances might have correct answers in F t in addition to the originally annotated correct answer a t . The final dataset statistics after this step are given in <ref type="table" target="#tab_5">Table 3</ref>.</p><p>Task 2.1: Single Answer Selection. Consider instances where F t doesn't contain any correct answer. The task is to select the correct answer a t among the five choices given D, u t , and q.</p><p>Task 2.2: All Answers Selection. This task is performed on the entire dataset (including the subset of data which is used in Task 2.1. There might be one or more correct answers for a particular instance resulting from the AF algorithm. The task is to select all the correct answer(s) (including a t ) among the five choices given D, u t , and q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CICERO Tasks: Experimental Results</head><p>We split our dataset in dialogue level where the training, validation and test instances are obtained from a total of 3477, 1097, 1098 distinct dialogues respectively. This results in a 60:20:20 proportion of total annotation instances. The three sets have 17365, 5370, and 5331 unique target utterances respectively. We tune on the validation dataset and report results on the test dataset (average of 5 runs). For the sake of brevity, the detailed hyperparameters are given in the supplementary material. We use the following questions (q) for the five inference types for all the tasks: Cause: What is or could be the cause of target? Subsequent Event: What subsequent event happens or could happen following the target? Prerequisite: What is or could be the prerequisite of target? Motivation: Question: What subsequent event happens or could happen following the Target?</p><p>The salesman packed five California oranges.</p><p>The salesman packed two california oranges.</p><p>The salesman packed five california limes.</p><p>The salesman packed one california orange.</p><p>His friend packed five california oranges.</p><formula xml:id="formula_1">(five two) (orange lime) (five one) (salesman friend)</formula><p>Target: Then give me five California oranges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4:</head><p>A data sample of CICERO for the Plausible Alternative Selection task. Here, commonsense is required to infer -a salesman packs the items that buyers want to purchase. In this particular dialogue, the buyer wants to purchase five California oranges and four bananas which can be inferred from the context.</p><p>What is or could be the motivation of target? Reaction: What is the possible emotional reaction of the listener in response to target?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baseline Models</head><p>CICERO NLG -(1.1-1.2). We use large versions of T5 (Raffel et al., 2020) and GLUCOSE-T5 (Mostafazadeh et al., 2020) as our models. GLUCOSE-T5 is a T5 large model that is pre-trained on the GLUCOSE dataset. We concatenate q, u t , and the context c with separators to form the input to the model: q &lt;sep&gt; u t &lt;sep&gt; c. The context c is formed by concatenating utterances of D :u t (subsequent event clipped) or D (all other tasks). For the chained generation task, we additionally provide the cause/subsequent event as input. The inputs are q &lt;sep&gt; u t &lt;sep&gt; subsequent event: e t &lt;sep&gt; c and q &lt;sep&gt; u t &lt;sep&gt; cause: c t &lt;sep&gt; c for cause and subsequent event generation, respectively. The objective is to generate the answer as output in the sequence-to-sequence setup. We use teacher forcing during training and beam search during inference.</p><p>CICERO MCQ -Single Answer Selection (2.1).</p><p>We use RoBERTa-large, ELECTRA-large, T5-large, and Unified QA Large for this task. The input to the models for RoBERTa-large, ELECTRA-large is the concatenation of question q, target u t , dialogue D, and candidate answers x j , j ? {1, ..., 5}: &lt;cls&gt; q &lt;sep&gt; u t &lt;sep&gt; D &lt;sep&gt; x j . Each score is predicted from the corresponding &lt;cls&gt; vector and the highest scoring one is selected as the answer. For seq2seq models T5-large, and Unified QA Large, we use the following inputq &lt;sep&gt; 1) x 1 2)</p><formula xml:id="formula_2">x 2 3) x 3 4) x 4 5) x 5 &lt;sep&gt; u t &lt;sep&gt; D.</formula><p>The output to be generated is the correct answersuch as x 1 or x 2 .</p><p>CICERO MCQ -All Answers Selection (2.2). We use seq2seq models T5-large, and Unified QA Large as they can generate both single and multiple-answers (with separator tokens) as output. The input is q &lt;sep&gt; 1) x 1 2) x 2 3) x 3 4) x 4 5) x 5 &lt;sep&gt; u t &lt;sep&gt; D. The output to be generated are the correct answer(s), such as x 2 (single answer) or x 1 &lt;sep&gt; x 3 &lt;sep&gt; x 4 (multiple answers). Here, x 1 ? x 5 denotes the five possible choices shuffled randomly.   <ref type="bibr">, 2015)</ref>, and Sem-Sim which computes the semantic cosine similarity of two sentences using the supervised RoBERTa-large sentence embedding model <ref type="bibr">(Gao et al., 2021)</ref>. All scores are reported in the range of 0-1.</p><p>Human Evaluation Metrics. Due to significant dissonance with human evaluation, automatic evaluation metrics are often considered not reliable for generation quality evaluation in literature. Hence, we resort to human evaluation met-rics. The human annotators rate on an integer scale from 1 (worst) to 5 (best) on three coarse attributes: Creativity: As the majority of the inferences require speculation, this metric measures how creative the models and the annotators are. Contextuality: Whether the generated or annotated inferences fit the context. Fluency: Whether the generated or annotated inferences are grammatically correct.</p><p>Results of Automatic Evaluation. The results for the generative tasks are reported in <ref type="table" target="#tab_8">Table 4</ref> and <ref type="table" target="#tab_10">Table 5</ref>. We observe that the fine-tuned models perform quite similarly across various metrics in  challenging to infer than the Motivation, and Reaction. However, the models are posed to the most challenging instances in the case of Prerequisite type as inferring this type requires rich commonsense and background knowledge. Hence, for this category, the models achieve a low score compared to rest of the inference categories. We also notice that exposing the future utterances to the models help in attaining better inference performance for the relation type Subsequent Event. The trained models perform worse when the future utterances are not available in the input as seen in the Subsequent Event Clipped task. A significant drop of performance is noticed in the CIDEr metric. For the chained generation tasks (1.1.4 and 1.1.5), we notice (refer to <ref type="table" target="#tab_10">Table 5</ref>) a very similar trend in models' performance i.e., the models tend to perform better for these two experimental settings compared to only Cause (1.1.1) and Subsequent Event (1.1.2) predictions. We can surmise that the additional cues from the available annotations of Subsequent Event type in the Chained Cause setting, and the Cause type in the Chained Subsequent Event setting are the key to such performance improvement. As depicted in <ref type="table" target="#tab_8">Table 4</ref> (and also  ferences generated by each model and their corresponding gold inferences. These inferences are then manually rated by three independent annotators based on the human-evaluated metrics. As suggested by <ref type="table" target="#tab_11">Table 6</ref>, we observe that most of the fine-tuned models on CICERO perform similarly but fail to reach gold annotation performance. Moreover, as expected, the fine-tuned models significantly outperform their non fine-tuned counterparts. We provide some examples of the generated inferences in <ref type="table">Table 7</ref>. Inspection of the model generated inferences reveal that usage of keywords from the dialogue without generalizing the events is more frequent. Generated inferences are significantly less diverse and creative than gold annotations.</p><p>Performance of GLUCOSE. GLUCOSE contains contextual commonsense inferences on events in monologues. Comparing the results ( Targetu 1 u 1 u 1 ; Inference: Cause; Answers: Gold -The speaker did not eat breakfast. T5 -The speaker and the listener are going to a restaurant for lunch. GLUCOSE-T5 -The speaker and the listener are at a restaurant.</p><p>Targetu 2 u 2 u 2 ; Inference: Cause; Answers: Gold -The restaurant down the street is famous for its delicious food. T5 -The listener is hungry and wants to eat something. GLUCOSE-T5 -The speaker is hungry and wants to order something to eat.</p><p>Targetu 3 u 3 u 3 ; Inference: Motivation; Answers: Gold -The speaker is craving some savory fast food. T5 -The speaker is craving for a hamburger, fries and a chocolate sundae. GLUCOSE-T5 -The speaker is craving for a burger, fries and sundae.</p><p>Targetu 6 u 6 u 6 ; Inference: Prerequisite; Answers: Gold -The speaker is a fitness freak and keeps track of his daily diet. T5 -The speaker is a healthy person. GLUCOSE-T5 -The speaker is a health conscious person.</p><p>Targetu 7 u 7 u 7 ; Inference: Subsequent Event; Answers: Gold -The listener refused to eat anything that is unhealthy. T5 -The speaker and the listener decided to order some hot dogs. GLUCOSE-T5 -The speaker and the listener decided to order some hot dogs.</p><p>Targetu 8 u 8 u 8 ; Inference: Reaction; Answers: Gold -The listener felt embarrassed by the statement of the speaker. T5 -The listener is shocked to hear the speaker's comment. GLUCOSE-T5 -The listener is disappointed with the speaker's decision. <ref type="table">Table 7</ref>: Inferences by different models extracted from a sample dialogue for the CICERO NLG task. the same task for dialogues. Akin to the non finetuned T5, non fine-tuned GLUCOSE-T5 produces gibberish outputs for all the commonsense inference types but the causal and motivation types. We surmise this happens as these two commonsense types exist in the GLUCOSE dataset. Although the generated text for these two commonsense inference types are grammatically correct and sometimes contain contextual words, they are far from the desired quality, semantically very much dissimilar from the annotated gold instances, and rated low in the qualitative evaluation, as shown in <ref type="table" target="#tab_11">Table 6</ref>. We also confirm the efficacy of finetuning the models on CICERO through human evaluation, as explained in ?4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results of the CICERO MCQ Task</head><p>Evaluation Metrics. 1) RoBERTa and ELECTRA: The accuracy of selecting the correct answer is used to evaluate the performance of these models.</p><p>2) T5 and Unified QA: The output is considered as a single answer if it doesn't contain any separator token. Otherwise, the output is segmented at separator tokens to obtain multiple answers. We then follow the method in <ref type="bibr">Khashabi et al. (2020)</ref>, where match is computed by comparing each of the generated answer(s) with the candidate choices based on their token-level overlap. For each generated answer, the most similar candidate choice is considered as the corresponding output. The prediction is considered as correct if the final output(s) is an exact match (EM) with the gold annotated answer(s).</p><p>Single Answer Selection (2.1). We report the results of this setting in   sequently as a solver for the final CICERO MCQ task. We think, this results expose the model dependency of the AF process. In other words, the negative samples chosen by the backbone model X for the AF algorithm will be difficult to distinguish from the human-annotated true samples using the same model X. These negative samples, however, could be relatively easier to identify using another model Y. The seq2seq models T5 and Unified QA perform significantly better than RoBERTa and ELECTRA as can be seen in <ref type="table" target="#tab_14">Table 8</ref>. While models like RoBERTa, ELECTRA encode each candidate answer separately, T5 and Unified QA encode them together. Thanks to this joint encoding of candidate answers, T5 and Unified QA can take advantage of more task-related information that RoBERTa and ELECTRA might miss due to the separate encoding scheme. We surmise it could be one of the reasons why the seq2seq models have an edge over RoBERTa and ELECTRA for this particular task. T5 and Unified QA attain almost the same score for single answer selection. This is surprising as Unified QA is initialized from the T5-large checkpoint and then further trained on other QA datasets. As such, we think, the different fine-tuned domains of Unified QA does not help in the CICERO MCQ task.</p><p>All Answers Selection (2.2). We train and evaluate T5 and Unified QA on the entire dataset of both single and multiple correct answers and report the results in <ref type="table" target="#tab_16">Table 9</ref>. Overall, T5 and Unified QA perform similarly. The general performance, across the models, on instances with multiple correct answers is much worse than instances with a single correct answer. We confirm this by reporting the results only on instances with multiple answers in <ref type="table" target="#tab_16">Table 9</ref>, where T5 and Unified QA achieve only 3.38% and 3.60% exact match, respectively. This could probably be attributed to the stark data imbalance of ?86/14% between single-and multi-answer instances, respectively (see <ref type="table" target="#tab_5">Table 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Commonsense knowledge has received more attention compared with factual knowledge, as it is usually not mentioned explicitly in the con-  <ref type="bibr">et al., 2019)</ref>. However, ConceptNet is contextfree, meaning that they only capture relationships around a selected set of entities, without paying attention to the context where the entity occurs. Moreover, inference is often needed in discourse level, which do not always align with the entities in knowledge bases. Knowledge models such as <ref type="bibr">COMET (Bosselut et al., 2019)</ref> is a way to circumvent this issue and make inferences on an utterance (sentence) level. But the generated knowledge still lacks the detail from the dialogue, as it is trained on the aforementioned knowledge base. Our approach, instead, centers on the dialogue dataset and provides more detailed commonsense inference at an utterance level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We introduced CICERO, a new dataset for dialogue reasoning with contextualized commonsense inference. It contains ?53K inferences for five commonsense dimensions -cause, subsequent event, prerequisite, motivation, and emotional reaction -collected from ?5.6K dialogues.</p><p>To show the usefulness of CICERO for dialogue reasoning, we design several challenging generative and multi-choice answer selection tasks for state-of-the-art NLP models to solve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported by the A*STAR under its RIE 2020 AME programmatic grant RGAST2003 and project T2MOE2008 awarded by Singapore's MoE under its Tier-2 grant scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>The annotators for CICERO were hired through a data annotation service. The compensation was derived based on the country of residence of the annotators, as deemed by the company. The study has been categorized as "exempt" by the IRB. Annotators were strictly asked not to write any toxic content (hateful or offensive toward any gender, race, sex, religion). They were asked to consider gender-neutral settings in dialogues whenever possible.</p><p>The source dialogue datasets -DailyDialog, MuTual, and DREAM are high quality multi-turn dialogue datasets manually annotated by experts in dialogue, communication theory and linguistics. All three datasets have been extensively used and studied in the natural language processing literature. The three source datasets and our annotations in CICERO do not contain any personal data or any information that can uniquely identify individual people or groups. Target: Gate 36, but I'm sorry to tell you that it's been changed to Gate 7 and your planeis taking off in 20 minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><p>The speaker and her family rushed towards the gate number 7 to catch the flight.</p><p>The speaker and her family rushed towards the gate number 7 and they missed the flight.</p><p>The family rushed towards gate number 8 to catch the flight.</p><p>The speaker and her family rushed towards the gate number 7 after their flight had been canceled.</p><p>The speaker and her family rushed towards the gate number 7 but could not catch the flight.</p><p>(b) <ref type="figure">Figure 6</ref>: Instances of temporal commonsense in CICERO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Details on CICERO</head><p>The total compensation for the complete annotation process of CICERO including all the manual labeling ( ?2), and verification stages in AF ( ?3.2) was USD 13, 500. The annotators were hired through a data annotation company. The total compensation was derived based on the country of residence of the annotators, as deemed by the company. Being a dialogue-centric dataset, CICERO encompasses various aspects of human to human conversations such as temporal commonsense awareness in <ref type="figure">Fig. 5, Fig. 6</ref>, physical commonsense in <ref type="figure">Fig. 7</ref>, general commonsense in <ref type="figure" target="#fig_5">Fig. 8</ref>, and social commonsense in <ref type="figure" target="#fig_8">Fig. 10</ref>. In <ref type="figure">Fig. 6a</ref>, commonsense is required to infer that a familiar face may look different to us if we meet that person after a long time. There could be other potential reasons why a person might look different to his/her friends such as facial surgery, sickness, makeup, etc. However, in this particular dialogue context, the most appropriate speculative cause of the target is meeting the person after a long time. Similarly in <ref type="figure">Fig. 6b</ref>, the person hurries to the boarding gate as only 20 minutes is left before the flight takes off. Leveraging commonsense inference, we can infer that going to a place in a very short period requires us to rush. In <ref type="figure">Fig. 7</ref>, physical commonsense knowledge is required to infertouching a hot element can burn our fingers and pans or microwaves are used for cooking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B CICERO NLG Task: Extended Results</head><p>We report BLEU1 scores <ref type="bibr">(Papineni et al., 2002)</ref> in addition to the automatic evaluation metrics described in ?4.2. We also report results for generative tasks with the BART-large <ref type="bibr">(Lewis et al., 2020), and</ref><ref type="bibr">COMET (Hwang et al., 2021)</ref> model. COMET is a commonsense generation model from free text input. It is a pre-trained BART-large model fine-tuned on the ATOMIC dataset <ref type="bibr">(Hwang et al., 2021)</ref>. In our work, we have used all the models in two distinct ways -i) with fine-tuning and ii) without fine-tuning on CICERO. The results are shown in <ref type="table" target="#tab_19">Table 10, and Table 11</ref>. Surprisingly, despite being pre-trained on a large com-    <ref type="table" target="#tab_3">Table 12</ref>: Results of the human evaluation for the CICERO NLG task. T5 * , BART * , COMET * , and GLUCOSE-T5 * represent non fine-tuned versions.</p><p>parate inputs, which are at odds with ATOMIC. Further research is needed to draw any conclusion.</p><p>The results of human evaluation of the models are illustrated in <ref type="table" target="#tab_3">Table 12</ref>. It can be seen that all the models perform almost similarly on CICERO and stand far from reaching human-level performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-tuned vs non Fine-tuned Evaluations.</head><p>All the models perform very poorly when they are not fine-tuned on CICERO. The non fine-tuned models generate gibberish sentences across all five inference categories. The automatic and human evaluation results of these models are also reported in <ref type="table" target="#tab_3">Table 10 and Table 12</ref>, respectively. The results confirm that fine-tuning is necessary for dialoguelevel commonsense inference thus reaffirming the importance of our curated dataset CICERO. The non fine-tuned COMET produces very short outputs (1-3 words, akin to ATOMIC annotations) that are not readily comparable with CICERO, resulting in poor evaluation scores.</p><p>Finally, we provide some additional examples to depict the inference generation quality of the models in <ref type="table" target="#tab_5">Table 13</ref> No, I don't think so. My parents have offered to take care of him, and I don't think he'd be happy in the city. A (u 9 ) (u 9 ) (u 9 ): You're probably right. But aren't you afraid of moving to such a big place, especially after living in a small village? B (u 10 ) (u 10 ) (u 10 ): Not really. I think I'll enjoy myself. There's so much to do there; I expect I won't miss the countryside much and I can always come back and visit. A (u 11 ) (u 11 ) (u 11 ): Well, I just hope you'll invite me to stay when you get settled. B (u 12 ) (u 12 ) (u 12 ): Of course I will.</p><p>Targetu 6 u 6 u 6 ; Inference: Cause; Answers: Gold -Being an expensive city, it is quite difficult to find an affordable place to live in London. T5 -The listener asked Jenny where she was going to live. COMET -The speaker is looking for a flat to live in London. GLUCOSE-T5-Jenny has decided to move to London for her job.</p><p>Targetu 10 u 10 u 10 ; Inference: Cause; Answers: Gold -Jenny realizes that a city like London will provide a great quality of life for her. T5 -The listener asked Jenny if she was afraid of moving to London after living in a small village. COMET -The speaker is moving to London for a job. GLUCOSE-T5-The listener asked Jenny if she was afraid of moving to such a big place.</p><p>Targetu 6 u 6 u 6 ; Inference: Subsequent Event; Answers: Gold -The listener suggests Jenny to find potential flats or flatmates online. T5 -The speaker will find a flat to share with a friend. COMET -The speaker informed the listener that she will share the flat with someone else. GLUCOSE-T5-Jenny will find a flat to share with her friend.</p><p>Targetu 10 u 10 u 10 ; Inference: Subsequent Event; Answers: Gold -Jenny inquired a social club in London and ask for their membership to utilize her free time. T5 -The speaker told the listener that he would love to visit London. COMET -The speaker informed the listener that he will miss the countryside very much. GLUCOSE-T5-The speaker informed the listener that he would love to come back to London.</p><p>Targetu 4 u 4 u 4 ; Inference: Prerequisite; Answers: Gold -Jenny has completed her studies. T5 -The speaker has a job in London. COMET -Jenny has applied for a job in London. GLUCOSE-T5-The speaker has a job in London.</p><p>Targetu 12 u 12 u 12 ; Inference: Prerequisite; Answers: Gold -Jenny and the listener are good friends. T5 -Jenny has invited her friend to stay with her in London. COMET -Jenny has a place to stay in London. GLUCOSE-T5-The listener invited Jenny to stay in London.</p><p>Targetu 6 u 6 u 6 ; Inference: Motivation; Answers: Gold -Jenny is optimistic about having someone as her flatmate to save on rent. T5 -Jenny is hopeful of finding a flat to share with someone. COMET -Jenny is optimistic about having someone as her flatmate. GLUCOSE-T5-Jenny is hopeful that she will find a flat to share with somebody.</p><p>Targetu 12 u 12 u 12 ; Inference: Reaction; Answers: Gold -The listener is happy for Jenny and looks forward to being invited to London by Jenny. T5 -The listener is happy for Jenny. COMET -The listener is happy to know that the speaker is moving to London. GLUCOSE-T5-The listener is excited to meet Jenny in London. models under three distinct settings: 1) Setting 1: train models only on instances with a singleanswer, 2) Setting 2: train models only on instances with multiple-answers, 3) Setting 3: train models on the entire dataset comprising both single and multiple-answers.</p><p>The performances of both the generative models T5 and Unified QA on instances with multiple answers are very poor (see <ref type="table" target="#tab_8">Table 14</ref>, <ref type="table" target="#tab_10">Table 15</ref> and <ref type="figure">Fig. 11a, Fig. 11b</ref>). Further, we can also see instances where the predicted answers by these models contradict (see <ref type="figure">Fig. 11b</ref>). While T5 surpasses   Unified QA for Setting 3, Unified QA shines over T5 for the other two settings.</p><p>Performance of ELECTRA vs RoBERTa. We also extend upon the results reported earlier for ELECTRA and RoBERTa in ?4.3 for the single answer selection (Task 2.1) in CICERO MCQ . The performance of ELECTRA is notably better than RoBERTa on this task. We reckon this could be due to the fact that we train our adversarial filtering (AF) method using RoBERTa. As such the efficacy of AF to prevent exposing stylistic artifacts to the discriminators is lesser for ELEC-TRA compared to RoBERTa. In other words, ELECTRA is more efficient than RoBERTa for the CICERO MCQ task due to its ability to better discriminate machine-generated negative answers from human-annotated true answers by leveraging stylistic artifacts as observed in <ref type="bibr">Zellers et al. (2018)</ref>.</p><p>Despite performing decently on the single answer selection task for CICERO MCQ , RoBERTa does make mistakes in understanding some very interesting commonsense-based inferences such as the ones illustrated in <ref type="figure">Fig. 12</ref>. In these two examples, commonsense inference is required to detect the bluff by Tim Smith. Among other kinds of errors, we find RoBERTa failing to capture contextual commonsense cues such as in <ref type="figure" target="#fig_6">Fig. 9</ref> if a person wanting to buy new batteries is informed about the availability of batteries at photocopy stores, that person will search for photocopy stores instead of ad stores. Dr. bean is a health and fitness, conscious person.</p><p>Dr. bean is not as conscious as you think.</p><p>Dr. bean is a sports fan.</p><p>Dr. bean is not a health and fitness-conscious person.</p><p>The doctor has no health and fitness goals.</p><p>Dr. bean is a health and fitness, conscious person. MCQ using RoBerta: Generation using T5: Dr. Bean loves to play tennis and handball. The listener was cheerful to see linda's passion and dedication for her profession.</p><p>The listener was cheerful to hear linda's pragmatism and lack of dedication.</p><p>The listener was pleased to see linda's passion and dedication for her profession.</p><p>Linda's passion and dedication for her profession made the listener laugh.</p><p>Linda told the story of how she was depressed.</p><p>The listener was cheerful to see linda's passion and dedication for her profession. MCQ using RoBerta: Generation using T5: The listener is excited to know about Linda's future plans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b)</head><p>Question: What subsequent event happens or could happen following the Target?</p><p>A: Sorry, Kevin. I am sorry for breaking your glass. I was tidying up your desk. B: You're supposed to be more careful. A: I'm on duty today. I'm really sorry. I'll pay for it and try to be more careful in the future. B: There is no need to pay, but be sure you're more careful from now on. A: I will. Sorry again.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target:</head><p>There is no need to pay, but be sure you're more careful from now on. Generation using T5: Kevin will try to be more careful in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(c)</head><p>Tom's car met with an accident recently.</p><p>Tom's car is in perfect condition.</p><p>Tom's bike meet with an accident recently.</p><p>Tom's car met with no accident recently.</p><p>His car has never had an accident.   Target: All photography shops carry them .</p><p>They will search the ad store to buy a new battery.</p><p>They will search the photocopy shop for a good paper.</p><p>They will search the photocopy shop to buy new paper.</p><p>They search the photocopy shop to buy a new camera.</p><p>They will search the photocopy shop to buy a new battery.</p><p>They will search the ad store to buy a new battery. MCQ using RoBerta: Generation using T5: The speaker informed the listener that all photography shops carry batteries. Zero-shot Setting. We also set up a zero-shot setting for Task 2.1 -Single Answer Selection and Task 2.2 -All Answers Selection. Under this setting, we only keep instances pertaining to cause, prerequisite, and emotional reaction in the train, validation data while instances with subsequent event, and motivation are kept in the test data. All the models underperform in the zero-shot setting, as can be seen in <ref type="table" target="#tab_10">Table 15</ref>. Like the all and single answer(s) prediction, T5 and Unified QA perform similarly. On the other hand, ELECTRA's zero-shot performance surpasses that of RoBERTa. Notably, performance of T5 and Unified QA only drop around 1% in this setting, as compared to 3% drop observed for RoBERTa and ELECTRA. Hence, it is fair to conclude that for the CICERO MCQ task, T5 and Unified QA are more robust to zero-shot scenarios than RoBERTa and ELECTRA. In the case of zero-shot single answer prediction, the best model is Unified QA which outperforms RoBERTa and ELECTRA by 11% and 15% respectively.</p><p>Performance on Single-vs Multi-answer Instances. It is evident from Tables 14 and 15, that in both regular and zero-shot settings, all the models exclusively trained on single-and multianswer instances perform better on single-and multi-answer test instances, respectively, as compared to models trained on both types of instances. This is likely a side-effect of the data imbalance</p><p>The speaker desires to calm the listener and help him forget his worries.</p><p>The speaker desires to help the listener remember his worries.</p><p>The speaker desires to make the listener feel nervous.</p><p>The speaker wants to make the listener think about his worries.</p><p>The speaker desires to make the listener laugh.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question:</head><p>What is or could be the motivation of target? Target: I understand how you're feeling. Just take it easy. You'll make a lot of friends very soon.</p><p>The speaker desires to calm the listener and help him forget his worries. MCQ using RoBerta: Generation using T5: The speaker is encouraging the listener. The listener is relaxed now that they won't have to travel by bus anymore.</p><p>The listener is relaxed now that they will make more money by traveling by bus.</p><p>The listener is relaxed now that they will be able to travel by bus again.</p><p>The listener is relieved that they will still use the bus.</p><p>The listener is not relaxed since he still has to travel by bus.</p><p>The listener is relaxed now that they won't have to travel by bus anymore. MCQ using RoBerta: Generation using T5: The listener is excited to visit the Smiths. between the single-and multi-answer instances (?86/14%) in the training set which causes the scarce multi-answer instances to have confounding effect on the training process, degrading the performance on both types of test instances.</p><p>Performance of CICERO NLG vs CICERO MCQ . We present the qualitative analysis for generative (CICERO NLG ) and discriminative (CICERO MCQ ) experiments in <ref type="figure" target="#fig_5">Fig. 8a, Fig. 8b,  Fig. 8c, Fig. 8d, Fig. 9, Fig. 10a, and Fig. 10b</ref>. Except for <ref type="figure" target="#fig_6">Fig. 9</ref>, RoBERTa provides the accurate answer on all instances. Contrary to this, the performance of T5 is far from being sublime on those samples for the CICERO NLG task. This depicts that the commonsense-based generative task CICERO NLG poses more challenge than the commonsense-based discriminative task CICERO MCQ . We surmise this could happen due to two potential reasons -1. Machine-generated negative answers may carry stylistic biases (Zellers et al., 2018), thus making the task of discriminators easier.</p><p>2. We collate the negative answers by generating counterfactual and contradictory sen- The listener tries to forget whether he knows mr. brown personally or not.</p><p>The listener tries to recall what mr. brown was saying.</p><p>The listener isn't supposed to be able to recall if he knows mr. brown personally.</p><p>The listener decides to ask mr. brown if he knows him personally.</p><p>The listener tries to recall if he knows mr. brown personally.</p><p>The listener tries to recall what mr. brown was saying. MCQ using T5:</p><p>Question: What subsequent event happens or could happen following the target?</p><p>Target: Just one, Mr. Blank. You had a telephone call from someone called Brown, David Brown.</p><p>The listener tries to recall if he knows mr. brown personally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a)</head><p>A: I want to take the children out next Saturday. B: Next Saturday? That's eleventh, isn't it? A: No, it's the twelfth. B: Oh, yes, the twelfth. Where do you want to take them? A: To the zoo. B: To the zoo? You took them, there last month. I didn't think they enjoyed that visit. A: That's not what they told me. B: I think the beach is a better place. A: OK. That's the beach. B: What time are you going to pick them up? A: At 7 in the morning. B: Then I'll get ready for them half an hour earlier.</p><p>The speaker is worried that the children would not enjoy the play like before.</p><p>The speaker is worried that the children would not enjoy the zoo visit like before.</p><p>The speaker is worried that the children would not enjoy the beach visit like before.</p><p>The speaker does not want the children to go to the zoo.</p><p>The speaker is concerned that the children would enjoy the zoo visit like before.</p><p>The speaker is concerned that the children would enjoy the zoo visit like before. MCQ using T5: Question: What is or could be the motivation of target?</p><p>Target: To the zoo? You took them, there last month. I didn't think they enjoyed that visit.</p><p>The speaker is worried that the children would not enjoy the zoo visit like before.</p><p>(b) <ref type="figure">Figure 11</ref>: Multiple-answer predictions by T5 for the CICERO MCQ task. tences from the annotated true inferences. As a result, the generated negative answers are lexically very similar to the annotated sentences resulting in less diversity in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>RoBERTa-Large  A: But he told me it was true. You see, his grandfather used to be an army officer during the war. And because he didn't return home after the war, everybody thought he had been killed in the war. B: But then, he suddenly appeared alive, like in those films. A: Exactly. Tom, oh no, Tim, told me that by chance he saw an old man at the railway station selling newspapers.</p><p>And he was surprised to see someone like his grandfather in a picture he had seen. So naturally he went to the man and asked him whether his name was Smith. And the man, I mean, his grandfather, said yes, and after that everything happened just like a film. B: Amazing. But why didn't the old man go back to his hometown after the war? A: Well, that's another long story. I'll tell you later.</p><p>The listener would tell the speaker that this story is actually true.</p><p>The listener would tell the speaker that this story is based on true events.</p><p>The listener would tell the speaker that this story is not believable at all.</p><p>The listener would tell the speaker that this story is very enticing.</p><p>The listener would tell the speaker that this story is very true.</p><p>The listener would tell the speaker that this story is based on true events. MCQ using RoBerta: Generation using T5: Tim Smith told Tom that he saw his grandfather in London.</p><p>Question: What subsequent event happens or could happen following the target?</p><p>Target: Well, he told me he saw his dead grandfather in London.</p><p>Tim's grandfather was shot during war.</p><p>Tim's grandfather was not shot during the war, it was only a rumor.</p><p>Tim's grandfather was shot during the war and he knows it.</p><p>Tim's grandfather was shot during the war and he never heard of it.</p><p>Tim's grandfather was shot a lot in the war.</p><p>Tim's grandfather was shot during war. MCQ using RoBerta: Generation using T5: Tom's grandfather used to be an army officer during the war.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question:</head><p>What is or could be the prerequisite of target?</p><p>Target: But then, he suddenly appeared alive, like in those films. <ref type="figure">Figure 12</ref>: Examples of some incorrect predictions by RoBERTa for the CICERO MCQ task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D CICERO vs Other Commonsense Datasets</head><p>The key differences that set CICERO apart from the rest of the commonsense datasets are following:</p><p>? To the best of our knowledge, CICERO is the only publicly available dialogue-centric commonsense inference dataset.</p><p>? The speculative nature of the questions posed to the annotators enforces employment of rich commonsense knowledge in the inferences, thereby, making CICERO commonsense-rich and, thus, difficult inferences for models without relevant commonsense knowledge.</p><p>? While the performance of the strong baseline models on CICERO for CICERO MCQ task are comparable (see <ref type="table" target="#tab_11">Table 16</ref>) with the performance on other available commonsensebased question-answering datasets, unlike the others, around 14% of the instances in CICERO contain multiple correct inferences/answers. These are more challenging to the baselines, as can be seen in <ref type="table" target="#tab_8">Table 14</ref>.</p><p>? Dialogue-centric commonsense inference/answer generation task, i.e., CICERO NLG is novel and hard to solve. Strong baselines, such as, T5, BART, and their checkpoints pre-trained on large external commonsense datasets, such as, ATOMIC and GLUCOSE, perform poorly at this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Hyperparameter Details</head><p>All models for the CICERO NLG generative tasks were trained with the Adafactor optimizer (Shazeer and Stern, 2018) with a learning rate of 5e-6.</p><p>The models CICERO MCQ alternative selection were trained with the AdamW (Loshchilov and Hutter, 2018) optimizer with a learning rate of 1e-5. We used a batch size of 4 for all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Computational Resources</head><p>The T5 Large and GLUCOSE-T5 Large have 770M parameters each.</p><p>The RoBERTa-Large and ELECTRA-Large have 355M and 335M parameters, respectively. We also use a BART-Large and COMET-Large models for more extensive experiments (Appendix B). Both the models have 406M parameters. We use a single RTX 8000 GPU for our experiments. All models were trained for 5 epochs. Training and inference for the generative tasks i.e., CICERO NLG require between 1.5-6 hours in this GPU. Training and inference for the alternative selection task i.e., CICERO MCQ require a total of 15 hours. Training and inference times are 40% less for zero-shot setting experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>?????? ? eating at McDonald's.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Intermediate commonsense inference steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>text. It is demonstrated to be essential in openended generation tasks, such as story explanation generation (Mostafazadeh et al., 2020), story end generation (Guan et al., 2019) and abductive reasoning (Bhagavatula et al., 2019). To infuse commonsense knowledge in NLP models, several approaches to tasks like sentence ordering (Ghosal et al., 2021b), emotion recogni-tion (Ghosal et al., 2020), story generation (Guanet al., 2020; Xu et al., 2020)  and dialogue generation(Zhou et al., 2018)  use prevalent commonsense knowledge bases (CSKB) likeCon- ceptNet (Speer et al., 2017)  or ATOMIC (Sap</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>The speaker touched the hot handle of the pan.The speaker touched the ice.The speaker touched the cold handle of the skillet.The speaker touched the cold handle of the pan and it burnt.The speaker touched the hot handle of the microwave.Question: What is or could be cause of the Target? A: Well, the salad's almost ready. How's the beef going? I'm starving. B: So am I. The beef looks just about ready. Just one minute ... ow! A: What's the matter? B: Oh, my finger, I burned my finger! A: Oh, wait, I'll get some ice and put it on your finger. B: OK. A: There. B: Ah, ah, much better. The ice really works. A: How does it feel? B: Oh, I feel good. Thanks. Let's eat. Target: Oh, my finger, I burned my finger!Figure 7: A data sample of CICERO where physical commonsense inference is prevalent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>What is the possible emotional reaction of the listener in response to target? A: Linda, what do you do for a living? B: I am a dancer. A: Oh,and what do you do for fun? B: I like to enjoy classical music A: And what's the most exciting thing that happened to you recently? B: Oh, this is so great! Some of my friends and I went to a famous piano concert. A: And who do you admire most in the world? B: I guess, my dad. A: And what do you want to be doing five years from now? B: I would love to have my own dancers' school if I could. Target: I would love to have my own dancers' school if I could.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Instances of general commonsense in CICERO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>An instance where RoBERTa fails to capture the contextual commonsense cue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>What is the possible emotional reaction of the listener in response to target? A: I'd like to pay a visit to the Smiths at 3:30 p.m. Will you go with me, Mary? B: I'd love to, but I won't be off work from my factory until 4:00 p.m. How about 4:15? I'll be free then, Jack. A: OK. Let's meet at the bus stop and take the No.5 bus to go there. B: Why not by bike? The bus would be crowded at that time. A: But my bike is broken. B: You can use your sister's new bike, can't you? A: Yes. I'll wait for you in front of the bookstore opposite the cinema. Target: Yes. I'll wait for you in front of the bookstore opposite the cinema.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Instances of social commonsense in CI-CERO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>But that is the only thing I can cook. What do you want to have then? I'd like to go to McDonald's this time. Tom is bored of eating the same dishes and want to try something different in his meal.</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Tom's father</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">is relieved</cell></row><row><cell></cell><cell></cell><cell>Dialogue</cell><cell></cell></row><row><cell>Causes eating same dish boredom change of diet Causes Desire Cau ses Causes Desire</cell><cell></cell><cell cols="2">Em oti on al Re ac tio n</cell><cell>No cooking at home Serves food Relaxation Implies Causes Desire</cell></row><row><cell>dine at</cell><cell></cell><cell></cell><cell></cell><cell>Has Property</cell></row><row><cell>McDonald's</cell><cell></cell><cell cols="3">McDonald's</cell></row><row><cell>Dine at McDonald's serves tasty food McDonald's Has Property S u b s e q u e n t E v e n t Causes Desire</cell><cell>go McDonald's now located nearby distance within driving Implies Implies H a s P r e r e q u i s i t e</cell><cell>routine not part of eliminates diversifies experience repetition Implies Implies</cell><cell cols="2">M o t i v a t e s</cell></row><row><cell>Tom's father agrees to go to</cell><cell></cell><cell></cell><cell cols="2">Has Property</cell></row><row><cell>McDonald's</cell><cell></cell><cell cols="2">going to</cell></row><row><cell></cell><cell>A McDonald's is open</cell><cell cols="2">McDonalds</cell></row><row><cell></cell><cell>nearby</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Going to McDonald's</cell><cell></cell></row><row><cell></cell><cell></cell><cell>diversifies experience</cell><cell></cell></row><row><cell></cell><cell cols="4">2.2 Dialogue Selection for CICERO</cell></row><row><cell></cell><cell cols="2">2.2.1 Source Datasets</cell><cell></cell></row><row><cell></cell><cell cols="4">To build CICERO, we use the dyadic dialogues</cell></row><row><cell></cell><cell cols="3">of the following three datasets:</cell></row><row><cell></cell><cell cols="4">DailyDialog (Li et al., 2017) covers dialogues</cell></row><row><cell></cell><cell cols="4">from wide range of topics -life, work, relation-</cell></row><row><cell></cell><cell cols="4">ships, tourism, finance, etc. The constituent utter-</cell></row><row><cell></cell><cell cols="4">ances are labelled with emotion and dialogue-act.</cell></row></table><note>MuTual (Cui et al., 2020) is a multi-turn dialogue reasoning dataset. Given a dialogue history, the objective is to predict the next utterance by con- sidering aspects such as intent, attitude, algebraic, multi-fact, and situation reasoning.I'm sorry.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Annotated examples in CICERO marked with the target utterance and the inference type. Inference types Cause, Effect, Prerequisite, Motivation, and Reaction correspond to questions Q1, Q2, Q3, Q4, and Q5, respectively, in ?2.1.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Statistics of the annotated CICERO dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Can I help you? B: Yes, please. I'd like some oranges. A: Do you want Florida or California oranges? B: Which do you think are better? A: Florida oranges are sweet but they are small. But California oranges have no seeds. B: Then give me five California oranges.</figDesc><table><row><cell>A: Anything else?</cell></row><row><cell>B: I also want some bananas. How do you sell them?</cell></row><row><cell>A: One dollar a pound. How many do you want?</cell></row><row><cell>B: Give me four and see how much they are.</cell></row><row><cell>A: They are just one pound.</cell></row><row><cell>B: Good. How much do I owe you?</cell></row><row><cell>A: Three dollars.</cell></row><row><cell>B: Here you are.</cell></row><row><cell>A: Thank you.</cell></row></table><note>A:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Results of the CICERO NLG task. T5 Results of the CICERO NLG Task Automatic Evaluation Metrics. For generative tasks, we report the following metrics: BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE (Lin, 2004), CIDEr (Vedantam et al.</figDesc><table /><note>* and GLUCOSE-T5* are not fine-tuned on our dataset. All models are Large models. SE denotes Subsequent Event.4.2</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 .</head><label>4</label><figDesc>The T5 model achieves the best performance in most of the experimental settings.The results indicate that the causal types are more</figDesc><table><row><cell>Model</cell><cell cols="5">BLEU2 METEOR ROUGE CIDEr Sem-Sim</cell></row><row><cell>(1.1.4) Chained Cause</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>T5</cell><cell>0.1566</cell><cell>0.1675</cell><cell>0.2757</cell><cell>0.5303</cell><cell>0.6518</cell></row><row><cell>GLUCOSE-T5</cell><cell>0.1600</cell><cell>0.1697</cell><cell>0.2796</cell><cell>0.5633</cell><cell>0.6557</cell></row><row><cell>(1.1.1)* Cause</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>T5</cell><cell>0.1503</cell><cell>0.1635</cell><cell>0.2634</cell><cell>0.4591</cell><cell>0.6284</cell></row><row><cell>GLUCOSE-T5</cell><cell>0.1564</cell><cell>0.1636</cell><cell>0.2709</cell><cell>0.4915</cell><cell>0.6310</cell></row><row><cell>(1.1.5) Chained SE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>T5</cell><cell>0.1813</cell><cell>0.1784</cell><cell>0.2940</cell><cell>0.5136</cell><cell>0.6469</cell></row><row><cell>GLUCOSE-T5</cell><cell>0.1789</cell><cell>0.1776</cell><cell>0.2943</cell><cell>0.5218</cell><cell>0.6516</cell></row><row><cell>(1.1.2)* SE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>T5</cell><cell>0.1622</cell><cell>0.0841</cell><cell>0.2764</cell><cell>0.4167</cell><cell>0.6279</cell></row><row><cell>GLUCOSE-T5</cell><cell>0.1612</cell><cell>0.1628</cell><cell>0.2778</cell><cell>0.4471</cell><cell>0.6294</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Results of the CICERO NLG subtasks -chained cause and subsequent event generation. (1.1.1)* and (1.1.2)* indicates results from Task 1.1.1 and 1.1.2 (as in Table 4), but only for targets which have both cause and effect annotated, ensuring a fair comparison with (1.1.4) and (1.1.5). SE denotes Subsequent Event.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6</head><label>6</label><figDesc>), the non fine-tuned versions of T5 and GLUCOSE-T5 perform poorly as they produce gibberish outputs across all the five inference categories indicating the importance of fine-tuning on CICERO.</figDesc><table><row><cell>Creativity</cell><cell>4.7</cell><cell>3.8</cell><cell>3.9</cell><cell>2.4</cell><cell>1.9</cell></row><row><cell>Contextuality</cell><cell>4.8</cell><cell>4.1</cell><cell>4.3</cell><cell>2.1</cell><cell>2.1</cell></row><row><cell>Fluency</cell><cell>5.0</cell><cell>4.8</cell><cell>4.9</cell><cell>1.9</cell><cell>2.9</cell></row></table><note>Results of Human Evaluation. For each of the five inference types, we randomly sample 40 in- Metric Gold T5 GLUCOSE T5* GLUCOSE*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Results of the human evaluation for CICERO NLG . T5</figDesc><table /><note>* and GLUCOSE-T5* represent non fine-tuned versions.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 4</head><label>4</label><figDesc>): I'm hungry, let's order up something to eat. B (u 2 ) (u 2 ) (u 2 ): Ok, maybe we can order a soup and a salad from the restaurant down the street. A (u 3 ) (u 3 ) (u 3 ): I was thinking of getting a hamburger, fries and a chocolate sundae. B (u 4 ) (u 4 ) (u 4 ): You eat too much junk food. That sort of stuff clogs up your arteries and is very high in cholesterol. A (u 5 ) (u 5 ) (u 5 ): Well I never seem to gain weight so I don't mind. B (u 6 ) (u 6 ) (u 6 ): It's not only about getting fat or not, it's about being healthy. You could really have some health problems later on. A (u 7 ) (u 7 ) (u 7 ): How about pizza or maybe some fried chicken! Better yet, let's order some hot dogs! B (u</figDesc><table><row><cell>, Table 6) of fine-tuned and non fine-</cell></row><row><cell>tuned checkpoints suggests that pre-training on a</cell></row><row><cell>monologue-based contextual commonsense infer-</cell></row><row><cell>ence dataset does not ensure good performance on</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 .</head><label>8</label><figDesc>The reported metric is accuracy of selecting the correct answer. The overall score is 83.28% for RoBERTa and 86.82% for ELECTRA. ELECTRA has an edge over RoBERTa on all the five inference types. This could be a side effect of using RoBERTa as the backbone model for the AF algorithm and sub-</figDesc><table><row><cell>Model</cell><cell>Cause</cell><cell>SE</cell><cell cols="2">Prerequisite Motivation</cell><cell>Emotional Reaction</cell><cell>Average</cell></row><row><cell>RoBERTa</cell><cell cols="2">83.34 83.17</cell><cell>79.48</cell><cell>86.33</cell><cell>84.26</cell><cell>83.28</cell></row><row><cell>ELECTRA</cell><cell cols="2">87.09 86.09</cell><cell>85.15</cell><cell>90.31</cell><cell>86.11</cell><cell>86.82</cell></row><row><cell>T5</cell><cell cols="2">95.19 95.29</cell><cell>94.93</cell><cell>96.52</cell><cell>96.99</cell><cell>95.54</cell></row><row><cell cols="3">Unified QA 95.85 94.99</cell><cell>95.55</cell><cell>96.35</cell><cell>97.22</cell><cell>95.70</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 :</head><label>8</label><figDesc>Accuracy scores for Task 2.1. Models are trained and evaluated on instances with a single correct answer.</figDesc><table><row><cell>Model</cell><cell>Evaluated On</cell><cell>Cause</cell><cell>SE</cell><cell cols="2">Prerequisite Motivation</cell><cell>Emotional Reaction</cell><cell>Average</cell></row><row><cell>T5 Unified QA</cell><cell>S + M</cell><cell cols="2">78.18 74.72 78.12 74.79</cell><cell>75.50 75.36</cell><cell>82.51 81.58</cell><cell>84.59 84.08</cell><cell>77.68 77.51</cell></row><row><cell>T5 Unified QA</cell><cell>S</cell><cell cols="2">93.20 91.28 93.12 91.16</cell><cell>91.27 91.00</cell><cell>95.19 94.28</cell><cell>95.14 94.79</cell><cell>92.71 92.45</cell></row><row><cell>T5 Unified QA</cell><cell>M</cell><cell>3.50 3.50</cell><cell>2.77 3.69</cell><cell>3.59 3.98</cell><cell>3.61 2.58</cell><cell>6.03 4.31</cell><cell>3.38 3.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table /><note>Exact match scores for Task 2.2. Models are trained on instances with both single and multiple correct answers, i.e., the entire dataset. SE ? Subsequent Event; S ? Single-Answer Instances; M ? Multi-Answer Instances.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 301-313, Singapore and Online. Association for Computational Linguistics. Long Papers, pages 986-995. Asian Federation of Natural Language Processing. Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81.</figDesc><table><row><cell>Deepanway Ghosal, Navonil Majumder, Alexander</cell><cell></cell></row><row><cell>Gelbukh, Rada Mihalcea, and Soujanya Poria. 2020. Cosmic: Commonsense knowledge for emotion identification in conversations. In Findings of the Association for Computational Linguistics: EMNLP</cell><cell>Ilya Loshchilov and Frank Hutter. 2018. Decoupled weight decay regularization. In International Con-ference on Learning Representations.</cell></row><row><cell>2020, pages 2470-2481.</cell><cell>Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring-</cell></row><row><cell>Deepanway Ghosal, Navonil Majumder, Rada Mihal-cea, and Soujanya Poria. 2021b. Stack: Sentence ordering with temporal commonsense knowledge.</cell><cell>ing order into texts. In Proceedings of the Con-ference on Empirical Methods in Natural Language Processing.</cell></row><row><cell>In Proceedings of the 2021 Conference on Empiri-cal Methods in Natural Language Processing, pages 8676-8686.</cell><cell>Nasrin Mostafazadeh, Aditya Kalyanpur, Lori Moon, Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An David Buchanan, Lauren Berkowitz, Or Biran, and automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings Jennifer Chu-Carroll. 2020. GLUCOSE: GeneraL-</cell></row><row><cell>Herbert P. Grice. 1975. Logic and conversation. Speech acts, pages 41-58.</cell><cell>of the acl workshop on intrinsic and extrinsic evalu-ation measures for machine translation and/or sum-marization, pages 65-72. ized and COntextualized story explanations. In Pro-ceedings of the 2020 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP), On-</cell></row><row><cell>Jian Guan, Fei Huang, Zhihao Zhao, Xiaoyan Zhu, and Minlie Huang. 2020. A knowledge-enhanced pre-training model for commonsense story generation. Transactions of the Association for Computational Linguistics, 8:93-108. Jian Guan, Yansen Wang, and Minlie Huang. 2019.</cell><cell>line. Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Han-nah Rashkin, Doug Downey, Wen-tau Yih, and Yejin Choi. 2019. Abductive commonsense reasoning. In Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic eval-uation of machine translation. In Proceedings of the 40th annual meeting of the Association for Compu-International Conference on Learning Representa-tions. tational Linguistics, pages 311-318.</cell></row><row><cell>Story ending generation with incremental encoding and commonsense knowledge. In Proceedings of the AAAI Conference on Artificial Intelligence, vol-(a) ume 33, pages 6473-6480. Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Jeff Da, Keisuke Sakaguchi, Antoine Bosse-lut, and Yejin Choi. 2020. COMET-ATOMIC 2020: Question: What subsequent event happens or could happen following the target? On symbolic and neural commonsense knowledge A: I'm trying to get on Flight FA2028. Am I on time? B: Not exactly. It's 6:20 pm now. But lucky for you, that flight has been delayed. A: I never thought I'd be happier about a delay. But hey, that's great news. B: OK, may I check your luggage and tickets, please? A: Here you are. Which gate do I leave from? B: Gate 36, but I'm sorry to tell you that it's been changed to Gate 7 and your plane is taking off in 20 minutes.</cell><cell>Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, and Yejin Choi. 2019. Comet: Commonsense transformers for automatic knowledge graph construction. In Pro-ceedings of the 57th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 4762-4779. Lianhui Qin, Antoine Bosselut, Ari Holtzman, Chandra Bhagavatula, Elizabeth Clark, and Yejin Choi. 2019. Counterfactual story reasoning and generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-guage Processing (EMNLP-IJCNLP), pages 5043-5053.</cell></row><row><cell>graphs. CoRR, abs/2010.05953.</cell><cell>Samuel R Bowman, Gabor Angeli, Christopher Potts, Colin Raffel, Noam Shazeer, Adam Roberts, Katherine</cell></row><row><cell>Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Jeff Da, Keisuke Sakaguchi, Antoine Bosselut, and Yejin Choi. 2021. Comet-atomic 2020: On sym-bolic and neural commonsense knowledge graphs. In AAAI.</cell><cell>and Christopher D Manning. 2015. A large an-notated corpus for learning natural language infer-ence. In Conference on Empirical Methods in Natu-ral Language Processing, EMNLP 2015, pages 632-642. Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text trans-former. Journal of Machine Learning Research, 21:1-67.</cell></row><row><cell>D. Khashabi, S. Min, T. Khot, A. Sabhwaral, O. Tafjord, P. Clark, and H. Hajishirzi. 2020. Uni-fiedqa: Crossing format boundaries with a single qa system. EMNLP -findings. Mike Lewis, Yinhan Liu, Naman Goyal, Mar-jan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation,</cell><cell>Leyang Cui, Yu Wu, Shujie Liu, Yue Zhang, and Ming Zhou. 2020. Mutual: A dataset for multi-turn dia-logue reasoning. In Proceedings of the 58th Confer-ence of the Association for Computational Linguis-Nils Reimers and Iryna Gurevych. 2019a. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-tics. Association for Computational Linguistics. G?nes Erkan and Dragomir R Radev. 2004. Lexrank: ral Language Processing (EMNLP-IJCNLP), pages 3982-3992. Graph-based lexical centrality as salience in text Nils Reimers and Iryna Gurevych. 2019b. Sentence-summarization. Journal of artificial intelligence re-bert: Sentence embeddings using siamese bert-search, 22:457-479. networks. In Proceedings of the 2019 Conference on</cell></row><row><cell>and comprehension. In Proceedings of the 58th An-Linguistics, pages 7871-7880. nual Meeting of the Association for Computational</cell><cell>Empirical Methods in Natural Language Process-Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. embeddings. In Empirical Methods in Natural Lan-SimCSE: Simple contrastive learning of sentence ing. Association for Computational Linguistics.</cell></row><row><cell></cell><cell>guage Processing (EMNLP).</cell></row></table><note>Deepanway Ghosal, Pengfei Hong, Siqi Shen, Navonil Majumder, Rada Mihalcea, and Soujanya Poria. 2021a. CIDER: Commonsense inference for dia- logue explanation and reasoning. InYanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. 2017. Dailydialog: A manually labelled multi-turn dialogue dataset. In Proceedings of the Eighth International Joint Conference on Nat- ural Language Processing, IJCNLP 2017, Taipei, Taiwan, November 27 -December 1, 2017 -Volume 1:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 10 :</head><label>10</label><figDesc>Results for Task 1. T5 * , BART * , COMET * and GLUCOSE-T5 * are not fine-tuned on CICERO. SE denotes Subsequent Event.</figDesc><table><row><cell>Model</cell><cell cols="6">BLEU1 BLEU2 METEOR ROUGE CIDEr Sem-Sim</cell></row><row><cell>(1.1.4) Chained Cause</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>T5</cell><cell>0.2781</cell><cell>0.1566</cell><cell>0.1675</cell><cell>0.2757</cell><cell>0.5303</cell><cell>0.6518</cell></row><row><cell>BART</cell><cell>0.1960</cell><cell>0.1104</cell><cell>0.1382</cell><cell>0.2242</cell><cell>0.4231</cell><cell>0.6074</cell></row><row><cell>COMET</cell><cell>0.2893</cell><cell>0.1633</cell><cell>0.1674</cell><cell>0.2742</cell><cell>0.5247</cell><cell>0.6488</cell></row><row><cell>GLUCOSE-T5</cell><cell>0.2820</cell><cell>0.1600</cell><cell>0.1697</cell><cell>0.2796</cell><cell>0.5633</cell><cell>0.6557</cell></row><row><cell>(1.1.1)* Cause</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>T5</cell><cell>0.2884</cell><cell>0.1503</cell><cell>0.1635</cell><cell>0.2634</cell><cell>0.4591</cell><cell>0.6284</cell></row><row><cell>BART</cell><cell>0.2548</cell><cell>0.1400</cell><cell>0.1530</cell><cell>0.2590</cell><cell>0.4279</cell><cell>0.6225</cell></row><row><cell>COMET</cell><cell>0.2769</cell><cell>0.1522</cell><cell>0.1584</cell><cell>0.2654</cell><cell>0.4510</cell><cell>0.6257</cell></row><row><cell>GLUCOSE-T5</cell><cell>0.2938</cell><cell>0.1564</cell><cell>0.1636</cell><cell>0.2709</cell><cell>0.4915</cell><cell>0.6310</cell></row><row><cell>(1.1.5) Chained SE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>T5</cell><cell>0.3322</cell><cell>0.1813</cell><cell>0.1784</cell><cell>0.2940</cell><cell>0.5136</cell><cell>0.6469</cell></row><row><cell>BART</cell><cell>0.3131</cell><cell>0.1649</cell><cell>0.1672</cell><cell>0.2795</cell><cell>0.4106</cell><cell>0.6314</cell></row><row><cell>COMET</cell><cell>0.3057</cell><cell>0.1626</cell><cell>0.1673</cell><cell>0.2742</cell><cell>0.4515</cell><cell>0.6321</cell></row><row><cell>GLUCOSE-T5</cell><cell>0.3258</cell><cell>0.1789</cell><cell>0.1776</cell><cell>0.2943</cell><cell>0.5218</cell><cell>0.6516</cell></row><row><cell>(1.1.2)* SE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>T5</cell><cell>0.3088</cell><cell>0.1622</cell><cell>0.0841</cell><cell>0.2764</cell><cell>0.4167</cell><cell>0.6279</cell></row><row><cell>BART</cell><cell>0.2919</cell><cell>0.1490</cell><cell>0.1617</cell><cell>0.2667</cell><cell>0.3719</cell><cell>0.6165</cell></row><row><cell>COMET</cell><cell>0.3036</cell><cell>0.1557</cell><cell>0.1580</cell><cell>0.2727</cell><cell>0.3790</cell><cell>0.6187</cell></row><row><cell>GLUCOSE-T5</cell><cell>0.2998</cell><cell>0.1612</cell><cell>0.1628</cell><cell>0.2778</cell><cell>0.4471</cell><cell>0.6294</cell></row></table><note>monsense inference dataset, the fine-tuned COMET model fails to outperform both fine-tuned T5 and BART in most of the experiments. This could be due to catastrophic forgetting triggered by dis-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 11</head><label>11</label><figDesc></figDesc><table><row><cell cols="4">: Results for chained cause effect generation. (1.1.1)* and (1.1.2)* indicates results from Task 1.1.1, and</cell></row><row><cell cols="4">1.1.2 (as in Table 10), but only for target instances which have both cause and effect annotated, ensuring a fair</cell></row><row><cell cols="4">comparison with (1.2). SE denotes Subsequent Event.</cell></row><row><cell>Model</cell><cell cols="3">Creativity Contextuality Fluency</cell></row><row><cell>Gold</cell><cell>4.7</cell><cell>4.8</cell><cell>5.0</cell></row><row><cell>T5</cell><cell>3.8</cell><cell>4.1</cell><cell>4.9</cell></row><row><cell>BART</cell><cell>3.6</cell><cell>4.3</cell><cell>4.9</cell></row><row><cell>COMET</cell><cell>3.8</cell><cell>4.1</cell><cell>4.8</cell></row><row><cell>GLUCOSE-T5</cell><cell>3.9</cell><cell>4.3</cell><cell>4.9</cell></row><row><cell>T5  *</cell><cell>2.4</cell><cell>2.1</cell><cell>1.9</cell></row><row><cell>BART  *</cell><cell>2.6</cell><cell>2.5</cell><cell>1.8</cell></row><row><cell>COMET  *</cell><cell>2.2</cell><cell>2.3</cell><cell>2.5</cell></row><row><cell>GLUCOSE-T5  *</cell><cell>1.9</cell><cell>2.1</cell><cell>2.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head></head><label></label><figDesc>Work, mainly. I'm sure I'll be able to find a job there. A (u 5 ) (u 5 ) (u 5 ): You're probably right. But where are you going to live? B (u 6 ) (u 6 ) (u 6 ): I hope I'll find a flat to share with somebody. That way it will be cheaper. A (u 7 ) (u 7 ) (u 7 ): Yes, that's a good idea. Are you taking your dog with you? B (u</figDesc><table><row><cell>A (u 1 ) (u 1 ) (u 1 ): Hi, Jenny. Is it true you're moving to London? B (u 2 ) (u 2 ) (u 2 ): Yes, it is. A (u 3 ) (u 3 ) (u 3 ): What made you decide</cell></row><row><cell>to do that? B (u 4 ) (u 4 ) (u 4 ): 8 ) (u 8 ) (u 8 ):</cell></row></table><note>.C CICERO MCQ : Extended Results, Quantitative and Qualitative Analysis For answer selection with generative models in CICERO MCQ , we train T5 and Unified QA</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 13 :</head><label>13</label><figDesc>Inferences extracted from a sample dialogue.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 14 :</head><label>14</label><figDesc>Results of the CICERO MCQ task. SE denotes subsequent event.</figDesc><table><row><cell>Single ? ? Instances with single answer.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 15 :</head><label>15</label><figDesc>Results of the CICERO MCQ task under the zero-shot setting. SE denotes subsequent event. Instance corresponding to cause, prerequisite, and emotional reaction are used for training.</figDesc><table><row><cell>Instance corresponding to</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head></head><label></label><figDesc>Hello. Is that Dr. Bean? A: Yes, it is. B: Dr. Bean, I'm making a survey for the National Research Company. I'd like to ask you a few questions about your health habits. A: OK. B: First question: How often do you take medicine? A: I sometimes take aspirin, but that's all.</figDesc><table><row><cell>A: Hello?</cell></row><row><cell>B: : Do you take vitamins?</cell></row><row><cell>A: No, I never do.</cell></row><row><cell>B: How about exercise?</cell></row><row><cell>A: Well, I often play tennis or handball.</cell></row><row><cell>B: Do you eat any healthy food?</cell></row><row><cell>A: No, I just try to eat good food.</cell></row><row><cell>B: Well, I've finished. Thank you for your help.</cell></row><row><cell>Question: What is the prerequisite of target?</cell></row><row><cell>Target: Well, I often play tennis or handball.</cell></row></table><note>B</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head></head><label></label><figDesc>Kevin would search for a new table glass. Kevin will search for a new table cloth. Kevin would search for a new table trough. Kevin would search for a new table mat. Kevin would search for a new desk lamp to replace the one he had.</figDesc><table><row><cell>MCQ using RoBerta:</cell><cell>Kevin would search for a new table glass.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head></head><label></label><figDesc>I want to go to New York by train today. Would you please look up a train time for me? A: Certainly, Hold on, please. Um... there's one at eleven p. m. It's a little late. Why don't you go there by car? B: My car is being repaired now. I have to go there by train. A: Do you think you have enough time? B: Yes. I'll try it. Thank you. Bye. My car is being repaired now. I have to go there by train.</figDesc><table><row><cell>A: Hello. This is Amy.</cell></row><row><cell>B: Hello, Amy. This is Tom.</cell></row><row><cell>A: Yes. What can I do for you?</cell></row><row><cell>Question: What is the prerequisite of target? B: Tom's car met with an accident recently. MCQ using RoBerta:</cell></row><row><cell>Generation using T5: The speaker's car is not working properly.</cell></row></table><note>Target:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head></head><label></label><figDesc>What subsequent event happens or could happen following the Target?</figDesc><table><row><cell>A: Lucy , take my picture here , OK ?</cell></row><row><cell>B: Sure . Just a minute . Let me take my camera out .</cell></row><row><cell>A: What's the matter ?</cell></row><row><cell>B: I'm not sure .</cell></row><row><cell>A: Is it broken ?</cell></row><row><cell>B: I hope not ! Oh , I see .</cell></row><row><cell>A: What is it ?</cell></row><row><cell>B: The batteries are worn down . I need replace them .</cell></row><row><cell>A: Where can we get batteries ?</cell></row><row><cell>B: All photography shops carry them .</cell></row><row><cell>A: OK . Let's take a walk and look for a shop that does .</cell></row></table><note>Question:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_30"><head></head><label></label><figDesc>Hello, Ben. You're getting ready for tomorrow's lessons, aren't you? B: Yes, but I'm a bit nervous. I have no idea what'll happen in class and how I'll get along with my classmates. A: I understand how you're feeling. Just take it easy. You'll make a lot of friends very soon. B: Thank you. I'll try my best to get used to my new school life as soon as possible. By the way, what time does the first class begin? A: At 8 o'clock. But before that we have 10 minutes to hand in homework and then 20 minutes for morning reading. B: So we must get to school before 7:30, right? A: Right. B: How long does each class last? A: 45 minutes, I think, with a 10 or 15 minutes' break. B: Well, I hear that lunchtime is nearly 12 o'clock and I'll be starving by then. A: Don't worry. During the break after the second class, we can buy something to eat. B: That's good.</figDesc><table /><note>A:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_31"><head></head><label></label><figDesc>Any messages, Miss Grey? B: Just one, Mr. Blank. You had a telephone call from someone called Brown, David Brown. A: Brown? I don't seem to know anyone called Brown. What did he say? B: He wouldn't say. But it sounded important. I told him you'd phone him as soon as you got back. A: Well, I'd better do it then, I suppose. Er...you've got his phone number, haven't you? B: Yes, it's 633201. A: 622301. B: No, 633201. A: Oh, I'd better write it down, otherwise I'll probably forget it. B: I have already done it, Mr. Blank. It's on your desk.</figDesc><table /><note>A:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_33"><head>Table 16 :</head><label>16</label><figDesc>Results of baseline models in other CSK datasets. Note: This result on CICERO using RoBERTa-large is obtained for only instances with single answer. No. But I know a Tim Smith. A: Oh, yes, you are right. It was Tim Smith I meant. You know what happened to him the other day? B: No, what happened then? A: Well, he told me he saw his dead grandfather in London. B: Oh, come on. You are not telling a ghost story, are you?</figDesc><table><row><cell>A: Do you know Tom?</cell></row><row><cell>B: Tom what?</cell></row><row><cell>A: Tom Smith.</cell></row><row><cell>B:</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
