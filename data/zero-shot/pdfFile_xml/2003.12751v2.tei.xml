<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Physics-based Noise Formation Model for Extreme Low-light Raw Denoising</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixuan</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Physics-based Noise Formation Model for Extreme Low-light Raw Denoising</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lacking rich and realistic data, learned single image denoising algorithms generalize poorly to real raw images that do not resemble the data used for training. Although the problem can be alleviated by the heteroscedastic Gaussian model for noise synthesis, the noise sources caused by digital camera electronics are still largely overlooked, despite their significant effect on raw measurement, especially under extremely low-light condition. To address this issue, we present a highly accurate noise formation model based on the characteristics of CMOS photosensors, thereby enabling us to synthesize realistic samples that better match the physics of image formation process. Given the proposed noise model, we additionally propose a method to calibrate the noise parameters for available modern digital cameras, which is simple and reproducible for any new device. We systematically study the generalizability of a neural network trained with existing schemes, by introducing a new low-light denoising dataset that covers many modern digital cameras from diverse brands. Extensive empirical results collectively show that by utilizing our proposed noise formation model, a network can reach the capability as if it had been trained with rich real data, which demonstrates the effectiveness of our noise formation model. arXiv:2003.12751v2 [eess.IV] 9 Apr 2020</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Light is of paramount importance to photography. Night and low light place very demanding constraints on photography due to limited photon count and inescapable noise. The natural reaction is to gather more light by, e.g., enlarging aperture setting, lengthening exposure time and opening flashlight. However, each method is a tradeoff -large aperture incurs small depth of field, and is unavailable in smartphone cameras; long exposure can induce blur due to scene variations or camera motions; flash can cause color aberrations and is useful only for nearby objects.</p><p>A practical rescue for low-light imaging is to use burst capturing <ref type="bibr" target="#b47">[46,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b43">42,</ref><ref type="bibr" target="#b41">40]</ref>, in which a burst of images are aligned and fused to increase the signal-to-noise ratio * Corresponding author: fuying@bit.edu.cn  <ref type="bibr" target="#b8">[9]</ref>, where we present (a) the short-exposure noisy input image; (f) the long-exposure reference image; (b-e) the outputs of UNets <ref type="bibr" target="#b53">[51]</ref> trained with (b) synthetic data generated by the homoscedastic Gaussian noise model (G), (c) synthetic data generated by the signal-dependent heteroscedastic Gaussian noise model (G+P) <ref type="bibr" target="#b21">[22]</ref>, (d) paired real data of <ref type="bibr" target="#b8">[9]</ref>, and (e) synthetic data generated by our proposed noise model respectively. All images were converted from raw Bayer space to sRGB for visualization; similarly hereinafter.</p><p>(SNR). However, burst photography can be fragile, suffering from ghosting effect <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b59">56]</ref> when capturing dynamic scenes in the presence of vehicles, humans, etc. An emerging alternative approach is to employ a neural network to automatically learn the mapping from a low-light noisy image to its long-exposure counterpart <ref type="bibr" target="#b8">[9]</ref>. However, such a deep learning approach generally requires a large amount of labelled training data that resembles low-light photographs in the real world. Collecting rich high-quality training samples from diverse modern camera devices is tremendously labor-intensive and expensive. In contrast, synthetic data is simple, abundant and inexpensive, but its efficacy is highly contingent upon how accurate the adopted noise formation model is. The heteroscedastic Gaussian noise model <ref type="bibr" target="#b21">[22]</ref>, instead of the commonly-used homoscedastic one, approximates well the real noise occurred in daylight or moderate low-light settings <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b28">28]</ref>. However, it cannot delineate the full picture of sensor noise under severely low illuminance. An illustrative example is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, where the objec-tionable banding pattern artifacts, an unmodeled noise component that is exacerbated in dim environments, become clearly noticeable by human eyes.</p><p>In this paper, to avoid the effect on noise model from the image processing pipeline (ISP) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b47">46]</ref> converting raw data to sRGB, we mainly focus on the noise formation model for raw images. We propose a physics-based noise formation model for extreme low-light raw denoising, which explicitly leverages the characteristics of CMOS photosensors to better match the physics of noise formation. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, our proposed synthetic pipeline derives from the inherent process of electronic imaging by considering how photons go through several stages. It models sensor noise in a fine-grained manner that includes many noise sources such as photon shot noise, pixel circuit noise, and quantization noise. Besides, we provide a method to calibrate the noise parameters from available digital cameras. In order to investigate the generality of our noise model, we additionally introduce an extreme low-light denoising (ELD) dataset taken by various camera devices to evaluate our model. Extensive experiments show that the network trained only with the synthetic data from our noise model can reach the capability as if it had been trained with rich real data.</p><p>Our main contributions can be summarized as follows:</p><p>? We formulate a noise model to synthesize realistic noisy images that can match the quality of real data under extreme low-light conditions.</p><p>? We present a noise parameter calibration method that can adapt our model to a given camera.</p><p>? We collect a dataset with various camera devices to verify the effectiveness and generality of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Noise removal from a single image is an extensivelystudied yet still unresolved problem in computer vision and image processing. Single image denoising methods generally rely on the assumption that both signal and noise exhibit particular statistical regularities such that they can be separated from a single observation. Crafting an analytical regularizer associated with image priors (e.g. smoothness, sparsity, self-similarity, low rank), therefore, plays a critical role in traditional design pipeline of denoising algorithms <ref type="bibr" target="#b54">[52,</ref><ref type="bibr" target="#b49">48,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b44">43,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b26">26]</ref>. In the modern era, most single image denoising algorithms are entirely data-driven, which consist of deep neural networks that implicitly learn the statistical regularities to infer clean images from their noisy counterparts <ref type="bibr" target="#b56">[53,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b46">45,</ref><ref type="bibr" target="#b64">61,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b60">57,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b27">27]</ref>. Although simple and powerful, these learning-based approaches are often trained on synthetic image data due to practical constraints. The most widely-used additive, white, Gaussian noise model deviates strongly from realistic evaluation scenarios, resulting in significant performance declines on photographs with real noise <ref type="bibr" target="#b51">[49,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>To step aside the domain gap between synthetic images and real photographs, some works have resorted to collecting paired real data not just for evaluation but for training <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b57">54,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b33">33]</ref>. Notwithstanding the promising results, collecting sufficient real data with ground-truth labels to prevent overfitting is exceedingly expensive and time-consuming. Recent works exploit the use of paired (Noise2Noise <ref type="bibr" target="#b38">[38]</ref>) or single (Noise2Void <ref type="bibr" target="#b37">[37]</ref>) noisy images as training data instead of paired noisy and noise-free images. However, they can not substantially ease the burden of labor requirements for capturing a massive amount of real-world training data.</p><p>Another line of research has focused on improving the realism of synthetic training data to circumvent the difficulties in acquiring real data from cameras. By considering both photon arrival statistics ("shot" noise) and sensor readout effects ("read" noise), the works of <ref type="bibr" target="#b47">[46,</ref><ref type="bibr" target="#b4">5]</ref> employed a signal-dependent heteroscedastic Gaussian model <ref type="bibr" target="#b21">[22]</ref> to characterize the noise properties in raw sensor data. Most recently, Wang et al. <ref type="bibr" target="#b62">[59]</ref> proposes a noise model, which considers the dynamic streak noise, color channel heterogeneous and clipping effect, to simulate the high-sensitivity noise on real low-light color images. Concurrently, a flow-based generative model, namely Noiseflow <ref type="bibr" target="#b0">[1]</ref> is proposed to formulate the distribution of real noise using latent variables with tractable density 1 . However, these approaches oversimplify the modern sensor imaging pipeline, especially the noise sources caused by camera electronics, which have been extensively studied in the electronic imaging community <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b61">58,</ref><ref type="bibr" target="#b13">14]</ref>. In this work, we propose a physics-based noise formation model stemming from the essential process of electronic imaging to synthesize the noisy dataset and show that sizeable improvements of denoising performance on real data, particularly under extremely low illuminance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Physics-based Noise Formation Model</head><p>The creation of a digital sensor raw image D can be generally formulated by a linear model</p><formula xml:id="formula_0">D = KI + N,<label>(1)</label></formula><p>where I is the number of photoelectrons that is proportional to the scene irradiation, K represents the overall system gain composed by analog and digital gains, and N denotes the summation of all noise sources physically caused by light or camera. We focus on the single raw image denoising problem under extreme low-light conditions. In this context, the characteristics of N are formated in terms of the sensor physical process beyond the existing noise models. Deriving an optimal regularizer to tackle such noise is infeasible, as there is no analytical solver for such a noise distribution 2 . Therefore, we rely on a learning-based neural network pipeline to implicitly learn the regularities from data. Creating training samples for this task requires careful considerations of the characteristics of raw sensor data.</p><p>In the following, we first describe the detailed procedures of the physical formation of a sensor raw image as well as the noise sources introduced during the whole process. An overview of this process is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Sensor Raw Image Formation</head><p>Our photosensor model is primarily based upon the CMOS sensor, which is the dominating imaging sensor nowadays <ref type="bibr" target="#b52">[50]</ref>. We consider the electronic imaging pipeline of how incident light is converted from photons to electrons, from electrons to voltage, and finally from voltage to digital numbers, to model noise. From Photon to Electrons. During exposure, incident lights in the form of photons hit the photosensor pixel area, which liberates photon-generated electrons (photoelectrons) proportional to the light intensity. Due to the quantum nature of light, there exists an inevitable uncertainty in the number of electrons collected. Such uncertainty imposes a Poisson distribution over this number of electrons, which follows</p><formula xml:id="formula_1">(I + N p ) ? P (I) ,<label>(2)</label></formula><p>where N p is termed as the photon shot noise and P denotes the Poisson distribution. This type of noise depends on the light intensity, i.e., on the signal. Shot noise is a fundamental limitation and cannot be avoided even for a perfect sensor. There are other noise sources introduced during the photon-to-electron stage, such as photo response nonuniformity and dark current noise, reported by many previous literatures <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b61">58,</ref><ref type="bibr" target="#b2">3]</ref>. Over the last decade, technical advancements in CMOS sensor design and fabrication, e.g., on-sensor dark current suppression, have led to a new generation of digital single lens reflex (DSLR) cameras with lower dark current and better photo response uniformity <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b42">41]</ref>. Therefore, we assume a constant photo response and absorb the effect of dark current noise N d into read noise N read , which will be presented next. From Electrons to Voltage. After electrons are collected at each site, they are typically integrated, amplified and read out as measurable charge or voltage at the end of exposure time. Noise present during the electrons-to-voltage stage depends on the circuit design and processing technology used, and thus is referred to as pixel circuit noise <ref type="bibr" target="#b24">[25]</ref>. It includes thermal noise, reset noise <ref type="bibr" target="#b36">[36]</ref>, source follower noise <ref type="bibr" target="#b39">[39]</ref> and banding pattern noise <ref type="bibr" target="#b24">[25]</ref>. The physical origin of these noise components can be found in the electronic imaging literatures <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b61">58,</ref><ref type="bibr" target="#b39">39]</ref>. For instance, source follower noise is attributed to the action of traps in silicon lattice which randomly capture and emit carriers; banding pattern noise is associated with the CMOS circuit readout pattern and the amplifier. By leveraging this knowledge, we consider the thermal noise N t , source follower noise N s and banding pattern noise N b in our model. The noise model of N b will be presented later. Here, we absorb multiple noise sources into a unified term, i.e. read noise</p><formula xml:id="formula_2">N read = N d + N t + N s .<label>(3)</label></formula><p>Read noise can be assumed to follow a Gaussian distribution, but the analysis of noise data (in Section 3.2) tells a long-tailed nature of its shape. This can be attributed by the flicker and random telegraph signal components of source follower noise <ref type="bibr" target="#b24">[25]</ref>, or the dark spikes raised by dark current <ref type="bibr" target="#b36">[36]</ref>. Therefore, we propose using a statistical distribution that can better characterize the long-tail shape. Specifically, we model the read noise by a Tukey lambda distribution (T L) <ref type="bibr" target="#b34">[34]</ref>, which is a distributional family that can approximate a number of common distributions (e.g., a heavy-tailed Cauchy distribution):</p><formula xml:id="formula_3">N read ? T L (?; 0, ? T L ) ,<label>(4)</label></formula><p>where ? and ? indicate the shape and scale parameters respectively, while the location parameter is set to be zero given the zero-mean noise assumption. Banding pattern noise N b appears in images as horizontal or vertical lines. We only consider the row noise component (horizontal stripes) in our model, as the column noise component (vertical stripes) is generally negligible when measuring the noise data (Section 3.2). We simulate the row noise N r by sampling a value from a zero-mean Gaussian distribution with a scale parameter ? r , then adding it as an offset to the whole pixels within a single row. From Voltage to Digital Numbers. To generate an image that can be stored in a digital storage medium, the analog voltage signal read out during last stage is quantized into discrete codes using an ADC. This process introduces quantization noise N q given by</p><formula xml:id="formula_4">N q ? U (?1/2q, 1/2q) ,<label>(5)</label></formula><p>where U (?, ?) denotes the uniform distribution over the range [?1/2q, 1/2q] and q is the quantization step.</p><p>To summarize, our noise formation model consists of four major noise components:</p><formula xml:id="formula_5">N = KN p + N read + N r + N q ,<label>(6)</label></formula><p>where K, N p , N read , N r and N q denotes the overall system gain, photon shot noise, read noise, row noise and quantization noise, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Sensor Noise Evaluation</head><p>In this section, we present a noise parameter calibration method attached to our proposed noise formation model. According to Eq. (2) (4) (6), the necessary parameters to specify our noise model include overall system gain K for photon shot noise N p ; shape and scale parameters (? and ? T L ) for read noise N read ; scale parameter ? r for row noise N r . Given a new camera, our noise calibration method consists of two main procedures, i.e. (1) estimating noise parameters at various ISO settings 3 , and (2) modeling joint distributions of noise parameters. Estimating noise parameters. We record two sequences of raw images to estimate K and other noise parameters: flat-field frames and bias frames.</p><p>Flat-field frames are the images captured when sensor is uniformly illuminated. They can be used to derive K according to the Photon Transfer method. <ref type="bibr" target="#b32">[32]</ref>Once we have K, we can firstly convert a raw digital signal D into the number of photoelectrons I, then impose a Poisson distribution on it, and finally revert it to D -this simulates realistic photon shot noise.</p><p>Bias frames are the images captured under a lightless environment with the shortest exposure time. We took them at a dark room and the camera lens was capped on. Bias frames delineate the read noise picture independent of light, blended by the multiple noise sources aforementioned. The banding pattern noise can be tested via performing discrete Fourier transform on a bias frame. In <ref type="figure" target="#fig_2">Fig.3</ref>, the highlighted vertical pattern in the centralized Fourier spectrum reveals the existence of row noise component. To analyze the distribution of row noise, we extract the mean values of each row from raw data. These values, therefore, serve as good estimates to the underlying row noise intensities, given the zero-mean nature of other noise sources. The normality of the row noise data is tested by a Shapiro-Wilk test <ref type="bibr" target="#b58">[55]</ref>: the resulting p-value is higher than 0.05, suggesting the null hypothesis that the data are normally distributed cannot be rejected. The related scale parameter ? r can be easily estimated by maximizing the log-likelihood.</p><p>After subtracting the estimated row noise from a bias frame, statistical models can be used to fit the empirical distribution of the residual read noise. A preliminary diagnosis ( <ref type="figure" target="#fig_3">Fig. 4</ref> Left) shows the main body of the data may follow a Gaussian distribution, but it also unveils the long-tail nature of the underlying distribution. In contrast to regarding extreme values as outliers, we observe an appropriate long-tail statistical distribution can characterize the noise data better.</p><p>We generate a probability plot correlation coefficient (PPCC) plot <ref type="bibr" target="#b19">[20]</ref> to identify a statistical model from a Tukey lambda distributional family <ref type="bibr" target="#b34">[34]</ref> that best describes the data. The Tukey lambda distribution is a family of distributions that can approximate many distributions by varying its shape parameter ?. It can approximate a Gaussian distribution if ? = 0.14, or derive a heavy-tailed distribution if ? &lt; 0.14. The PPCC plot ( <ref type="figure" target="#fig_3">Fig. 4</ref> Middle) is used to find a good value of ?. The probability plot <ref type="bibr" target="#b63">[60]</ref>  <ref type="figure" target="#fig_3">(Fig. 4</ref> Right) is then employed to estimate the scale parameter ? T L . The goodness-of-fit can be evaluated by R 2 -the coefficient of determination w.r.t. the resulting probability plot <ref type="bibr" target="#b48">[47]</ref>. The R 2 of the fitted Tukey Lambda distribution is much higher than the Gaussian distribution (e.g., 0.972 vs. 0.886), indicating a much better fit to the empirical data.</p><p>Although we use a unified noise model for different cam-  eras, the noise parameters estimated from different cameras are highly diverse. <ref type="figure" target="#fig_3">Figure 4</ref> shows the selected optimal shape parameter ? differs camera by camera, implying distributions with varying degree of heavy tails across cameras. The visual comparisons of real and simulated bias frames are shown in <ref type="figure">Fig. 5</ref>. It shows that our model is capable of synthesizing realistic noise across various cameras, which outperforms the Gaussian noise model both in terms of the goodness-of-fit measure (i.e., R 2 ) and the visual similarity to real noise.</p><p>Modeling joint parameter distributions. To choose noise parameters for our noise formation model, we infer the joint distributions of (K, ? T L ) and (K, ? r ), from the parameter samples estimated at various ISO settings. As shown in <ref type="figure" target="#fig_4">Fig. 6</ref>, we use the linear least squares method to find the line of best fit for two sets of log-scaled measurements. Our noise parameter sampling procedure is</p><formula xml:id="formula_6">log (K) ? U log(K min ), log(K max ) , log (? T L ) | log (K) ? N (a T L log(K) + b T L ,? T L ) , (7) log (? r ) | log (K) ? N (a r log(K) + b r ,? r ) ,</formula><p>where U (?, ?) denotes a uniform distribution and N (?, ?) denotes a Gaussian distribution with mean ? and standard  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Extreme Low-light Denoising (ELD) Dataset</head><p>To systematically study the generality of the proposed noise formation model, we collect an extreme low-light denoising (ELD) dataset that covers 10 indoor scenes and 4 camera devices from multiple brands (SonyA7S2, NikonD850, CanonEOS70D, CanonEOS700D). We also record bias and flat field frames for each camera to calibrate our noise model. The data capture setup is shown in <ref type="figure" target="#fig_5">Fig. 7</ref>. For each scene and each camera, a reference image at the base ISO was firstly taken, followed by noisy images whose exposure time was deliberately decreased by low light factors f to simulate extreme low light conditions. Another reference image then was taken akin to the first one, to ensure no accidental error (e.g. drastic illumination change or accidental camera/scene motion) occurred. We choose three ISO levels (800, 1600, 3200) <ref type="bibr" target="#b3">4</ref> and two low light factors (100, 200) for noisy images to capture our dataset, resulting in 240 (3?2?10?4) raw image pairs in total. The hardest example in our dataset resembles the image captured at a "pseudo" ISO up to 640000 (3200?200).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental setting</head><p>Implementation details. A learning-based neural network pipeline is constructed to perform low-light raw denoising. We utilize the same U-Net architecture <ref type="bibr" target="#b53">[51]</ref> as <ref type="bibr" target="#b8">[9]</ref>. Raw Bayer images from SID Sony training dataset <ref type="bibr" target="#b8">[9]</ref> are used to create training data. We pack the raw Bayer images into four channels (R-G-B-G) and crop non-overlapped 512 ? 512 regions augmented by random flipping/rotation. Our approach only use the clean raw images, as the paired noisy images are generated by the proposed noise model on-the-fly. Besides, we also train networks based upon other training schemes as references, including training with paired real data (short exposure and long exposure counterpart) and training with paired real noisy images (i.e., Noise2Noise <ref type="bibr" target="#b38">[38]</ref>).</p><p>Our implementation 5 is based on PyTorch. We train the models with 200 epoch using L 1 loss and Adam optimizer <ref type="bibr" target="#b35">[35]</ref> with batch size 1. The learning rate is initially set to 10 ?4 , then halved at epoch 100, and finally reduced to 10 ?5 at epoch 180.</p><p>Competing methods. To understand how accurate our proposed noise model is, we compare our method with:</p><p>1. The approaches that use real noisy data for training, i.e. "paired real data" <ref type="bibr" target="#b8">[9]</ref>  <ref type="bibr" target="#b5">6</ref> and Noise2Noise <ref type="bibr" target="#b38">[38]</ref>; 2. Previous noise models, i.e. homoscedastic (G) and heteroscedastic Gaussian noise models (G+P) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21]</ref>; 3. The representative non-deep methods, i.e. BM3D <ref type="bibr" target="#b14">[15]</ref> and Anscombe-BM3D (A-BM3D) <ref type="bibr" target="#b45">[44]</ref> 7 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results on SID Sony dataset</head><p>Single image raw denoising experiment is firstly conducted on images from SID Sony validation and test sets. For quantitative evaluation, we focus on indoor scenes illuminated by natural lights, to avoid flickering effect of alternating current lights <ref type="bibr" target="#b1">[2]</ref>  <ref type="bibr" target="#b7">8</ref> . To account for the imprecisions of shutter speed and analog gain <ref type="bibr" target="#b1">[2]</ref>, a single scalar is calculated and multiplied into the reconstructed image to minimize the mean square error evaluated by the ground truth. Ablation study on noise models. To verify the efficacy of the proposed noise model, we compare the performance of networks trained with different noise models developed in Section 3.1. All noise parameters are calibrated using the ELD dataset, and sampled with a process following (or similar to) Eq. <ref type="bibr" target="#b6">(7)</ref>. The results of the other methods described in Section 5.1 are also presented as references.</p><p>As shown in <ref type="table" target="#tab_1">Table 1</ref>, the domain gap is significant between the homoscedastic/heteroscedastic Gaussian models and the de facto noise model (characterized by the model trained with paired real data). This can be attributed to <ref type="bibr" target="#b0">(1)</ref> the Gaussian approximation of Possion distribution is not justified under extreme low illuminance; (2) horizontal bandings are not considered in the noise model; (3) longtail nature of read noise is overlooked. By taking all these factors into account, our final model, i.e. G * +P * +R+U gives rise to a striking result: the result is comparable to or sometimes even better than the model trained with paired real data. Besides, training only with real low-light noisy data is not effective enough, due to the clipping effects (that violates the zero-mean noise assumption) and the large variance of corruptions (that leads to a large variance of the Noise2Noise solution) <ref type="bibr" target="#b38">[38]</ref>. A visual comparison of our final model and other methods is presented in <ref type="figure" target="#fig_6">Fig. 8</ref>, which  shows the effectiveness of our noise formation model.</p><p>Though we only quantitatively evaluate the results on indoor scenes of the SID Sony set, our method can be applied to outdoor scenes as well. The visual comparisons of both indoor and outdoor scenes from SID Sony set are presented in <ref type="figure">Fig. 9</ref>. It can be seen that the random noise can be sup-   pressed by the model learned with heteroscedastic Gaussian noise (G+P) <ref type="bibr" target="#b21">[22]</ref>, but the resulting colors are distorted, the banding artifacts become conspicuous, and the image details are barely discernible. By contrast, our model produces visually appealing results as if it had been trained with paired real data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results on our ELD dataset</head><p>Method comparisons. To see whether our noise model can be applicable to other camera devices as well, we assess model performance on our ELD dataset. <ref type="table" target="#tab_2">Table 2</ref> and <ref type="figure" target="#fig_0">Fig. 10</ref> summarize the results of all competing methods. It can be seen that the non-deep denoising methods, i.e. BM3D and A-BM3D, fail to address the banding residuals, the color bias and the extreme values presented in the noisy input, whereas our model recovers vivid image details which can be hardly perceived on the noisy image by human observers. Moreover, our model trained with synthetic data even often outperforms the model trained with paired real data. We note the finding here conforms with the evaluation of sensor noise presented in Section 3.2, especially in <ref type="figure" target="#fig_3">Fig. 4</ref> and 5, where we show the underlying noise distribution varies camera by camera. Consequently, training with paired real data from SID Sony camera inevitably overfits to the noise pattern merely existed on the Sony camera, leading to suboptimal results on other types of cameras. In contrast, our model relies on a very flexible noise model and a noise calibration process, making it adapts to noise characteristics of other (calibrated) camera models as well. Additional evidence can be found in <ref type="figure" target="#fig_0">Fig. 11</ref>, where we apply these two models to an image captured by a smartphone camera. Our reconstructed image is clearer and cleaner than what is re- Training with more synthesized data. A useful merit of our approach against the conventional training with paired real data, is that our model can be easily incorporated with more real clean samples to train. <ref type="figure" target="#fig_0">Fig. 12(a)</ref> shows the relative improvements of our model when training with the dataset synthesized by additional clean raw images from MIT5K dataset <ref type="bibr" target="#b6">[7]</ref>. We find the major improvements, as shown in <ref type="figure" target="#fig_0">Fig. 13</ref>, are owing to the more accurate color and brightness restoration. By training with more raw image samples from diverse cameras, the network learns to infer picture appearances more naturally and precisely.</p><p>Sensitivity to noise calibration. Another benefit of our approach is we only need clean samples and a noise calibration process to adapt to a new camera, in contrast to capturing real noisy images accompanied with densely-labeled ground truth. Besides, the noise calibration process can be simplified once we already have a collection of parameter samples from various cameras. <ref type="figure" target="#fig_0">Fig. 12(b)</ref> shows models can reach comparable performance on target cameras without noise calibration, by simply sampling parameters from other three calibrated cameras instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have presented a physics-based noise formation model together with a noise parameter calibration method to help resolve the difficulty of extreme low-light denoising. We revisit the electronic imaging pipeline and investigate the influential noise sources overlooked by existing noise models. This enables us to synthesize realistic noisy raw data that better match the underlying physical process of noise formation. We systematically study the efficacy of our noise formation model by introducing a new dataset that covers four representative camera devices. By training only with our synthetic data, we demonstrate a convolutional neural network can compete with or sometimes even outperform the network trained with paired real data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An image from the See-in-the-Dark (SID) Dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overview of electronic imaging pipeline and visualization of noise sources and the resulting image at each stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Centralized Fourier spectrum of bias frames captured by SonyA7S2 (left) and (right) NikonD850 cameras</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Distribution fitting of read noise for SonyA7S2 (top) and NikonD850 (bottom) cameras. Left: probability plot against the Gaussian distribution; Middle: Tukey lambda PPCC plot that determines the optimal ? (shown in red line); Right: probability plot against the Tukey Lambda distribution. A higher R 2 indicates a better fit. (Best viewed with zoom)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Linear least squares fitting from estimated noise parameter samples (blue dots) from a NikonD850 camera. Left and right figures show the joint distributions of (K, ?T L) and (K, ?r) respectively, where we sample the noise parameters from the blue shadow regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Capture setup and example images from our dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>(a) Noise2Noise (b) Paired real data (c) Ground Truth (d) G (e) G+P (f) G * +P * +R+U Visual result comparison of different training schemes. Our final model (G * +P * +R+U ) suppresses the "purple" color shift, residual bandings and chroma artifacts compared to other baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Raw image denoising results on both indoor and outdoor scenes from SID Sony dataset. (Best viewed with zoom) Raw image denoising results on our ELD dataset. (Best viewed with zoom)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>Denoising results of a low-light image captured by a Huawei Honor 10 camera.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>(a) Performance boost when training with more synthesized data. (b) Noise parameter sensitivity test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 :</head><label>13</label><figDesc>Denoising results of a low-light image captured by a NikonD850 camera. stored by the model trained with paired real data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Quantitative Results on Sony set of the SID dataset. The noise models are indicated as follows. G: the Gaussian model for read noise N read ; G * : the tukey lambda model for N read ; P : the Gaussian approximation for photon shot noise Np; P * : the true Poisson model for Np; R: the Gaussian model for row noise Nr; U : the uniform distribution model for quantization noise Nq. The best results are indicated by red color and the second best results are denoted by blue color. min andK max are the estimated overall system gains at the minimum and maximum ISO of a camera respectively. a and b indicate the fitted line's slope and intercept respectively.? is an unbiased estimator of standard deviation of the linear regression under the Gaussian error assumption. For shape parameter ?, we simply sample it from the empirical distribution of the estimated parameter samples. Noisy image synthesis. To synthesize noisy images, clean images are chosen and divided by low light factors sampled uniformly from [100, 300] to simulate low photon count in the dark. Noise is then generated and added to the scaled clean samples, according to Eq. (6)<ref type="bibr" target="#b6">(7)</ref>. The created noisy images are finally normalized by multiplying the same low light factors to expose bright but excessively noisy contents.</figDesc><table><row><cell></cell><cell>?100</cell><cell>?250</cell><cell>?300</cell></row><row><cell>Model</cell><cell cols="3">PSNR / SSIM PSNR / SSIM PSNR / SSIM</cell></row><row><cell>BM3D</cell><cell>32.92 / 0.758</cell><cell>29.56 / 0.686</cell><cell>28.88 / 0.674</cell></row><row><cell>A-BM3D</cell><cell>33.79 / 0.743</cell><cell>27.24 / 0.518</cell><cell>26.52 / 0.558</cell></row><row><cell>Paired real data</cell><cell>38.60 / 0.912</cell><cell>37.08 / 0.886</cell><cell>36.29 / 0.874</cell></row><row><cell>Noise2Noise</cell><cell>37.42 / 0.853</cell><cell>33.48 / 0.725</cell><cell>32.37 / 0.686</cell></row><row><cell>G</cell><cell>36.10 / 0.800</cell><cell>31.87 / 0.640</cell><cell>30.99 / 0.624</cell></row><row><cell>G+P</cell><cell>37.08 / 0.839</cell><cell>32.85 / 0.697</cell><cell>31.87 / 0.665</cell></row><row><cell>G+P  *</cell><cell>38.31 / 0.884</cell><cell>34.39 / 0.765</cell><cell>33.37 / 0.730</cell></row><row><cell>G  *  +P  *</cell><cell>39.10 / 0.911</cell><cell>36.46 / 0.869</cell><cell>35.69 / 0.855</cell></row><row><cell>G  *  +P  *  +R</cell><cell>39.23 / 0.912</cell><cell>36.89 / 0.877</cell><cell>36.01 / 0.864</cell></row><row><cell>G  *  +P  *  +R+U</cell><cell>39.27 / 0.914</cell><cell>37.13 / 0.883</cell><cell>36.30 / 0.872</cell></row><row><cell>deviation ?.K</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Quantitative results (PSNR/SSIM) of different methods on our ELD dataset containing four representative cameras.</figDesc><table><row><cell>Camera</cell><cell>f</cell><cell>Index</cell><cell cols="2">Non-deep</cell><cell cols="2">Training with real data</cell><cell cols="3">Training with synthetic data</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">BM3D [15] A-BM3D [44] Paired data [9] Noise2Noise [38]</cell><cell>G</cell><cell>G+P [22]</cell><cell>Ours</cell></row><row><cell>SonyA7S2</cell><cell>?100 ?200</cell><cell>PSNR SSIM PSNR SSIM</cell><cell>37.69 0.803 34.06 0.696</cell><cell>37.74 0.776 35.26 0.721</cell><cell>44.50 0.971 42.45 0.945</cell><cell>41.63 0.856 37.98 0.775</cell><cell>42.35 0.893 38.93 0.813</cell><cell>42.46 0.889 38.88 0.812</cell><cell>45.36 0.972 43.27 0.949</cell></row><row><cell>NikonD850</cell><cell>?100 ?200</cell><cell>PSNR SSIM PSNR SSIM</cell><cell>33.97 0.725 31.36 0.618</cell><cell>36.60 0.779 32.59 0.723</cell><cell>41.28 0.938 39.44 0.910</cell><cell>40.47 0.848 37.98 0.820</cell><cell>39.57 0.823 36.68 0.757</cell><cell>40.29 0.845 37.26 0.786</cell><cell>41.79 0.912 39.69 0.875</cell></row><row><cell>CanonEOS70D</cell><cell>?100 ?200</cell><cell>PSNR SSIM PSNR SSIM</cell><cell>30.79 0.589 28.06 0.540</cell><cell>31.88 0.692 28.66 0.597</cell><cell>40.10 0.931 37.32 0.867</cell><cell>38.21 0.826 34.33 0.704</cell><cell>40.59 0.925 37.49 0.871</cell><cell>40.94 0.934 37.64 0.873</cell><cell>40.62 0.937 38.17 0.890</cell></row><row><cell>CanonEOS700D</cell><cell>?100 ?200</cell><cell>PSNR SSIM PSNR SSIM</cell><cell>29.70 0.556 27.52 0.537</cell><cell>30.13 0.640 27.68 0.579</cell><cell>39.05 0.906 36.50 0.850</cell><cell>38.29 0.859 34.94 0.766</cell><cell>39.77 0.884 37.67 0.870</cell><cell>40.08 0.897 37.86 0.879</cell><cell>39.84 0.921 37.59 0.879</cell></row><row><cell>Input</cell><cell cols="2">BM3D</cell><cell>A-BM3D</cell><cell>G</cell><cell>G+P</cell><cell cols="3">Noise2Noise Paired real data</cell><cell>Ours</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that Noiseflow requires paired real data to obtain noise data (by subtracting the ground truth images from the noisy ones) for training.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Even if each noise component has an analytical formulation, their summation can generally be intractable.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Noise parameters are generally stationary at a fixed ISO.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Most modern digital cameras are ISO-invariant when ISO is set higher than 3200<ref type="bibr" target="#b12">[13]</ref>.<ref type="bibr" target="#b4">5</ref> Code is released at https://github.com/Vandermode/NoiseModel 6<ref type="bibr" target="#b8">[9]</ref> used paired real data to perform raw-to-sRGB low-light image processing. Here we adapt its setting to raw-to-raw denoising.<ref type="bibr" target="#b6">7</ref> The noise level parameters required are provided by the off-the-shelf image noise level estimators<ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b10">11]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Alternating current light is not noise, but a type of illumination that breaks the irradiance constancy between short/long exposure pairs, making the quantitative evaluation inaccurate.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments We thank Tianli Tao for the great help in collecting the ELD dataset. This work was partially supported by the National Natural Science Foundation of China under Grants No. 61425013 and No. 61672096.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Noise flow: Noise modeling with conditional normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Abdelhamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><forename type="middle">A</forename><surname>Brubaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A high-quality denoising dataset for smartphone cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Abdelhamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A model for dark current characterization and simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">L</forename><surname>Baer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE -The International Society for Optical Engineering</title>
		<meeting>SPIE -The International Society for Optical Engineering</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6068</biblScope>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An analysis of camera noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingemar</forename><forename type="middle">J</forename><surname>Boie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="671" to="674" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unprocessing images for learned raw denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dillon</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11036" to="11045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartomeu</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning photographic global tonal adjustment with a database of input / output image pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Bychkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?do</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Seeing motion in the dark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to see in the dark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep boosting for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An efficient statistical method for image noise level estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengyuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On learning optimized reaction diffusion processes for effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Exposure and digital cameras, part 1: What is iso on a digital camera? when is a camera isoless? iso myths and digital cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><forename type="middle">N</forename><surname>Clark</surname></persName>
		</author>
		<ptr target="http://www.clarkvision.com/articles/iso/" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Virtual sensor design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Costantini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE -The International Society for Optical Engineering</title>
		<meeting>SPIE -The International Society for Optical Engineering</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5301</biblScope>
			<biblScope unit="page" from="408" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-d transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostadin</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sparsity-based image denoising via dictionary learning and structural clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weisheng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangming</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="457" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cmos image sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbas</forename><forename type="middle">El</forename><surname>Gamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmy</forename><surname>Eltoukhy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Circuits and Devices Magazine</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="6" to="20" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image denoising via sparse and redundant representations over learned dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Aharon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3736" to="3745" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sensor calibration and simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joyce</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manu</forename><surname>Parmar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE -The International Society for Optical Engineering</title>
		<meeting>SPIE -The International Society for Optical Engineering</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The probability plot correlation coefficient test for normality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Filliben</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="111" to="117" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Clipped noisy images: Heteroskedastic modeling and practical denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="2609" to="2629" />
		</imprint>
	</monogr>
	<note>Signal Processing</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Practical poissonian-gaussian noise modeling and fitting for single-image rawdata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mejdi</forename><surname>Trimeche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1737" to="1754" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A review of the pinned photodiode for ccd and cmos image sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">R</forename><surname>Fossum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><forename type="middle">B</forename><surname>Hondongwa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of the Electron Devices Society</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="33" to="43" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep joint demosaicking and denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?do</forename><surname>Durand</surname></persName>
		</author>
		<idno>191:1- 191:12</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">D</forename><surname>Gow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Renshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Findlater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lindsay</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">J</forename><surname>Mcleod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A comprehensive tool for modeling cmos image-sensor-noise performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nicol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Electron Devices</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1321" to="1329" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Weighted nuclear norm minimization with application to image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangchu</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Toward convolutional blind denoising of real photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifei</forename><surname>Shi Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Burst photography for high dynamic range and low-light imaging on mobile cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dillon</forename><surname>Hasinoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Geiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">192</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Radiometric ccd camera calibration and noise estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><forename type="middle">E</forename><surname>Healey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghava</forename><surname>Kondepudy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="267" to="276" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A model for measurement of noise in ccd digital-video cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">E</forename><surname>Mckinnon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Unsworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">M</forename><surname>Woodhead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Measurement Science and Technology</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="334" to="340" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A technique for evaluation of ccd video-camera noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">E</forename><surname>Mckinnon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Unsworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">M</forename><surname>Woodhead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="280" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ccd charge collection efficiency and the photon transfer technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Janesick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Klaasen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Elliott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE -The International Society for Optical Engineering</title>
		<meeting>SPIE -The International Society for Optical Engineering</meeting>
		<imprint>
			<date type="published" when="1985" />
			<biblScope unit="volume">570</biblScope>
			<biblScope unit="page" from="7" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to see moving objects in the dark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinqiang</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV), October</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Some properties of the range in samples from tukey&apos;s symmetric lambda distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">L</forename><surname>Joiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><forename type="middle">R</forename><surname>Rosenblatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Publications of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">334</biblScope>
			<biblScope unit="page" from="394" to="399" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">High-level numerical simulations of noise in ccd and cmos photosensors: review and tutorial</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mikhail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James S</forename><surname>Konnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welsh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.4031</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Noise2void -learning denoising from single noisy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim-Oliver</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Jug</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Noise2noise: Learning image restoration without clean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Munkberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Hasselgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2971" to="2980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cedric</forename><surname>Leyris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Valenza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Trap competition inducing r.t.s noise in saturation range in n-mosfets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vildeuil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE -The International Society for Optical Engineering</title>
		<meeting>SPIE -The International Society for Optical Engineering</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">5844</biblScope>
			<biblScope unit="page" from="41" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Handheld mobile photography in very low light</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orly</forename><surname>Liba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiran</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Ta</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Karnad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiurui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dillon</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Geiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">High performance cmos light detector with dark current suppression in variable-temperature systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wensheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoming</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyunlong</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fast burst images denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Uyttendaele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename><surname>Jian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sparse representation for color image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="69" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Optimal inversion of the anscombe transformation in low-count poisson image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markku</forename><surname>Makitalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="109" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Image restoration using very deep convolutional encoderdecoder networks with symmetric skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiao</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Bin</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2802" to="2810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Burst denoising with kernel prediction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dillon</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Probability distributions for offshore wind speeds. Energy Conversion and Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><forename type="middle">C</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lackner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">M</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurie</forename><forename type="middle">G</forename><surname>Baise</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="15" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">An iterative regularization method for total variation-based image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Goldfarb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wotao</forename><surname>Yin</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Multiscale Modeling and Simulation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="460" to="489" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Benchmarking denoising algorithms with real photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Plotz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Grand View Research. Image sensors market analysis</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Nonlinear total variation based noise removal algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><forename type="middle">I</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emad</forename><surname>Fatemi</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Physica D Nonlinear Phenomena</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Shrinkage fields for effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deepisp: Learning end-to-end image processing pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">An approximate analysis of variance test for normality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Francia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">337</biblScope>
			<biblScope unit="page" from="215" to="216" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Human-aware motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiankai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingfa</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Noise modeling for design and simulation of computational imaging systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Wach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">R</forename><surname>Dowski</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE -The International Society for Optical Engineering</title>
		<meeting>SPIE -The International Society for Optical Engineering</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5438</biblScope>
			<biblScope unit="page" from="159" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Enhancing low light videos by exploring high sensitivity camera noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuemei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Probability plotting methods for the analysis of data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Wilk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gnanadesikan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
