<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Loss Function Entropy Regularization for Diverse Decision Boundaries</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sue</forename><forename type="middle">Sin</forename><surname>Chong</surname></persName>
							<email>z18516937829@163.com</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Interdisciplinary Information Sciences</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Loss Function Entropy Regularization for Diverse Decision Boundaries</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>2</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Unsupervised Learning</term>
					<term>Diversified Deci- sion Boundaries</term>
					<term>Entropy Regularization</term>
					<term>Feature Set</term>
					<term>Loss Function</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Is it possible to train several classifiers to perform meaningful crowd-sourcing to produce a better prediction label set without ground-truth annotation? This paper will modify the contrastive learning objectives to automatically train a self-complementing ensemble to produce a state-of-the-art prediction on the CIFAR10 and CIFAR100-20 tasks. This paper will present a straightforward method to modify a single unsupervised classification pipeline to automatically generate an ensemble of neural networks with varied decision boundaries to learn a more extensive feature set of classes. Loss Function Entropy Regularization (LFER) are regularization terms to be added to the pre-training and contrastive learning loss functions. LFER is a gear to modify the entropy state of the output space of unsupervised learning, thereby diversifying the latent representation of decision boundaries of neural networks. Ensemble trained with LFER has higher successful prediction accuracy for samples near decision boundaries. LFER is an adequate gear to perturb decision boundaries and has produced classifiers that beat stateof-the-art at the contrastive learning stage. Experiments show that LFER can produce an ensemble with accuracy comparable to the state-of-the-art yet have varied latent decision boundaries. It allows us to perform meaningful verification for samples near decision boundaries, encouraging the correct classification of near-boundary samples. By compounding the probability of correct prediction of a single sample amongst an ensemble of neural network trained, our method can improve upon a single classifier by denoising and affirming correct feature mappings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>How to make unsupervised algorithms converge and learn different features at will at the same time has always been difficult. Recently, a combination of the representation learning method and end-to-end learning has given promising results in the coarse classification of reasonably large datasets such as CIFAR100 <ref type="bibr" target="#b0">[1]</ref>. SCAN's <ref type="bibr" target="#b0">[1]</ref> three-step process involves two contrastive learning stages and a fine-tuning stage.</p><p>Recently, <ref type="bibr" target="#b1">[2]</ref> works on feature selection in unsupervised learning. <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> works on making the pretext training phase of unsupervised learning more robust. The properties of similarity search vectors and application in unsupervised image classification task has also been much studied in <ref type="bibr" target="#b5">[5]</ref>, <ref type="bibr" target="#b6">[6]</ref>. The integration of noise and features into the unsupervised learning pipeline has also proven to be useful <ref type="bibr" target="#b7">[7]</ref>. <ref type="bibr" target="#b8">[8]</ref>  <ref type="bibr" target="#b9">[9]</ref> has studied the possibility Published in International Conferernce on Big Data Analytics 2022 in Guangzhou, China <ref type="bibr">(ICBDA 2022)</ref> of searching in a quantized sparse space representation. <ref type="bibr" target="#b10">[10]</ref> has studied the momentum of the unsupervised clustering process. Recently, <ref type="bibr" target="#b11">[11]</ref> introduced a simple contrastive learning framework. Whereas <ref type="bibr" target="#b12">[12]</ref> makes the contrastive learning stage robust. Asano's work <ref type="bibr" target="#b13">[13]</ref> is a fine-tuning technique that has proven to be very effective in the final stages of unsupervised learning. In contrast, Vo's work <ref type="bibr" target="#b14">[14]</ref> attempts to find new objects or features as an optimization objective.</p><p>Entropy regularization is an important technique in machine learning and has many applications. Several studies <ref type="bibr" target="#b15">[15]</ref>, <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b17">[17]</ref> utilized entropy regularization enhance unsupervised learning, entropy regularization can either maximize marginal entropy of bits or speedup classification. In searching, <ref type="bibr" target="#b18">[18]</ref> also utilized entropy regularization on one-hot codes. Whereas <ref type="bibr" target="#b20">[19]</ref> has studied the mass-spring-damper system without a singular kernel, which might be able to play the role of controller in unsupervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Related Work</head><p>Out-of-Distribution (OOD) Detection is an increasingly prominent field in machine learning. <ref type="bibr" target="#b21">[20]</ref> is a method for generating a state-of-the-art OOD detector with the adversarial method. <ref type="bibr" target="#b22">[21]</ref> used a method based on Gaussian to extract features from a trained neural network to train an OOD detector with relatively simple tools. Via <ref type="bibr" target="#b22">[21]</ref>, one can link the certainty of features learnt to OOD classifier accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Motivation</head><p>If we can have a check to see whether an unsupervised learner has learned a feature, that would be great. The check serves as a fill-in-the-blank check for an unsupervised classifier. It will allow us to paint a class as a set of more specific and more refined and, at the same time, correct features instead of having to accept a class as a very generic template of features.</p><p>Secondly, the ability to separate weakly-classified samples from well-classified samples only by using a similarly-unsupervised-trained neural network or pipeline allows the unsupervised learner to self-check and improve. When it is hard to differentiate, it will be beneficial to be able to have neural networks trained with varied decision boundaries to vote on a particular sample. This motivates us to modify the entropy configuration of output space to train neural networks with varied decision boundaries to help partition the dataset arXiv:2205.00224v2 [cs.</p><p>LG] 23 May 2022 into easy-to-classify and possibly hard-to-classify samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PRELIMINARIES</head><p>We attempt to introduce the possibility of searching in the space of decision boundaries in the unsupervised image classification task. This work attempts to combine the best representation learning, end-to-end learning, and the properties of spreading vector and entropy control to give users a simple yet intuitive way to explore and exploit the entropy dimension in the output space of contrastive learning. This paper presents a straightforward method for training reasonably good ensembles from a single pipeline with diverse decision boundaries and can learn varied latent representations of the dataset. We think the unsupervised exploration of decision boundaries under varied entropy configurations can improve the unsupervised classification process. We present a framework where we can generate an ensemble that has varied yet complementing decision boundaries simply by changing constants. Our objective is to maximize the number of possibilities in output space entropy distribution of converging neural networks, which allows for a large variety of decision boundaries and, therefore, a highly diversified set of learned latent representations of class features. LFER is a set of regularization terms added to unsupervised learning objective functions, which will enable unsupervised neural network latent representation exploration. We offer a simple-to-implement yet sensitive gear, allowing for unsupervised learning that exploits differences in the latent representation of decision boundaries to improve accuracy. We have shown that generating an ensemble from an arbitrary deep learning architecture and a dataset (pipeline) is simple and has improved accuracy compared to the best neural network possibly trained from the same pipeline.</p><p>The main contributions of our paper are as follows:</p><p>? We identified a method to exploit entropy in decision boundary formation in the unsupervised classification problem, the LFER method. ? LFER is simple to implement and use and almost always guarantees to train a neural network with varied decision boundaries from the same dataset from a single architecture. Without LFER, unsupervised learning always converges to the same set of features with the exact decision boundaries, reproducibly. ? LFER method can produce networks with classification accuracy compared to state-of-the-art, but with varied latent decision boundaries. It can also serve as the uniqueness of convergence check on an unsupervised classification pipeline. ? Ensemble trained with LFER can meaningfully encourage prediction for samples which lie near decision boundaries. ? On the CIFAR100-20 task, our ensemble can capture a more extensive super-class feature set. ? LFER can mine for neural networks, which are better out-of-distribution detectors.</p><p>In the following section, we will discuss the definition and implications of solving the problem of diversified unsupervised learning with LFER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROBLEM DEFINITION Given an unsupervised classification pipeline</head><p>Pipeline, how can we automatically train an ensemble that can improve prediction accuracy, where each neural network in the ensemble is independently a reasonable classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Applications</head><p>LFER is a natural filter for weakly-classified samples in unsupervised learning. With a series of neural networks, which each converge to a different set of correct features, it can immediately identify weakly-classified samples. Secondly, LFER checks for convergence in features learned in unsupervised classification pipelines. When the dataset has multiple ways of matching different features to the same classes, LFER can list the different mappings between features and classes. Finally, it serves as a neighbourhood exploration tool that allows for searching with constants on some arbitrary entropy structure in the output space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implications</head><p>When a Pipeline (machine learning architecture + dataset) is sufficient to produce a reasonable classifier by unsupervised learning on a dataset, by using LFER, we can exhaust the feature discovery possibilities of the target Pipeline. Furthermore, suppose there exists any subclass within a superclass. In that case, the LFER will effectively mine for sub-class representations in the original dataset. Finally, there will very likely exist a neural network that converges to an alternative set of features; it serves as a checker for the uniqueness of feature convergence in an unsupervised classification process.</p><p>The first section will discuss the LFER method on the Unsupervised Semantic Clustering pipeline. At the same time, the second section will discuss reasoning about entropy regularization and the convergence of neural networks trained with LFER. The third section will discuss the implications of finding complementing neural networks which have learnt different features of the same dataset. The fourth section will discuss the application of combinations of neural networks with different latent decision boundaries. Finally, this paper will conclude with the applications and implications of using LFER as an output space entropy controlling tool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. LOSS FUNCTION ENTROPY REGULARIZATION</head><p>(LFER) Loss Function Entropy Regularization (LFER) is a series of entropy regularization terms added to the contrastive learning loss functions. LFER is added to the contrastive learning and pre-training stages of unsupervised classification to encourage different decision boundaries and entropy distribution in the output space, thereby resulting in neural networks with similar accuracy but different latent representations of features. Experiments show that training neural networks with different confusion matrices is impossible without LFER. The implementation of LFER merely requires an additional ten lines of code for the loss function.</p><formula xml:id="formula_0">(?? 0 , ?? 1 , +? 2 , ?? 3 ) ( ? c ? log ? c ? , c?C ? c ? log ? c ? , c?C k?N X ? c ? ? k ? log ? c ? ? k ? , c?C k?N X ? c ? ? k ? log ? c ? ? k ? ) , ? c ? = 1 |D| X?D ? c ? (X))<label>(1)</label></formula><p>Entropy terms were previously added in semantic clustering to encourage uniform prediction amongst the contrastive learning stage classes. Our experiment shows that regularization in unsupervised classification objective functions is indispensable for identifying a more extensive set of features, ensemble trained with LFER demonstrates an improvement in the latent representation of features of the neural network.</p><p>Higher-order entropy terms play the role of the controller in controlling the distances between clusters in the output space. Hence it is possible to produce neural networks with varying confusion matrices. Moreover, it is possible to both maximize or minimize between-cluster spaces and the smoothing of the clustering process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. LFER Acting Upon Unsupervised Semantic Clustering</head><p>There are 3 portions to unsupervised classification in the <ref type="bibr" target="#b0">[1]</ref>, SimCLR, Semantic Clustering(SCAN), and Selflabel(SLL).SCAN being contrastive learning stages. Adding entropy terms in the objective functions of contrastive learning stages can improve the diversity of neural networks trained. The regularization term in SimCLR is as follows.</p><formula xml:id="formula_1">min ? d(? ? (X i ), ? ? (T [X i ])) ? ? 0 ? ? (X i ), ? ? (T [X i ]) log ? ? (X i ), ? ? (T [X i ])<label>(2)</label></formula><p>The regularization term in the SCAN portion of the pipeline is as follows. ? 2 and ? 3 plays the role of a spring term and damper term for between-clusters entropy.</p><formula xml:id="formula_2">? = ? 1 |D| X?D k?N X log ? ? (X), ? ? (k) + ? 1 ? c ? log ? c ? ? ? 2 c?C k?N X ? c ? ? k ? log ? c ? ? k ? + ? 3 c?C k?N X ? c ? ? k ? log ? c ? ? k ? , ? c ? = 1 |D| X?D ? c ? (X)<label>(3)</label></formula><p>The scalar terms (? i ) i?0 = f (classes) , is a function of number of classes. Below is a table of scalar terms and the classification accuracy of different lambda combinations. The new optimization function presents a control function related to the number of classes to modify the entropy in the input space. Hence we can specify a specific behaviour desired for the output neural network, and then simply by modifying relative values of (? i ) i?1 train a converging network with the desired property.  Reflecting the sensitivity of spring constant and dampening ratio in a physical spring system, neural networks trained with objective functions with regularization terms that are scalar multiples of each other can also have significantly different decision boundaries. We can also choose to improve the performance of a single classifier in the contrastive learning stage.</p><p>1) Reasoning About Entropy State: By optimizing for entropy exploration, we add the constraint of the number of classes to the output space, forcing different values and structures of classes in the trained neural network. Furthermore, we have introduced an abridged notation to reason about the resulting state of entropy. It will allow for the convenience of solving for deducing the stability of the training process and the properties of the output neural network. The 3-step unsupervised classification process is essentially a cascading function of the entropy exploration and exploitation of the dataset. Let g(x) be the pretext regularization function. Similarly, we define the following for SCAN.</p><formula xml:id="formula_3">(? i ) i?1 Value Notation ? 0 0 g(x) = x ? 0 &gt; 0 g(x) =x</formula><formula xml:id="formula_4">? = ? 1 |D| X?D k?N X log ? ? (X), ? ? (k) + ? 2 x ? ? 3 x + ? 4 x<label>(4)</label></formula><p>Let ? 2 x ? ? 3 x + ? 4 x = h(x) be the SCAN regularization function. Then compounding the two regularization functions, the state of entropy resulting from the training process can be expressed as h(g(x)). Hence, the contrastive training process maximizes similarity with a second-order differential equation of entropy, offering fine-grained knobs to the desired output entropy state and the decision boundary formation within clusters. LFER serves as a control function for entropy in the contrastive learning stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pseudo Entropy Control Functions</head><p>(? i ) i?1 is a function of number of classes, aka (? i ) i?1 = f i?1 (n). Expressing the entropy control function as an inner product between a series of functions of number of classes, we have the following.</p><p>h(g(x)) = h(n, g(x))</p><formula xml:id="formula_5">= ? 1 g(x) ? ? 2 g(x) + ? 3 g(x) = f 1 (n)g(x) ? f 2 (n)g(x) + f 3 (n)g(x) = f i?1 (n), g(x)<label>(5)</label></formula><p>Self-labelling, which attempts to classify noisy neighbours near clusters by minimizing cross-entropy loss correctly, is the only step in the pipeline that optimizes for cross-entropy. LFER makes it possible to systematically produce converging neural networks with different decision boundaries simply by modifying (f i (n)) n?1 . Experiments show that neural networks trained with different objective functions are most confident about a wide variety of prototype images for each super-class. LFER offers a unique edge to the task of CIFAR100-20, as it allows for the mining of more subclass prototypes within a super-class, thereby improving overall classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Searching for Constants to Train the Best Performing Neural Network</head><p>In contrastive learning stages, LFER has consistently been able to mine for neural networks that beat state-ofthe-art by 1 ? 2% at the end of the contrastive learning stage on the CIFAR10 and CIFAR100-20 classification task. However, the slight edge gained does not persist through the self-labelling stage. LFER can also produce neural networks that train better out-of-distribution classifiers on the same dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ENTROPY PERTURBATION FOR DECISION BOUNDARIES</head><p>LFER is necessary to train neural networks to learn different features. Experiment shows that repeated training on the model without any regularization term results in a similar confusion matrix. The terms force different structures on the neural network decision boundaries, which results in differences in the confusion matrix.</p><p>LFER is a sensitive gear for modifying the entropy environment/configuration in output space. In this section, we will discuss an example ensemble, trained with ? 3 <ref type="figure">= {4, 8, 16</ref>, 32}. To demonstrate that LFER can produce neural networks that learn a different set of features at ease, we introduce the notion of n guess accuracy.</p><p>The top K accuracy is calculated as follows. Given n neural networks, if any n neural network predicts the label correctly, it is calculated as a correct prediction. There is no hierarchy or preference to the set of prediction labels produced by the set of n neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Self-complementing Ensemble</head><p>The ? 3 term is an effective knob for controlling the fine-grained cluster formation process. It is easy to train a set of neural networks simply by multiplying the ? 3 with a geometric series. By changing ? 3 , we are almost certain to find a neural network which has learnt a different set of correct features.   We sieve for confident samples with majority votes from the ensemble for at least three-quarters of the samples within the ensemble. We can further clamp down on samples that all networks do not agree on, aka samples with very high confusion. Below is an image of the most confident prototypes of an LFER ensemble. This ensemble has learnt the many sub-classes within the 20 superclasses of the CIFAR100 dataset. For instance, in the large carnivores superclass, neural networks in the ensemble have learnt bear, tiger, leopard and lion, respectively, as their most confident prototype for the same superclass. <ref type="figure">Fig. 1: Figure 1</ref>. Different Prototype Images of Superclasses LFER can mine for a more comprehensive representation of features of classes when there are many subclasses within a super-class. When sub-classes differ drastically, the advantage will be prominent. 4 neural networks are sufficient for verification checks for weakly-classified samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. MORE ENTROPY CONFIGURATIONS</head><p>It is also possible to explore the entropy space further with even larger classes of LFER functions. We note that lambdas templates which result in converging neural networks, correspond to spring damper constants which result in spring-damper systems with forced harmonic motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Extended Stability</head><p>Experimentation has proven that the following lambda bounds will produce optimization functions that can produce a converging neural network. The number of possible LFER functions which can train converging neural networks in the contrastive learning stage is vast.  Decision boundaries of a neural network trained are very sensitive to small changes in (? i ) i?1 . Therefore, it is reasonable to expect to find neural networks that can identify all other subclass prototypes in the superclass by enumerating the set of ?s. With LFER, it is possible to have multiple neural networks which are confident about more sub-classes within the superclass to learn the sub-classes within a superclass.</p><formula xml:id="formula_6">(? i ) i?1 Big O Bound ? 0 O(1) ? 1 O(1) ? 2 O(n) ? 3 O(n (n))</formula><formula xml:id="formula_7">? 0 ? 1 ? 2 ? 3 0 O(1) 0 0 0 O(1) O(1) 0 O(1) O(1) O(1) 0 O(1) O(1) O(1) O(1) O(1) O(1) O( ? n) 0 O(1) O(1) 0 O(n) O(1) O(1) O( ? n) O( ? n) O(1) O(1) O(1) O( 1 n ) O(1) O(n) O( 1 n ) O(n ? n)</formula><p>We observed that the accuracy of a single classifier in CIFAR20 could hardly surpass 0.5 due to limits in the number of classes and the lack of labels. However, having multiple networks learning the 20 classes in CIFAR100-20 makes it possible to have different neural networks learn the features of smaller classes, which results in a better superclass classification accuracy.  Each classifier is independently an at least &gt; 0.40 accuracy classifier. Moreover, &gt; 0.85 of the classifiers show at least 0.1 improvement in classification guess. When we compound the LFER of different classes, it is much more likely to make the n guess of the neural networks more robust. Below is a listing of some of the best combinations of neural networks. Some of the best combinations often involve neural networks with non-zero ? 0 , ? 2 , ? 3 , demonstrating the indispensability of the LFER in training neural networks with diverse decision boundaries.   In the CIFAR100-20 task, there are five subclasses in each superclass; there are multiple ways of mapping sub-classes to a superclass. LFER mines the input dataset for multiple sub-classes within the superclass. By compounding 2 to 3 neural networks whereby each network learns a subclass within a superclass, the n guess predictions will, in expectation, make reasonable guesses which will include each subclass.  1) Learning Sub-classes Within Super-class: With the LFER, we can mine for different prototypical images within a super-class in the CIFAR100-20 task. Instead of being forced to view a super-class as a set of nondescriptive generic features, LFER can mine for decision boundaries which respect specific subclass features within a super-class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Combined Ensembles</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. MAJORITY VOTE</head><p>Training several neural networks and having them vote on classes of a sample can produce state-of-theart prediction accuracy. N guess accuracy is the upper bound of majority vote accuracy. We can improve upon the accuracy of a single prediction set by compounding the accuracy of several classifiers. Our best combination, which involves 27 different neural networks voting for the label of every single sample, reaches a new state-ofthe-art accuracy rate of 0.58.</p><p>Experiment results demonstrate that the fuzziness induced by LFER allows for meaningful verification checks across neural networks trained from the same architecture. We also observed that dividing the classifiers into three tiers according to their accuracy rate and organising voting amongst neural networks drawn from each of the tiers more easily produce an accuracy rate higher than randomly selected neural networks.  Neural networks trained with LFER have different latent representations of decision boundaries. The knobs presented by (? i ) i?1 are fine-grained decision boundaries gear for modifying the entropy environment of output space, which influences decision boundary formation. LFER presents the possibility of using simple search techniques on ?s to automatically mine-train neural ensembles with varied decision boundaries by exploiting the differences in a wide array of latent representations of decision boundaries about a single dataset. As a result, ensembles trained with LFER can often beat the accuracy of the best neural network possible for a particular architecture on a dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Prototype Images for 2 Guess Combined Ensembles Best 3 combinations often have confident prototypes drawn from a larger variety of sub-classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Prototype Images for 3 Guess Combined EnsemblesWhile there may be confusion across super-classes, each subclass is only represented once in each classifier confident prototype. Combinations in aggregation learn to be confident about a large set of sub-classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc></figDesc><table /><note>Experiment Results</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc></figDesc><table /><note>Entropy Regularization State Notation</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>Top 2 ACC Of a ? 3 Series The 2-ensemble agrees on samples in which they are both confident and only disagrees on samples where there is confusion or lies near the decision boundaries. With LFER, we can identify classes and samples where there are second opinions. Therefore it can serve as a filtering method for noisy samples and noisy classes.</figDesc><table><row><cell cols="3">3/4 guess ACC 2-agreement ? 0</cell><cell>? 1</cell><cell>? 2</cell><cell>? 3</cell></row><row><cell>69.35</cell><cell>75.86</cell><cell>2</cell><cell>5</cell><cell>4</cell><cell>4</cell></row><row><cell>1</cell><cell></cell><cell>2</cell><cell>5</cell><cell>4</cell><cell>8</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell>5</cell><cell>4</cell><cell>32</cell></row><row><cell>67.80</cell><cell>83.97</cell><cell>2</cell><cell>5</cell><cell>4</cell><cell>4</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell>5</cell><cell>4</cell><cell>8</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell>5</cell><cell>4</cell><cell>16</cell></row><row><cell>67.49</cell><cell>82.50</cell><cell>2</cell><cell>5</cell><cell>4</cell><cell>4</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell>5</cell><cell>4</cell><cell>16</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell>5</cell><cell>4</cell><cell>32</cell></row><row><cell>71.92</cell><cell>NA</cell><cell>2</cell><cell>5</cell><cell>4</cell><cell>4</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell>5</cell><cell>4</cell><cell>8</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell>5</cell><cell>4</cell><cell>16</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell>5</cell><cell>4</cell><cell>32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV :</head><label>IV</label><figDesc></figDesc><table /><note>3/4 Guess Of a ? 3 Series</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V :</head><label>V</label><figDesc></figDesc><table /><note>Big O Bounds for f (n)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI :</head><label>VI</label><figDesc>Stable Templates</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VII :</head><label>VII</label><figDesc></figDesc><table /><note>Top K Performances</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE VIII</head><label>VIII</label><figDesc></figDesc><table><row><cell></cell><cell cols="5">: Top 2 ACC of Combined Ensembles</cell></row><row><cell></cell><cell cols="2">Top 3 ACC ? 0</cell><cell>? 1</cell><cell>? 2</cell><cell>? 3</cell></row><row><cell>1</cell><cell>70.79</cell><cell>0 2 2</cell><cell>5 5 5</cell><cell>0 4 1/4n</cell><cell>0 0 0.5/ ? n</cell></row><row><cell></cell><cell>70.66</cell><cell>0</cell><cell>5</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell>5</cell><cell>4</cell><cell>0</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell>5</cell><cell>4</cell><cell>4</cell></row><row><cell></cell><cell>70.60</cell><cell>0 2</cell><cell>5 5</cell><cell>0 1/4n</cell><cell>0 0.5/ ? n</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell>5</cell><cell>1/4n 2</cell><cell>1/2n</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE IX :</head><label>IX</label><figDesc>Top 3 ACC of Combined Ensembles</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE X :</head><label>X</label><figDesc>State-of-the-Art Results on CIFAR100-20 by Majority Voting VIII. CONCLUSION</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scan: Learning to classify images without labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Preserving ordinal consensus: Towards feature selection for unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artificial Intell. (AAAI)</title>
		<meeting>AAAI Conf. Artificial Intell. (AAAI)<address><addrLine>New York City; New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-02" />
			<biblScope unit="page" from="75" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretextinvariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Leveraging large-scale uncurated data for unsupervised pre-training of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<ptr target="http://arxiv.org/abs/1905.01278" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Spreading vectors for similarity search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning spreadout local feature descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<idno>abs/1708.06320</idno>
		<ptr target="http://arxiv.org/abs/1708.06320" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Unsupervised learning by predicting noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Approximate search with quantized sparse representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zepeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sparse composite quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4548" to="4556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Debiased contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yen-Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00224</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Ym</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised image matching and object discovery as optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8287" to="8296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Regularizing neural networks by penalizing confident output distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Sinkhorn distances: Lightspeed computation of optimal transportation distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep hashing for compact binary codes learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">E</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2475" to="2483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">SUBIC: A supervised, structured binary code for image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zepeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1708.02932</idno>
		<ptr target="http://arxiv.org/abs/1708.02932" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Modeling of a mass-spring-damper system by fractional derivatives with and without a singular kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gomez-Aguilar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="6289" to="6303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generative probabilistic novelty detection with adversarial autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pidhorskyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Almohsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6822" to="6833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A simple unified framework for detecting out-of-distribution samples and adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
