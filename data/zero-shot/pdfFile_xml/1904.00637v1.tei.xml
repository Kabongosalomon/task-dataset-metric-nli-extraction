<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Single Image Reflection Removal Exploiting Misaligned Training Data and Network Enhancements</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixuan</forename><surname>Wei</surname></persName>
							<email>kaixuanwei@bit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Fu</surname></persName>
							<email>fuying@bit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wipf</surname></persName>
							<email>davidwip@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Huang</surname></persName>
							<email>huahuang@bit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Single Image Reflection Removal Exploiting Misaligned Training Data and Network Enhancements</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Removing undesirable reflections from a single image captured through a glass window is of practical importance to visual computing systems. Although state-of-theart methods can obtain decent results in certain situations, performance declines significantly when tackling more general real-world cases. These failures stem from the intrinsic difficulty of single image reflection removal -the fundamental ill-posedness of the problem, and the insufficiency of densely-labeled training data needed for resolving this ambiguity within learning-based neural network pipelines. In this paper, we address these issues by exploiting targeted network enhancements and the novel use of misaligned data. For the former, we augment a baseline network architecture by embedding context encoding modules that are capable of leveraging high-level contextual clues to reduce indeterminacy within areas containing strong reflections. For the latter, we introduce an alignment-invariant loss function that facilitates exploiting misaligned real-world training data that is much easier to collect. Experimental results collectively show that our method outperforms the state-ofthe-art with aligned data, and that significant improvements are possible when using additional misaligned data. arXiv:1904.00637v1 [cs.CV]  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Reflection is a frequently-encountered source of image corruption that can arise when shooting through a glass surface. Such corruptions can be addressed via the process of single image reflection removal (SIRR), a challenging problem that has attracted considerable attention from the computer vision community <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b37">38]</ref>. Traditional optimization-based methods often leverage manual intervention or strong prior assumptions to render the problem more tractable <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25]</ref>. Recently, alternative learningbased approaches rely on deep Convectional Neural Networks (CNNs) in lieu of the costly optimization and handcrafted priors <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b37">38]</ref>. But promising results notwith-standing, SIRR remains a largely unsolved problem across disparate imaging conditions and varying scene content.</p><p>For CNN-based reflection removal, our focus herein, the challenge originates from at least two sources: (i) The extraction of a background image layer devoid of reflection artifacts is fundamentally ill-posed, and (ii) Training data from real-world scenes, are exceedingly scarce because of the difficulty in obtaining ground-truth labels.</p><p>Mathematically speaking, it is typically assumed that a captured image I is formed as a linear combination of a background or transmitted layer T and a reflection layer R, i.e., I = T + R. Obviously, when given access only to I, there exists an infinite number of feasible decompositions. Further compounding the problem is the fact that both T and R involve content from real scenes that may have overlapping appearance distributions. This can make them difficult to distinguish even for human observers in some cases, and simple priors that might mitigate this ambiguity are not available except under specialized conditions.</p><p>On the other hand, although CNNs can perform a wide variety visual tasks, at times exceeding human capabilities, they generally require a large volume of labeled training data. Unfortunately, real reflection images accompanied with densely-labeled, ground-truth transmitted layer intensities are scarce. Consequently, previous learning-based approaches have resorted to training with synthesized images <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b47">48]</ref> and/or small real-world data captured from specialized devices <ref type="bibr" target="#b47">[48]</ref>. However, existing image synthesis procedures are heuristic and the domain gap may jeopardize accuracy on real images. On the other hand, collecting sufficient additional real data with precise ground-truth labels is tremendously labor-intensive. This paper is devoted to addressing both of the aforementioned challenges. First, to better tackle the intrinsic ill-posedness and diminish ambiguity, we propose to leverage a network architecture that is sensitive to contextual information, which has proven useful for other vision tasks such as semantic segmentation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b12">13]</ref>. Note that at a high level, our objective is to efficiently convert prior information mined from labeled training data into network structures capable of resolving this ambiguity. Within a traditional CNN model, especially in the early layers where the effective receptive field is small, the extracted features across all channels are inherently local. However, broader non-local context is necessary to differentiate those features that are descriptive of the desired transmitted image, and those that can be discarded as reflection-based. For example, in image neighborhoods containing a particularly strong reflection component, accurate separation by any possible method (even one trained with arbitrarily rich training data) will likely require contextual information from regions without reflection. To address this issue, we utilize two complementary forms of context, namely, channel-wise context and multi-scale spatial context. Regarding the former, we apply a channel attention mechanism to the feature maps from convolutional layers such that different features are weighed differently according to global statistics of the activations. For the latter, we aggregate information across a pyramid of feature map scales within each channel to reach a global contextual consistency in the spatial domain. Our experiments demonstrate that significant improvement can be obtained by these enhancements, leading to state-of-the-art performance on two real-image datasets.</p><p>Secondly, orthogonal to architectural considerations, we seek to expand the sources of viable training data by facilitating the use of misaligned training pairs, which are considerably easier to collect. Misalignment between an input image I and a ground-truth reflection-free version T can be caused by camera and/or object movements during the acquisition process. In the previous works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b46">47]</ref>, data pairs (I, T ) were obtained by taking an initial photo through a glass plane, followed by capturing a second one after the glass has been removed. This process requires that the camera, scene, and even lighting conditions remain static. Adhering to these requirements across a broad acquisition campaign can significantly reduce both the quantity and diversity of the collected data. Additionally, post-processing may also be necessary to accurately align I and T to compensate for spatial shifts caused by the refractive effect <ref type="bibr" target="#b36">[37]</ref>. In contrast, capturing unaligned data is considerably less burdensome, as shown in <ref type="figure">Fig. 1</ref>. For example, there is no need for a tripod, table, or other special hardware; the camera can be hand-held and the pose can be freely adjusted; dynamic scenes in the presence of vehicles, humans, etc. can be incorporated; and finally no post-processing of any type is needed.</p><p>To handle such misaligned training data, we require a loss function that is, to the extent possible, invariant to the alignment, i.e., the measured image content discrepancy between the network prediction and its unaligned reference should be similar to what would have been observed if the reference was actually aligned. In the context of image style transfer <ref type="bibr" target="#b16">[17]</ref> and others, certain perceptual loss <ref type="bibr" target="#b46">[47]</ref> Ours <ref type="figure">Figure 1</ref>: Comparison of the reflection image data collection methods in <ref type="bibr" target="#b46">[47]</ref> and this paper.</p><p>functions have been shown to be relatively invariant to various transformations. Our study shows that the using only the highest-level feature from a deep network (VGG-19 in our case) leads to satisfactory results for our reflection removal task. In both simulation tests and experiments using a newly collected dataset, we demonstrate for the first time that training/fine-tuning a CNN with unaligned data improves the reflection removal results by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This paper is concerned with reflection removal from a single image. Previous methods utilizing multiple input images of, e.g., flash/non-flash pairs <ref type="bibr" target="#b0">[1]</ref>, different polarization <ref type="bibr" target="#b19">[20]</ref>, multi-view or video sequences <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b45">46]</ref> will not be considered here.</p><p>Traditional methods. Reflection removal from a single image is a massively ill-posed problem. Additional priors are needed to solve the otherwise prohibitively-difficult problem in traditional optimization-based method <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b35">36]</ref>. In <ref type="bibr" target="#b21">[22]</ref>, user annotations are used to guide layer separation jointly with a gradient sparsity prior <ref type="bibr" target="#b22">[23]</ref>. <ref type="bibr" target="#b24">[25]</ref> introduces a relative smoothness prior where the reflections are assumed to be blurry thus their large gradients are penalized. <ref type="bibr" target="#b38">[39]</ref> explores a variant of the smoothness prior where a multi-scale Depth-of-Field (DoF) confidence map is utilized to perform edge classification. <ref type="bibr" target="#b30">[31]</ref> exploits the ghost cues for layer separation. <ref type="bibr" target="#b1">[2]</ref> proposes a simple optimization formulation with an l 0 gradient penalty on the transmitted layer inspired by image smoothing algorithms <ref type="bibr" target="#b41">[42]</ref>. Despite decent results can be obtained by these methods where their assumptions hold, the vastly-different imaging conditions and complex scene content in the real world render their generalization problematic.</p><p>Deep learning based methods. Recently, there is an emerging interest in applying deep convolutional neural networks for single image reflection removal such that the handcrafted priors can be replaced by data-driven learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b44">45]</ref>. The first CNN-based method is due to <ref type="bibr" target="#b4">[5]</ref>, where a network structure is proposed to first pre-dict the background layer in the edge domain followed by reconstructing it the color domain. Later, <ref type="bibr" target="#b37">[38]</ref> proposes to predict the edge and image intensity concurrently by two cooperative sub-networks. The recent work of <ref type="bibr" target="#b44">[45]</ref> presents a cascade network structure which predicts the background layer and reflection layer in an interleaved fashion. The earlier CNN-based methods typical use the raw image intensity discrepancy such as mean squared error (MSE) to train the networks. Several recent works <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b2">3]</ref> adopt the perceptual loss <ref type="bibr" target="#b16">[17]</ref> which uses the multi-stage features of a deep network pre-trained on ImageNet <ref type="bibr" target="#b28">[29]</ref>. <ref type="bibr" target="#b47">[48]</ref>. Adversarial loss is investigated in <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b20">21]</ref> to improve the realism of the predicted background layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Given an input image I contaminated with reflections, our goal is to estimate a reflection-free trasmitted imageT . To achieve this, we train a feed-forward CNN G ? G parameterized by ? G to minimize a reflection removal loss function l. Given training image pairs {(I n , T n )}, n = 1, ? ? ? , N , this involves solving:</p><formula xml:id="formula_0">? G = arg min ? G 1 N N n=1 l(G ? G (I n ), T n ).<label>(1)</label></formula><p>We will first introduce the details of network architecture G ? G followed by the loss function l applied to both aligned data (the common case) and newly proposed unaligned data extensions. The overall system is illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Basic Image Reconstruction Network</head><p>Our starting point can be viewed as the basic image reconstruction neural network component from <ref type="bibr" target="#b4">[5]</ref> but modified in three aspects: (1) We simplify the basic residual block <ref type="bibr" target="#b11">[12]</ref> by removing the batch normalization (BN) layer <ref type="bibr" target="#b13">[14]</ref>; <ref type="bibr" target="#b1">(2)</ref> we increase the capacity by widening the network from 64 to 256 feature maps; and (3) for each input image I, we extract hypercolumn features <ref type="bibr" target="#b9">[10]</ref> from a pretrained VGG-19 network <ref type="bibr" target="#b31">[32]</ref>, and concatenate these features with I as an augmented network input. As explained in <ref type="bibr" target="#b47">[48]</ref>, such an augmentation strategy can help enable the network to learn semantic clues from the input image.</p><p>Note that removing the BN layer from our network turns out to be critical for optimizing performance in the present context. As shown in <ref type="bibr" target="#b40">[41]</ref>, if batch sizes become too small, prediction errors can increase precipitously and stability issues can arise. Moreover, for a dense prediction task such as SIRR, large batch sizes can become prohibitively expensive in terms of memory requirements. In our case, we found that within the tenable batch sizes available for reflection removal, BN led to considerably worse performance, including color attenuation/shifting issues as sometimes observed in image-to-image translation tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b49">50]</ref>. BN layers have similarly been removed from other dense prediction tasks such as image super-resolution <ref type="bibr" target="#b25">[26]</ref> or deblurring <ref type="bibr" target="#b27">[28]</ref>.</p><p>At this point, we have constructed a useful base architecture upon which other more targeted alterations will be applied shortly. This baseline, which we will henceforth refer to as BaseNet, performs quite well when trained and tested on synthetic data. However, when deployed on realworld reflection images we found that its performance degraded by an appreciable amount, especially on the 20 real images from <ref type="bibr" target="#b47">[48]</ref>. Therefore, to better mitigate the transition from the make-believe world of synthetic images to real-life photographs, we describe two modifications for introducing broader contextual information into otherwise local convolutional filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Context Encoding Modules</head><p>As mentioned previously, we consider both context between channels and multi-scale context within channels.</p><p>Channel-wise context. The underlying design principle here is to introduce global contextual information across channels, and a richer overall structure within residual blocks, without dramatically increasing the parameter count. One way to accomplish this is by incorporating a channel attention module originally developed in <ref type="bibr" target="#b12">[13]</ref> to recalibrate feature maps using global summary statistics.</p><p>Let U = [u 1 , . . . , u c , . . . , u C ] denote original, uncalibrated activations produced by a network block, with C feature maps of size of H ? W . These activations generally only reflect local information residing within the corresponding receptive fields of each filter. We then form scalar, channel-specific descriptors z c = f gp (u c ) by applying a global average pooling operator f gp to each feature map u c ? R H?W . The vector z = [z 1 , . . . , z C ] ? R C represents a simple statistical summary of global, per-channel activations and, when passed through a small network structure, can be used to adaptively predict the relative importance of each channel <ref type="bibr" target="#b12">[13]</ref>.</p><p>More specifically, the channel attention module first computes s = ?(W U ?(W D z)) where W D is a trainable weight matrix that downsamples z to dimension R &lt; C, ? is a ReLU non-linearity, W U represents a trainable upsampling weight matrix, and ? is a sigmoidal activation. Elements of the resulting output vector s ? R C serve as channel-specific gates for calibrating feature maps vi?</p><formula xml:id="formula_1">u c = s c ? u c .</formula><p>Consequently, although each individual convolutional filter has a local receptive field, the determination of which channels are actually important in predicting the transmission layer and suppressing reflections is based on the processing of a global statistic (meaning the channel descriptors computed as activations pass through the network during inference). Additionally, the parameter overhead introduced by this process is exceedingly modest given that W D and W U are just small additional weight matrices associated with each block. Multi-scale spatial context. Although we have found that encoding the contextual information across channels already leads to significant empirical gains on real-world images, utilizing complementary multi-scale spatial information within each channel provides further benefit. To accomplish this, we apply a pyramid pooling module <ref type="bibr" target="#b10">[11]</ref>, which has proven to be an effective global-scene-level representation in semantic segmentation <ref type="bibr" target="#b48">[49]</ref>. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, we construct such a module using pooling operations at sizes 4, 8, 16, and 32 situated in the tail of our network before the final construction ofT . Pooling in this way fuses features under four different pyramid scales. After harvesting the resulting sub-region representations, we perform a non-linear transformation (i.e. a Conv-ReLU pair) to reduce the channel dimension. The refined features are then upsampled via bilinear interpolation. Finally, the different levels of features are concatenated together as a final representation reflecting multi-scale spatial context within each channel; the increased parameter overhead is negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training Loss for Aligned Data</head><p>In this section, we present our loss function for aligned training pairs (I, T ), which consists of three terms similar to previous methods <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>Pixel loss. Following <ref type="bibr" target="#b4">[5]</ref>, we penalize the pixel-wise in-</p><formula xml:id="formula_2">tensity difference of T andT via l pixel = ? T ? T 2 2 + ?( ? xT ? ? x T 1 + ? yT ? ? y T 1 )</formula><p>where ? x and ? y are the gradient operator along x-and y-direction, respectively. We set ? = 0.2 and ? = 0.4 in all our experiments.</p><p>Feature loss. We define the feature loss based on the activations of the 19-layer VGG network <ref type="bibr" target="#b32">[33]</ref> pretrained on ImageNet <ref type="bibr" target="#b28">[29]</ref>. Let ? l be the feature from the l-th layer of VGG-19, we define the feature loss as l f eat = l ? l ? l (T ) ? ? l (T ) 1 where {? l } are the balancing weights. Similar to <ref type="bibr" target="#b47">[48]</ref>, we use the layers 'conv2 2', 'conv3 2', 'conv4 2', and 'conv5 2' of VGG-19 net.</p><p>Adversarial loss. We further add an adversarial loss to improve the realism of the produced background images. We define an opponent discriminator network D ? D and minimize the relativistic adversarial loss <ref type="bibr" target="#b17">[18]</ref> defined as</p><formula xml:id="formula_3">l adv = l G adv = ? log(D ? D (T,T ))?log(1?D ? D (T , T )) for G ? G and l D adv = ? log(1 ? D ? D (T,T )) ? log(D ? D (T , T )) for D ? D where D ? D (T,T ) = ?(C(T ) ? C(T )) with ?(?)</formula><p>being the sigmoid function and C(?) the non-transformed discriminator function (refer to <ref type="bibr" target="#b17">[18]</ref> for details).</p><p>To summarize, our loss for aligned data is defined as:</p><formula xml:id="formula_4">l aligned = ? 1 l pixel + ? 2 l f eat + ? 3 l adv<label>(2)</label></formula><p>where we empirically set the weights as ? 1 = 1, ? 2 = 0.1, and ? 3 = 0.01 respectively throughout our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Loss for Unaligned Data</head><p>To use misaligned data pairs (I, T ) for training, we need a loss function that is invariant to the alignment, such that the true similarity between T and the predictionT can be reasonably measured. In this regard, we note that human observers can easily assess the similarity of two images even if they are not aligned. Consequently, designing a loss measuring image similarity on the perceptual-level may serve our goal. This motivates us to directly use a deep feature loss for unaligned data.</p><p>Intuitively, the deeper the feature, the more likely it is to be insensitive to misalignment. To experimentally verify this and find a suitable feature layer for our purposes, we conducted tests using a pre-trained VGG-19 network as follows. Given an unaligned image pair (I, T ), we use gradient descent to finetune the weights of our network G ? G to minimize the feature difference of T andT , with features extracted at different layers of VGG-19. <ref type="figure" target="#fig_1">Figure 3</ref> shows that using low-level or middle-level features from 'conv2 2' to 'conv4 2' leads to blurry results (similar to directly using a pixel-wise loss), although the reflection is more thoroughly removed. In contrast, using the highest-level feature from 'conv5 2' gives rise to a striking result: the predicted background image is sharp and almost reflection-free. Recently, <ref type="bibr" target="#b26">[27]</ref> introduced a "contextual loss" which is also designed for training deep networks with unaligned data for image-to-image translation tasks like image style transfer. In <ref type="figure" target="#fig_1">Fig 3,</ref> we also present the finetuned result using this loss for our reflection removal task. Upon visual inspection, the results are similar to our highest-level VGG feature loss (quantitative comparison can be found in the experiment section). However, our adopted loss (formally defined below) is much simpler and more computationally efficient than the loss from <ref type="bibr" target="#b26">[27]</ref>.</p><p>Alignment-invariant loss. Based on the above study, we now formally define our invariant loss component designed for unaligned data as l inv = ? h (T ) ? ? h (T ) 1 , where ? h denotes the 'conv5 2' feature of the pretrained VGG-19 network. For unaligned data, we also apply an adversarial loss which is not affected by misalignment. Therefore, our overall loss for unaligned data can be written as</p><formula xml:id="formula_5">l unaligned = ? 4 l inv + ? 5 l adv<label>(3)</label></formula><p>where we set the weights as ? 4 = 0.1 and ? 5 = 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Training data. We adopt a fusion of synthetic and real data as our train dataset. The images from <ref type="bibr" target="#b4">[5]</ref> are used as sythetic  <ref type="bibr" target="#b3">[4]</ref>. 90 real-world training images from <ref type="bibr" target="#b47">[48]</ref> are adopted as real data. For image synthesis, we use the same data generation model as <ref type="bibr" target="#b4">[5]</ref> to create our synthetic data. In the following, we always use the same dataset for training, unless specifically stated.</p><p>Training details. Our implementation 1 is based on Py-Torch. We train the model with 60 epoch using the Adam optimizer <ref type="bibr" target="#b18">[19]</ref>. The base learning rate is set to 10 ?4 and halved at epoch 30, then reduced to 10 ?5 at epoch 50. The weights are initialized as in <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>In this section, we conduct an ablation study for our method on 100 synthetic testing images from <ref type="bibr" target="#b4">[5]</ref> and 20 real testing images from <ref type="bibr" target="#b47">[48]</ref> (denoted by 'Real20').</p><p>Component analysis. To verify the importance of our network design, we compare four model architectures as described in Section 3, including (1) Our basic image reconstruction network BaseNet; (2) BaseNet with channelwise context module (BaseNet + CWC); (3) BaseNet with multi-scale spatial context module (BaseNet + MSC); and (4) Our enhanced reflection removal network, denoted ER-RNet, i.e., BaseNet + CWC + MSC. The result from the CEILNet <ref type="bibr" target="#b4">[5]</ref> fine-tuned on our training data (denoted by CEILNet-F) is also provided as an additional reference.</p><p>As shown in <ref type="table" target="#tab_0">Table 1</ref>, our BaseNet has already achieved a much better result than CEILNet-F. The performance of our BaseNet could be obviously boosted by using channelwise context and multi-scale spatial context modules, especially by using them together, i.e. ERRNet. <ref type="figure" target="#fig_2">Figure 4</ref> visually shows the results from BaseNet and our ERRNet. It can be observed that BaseNet struggles to discriminate the reflection region and yields some obvious residuals, while the ERRNet removes the reflection and produces much cleaner transmitted images. These results suggest the effectiveness of our network design, especially the components tailored to encode the contextual clues.  experiment, we first train our ERRNet with only 'synthetic data', 'synthetic + 50 aligned real data', and 'synthetic + 90 aligned real data'. The loss function in Eq. (2) is used for aligned data. We can see that the testing results become better with the increasing real data in <ref type="table" target="#tab_1">Table 2</ref>. Then, we synthesize misalignment through performing random translations within [?10, 10] pixels on real data 2 , and train ERRNet with 'synthetic + 50 aligned real data + 40 unaligned data'. Pixel-wise loss l pixel and alignmentinvariant loss l inv are used for 40 unaligned images. <ref type="table" target="#tab_1">Table 2</ref> shows employing 40 unaligned data with l pixel loss degrades the performance, even worse than that from 50 aligned images without additional unaligned data.</p><p>In addition, we also investigate the contextual loss l cx of <ref type="bibr" target="#b26">[27]</ref>. Results from both contextual loss l cx and our alignment-invariant loss l inv (or combination of them l inv + l cx ) surpass analogous results obtained with only aligned images by appreciable margins, indicating that these losses provide useful supervision to networks granted unaligned data. Note although l inv and l cx perform equally well, our l inv is much simpler and computationally efficient than l cx , suggesting l inv is lightweight alternative to l cx in terms of our reflection removal task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Method Comparison on Benchmarks</head><p>In this section, we compare our ERRNet against state-ofthe-art methods including the optimization-based method of <ref type="bibr" target="#b24">[25]</ref> (LB14) and the learning-based approaches (CEILNet <ref type="bibr" target="#b4">[5]</ref>, Zhang et al. <ref type="bibr" target="#b47">[48]</ref>, and BDN <ref type="bibr" target="#b44">[45]</ref>). For fair comparison, we finetune these models on our training dataset and report results of both the original pretrained model and finetuned version (denoted with a suffix '-F').</p><p>The comparison is conducted on four real-world datasets, i.e. 20 testing images in <ref type="bibr" target="#b47">[48]</ref> and three sub-datasets from SIR 2 <ref type="bibr" target="#b36">[37]</ref>. These three sub-datasets are captured under different conditions: (1) 20 controlled indoor scenes composed by solid objects; (2) 20 different controlled scenes on postcards; and (3) 55 wild scenes 3 with ground truth provided. In the following, we denote these datasets by 'Real20', 'Objects', 'Postcard', and 'Wild', respectively. <ref type="table">Table 3</ref> summarizes the results of all competing methods on four real-world datasets. The quality metrics include PSNR, SSIM <ref type="bibr" target="#b39">[40]</ref>, NCC <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b36">37]</ref> and LMSE <ref type="bibr" target="#b7">[8]</ref>. Larger values of PSNR, SSIM, and NCC indicate better performance, while a smaller value of LMSE implies a better result. Our ERRNet achieves the state-of-the-art performance in 'Real20' and 'Objects' datasets. Meanwhile, our result is comparable to the best-performing BDN-F on 'Postcard' data. The quantitative results on 'Wild' dataset reveal a frustrating fact, namely, that no method could outperform the naive baseline 'Input', suggesting that there is still large room for improvement. <ref type="figure">Figure 5</ref> displays visual results on real-world images. It can be seen that all compared methods fail to handle some strong reflections, but our network more accurately removes many undesirable artifacts, e.g. removal of tree branches reflected on the building window in the fourth photo of <ref type="figure">Fig 5.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Training with Unaligned Data</head><p>To test our alignment-invariant loss on real-world unaligned data, we first collected a dataset of unaligned image pairs with cameras and a portable glass, as shown in <ref type="figure">Fig. 1</ref> . Both a DSLR camera and a smart phone are used to capture the images. We collected 450 image pairs in total, and some samples are shown in <ref type="figure" target="#fig_4">Fig 6.</ref> These image pairs are randomly split into a training set of 400 samples and a testing set with 50 samples.</p><p>We conduct experiments on the BDN-F and ERRNet models, each of which is first trained on aligned dataset (w/o unaligned) as in Section 4.3, and then finetuned with our alignment-invariant loss and unaligned training data. The resulting pairs before and after finetuning are assembled for human assessment, as no existing numerical metric is available for evaluating unaligned data.</p><p>We asked 30 human observers to provide a preference   score among {-2,-1,0,1,2} with 2 indicating the finetuned result is significantly better while -2 the opposite. To avoid bias, we randomly switch the image positions of each pair. In total, 3000 human judgments are collected (2 methods, 30 users, 50 images pairs). More details regarding this evaluation process can be found in the suppl. material. <ref type="table" target="#tab_4">Table 4</ref> shows the average of human preference scores for the resulting pairs of each method. As can be seen, human observers clearly tend to prefer the results produced by the finetuned models over the raw ones, which demonstrates the benefit of leveraging unaligned data for training independent of the network architecture. <ref type="figure" target="#fig_5">Figure 7</ref> shows some typical results of the two methods; the results are significantly improved by training on unaligned data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have proposed an enhanced reflection removal network together with an alignment-invariant loss function to help resolve the difficulty of single image reflection removal. We investigated the possibility to directly utilize misaligned training data, which can significantly alleviate the burden of capturing real-world training data. To efficiently extract the underlying knowledge from real train- <ref type="table">Table 3</ref>: Quantitative results of different methods on four real-world benchmark datasets. The best results are indicated by red color and the second best results are denoted by blue color. The results of 'Average' are obtained by averaging the metric scores of all images from these four real-world datasets.   ing data, we introduce context encoding modules, which can be seamlessly embedded into our network to help discriminate and suppress the reflection component. Extensive experiments demonstrate our approach set a new state-ofthe-art on real-world benchmarks of single image reflection removal, both quantitatively and visually.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Overview of our approach for single image reflection removal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The effect of using different loss to handle misaligned real data. (a) and (b) are the unaligned image pair (I, T ). (c) shows the reflection removal result of our network trained on synthetic data and a small number of aligned real data (see Section 4 for details). Reflection can still be observed in the predicted background image. (d) is the result finetuned on (I, T ) with pixelwise intensity loss. (e)-(h) are the results finetuned with features at different layers of VGG-19. Only the highest-level feature from 'conv5 2' yields satisfactory result. (i) shows the results finetuned with the loss of [27]. (Best viewed on screen with zoom)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Efficacy of the training loss for unaligned data. In this Input BaseNetERRNet Comparison of the results with (ERRNet) and without (BaseNet) the context encoding modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>#****. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Image samples in our unaligned image dataset. Our dataset covers a large variety of indoor and outdoor environments including dynamic scenes with vehicles, human, etc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Results of training with and without unaligned data. See suppl. material for more examples. (Best view on screen with zoom)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of different settings. Our full model (i.e. ERRNet) leads to best performance among all comparisons.</figDesc><table><row><cell>Model</cell><cell cols="2">Synthetic PSNR SSIM</cell><cell cols="2">Real20 PSNR SSIM</cell></row><row><cell>CEILNet-F [5]</cell><cell>24.70</cell><cell>0.884</cell><cell>20.32</cell><cell>0.739</cell></row><row><cell>BaseNet only</cell><cell>25.71</cell><cell>0.926</cell><cell>21.51</cell><cell>0.780</cell></row><row><cell>BaseNet + CSC</cell><cell>27.64</cell><cell>0.940</cell><cell>22.61</cell><cell>0.796</cell></row><row><cell>BaseNet + MSC</cell><cell>26.03</cell><cell>0.928</cell><cell>21.75</cell><cell>0.783</cell></row><row><cell>ERRNet</cell><cell>27.88</cell><cell>0.941</cell><cell>22.89</cell><cell>0.803</cell></row><row><cell cols="5">data, i.e. 7,643 cropped images with size 224 ? 224 from PASCAL VOC dataset</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell cols="3">Simulation experiment to verify the efficacy our</cell></row><row><cell>alignment-invariant loss</cell><cell></cell><cell></cell></row><row><cell cols="2">Training Scheme PSNR</cell><cell>SSIM</cell></row><row><cell>Synthetic only</cell><cell>19.79</cell><cell>0.741</cell></row><row><cell>+ 50 aligned</cell><cell>22.00</cell><cell>0.785</cell></row><row><cell>+ 90 aligned</cell><cell>22.89</cell><cell>0.803</cell></row><row><cell cols="3">+ 50 aligned, + 40 unaligned trained with:</cell></row><row><cell>l pixel l inv</cell><cell>21.85 22.38</cell><cell>0.766 0.797</cell></row><row><cell>lcx</cell><cell>22.47</cell><cell>0.796</cell></row><row><cell>l inv + lcx</cell><cell>22.43</cell><cell>0.796</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>More results can be found in the suppl. material.</figDesc><table><row><cell>Input</cell><cell>LB14 [25]</cell><cell>CEILNet-F [5] Zhang et al. [48] BDN-F [45]</cell><cell>ERRNet</cell><cell>Reference</cell></row><row><cell cols="5">Figure 5: Visual comparison on real-world images. The images are obtained from 'Real20' (Rows 1-3) and our collected unaligned dataset (Rows 4-6). 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>186</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>187</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>188</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>189</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>190</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>191</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>192</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>193</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>194</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>195</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>196</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>197</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>198</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>199</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>200</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>201</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>202</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>203</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>204</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>205</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>206</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>207</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>208</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>209</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>210</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>211</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>212</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Human preference scores of self-comparsion experiments. Left: results of BDN-F; Right: results of ERRNet. X axis of each sub-figure represents the image # of testing images (50 in total).</figDesc><table><row><cell>BDN-F</cell><cell>ERRNet</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code is released on https://github.com/Vandermode/ERRNet</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Our alignment-invariant loss l inv can handle shifts of up to 20 pixels. See suppl. material for more details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Images indexed by 1, 2, 74 are removed due to misalignment.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Removing photography artifacts using gradient projection and flashexposure sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="828" to="835" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Single image reflection suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Arvanitopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Single image reflection removal using deep encoder-decoder network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00094</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A generic deep architecture for single image reflection removal and image smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wipf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Separating reflections and lighting using independent components analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="262" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Blind separation of superimposed moving images using image statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="19" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Freeman. Ground truth dataset and baseline evaluations for intrinsic image algorithms. In Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2335" to="2342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust separation of reflection from multiple images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2187" to="2194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to see through reflections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computational Photography</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feifei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The relativistic discriminator: a key element missing from standard gan. arXiv: Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jolicoeurmartineau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A physically-based approach to reflection separation: from physical modeling to constrained optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="209" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Generative single image reflection separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04102</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">User assisted separation of reflections from a single image using a sparsity prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1647" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to perceive transparency from the statistics of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zomet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1271" to="1278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exploiting reflection change for automatic reflection removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2432" to="2439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Single image layer separation using relative smoothness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2752" to="2759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The contextual loss for image transformation with non-aligned data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mechrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Talmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Separating transparent layers through layer information exchange</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sarel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="328" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reflection removal using ghosting cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image-based rendering for scenes with reflections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goesele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="100" to="101" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Layer extraction from multiple images containing reflections and transparency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="246" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Region-aware reflection removal with unified content and gradient priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Benchmarking single-image reflection removal algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Crrn: Multi-scale guided concurrent reflection removal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Depth of field guided reflection removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Hwee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing (ICIP), 2016 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Image smoothing via L0 gradient minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">174</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A computational approach for obstruction-free photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">79</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A computational approach for obstruction-free photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH)</title>
		<meeting>SIGGRAPH)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Seeing deeply and bidirectionally: A deep learning approach for single image reflection removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Robust optical flow estimation of double-layer images under transparency or reflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1410" to="1419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Single image reflection separation with perceptual losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
