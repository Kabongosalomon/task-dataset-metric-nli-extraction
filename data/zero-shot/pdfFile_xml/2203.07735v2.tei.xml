<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Augmenting Document Representations for Dense Retrieval with Interpolation and Perturbation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soyeong</forename><surname>Jeong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinheon</forename><surname>Baek</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Korea Advanced Institute of Science and Technology 1</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sukmin</forename><surname>Cho</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
							<email>sjhwang82@kaist.ac.kr2</email>
							<affiliation key="aff2">
								<orgName type="institution">Korea Advanced Institute of Science and Technology 1</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">C</forename><surname>Park</surname></persName>
							<email>park@nlp.kaist.ac.kr1jinheon.baek</email>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of AI</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Augmenting Document Representations for Dense Retrieval with Interpolation and Perturbation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dense retrieval models, which aim at retrieving the most relevant document for an input query on a dense representation space, have gained considerable attention for their remarkable success. Yet, dense models require a vast amount of labeled training data for notable performance, whereas it is often challenging to acquire query-document pairs annotated by humans. To tackle this problem, we propose a simple but effective Document Augmentation for dense Retrieval (DAR) framework, which augments the representations of documents with their interpolation and perturbation. We validate the performance of DAR on retrieval tasks with two benchmark datasets, showing that the proposed DAR significantly outperforms relevant baselines on the dense retrieval of both the labeled and unlabeled documents.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Retrieval systems aim at retrieving the documents most relevant to the input queries, and have received substantial spotlight since they work as core elements in diverse applications, especially for open-domain question answering (QA) <ref type="bibr" target="#b27">(Voorhees, 1999)</ref>. Open-domain QA is a task of answering the question from a massive amount of documents, often requiring two components, a retriever and a reader <ref type="bibr" target="#b1">(Chen et al., 2017;</ref><ref type="bibr" target="#b8">Karpukhin et al., 2020)</ref>. Specifically, a retriever ranks the most questionrelated documents, and a reader answers the question using the retrieved documents.</p><p>Traditional sparse retrieval approaches such as BM25 <ref type="bibr" target="#b21">(Robertson et al., 1994)</ref> and TF-IDF rely on term-based matching, hence suffering from the vocabulary mismatch problem: the failure of retrieving relevant documents due to the lexical difference from queries. To tackle such a problem, recent research focuses on dense retrieval models to generate learnable dense representations for queries and documents <ref type="bibr" target="#b8">(Karpukhin et al., 2020)</ref>. * Corresponding author Despite their recent successes, some challenges still remain in the dense retrieval scheme for a couple of reasons. First, dense retrieval models need a large amount of labeled training data for a decent performance. However, as <ref type="figure" target="#fig_0">Figure 1</ref> shows, the proportion of labeled query-document pairs is extremely small since it is almost impossible to rely on humans for the annotations of a large document corpus. Second, in order to adapt a retrieval model to the real world, where new documents constantly emerge, handling unlabeled documents that are not seen during training should obviously be considered, but remains challenging.</p><p>To automatically expand the query-document pairs, recent work generates queries from generative models <ref type="bibr" target="#b14">(Liang et al., 2020;</ref><ref type="bibr" target="#b16">Ma et al., 2021)</ref> or incorporates queries from other datasets , and then generates extra pairs of augmented queries and documents. However, these query augmentation schemes have serious and obvious drawbacks. First, it is infeasible to augment queries for every document in the dataset (see the number of unlabeled documents in <ref type="figure" target="#fig_0">Figure 1</ref>), since generating and pairing queries are quite costly. Second, even after obtaining new pairs, we need extra training steps to reflect the generated pairs on the retrieval model. Third, this query augmentation method does not add variations to the documents but only to the queries, thus it may be suboptimal to handle enormous unlabeled documents. Since augmenting additional queries is costly, the question is then if it is feasible to only manipulate the given query-document pairing to handle numerous unlabeled documents. To answer this, we first visualize the embeddings of labeled and unlabeled documents. <ref type="figure" target="#fig_0">Figure 1</ref> shows that there is no distinct distributional shift between labeled and unlabeled documents. Thus it could be effective to manipulate only the labeled documents to handle the nearby unlabeled documents as well as the labeled documents. Using this observation, we propose a novel document augmentation method for a dense retriever, which not only interpolates two different document representations associated with the labeled query ( <ref type="figure" target="#fig_1">Figure 2 (b)</ref>), but also stochastically perturbs the representations of labeled documents with a dropout mask (Figure 2 (c)). One notable advantage of our scheme is that, since it manipulates only the representations of documents, our model does not require explicit annotation steps of query-document pairs, which makes it highly efficient. We refer to our overall method as Document Augmentation for dense Retrieval (DAR).</p><p>We experimentally validate our method on standard open-domain QA datasets, namely Natural Question (NQ) <ref type="bibr" target="#b10">(Kwiatkowski et al., 2019)</ref> and Triv-iaQA <ref type="bibr" target="#b7">(Joshi et al., 2017)</ref> (TQA), against various evaluation metrics for retrieval models. The experimental results show that our method significantly improves the retrieval performances on both the unlabeled and labeled documents. Furthermore, a detailed analysis of the proposed model shows that interpolation and stochastic perturbation positively contribute to the overall performance.</p><p>Our contributions in this work are threefold: ? We propose to augment documents for dense retrieval models to tackle the problem of insufficient labels of query-document pairs. ? We present two novel document augmentation schemes for dense retrievers: interpolation and perturbation of document representations. ? We show that our method achieves outstanding retrieval performances on both labeled and unlabeled documents on open-domain QA tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Dense Retriever Dense retrieval models <ref type="bibr" target="#b8">Karpukhin et al., 2020)</ref> have gained much attention, which generate dense representations for queries and documents. However, dense retrieval faces a critical challenge from limited training data. Recent work has addressed such a problem by generating extra query-document pairs to augment those pairs to the original dense retrieval model <ref type="bibr" target="#b14">(Liang et al., 2020;</ref><ref type="bibr" target="#b16">Ma et al., 2021;</ref>, or by regularizing the model <ref type="bibr" target="#b22">(Rosset et al., 2019)</ref>. However, unlike ours that automatically augments data during a training phase, these methods require extensive computational resources for an additional generation step of explicitly querydocument pairing before training the retriever.</p><p>Data Augmentation Since data augmentation is crucial to the performance of deep neural networks, it is widely applied to diverse domains <ref type="bibr" target="#b23">(Shorten and Khoshgoftaar, 2019;</ref><ref type="bibr" target="#b5">Hedderich et al., 2021)</ref>, where interpolation and perturbation are dominant methods. Mixup interpolates two items, such as pixels of images, to augment the training data <ref type="bibr" target="#b32">(Zhang et al., 2018;</ref><ref type="bibr" target="#b26">Verma et al., 2019)</ref>, which is also adopted for NLP <ref type="bibr" target="#b30">Yin et al., 2021)</ref>. However, none of the previous work has shown the effectiveness of mixup when applied to retrieval tasks. Besides interpolation, <ref type="bibr" target="#b28">Wei and Zou (2019)</ref> and <ref type="bibr" target="#b15">Ma (2019)</ref> proposed perturbation over words, and <ref type="bibr" target="#b13">Lee et al. (2021b)</ref> proposed perturbation over word embeddings. <ref type="bibr" target="#b6">Jeong et al. (2021)</ref> and <ref type="bibr" target="#b4">Gao et al. (2021)</ref> perturbed text embeddings to generate diverse sentences and to augment positive sentence pairs in unsupervised learning. In contrast, we address dense retrieval, perturbing document representations with dropout <ref type="bibr" target="#b24">(Srivastava et al., 2014)</ref> in a supervised setting with labeled documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We begin with the definition of dense retrieval.</p><p>Dense Retrieval Given a pair of query q and document d, the goal of dense retrieval is to correctly calculate a similarity score between them from the dense representations q and d, as follows:</p><formula xml:id="formula_0">f (q, d) = sim(q, d), q = E Q (q; ? q ) and d = E D (d; ? d ),<label>(1)</label></formula><p>where f is a scoring function that measures the similarity between a query-document pair, sim is a applying data augmentation techniques to queries instead of documents. BM25 is the sparse retrieval model, whereas others are dense retrieval models. The best model and the second best model among dense retrievers are denoted in bold, which we aim to improve in this work. similarity metric such as cosine similarity, and E Q and E D are dense encoders for a query and document, respectively, with parameters ? = (? q , ? d ).</p><p>A dense retrieval scheme generally uses the negative sampling strategy to distinguish the relevant query-document pairs from irrelevant pairs, which generates an effective representation space for queries and documents. We specify a relevant query-document pair as (q, d + ) ? ? + , and an irrel-</p><formula xml:id="formula_1">evant pair as (q, d ? ) ? ? ? , where ? + ? ? ? = ?.</formula><p>The objective function is as follows:</p><formula xml:id="formula_2">min ? (q,d + )?? + (q,d ? )?? ? L(f (q, d + ), f (q, d ? )), (2)</formula><p>where a loss function L is a negative log-likelihood of the positive document. Our goal is to augment a set of query-document pairs, by manipulating documents with their interpolation or perturbation, which we explain in the next paragraphs.</p><p>Interpolation with Mixup As shown in interpolation of <ref type="figure" target="#fig_1">Figure 2</ref>, we aim at augmenting the document representation located between two labeled documents to obtain more query-document pairs, which could be useful to handle unlabeled documents in the middle of two labeled documents. To achieve this goal, we propose to interpolate the positive and negative documents (d + , d ? ) for the given query q, adopting mixup <ref type="bibr" target="#b32">(Zhang et al., 2018)</ref>. Note that, since the input documents to the encoder E D are discrete, we use the output embeddings of documents to interpolate them, as follows:</p><formula xml:id="formula_3">d = ?d + + (1 ? ?)d ? ,<label>(3)</label></formula><p>whered is the mixed representation of positive and negative documents for the given query q, and ? ? [0, 1]. We then optimize the model to estimate the similarity sim(q,d) between the interpolated document and the query as the soft label ? with a binary cross-entropy loss. The output of the crossentropy loss is added to the original loss in equation 2. One notable advantage of our scheme is   that the negative log-likelihood loss in equation 2 maximizes the similarity score of the positive pair, while minimizing the score of the negative pair; thus there are no intermediate similarities between arbitrary query-document pairs. However, ours can obtain query-document pairs having soft labels, rather than strict positive or negative classes, by interpolating the positive and negative documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stochastic Perturbation with Dropout</head><p>In addition to our interpolation scheme to handle unlabeled documents in the space of interpolation of two labeled documents, we further aim at perturbing the labeled document to handle its nearby unlabeled documents as shown in <ref type="figure" target="#fig_1">Figure 2</ref> (c). In order to do so, we randomly mask the representation of the labeled document, obtained by the document encoder E D , with dropout, where we sample masks from a Bernoulli distribution. In other words, if we sample n different masks from the distribution, we obtain n different query-document</p><formula xml:id="formula_4">pairs (q, d + i ) i=n i=1</formula><p>from one positive pair (q, d + ). By doing so, we augment n times more positive query-document pairs by replacing a single positive pair (q, d + ) in equation 2. Moreover, since the document perturbation is orthogonal to the interpolation, we further interpolate between the perturbed positive document d + i and the negative document d ? for the given query in equation 3, to augment a soft query-document pair from perturbation.</p><p>Efficiency Data augmentation methods are generally vulnerable to inefficiency, since they need a vast amount of resources to generate data and to forward the generated data into the large language model. However, since our interpolation and perturbation methods only manipulate the already   obtained representations of the documents from the encoder E D , we don't have to newly generate document texts and also to forward generated documents into the model, which greatly saves time and memory (see <ref type="table" target="#tab_3">Table 3</ref>). We provide a detailed analysis and discussion of efficiency in Appendix B.  <ref type="bibr" target="#b10">(Kwiatkowski et al., 2019)</ref>; 2) TriviaQA (TQA) is a QA collection scraped from the Web <ref type="bibr" target="#b7">(Joshi et al., 2017)</ref>.</p><p>Retrieval Models 1) BM25 is a sparse termbased retrieval model based on TF-IDF <ref type="bibr" target="#b21">(Robertson et al., 1994)</ref>. 2) Dense Passage Retriever (DPR) is a dense retrieval model with a dual-encoder of query-document pairs <ref type="bibr" target="#b8">(Karpukhin et al., 2020)</ref>. 3) DPR with Query Augmentation (DPR w/ QA) augments pairs with query generation for the document, adopting <ref type="bibr" target="#b14">(Liang et al., 2020;</ref><ref type="bibr" target="#b18">Mao et al., 2021a)</ref>. 4) DPR with Document Augmentation (DPR w/ DA) augments pairs by replacing words in the document <ref type="bibr" target="#b15">(Ma, 2019)</ref>. 5) DPR with Axiomatic Regularization (DPR w/ AR) regularizes the retrieval model to satisfy certain axioms <ref type="bibr" target="#b22">(Rosset et al., 2019)</ref>. 6) DAR is ours with interpolation and perturbation of document representations.</p><p>Metrics 1) Top-K Accuracy (T-K) computes whether a query's answer is included in Top-K retrieved documents. 2) Mean Reciprocal Rank (MRR) and 3) Mean Average Precision (MAP) measure the first rank and the average precision of query-relevant retrieved documents, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>For the dense retrieval model based on the DPR framework, we refer to the publicly available code from DPR <ref type="bibr" target="#b8">(Karpukhin et al., 2020)</ref>. We set the training epoch as 25 and batch size as 32 under academic budgets with a single GeForce RTX 3090 GPU having 24GB memory. We use in-batch negative sampling as our negative sampling strategy without hard negative samples. Also, we retrieve 100 passages per question. We use both interpolation and perturbation schemes for our augmentation methods. Specifically, for the interpolation method, we set ? ? [0, 1] in equation 3 to be sampled from the uniform distribution. Also, for the perturbation method, we set the dropping rate as 0.1, and the number of dropout masks n is selected in the range of 3 to 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>In this subsection, we show the overall performance of our DAR, and then give detailed analyses.</p><p>Overall Results As <ref type="table" target="#tab_0">Table 1</ref> shows, DAR outperforms dense retrieval baselines on all datasets on the DPR framework. Note that DAR contributes to more accurate retrieval performance, since the smaller K gives higher performance improvements. Furthermore, <ref type="figure" target="#fig_2">Figure 3</ref> shows that, with our method, the retrieval performance on unlabeled documents -not seen during training -together with the labeled ones is improved, where performance gains on unlabeled are remarkable. To see the robustness of DAR on other retrievers, we further evaluate our model on the recent ANCE framework (see Appendix A for setups). As <ref type="table" target="#tab_2">Table 2</ref> shows, we observe that the performance improvement is more dominant on MRR when given a smaller number of training queries (low-resource settings), that DAR effectively augments document representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on Query Augmentation</head><p>We focus on the problem of a notably small proportion of labeled documents in the training dataset, and propose to augment representations of unlabeled documents, which are not seen during training. However, it is also possible to augment representations of queries -likely to be unseen at the test time  -by applying our interpolation and perturbation methods directly to queries. Note that we refer to our query augmentation method as Query Augmentation for dense Retrieval (QAR). As shown in <ref type="table" target="#tab_0">Table 1</ref>, our proposed augmentation strategies also effectively improve the retrieval performance even when applied to queries. This result implies that our method is versatile, regardless of whether it is applied to documents or queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of Interpolation &amp; Perturbation</head><p>To understand how much our proposed interpolation and perturbation techniques contribute to the performance gain, we perform ablation studies. <ref type="table" target="#tab_5">Table 4</ref> shows that each of the interpolation and stochastic perturbation positively contributes to the performance. In particular, when both of them are simultaneously applied, the performance is much improved, which demonstrates that these two techniques are in a complementary relationship.</p><p>Batch Size We test DAR with varying numbers of batch sizes. <ref type="figure">Figure 4</ref> indicates that our DAR consistently improves the retrieval performance. Note that the smaller the batch size, the bigger the performance gap. Also, the batch size 16 of DAR outperforms the batch size 32 of the baseline, which highlights that DAR effectively augments document representations with a small batch.</p><p>Reader Performance To see whether accurately retrieved documents lead to better QA performance, we experiment with the same extractive reader from DPR without additional re-training. <ref type="figure" target="#fig_3">Figure 5</ref> illustrates the effectiveness of our method on passage reading with varying numbers of retrieved documents. We observe that our retrieval result with small retrieved documents (i.e., K = 10) significantly improves the performance of the reader. This implies that a more accurate retrieval on smaller K in <ref type="table" target="#tab_0">Table 1</ref> helps achieve the improved QA performance as <ref type="bibr" target="#b11">Lee et al. (2021a)</ref> described. Furthermore, our reader performance may be further enhanced with advanced reading schemes <ref type="bibr" target="#b18">(Mao et al., 2021a;</ref><ref type="bibr" target="#b19">Mao et al., 2021b)</ref>.</p><p>Negative Sampling Strategy To see the effectiveness of our DAR coupled with an advanced negative sampling scheme, we compare DAR against  <ref type="table">Table 6</ref>: Retrieval results on the NQ dataset, following the processing procedure of <ref type="bibr" target="#b25">Thakur et al. (2021).</ref> the baseline with the hard negative sampling strategy from BM25 <ref type="bibr" target="#b8">(Karpukhin et al., 2020)</ref>. <ref type="table" target="#tab_8">Table 5</ref> shows that DAR with hard negative sampling outperforms the baseline method. The results demonstrate that the performance of dense retrieval models could be further strengthened with a combination of our augmentation methods and advanced negative sampling techniques. Also, in all our experiments of the ANCE framework, we already use the strategy of negative sampling in , where we observe the clear performance improvement of our DAR on ANCE in <ref type="table" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on Different Data Processing</head><p>We additionally evaluate DAR on another NQ test dataset, following the processing procedure of <ref type="bibr" target="#b25">Thakur et al. (2021)</ref>. For experiments, we reuse the same training checkpoint used in <ref type="table" target="#tab_0">Table 1</ref>, as the training dataset is equal across the settings of <ref type="bibr" target="#b8">Karpukhin et al. (2020)</ref> and <ref type="bibr" target="#b25">Thakur et al. (2021)</ref>. As <ref type="table">Table 6</ref> shows, our DAR also consistently outperforms all baselines when tested on the NQ test set from <ref type="bibr" target="#b25">Thakur et al. (2021)</ref>. This confirms that our DAR robustly improves retrieval performances, regardless of the specific data processing strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented a novel method of augmenting document representations focusing on dense retrievers, which require an extensive amount of labeled query-document pairs for training. Specifically, we augment documents by interpolating and perturbing their embeddings with mixup and dropout masks. The experimental results and analyses on multiple benchmark datasets demonstrate that DAR greatly improves retrieval performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Statements</head><p>Retrieving the most relevant documents from the user's query is increasingly important in a realworld setting, as it is widely used from web search, to question answering, to dialogue generation systems. Notably, our work contributes to the accurate retrieval of documents with the proposed data augmentation strategies, thus improving the document retrieval performances on real-world applications. However, we have to still consider the failure of retrieval systems on low-resource but high-risk domains (e.g., biomedicine), where the labeled data for training retrieval models is limited yet one failure can yield a huge negative impact. While we strongly believe that our data augmentation strategies -interpolation and perturbation of document representations -are also helpful to improve the retrieval performances on such low-resource domains, the model's prediction performance is still far from perfect, and more efforts should be made to develop a reliable system.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experimental Setups</head><p>Datasets To evaluate the performance of retrieval models, we need two types of datasets: 1) a set of documents to retrieve, and 2) pairs of a query and a relevant document, having an answer for the query. We first explain the datasets that we used for the DPR framework <ref type="bibr" target="#b8">(Karpukhin et al., 2020)</ref>, and then describe the dataset for the ANCE framework . For documents to retrieve, we use the Wikipedia snapshot from December 20, 2018, which contains 21,015,324 passages consisting of 100 tokens, following <ref type="bibr" target="#b8">Karpukhin et al. (2020)</ref> for the DPR framework. For open-domain QA datasets, we use Natural Question (NQ) <ref type="bibr" target="#b10">(Kwiatkowski et al., 2019)</ref> and Trivia QA (TQA) <ref type="bibr" target="#b7">(Joshi et al., 2017)</ref>, following the dataset processing procedure of <ref type="bibr" target="#b8">Karpukhin et al. (2020)</ref>. We report the statistics of the training, validation, and test sets on NQ and TQA in <ref type="table" target="#tab_11">Table 7</ref>.</p><p>To see the performance gain of our DAR on other dense retrieval models, we evaluate DAR on the ANCE framework , which is one of the recent dense retrieval models. ANCE is evaluated on the MS MARCO dataset, thus we use MS MARCO for training and testing our model. Note that training ANCE with the full MS MARCO dataset requires 225 GPU hours even after excluding the excessive BM25 pre-training and inference steps. Thus we randomly sample the MS MARCO dataset to train the model under academic budgets. Specifically, the subset of our MS MARCO passage dataset contains 500,000 passages. Also, we randomly divide the training queries into two subsets: one for 10,000 training queries and the other for 50,000 training queries. Then we align the sampled training queries to the query-document pairs in the MS MARCO dataset. On the other hand, we do not modify the validation set (dev set) of query-document pairs for testing. We summarize the statistics of the dataset in <ref type="table" target="#tab_11">Table 7</ref>. Note that since the test set of MS MARCO is not publicly open, we evaluate the dense retrievers with the validation set, following .</p><p>Metrics Here, we explain the evaluation metrics for retrievers in detail. Specifically, given an input query, we measure the ranks of the correctly retrieved documents for the DPR framework with the following metrics: 1) Top-K Accuracy (T-K): It measures whether an answer of the given query is included in the retrieved Top-K documents.</p><p>2) Mean Reciprocal Rank (MRR): It computes the rank of the first correct document for the given query among the Top-100 retrieved documents, and then computes the average of the reciprocal ranks for all queries.</p><p>3) Mean Average Precision (MAP): It computes the mean of the average precision scores for all queries, where precision scores are calculated by the ranks of the correctly retrieved documents among Top-100 ranked documents.</p><p>We use the following evaluation metric for the reader, which identifies the answer from retrieved documents.</p><p>1) Exact Match (EM): It measures whether the reader exactly predicts one of the reference answers for each question.</p><p>Note that, for the ANCE framework, we follow the evaluation metrics, namely MRR@10 and Re-call@1k, in the original paper .</p><p>Experimental Implementation Details For dense retrieval models based on the DPR framework, we follow the dual-encoder structure of query and document by using the publicly available code from DPR 1 <ref type="bibr" target="#b8">(Karpukhin et al., 2020)</ref>. For all experiments, we set the batch size as 32, and train models on a single GeForce RTX 3090 GPU having 24GB memory. Note that, in contrast to the best reported setting of DPR which requires industrial-level resources of 8 V100 GPUs (8 ? 32GB = 256GB) for training with a batch size of 128, we use a batch size of 32 to train the model under academic budgets. We optimize the model parameters of all dense retrieval models with the Adam optimizer (Kingma and Ba, 2015) having a learning rate of 2e-05. We train the models for 25 epochs, following the analysis 2 that the training phases converge after 25 epochs.</p><p>For the retrievers based on the ANCE framework, we refer to the implementation from ANCE 3 . In order to directly measure the performance gain of the dense retrieval models based on ANCE from using our DAR, we use the pre-trained RoBERTa without warming up with the BM25 negatives. We train all the dense retrieval models for 50,000 steps with a single GeForce RTX 3090 GPU having 24GB memory, and simultaneously generate the ANN index with another GeForce RTX 3090 GPU, following . Following the standard implementation setting, we set the training batch size as 8, and optimize the model with the LAMB optimizer <ref type="bibr" target="#b31">(You et al., 2020)</ref> with a learning rate of 1e-6.</p><p>Architectural Implementation Details For our augmentation methods, we use both interpolation and perturbation schemes of document representations obtained from the document encoder E D in equation 1. Specifically, given a positive querydocument pair (q, d + ), we first perturb the document representation d + with dropout masks sampled from a Bernoulli distribution, which generates n numbers of perturbed document representa-</p><formula xml:id="formula_5">tions d + i i=n i=1</formula><p>. Then, we augment them to generate n numbers of positive query-document pairs</p><formula xml:id="formula_6">(q, d + i ) i=n i=1</formula><p>, which we use in equation 2. We search the number of perturbations n in the range from 3 to 9, and set the probability of the Bernoulli distribution as 0.1.</p><p>Instead of only using positive or negative pairs, we further augment query-document pairs having intermediate similarities with mixup. Specifically, we interpolate representations between the perturbed-positive document d + i and the negative document d ? for the given query q, with ? ? [0, 1] in equation 3 sampled from a uniform distribution. Note that, given a positive pair of a query and a document, we consider the documents not identified as positive in the batch as negative documents. In other words, if we set the batch size as 32, then we could generate 31 interpolated document representations from 1 positive pair and 31 negative pairs. To jointly train the interpolation scheme with the original objective, we add the loss obtained from interpolation to the loss in equation 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Efficiency</head><p>As described in the Efficiency paragraph of Section 3, compared to the existing query augmentation methods <ref type="bibr" target="#b14">(Liang et al., 2020;</ref><ref type="bibr" target="#b16">Ma et al., 2021;</ref>, document augmentation method <ref type="bibr" target="#b15">(Ma, 2019)</ref>, and word replacement method for regularization <ref type="bibr" target="#b22">(Rosset et al., 2019)</ref>, our method of augmenting document representations with interpolation and perturbation in a dense representation space is highly efficient. This is because, unlike the baselines above, we do not explicitly generate or replace a query or document text; but rather we only manipulate the representations of documents. This scheme greatly saves the time for training, since additional forwarding of the generated or replaced query-document pairs into the language model is not required for our data augmentation methods.</p><p>To empirically validate the efficiency of our methods against the baselines, we report the memory usage and time for training a retrieval model per epoch in <ref type="table" target="#tab_3">Table 3</ref>. As for memory efficiency, all the compared dense retrieval models using data augmentation methods, including ours, use the same amount of maximum GPU memory. This shows that the overhead of memory usage comes from operations in the large-size language model, such as BERT , not from manipulating the obtained document representations to augment the query-document pairs. Technically speaking, there are no additional parameters to augment document representations; thus our interpolation and perturbation methods do not increase the memory usage. On the other hand, DPR w/ AR excessively increases the memory usage, since it requires an extra forwarding process to the language model to represent the additional word-replaced sentences for regularization, instead of using the already obtained dense representations like ours.</p><p>We also report the training time for dense retrievers in <ref type="table" target="#tab_3">Table 3</ref>. Note that, for the explicit augmentation method based models, such as DPR w/ QA and DPR w/ DA, we exclude the extra time for training a generation model and generating a query or document for the given text. Also, we additionally generate the same number of query-document pairs in the training set, where the total amount of training data-points for DPR w/ QA and DPR w/ DA baselines are twice larger than the original dataset. Unlike these explicit query or document generation baselines, we perturb the document n times, but also interpolate the representations of positive and negative documents. As shown in Table 3, our DAR is about doubly more efficient than the explicit text augmentation methods, since DPR w/ QA and DPR w/ DA explicitly augment querydocument pairs instead of using the obtained dense T-5 T-20 T-100 DPR <ref type="bibr" target="#b8">(Karpukhin et al., 2020)</ref>   representations like ours. Also, our DAR takes a little more time to augment document representations than the base DPR model, while significantly improving retrieval performances as shown in <ref type="table" target="#tab_0">Table 1</ref>. Even compared to the term replacement based regularization model (DPR w/ AR), our DAR shows noticeable efficiency, since an additional embedding process of the document after the word replacement on it requires another forwarding step besides the original forwarding step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Reproduction of DPR</head><p>We strictly set the batch size as 32 for training all the dense retrievers using the DPR framework; therefore the retrieval performances are different from the originally reported ones in <ref type="bibr" target="#b8">Karpukhin et al. (2020)</ref> that use a batch size of 128. However, while we use the available code from the DPR paper, one may wonder if our reproduction result is accurate. Therefore, since <ref type="bibr" target="#b8">Karpukhin et al. (2020)</ref> provided the retrieval performances of the DPR with different batch sizes (e.g., a batch size of 32), evaluated on the development (validation) set of the NQ dataset, we compare the Top-K accuracy between the reported scores and our reproduced scores. <ref type="table" target="#tab_13">Table 8</ref> shows that our reproduced Top-K accuracy scores with three different Ks (e.g.,  are indeed similar to the reported ones, with ours even higher, thus showing that our reproductions are accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Experiment on WebQuestions</head><p>One may have a concern that, as a sparse retrieval model -BM25 -outperforms all the other dense retrieval models on the TQA dataset in <ref type="table" target="#tab_0">Table 1</ref>, TQA is not good enough to demonstrate the strength of our dense augmentation strategy. While we believe that sparse retrieval models are not our competitors as we aim to improve the dense retrieval models with data augmentation, in order to clear out such a concern, we additionally train and evaluate our DAR on the WebQuestions (WQ) dataset <ref type="bibr" target="#b0">(Berant et al., 2013)</ref>, following the data processing procedure from <ref type="bibr" target="#b8">(Karpukhin et al., 2020)</ref>. As Table 9 shows, our DAR outperforms both dense and sparse retrieval models. Thus, the best scheme  among sparse and dense retrievers still depends on the dataset, and combining sparse and dense models to complement each other will be a valuable research direction, which we leave as future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(Left) The number of labeled and unlabeled documents for the Natural Question dataset. (Right) T-SNE (Maaten and Hinton, 2008) visualization of randomly sampled document representations from the DPR model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Our document augmenting schemes of interpolation and perturbation on a dense representation space. Pos. and Neg. denote positive and negative documents to the query.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Retrieval results on the labeled and unlabeled documents in the NQ dataset with MRR as an evaluation metric.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FigureFigure 5 :</head><label>5</label><figDesc>Exact Match (EM) scores for a reader on the NQ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>BM25  32.46 20.78 78.25 62.94 43.77 22.11 55.28 34.85 83.15 76.41 66.28  46.30 DPR 39.55 25.61 83.77 72.94 54.02 27.45 44.29 27.24 80.50 71.07 57.74 33.63 DPR w/ QA 40.00 24.93 83.46 72.13 55.46 27.67 46.27 28.08 80.76 71.88 59.14 35.90 DPR w/ DA 41.28 26.60 83.68 72.83 55.51 29.31 46.08 27.82 80.42 71.55 58.64 35.85 DPR w/ AR 41.18 26.04 83.60 73.41 55.51 29.11 45.13 27.57 80.65 71.68 58.09 34.52 DAR (Ours) 42.92 27.12 84.18 75.04 57.62 30.42 47.32 28.70 81.30 72.66 59.88 36.94 QAR (Ours) 43.09 27.64 84.21 74.76 57.51 31.25 47.21 29.00 80.91 72.12 59.94 36.92 Retrieval results on NQ and TQA datasets, including the variant of our model -QAR:</figDesc><table><row><cell cols="2">Natural Questions (NQ)</cell><cell></cell><cell>TriviaQA (TQA)</cell><cell></cell><cell></cell></row><row><cell>MRR MAP T-100 T-20</cell><cell>T-5</cell><cell>T-1</cell><cell>MRR MAP T-100 T-20</cell><cell>T-5</cell><cell>T-1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on the MS MARCO subsets with ANCE as a denser retriever.</figDesc><table><row><cell></cell><cell cols="2">Time (Min.) Memory (MiB)</cell></row><row><cell>DPR</cell><cell>19</cell><cell>22,071</cell></row><row><cell>DPR w/ QA</cell><cell>41</cell><cell>22,071</cell></row><row><cell>DPR w/ DA</cell><cell>38</cell><cell>22,071</cell></row><row><cell>DPR w/ AR</cell><cell>29</cell><cell>38,986</cell></row><row><cell>DAR (Ours)</cell><cell>21</cell><cell>22,071</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Wall-clock time and</cell></row><row><cell>maximum memory usage for</cell></row><row><cell>training a DPR model per epoch.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>DAR (Ours) 42.92 27.12 75.04 57.62 w/o Perturbation 41.26 26.19 73.68 55.37 w/o Interpolation 40.40 25.70 73.41 55.29 DPR 39.55 25.61 72.94 54.02</figDesc><table><row><cell>MRR MAP T-20</cell><cell>T-5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies of our DAR on the NQ dataset by removing interpolation or perturbation.</figDesc><table><row><cell></cell><cell>0.74</cell><cell></cell></row><row><cell>T-20</cell><cell>0.66 0.70 0.72 0.68</cell><cell></cell><cell>DPR DAR (Ours)</cell></row><row><cell></cell><cell>8</cell><cell>16 Batch sizes</cell><cell>32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>DPR+HN 53.40 33.38 84.82 43.21 DAR+HN (Ours) 54.18 33.71 85.35 44.18</figDesc><table><row><cell>MRR MAP T-100</cell><cell>T-1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Retrieval results with hard negatives (HN) from BM25 on the NQ dataset for the DPR framework.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>BM25 29.60 28.05 77.87 61.30 42.27 18.86 DPR 31.79 29.94 88.30 70.48 45.48 19.18 DPR w/ QA 30.02 28.26 86.82 68.80 43.95 17.56 DPR w/ DA 31.96 30.25 87.75 71.29 46.55 19.03 DPR w/ AR 31.41 29.50 88.27 70.57 45.10 19.12 DAR (Ours) 33.37 31.49 88.93 73.70 48.38 20.16</figDesc><table><row><cell>MRR MAP T-100 T-20</cell><cell>T-5</cell><cell>T-1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Statistics for training, validation, and test sets onthe NQ, TQA, and randomly sampled MS MARCO datasets. Note that, for MS MARCO, we only sample the number of training query-document pairs except for the validation set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Comparison of the DPR models' Top-K accuracy between the reported and reproduced scores. Best performance is highlighted in bold.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>BM25 29.75 19.15 75.49 62.40 41.83 18.90 DPR 33.34 21.76 78.64 65.75 45.87 22.00 DAR (Ours) 34.48 22.16 78.79 67.37 47.54 23.23</figDesc><table><row><cell>MRR MAP T-100 T-20</cell><cell>T-5</cell><cell>T-1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 :</head><label>9</label><figDesc>Retrieval results on the WQ dataset, in which the best performance is highlighted in bold.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/facebookresearch/DPR 2 See footnote 1. 3 https://github.com/microsoft/ANCE</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by Institute for Information and communications Technology Promotion (IITP) grant funded by the Korea government (MSIT) (No. 2018-0-00582, Prediction and augmentation of the credibility distribution via linguistic analysis and automated evidence document collection).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic parsing on freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno>18-21 Octo- ber 2013</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Grand Hyatt Seattle, Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reading wikipedia to answer opendomain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1171</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1870" to="1879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mix-Text: Linguistically-informed interpolation of hidden space for semi-supervised text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.194</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2147" to="2157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SimCSE: Simple contrastive learning of sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingcheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A survey on recent approaches for natural language processing in low-resource scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Hedderich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heike</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jannik</forename><surname>Str?tgen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Klakow</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.201</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2545" to="2568" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised document expansion for information retrieval with stochastic text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soyeong</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinheon</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaehun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.sdp-1.2</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Scholarly Document Processing</title>
		<meeting>the Second Workshop on Scholarly Document Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7" to="17" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.550</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6769" to="6781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<title level="m">Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics</title>
		<editor>Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Phrase retrieval learns passage retrieval, too</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Wettig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Latent retrieval for weakly supervised open domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1612</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6086" to="6096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to perturb word embeddings for out-of-distribution qa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seanie</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minki</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Embedding-based zero-shot retrieval through query generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.10270</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Nlp augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Zero-shot neural passage retrieval via domain-targeted synthetic question generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Korotkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1075" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generation-augmented retrieval for opendomain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.316</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4089" to="4100" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reader-guided passage reranking for opendomain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.29</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="344" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rocketqa: An optimized training approach to dense passage retrieval for opendomain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingqi</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5835" to="5847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Okapi at TREC-3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micheline</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Third Text REtrieval Conference</title>
		<meeting>The Third Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-11-02" />
			<biblScope unit="volume">500</biblScope>
			<biblScope unit="page" from="109" to="126" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An axiomatic approach to regularizing neural ranking models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corby</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2019</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2019<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-07-21" />
			<biblScope unit="page" from="981" to="984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A survey on image data augmentation for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><surname>Shorten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Taghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khoshgoftaar</surname></persName>
		</author>
		<idno type="DOI">10.1186/s40537-019-0197-0</idno>
	</analytic>
	<monogr>
		<title level="j">J. Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">60</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">56</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nandan</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>R?ckl?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Abhishek Srivastava, and Iryna Gurevych. Round 2</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Manifold mixup: Better representations by interpolating hidden states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6438" to="6447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The TREC-8 question answering track report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Eighth Text REtrieval Conference, TREC 1999</title>
		<meeting>The Eighth Text REtrieval Conference, TREC 1999<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="1999-11-17" />
		</imprint>
		<respStmt>
			<orgName>NIST Special Publication. National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">EDA: Easy data augmentation techniques for boosting performance on text classification tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6383" to="6389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Approximate nearest neighbor negative contrastive learning for dense text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwok-Fung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junaid</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold</forename><surname>Overwijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">BatchMixup: Improving training by interpolating hidden states of the entire mini-batch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.434</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4908" to="4912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Large batch optimization for deep learning: Training BERT in 76 minutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<title level="m">mixup: Beyond empirical risk minimization. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
