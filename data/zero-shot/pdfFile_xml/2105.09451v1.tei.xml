<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Computer Vision and Image Understanding Anabranch Network for Camouflaged Object Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung-Nghia</forename><surname>Le</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">The Graduate University for Advanced Studies (SOKENDAI)</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tam</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Dayton</orgName>
								<address>
									<postCode>45469</postCode>
									<region>Ohio</region>
									<country>United States of America</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongliang</forename><surname>Nie</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Dayton</orgName>
								<address>
									<postCode>45469</postCode>
									<region>Ohio</region>
									<country>United States of America</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Triet</forename><surname>Tran</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">University of Science</orgName>
								<orgName type="institution" key="instit2">VNU-HCM</orgName>
								<address>
									<settlement>Ho Chi Minh</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Sugimoto</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">National Institute of Informatics</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Computer Vision and Image Understanding Anabranch Network for Camouflaged Object Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1 journal homepage: www.elsevier.com</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Camouflaged objects attempt to conceal their texture into the background and discriminating them from the background is hard even for human beings. The main objective of this paper is to explore the camouflaged object segmentation problem, namely, segmenting the camouflaged object(s) for a given image. This problem has not been well studied in spite of a wide range of potential applications including the preservation of wild animals and the discovery of new species, surveillance systems, search-and-rescue missions in the event of natural disasters such as earthquakes, floods or hurricanes. This paper addresses a new challenging problem of camouflaged object segmentation. To address this problem, we provide a new image dataset of camouflaged objects for benchmarking purposes. In addition, we propose a general end-to-end network, called the Anabranch Network, that leverages both classification and segmentation tasks. Different from existing networks for segmentation, our proposed network possesses the second branch for classification to predict the probability of containing camouflaged object(s) in an image, which is then fused into the main branch for segmentation to boost up the segmentation accuracy. Extensive experiments conducted on the newly built dataset demonstrate the effectiveness of our network using various fully convolutional networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ture patterns. These objects first evaluate their surrounding environment and then change to camouflaged textures via the colorization of cloths or cover. For either type of camouflaged objects, it is not obvious to identify them in images. <ref type="figure" target="#fig_0">Figure   1</ref> shows a few examples of camouflaged objects in real life.</p><p>Based on this figure, we can easily see how challenging camouflaged object segmentation is.</p><p>Camouflaged objects have naturally evolved to exploit weaknesses in the visual system of their prey or predator, and thus understanding such a mechanism will provide some insights for segmenting camouflaged objects. Without any prior, we human beings easily miss detecting camouflaged objects; however, once we are informed a camouflaged object exists in an image, we can carefully scan the entire image to detect it. This also comes up in nature: once the predator has an awareness of camouflaged animals in the scene, then it makes efforts to localize them for hunting. Therefore, utilizing the awareness as a prior can be an essential cue for camouflaged object segmentation. Then, the challenge is what awareness should be incorpo-rated in what way. We consider that classification scheme can be used as the awareness of existing of camouflaged objects in an image in order to combine with detection/segmentation scheme.</p><p>As reviewed in <ref type="bibr" target="#b40">Singh et al. (Singh et al., 2013)</ref>, there are a few methods that study camouflaged object detection in simple contexts. These methods use hand-crafted low-level features such as color, edge or texture to detect camouflaged objects. Such hand-crafted low-level features are designed to be as much discriminative as possible for detecting and segmenting objects, which is the opposite way to camouflage. Therefore, their performances are limited to camouflaged object segmentation. Furthermore, these methods can be applied only to relatively low-resolution images with a uniform background. Deep features, on the other hand, are known to outperform low-level features in many tasks in computer vision such as image classification <ref type="bibr" target="#b17">(Krizhevsky et al., 2012)</ref>, image semantic segmentation <ref type="bibr" target="#b37">(Shelhamer et al., 2016)</ref>, action recognition <ref type="bibr" target="#b43">(Tran et al., 2015)</ref>, and face recognition <ref type="bibr" target="#b48">(Wen et al., 2016)</ref>. We thus expect that deep features can replace low-level features even for camouflaged object segmentation by incorporating them into a new network. To use them, however, a large number of data of camouflaged objects are required. Nevertheless, there is no public dataset (both for training data and testing data) for the camouflaged object segmentation problem. This obviously poses serious problems to (1) train deep learning models and (2) evaluate the performance of the proposed network.</p><p>The overall contribution of this paper is two-fold:</p><p>? We provide a new image dataset of camouflaged objects to promote new methods for camouflaged object segmentation. Our newly constructed Camouflaged Object (CAMO) dataset consists of 1250 images, each of which contains at least one camouflaged object. Pixel-wise ground-truths are manually annotated to each image. Furthermore, images in the CAMO dataset involve a variety of challenging scenarios such as object appearance, background clutter, shape complexity, small object, object occlusion, multiple objects, and distraction. We emphasize that this is the very first dataset for camouflage segmentation.</p><p>? We propose a novel end-to-end network, called the Anabranch 1 Network (ANet), which is general, conceptually simple, flexible, and efficient for camouflaged object segmentation. Our proposed ANet leverages the advantages of different network streams, namely, the classification stream and the segmentation stream, in order to segment camouflaged objects. The classification stream is used as the awareness of existing of camouflaged object(s) in an image. This design is motivated by the observation that there is no guarantee that a camouflaged object is always present in an image and thus we first identify whether a camouflaged object is present in an image and then only if exists, the object(s) should be accurately segmented; nothing is segmented otherwise. Extensive experiments verify the effectiveness of our proposed ANet when 1 An anabranch is a section of a river or stream that diverts from the main channel or stem of the watercourse and rejoins the main stem downstream. the network is applied on various fully convolutional networks (FCNs) for camouflaged object segmentation.</p><p>The datasets, evaluation scripts, models, and results are publicly available at our websites 2,3</p><p>The remainder of this paper is organized as follows. Section 2 summarizes the related work. Next, Section 3 and Section 4 introduce the constructed dataset and the proposed network, respectively. Section 5 then reports extensive experiments over the proposed method and baselines on the newly constructed dataset. Section 5.5.2 discusses the joint training of ANet. Finally, Section 6 draws the conclusion and paves the way for the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This section first reviews existing works on camouflaged object segmentation and salient object segmentation, which share some similarities. We also clarify that the camouflaged object segmentation is more challenging than the salient object segmentation. Then, we briefly introduce some advancements of two-stream networks in several computer vision tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Camouflaged Object Segmentation</head><p>As addressed in the intensive survey compiled by <ref type="bibr" target="#b40">Singh et al. (Singh et al., 2013)</ref>, most of the related work <ref type="bibr" target="#b15">(Kavitha et al., 2011;</ref><ref type="bibr" target="#b32">Pan et al., 2011;</ref><ref type="bibr" target="#b41">Siricharoen et al., 2010;</ref><ref type="bibr" target="#b42">Song and Geng, 2010;</ref><ref type="bibr" target="#b52">Yin et al., 2011</ref>) uses handcrafted, low-level features for camouflaged region detection/segmentation where the similarity between the camouflaged region and the background is evaluated using low-level features, e.g., color, shape, orientation, and brightness <ref type="bibr" target="#b6">(Galun et al., 2003;</ref><ref type="bibr" target="#b42">Song and Geng, 2010;</ref><ref type="bibr" target="#b49">Xue et al., 2016)</ref>. We note that there is little work that directly deals with camouflaged object detection; most of the work is dedicated to detecting the foreground region even when some of its texture is similar to the background. <ref type="bibr" target="#b32">Pan et al. (Pan et al., 2011)</ref> proposed a 3D convexity based method to detect camouflaged objects in images. <ref type="bibr" target="#b27">Liu et al. (Liu et al., 2012)</ref> 4 integrated the top-down information based on the expectation maximization framework for foreground object detection. Sengottuvelan et al. (P. <ref type="bibr" target="#b31">Sengottuvelan and Shanmugam, 2008)</ref> applied a gray level co-occurrence matrix method to identify the camouflaged object in images with a simple background. In the case where some parts of a moving object and the background share the similar texture, optical flow is combined with color to detect/segment the moving object <ref type="bibr" target="#b52">(Yin et al., 2011;</ref><ref type="bibr" target="#b5">Gallego and Bertolino, 2014)</ref>. All these methods use handcrafted low-level features and work for only a few cases where images/videos have the simple and non-uniform background.</p><p>Their performances are also unsatisfactory in camouflage detection/segmentation when there is a strong similarity between the foreground and the background.</p><p>One of the main reasons why there has been very little work on camouflaged object segmentation is the lack of the standard dataset for this problem. A benchmarking dataset is thus mandatory to develop advanced techniques for camouflaged object segmentation. Note that the dataset has to include both training data and testing data. To our best of knowledge, there is so far no standard dataset for camouflage object segmentation, which has the sufficient number of data for training and evaluating deep networks. A few of datasets related to camouflaged animals have been proposed, but they have a limited number of samples. <ref type="bibr" target="#b33">Bideau et al. (Pia Bideau, 2016)</ref>  images of camouflaged animals. Therefore, we collect a new camouflaged object dataset. Note that our work is the first work solve the real camouflage segmentation problem in diverse images. Different from the two above datasets, our constructed dataset consists of both camouflaged animals and human.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Salient Object Segmentation</head><p>Salient object predictors aim to detect and segment salient objects in images. Though "saliency" is opposed to "camouflage", techniques developed for salient object segmentation may be useful for camouflaged object segmentation. This is because the two tasks highlight image regions with certain characteristics.</p><p>Early work on salient object segmentation is based on biologically-inspired approaches <ref type="bibr" target="#b11">(Itti et al., 1998;</ref><ref type="bibr" target="#b16">Koch and Ullman, 1987)</ref> where used features are the contrast of low-level features such as orientation of edges, or direction of movement.</p><p>Since human vision is sensitive to color, different approaches using color were proposed where the contrast of color features is locally or globally analyzed <ref type="bibr" target="#b0">(Achanta et al., 2009;</ref><ref type="bibr" target="#b2">Cheng et al., 2011)</ref>. Local analysis methods estimate the saliency of a particular image region against its neighborhoods based on color histogram comparison <ref type="bibr" target="#b2">(Cheng et al., 2011)</ref>; global analysis methods achieve globally consistent results by computing color dissimilarities to the mean image color <ref type="bibr" target="#b0">(Achanta et al., 2009</ref>). Various patch-based methods were also proposed that estimate dissimilarity between image patches <ref type="bibr" target="#b2">(Cheng et al., 2011;</ref><ref type="bibr" target="#b30">Margolin et al., 2013;</ref><ref type="bibr" target="#b53">Zhang et al., 2015)</ref>. To leverage the advantage of deep learning, recent methods first conduct superpixel segmentation and then feed segmented regions into a convolutional neural network (CNN) individually to obtain saliency scores <ref type="bibr" target="#b18">(Le and Sugimoto, 2018;</ref><ref type="bibr" target="#b22">Li and Yu, 2015;</ref><ref type="bibr" target="#b44">Wang et al., 2015)</ref>. More recent methods modified fully convolutional networks (FCNs) to compute a pixel-wise saliency map <ref type="bibr" target="#b10">(Hou et al., 2017;</ref><ref type="bibr" target="#b20">Le and Sugimoto, 2017;</ref><ref type="bibr" target="#b21">Li et al., 2017a;</ref><ref type="bibr" target="#b26">Liu and Han, 2016;</ref><ref type="bibr">Wang et al., 2017b,a)</ref>. Skip-layer structures are usually employed in FCNs to obtain multi-scale feature maps, which are critically needed to segment a salient object <ref type="bibr" target="#b20">(Le and Sugimoto, 2017;</ref><ref type="bibr" target="#b26">Liu and Han, 2016;</ref><ref type="bibr" target="#b10">Hou et al., 2017)</ref>. <ref type="bibr" target="#b23">Li et al. (Li and Yu, 2016)</ref>  based networks, which are originally developed for categoryspecific object detection, are also utilized to identify salient objects <ref type="bibr" target="#b8">(Han et al., 2018b;</ref><ref type="bibr" target="#b19">Le and Sugimoto, 2019)</ref>. In addition, salient objects from common categories can be co-segmented by exploiting correspondence relationship among multiple relevant images <ref type="bibr" target="#b7">(Han et al., 2018a;</ref><ref type="bibr" target="#b51">Yao et al., 2017)</ref>.</p><p>It is worth noting that camouflaged object segmentation is more challenging than salient object segmentation. In fact, salient objects tend to be outstanding and discriminative from the background. We thus only need to focus on identifying such outstanding and discriminative regions to segment salient objects. Camouflaged objects, however, tend to conceal themselves into the background environment by decreasing discriminativeness as much as possible. This makes it hard to identify boundaries of camouflaged objects. In particular, when the background is cluttered, camouflaged objects blend with the background too much, and discriminating them from the background becomes even harder.</p><p>We remark that the same object can be either a camouflaged object or a salient object, meaning that the differences between camouflaged objects and salient objects are ambiguous (cf. <ref type="figure" target="#fig_2">Fig. 2</ref>). This suggests that some salient object detection methods may work for the camouflaged object segmentation task; however, this is not the case. Indeed, existing methods on salient object segmentation are designed to segment objects for any reason because a salient object is assumed to always exist in a training/testing image. On the contrary, there is no guarantee that a camouflaged object is always present in an image. The approach to camouflaged object segmentation should be com-pletely different so that it allows us to handle practical scenarios where we do not know whether or not a target object exists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Two-Stream Network</head><p>There are several works that use two-stream architectures <ref type="bibr" target="#b38">(Simonyan and Zisserman, 2014a;</ref><ref type="bibr" target="#b4">Feichtenhofer et al., 2016;</ref><ref type="bibr" target="#b46">Wang et al., 2016;</ref><ref type="bibr" target="#b12">Jain et al., 2017;</ref><ref type="bibr" target="#b9">He et al., 2017;</ref><ref type="bibr" target="#b24">Li et al., 2017b)</ref>. Fusion networks were proposed for action recognition <ref type="bibr" target="#b38">(Simonyan and Zisserman, 2014a;</ref><ref type="bibr" target="#b4">Feichtenhofer et al., 2016;</ref><ref type="bibr" target="#b46">Wang et al., 2016)</ref> and object segmentation <ref type="bibr" target="#b12">(Jain et al., 2017)</ref> in videos. The network consists of an appearance stream and a motion stream, and the final result is fused from the outputs of the two streams. Two streams, however, have the same architecture for the same task despite the fact that they receive different inputs (i.e., video frame and optical flow). Two streams (i.e., a classification stream and a segmentation stream) of region-based networks for instance segmentation <ref type="bibr" target="#b9">(He et al., 2017;</ref><ref type="bibr" target="#b24">Li et al., 2017b)</ref> have different architectures for different tasks. The two streams are built up from the region proposal network <ref type="bibr" target="#b34">(Ren et al., 2015)</ref> and then work on specific regions.</p><p>In contrast with the above mentioned two-stream networks, the two streams (the classification stream and the segmentation stream) of our proposed ANet have different architectures mutually reinforce each other on the whole image for different tasks. The output of the classification stream is fused with that of the segmentation stream to enhance segmentation result. In this sense, the role of the classification stream is complementary to that of the segmentation stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Camouflaged Object Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motivation</head><p>In the literature, no dataset is available for camouflaged object segmentation. To promote camouflaged object segmentation, a publicly available dataset with pixel-wise ground-truth annotation is mandatory. Note that the dataset has to include both training data and testing data. The training data is suitable for training a deep neural network whereas the testing data is used for evaluation. Therefore, we aim to construct a dataset, Camouflaged Object (CAMO) dataset, to promote advancements in camouflaged object segmentation and its evaluation.</p><p>We stress that our CAMO dataset is the very first dataset for camouflaged object segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dataset Construction</head><p>To build the dataset, we initially collected 3000 images in which at least one camouflaged object exists. We collected the images of camouflaged objects from the Internet with keywords: "camouflaged objects", "camouflaged animals", "concealed objects", "hidden animals", "camouflaged soldier", "human body painting", etc. Note that we consider both types of camouflaged objects, namely, natural and artificial camouflage as mentioned in Section 1. Then, we manually discarded images with low resolution or duplication. Finally, we ended up with 1250 images. To avoid inconsistency in labeling groundtruth, we asked three people to annotate camouflaged objects in all 1250 images individually using a custom designed interactive segmentation tool. On average, each person takes 2-5 minutes to annotate one image depending on its complexity. The annotation stage spanned about one month.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dataset Description</head><p>We describe in this section the Camouflaged Object (CAMO) dataset specifically designed for the task of camouflaged object segmentation. Some examples are shown in <ref type="figure" target="#fig_0">Fig. 1</ref>   In our CAMO dataset, it is also noteworthy that multiple objects, including separate single objects and spatially connected/overlapping objects, possibly exist in some images (8%). This also makes our dataset more challenging for camouflaged object segmentation. The challenge of the CAMO dataset is also enhanced due to some attributes, i.e. object appearance, background clutter, shape complexity, small object, object occlusion, and distraction (cf. <ref type="figure" target="#fig_3">Fig. 4</ref>):</p><p>? Object appearance: The object has a similar color appearance with the background, causing large ambiguity in segmentation.</p><p>? Background clutter: The background is not uniform but contains small-scale structures or is composed of several complex parts.</p><p>? Shape complexity: The object has complex boundaries such as thin parts and holes, which is usually the case for the legs of insects.</p><p>? Small object: The ratio between the camouflaged object area and the whole image area is smaller than 0.1.</p><p>? Object occlusion: The object is occluded, resulting in disconnected parts or in touching the image border.</p><p>? Distraction: The image contains distracted objects, resulting in losing attention to camouflaged objects. This makes camouflaged objects more difficult to discover even by human beings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Anabranch Network</head><p>In this section, we give a detailed description of our Anabranch Network (ANet). Specifically, we first discuss the motivation of network design. Then, we introduce the architecture of ANet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Network Design</head><p>As mentioned above, there is no guarantee that a camouflaged object is always present in the scene. Therefore, a method that systematically segments objects for any image will not work. Moreover, directly applying discriminative features from segmentation models (i.e., semantic segmentation and salient object segmentation, etc.) to camouflaged object segmentation is not effective because camouflaged objects conceal their texture into the surrounding environment. In order to segment camouflaged objects, we need an additional function that identifies whether a camouflaged object exists in an image. To do so, each pixel needs to be classified as a part of camouflaged objects or not. Such classification can be used not only to enhance segmentation accuracy but also to segment multiple camouflaged objects. Indeed, this classification only strengthens features extracted from the camouflaged part and weakens features extracted from a non-camouflaged part. Accordingly, segmentation and classification tasks should be closely combined in the network with different architectures for camouflaged object segmentation. <ref type="figure" target="#fig_4">Figure 5</ref> depicts the overview of our proposed ANet, a general network for camouflaged object segmentation. The ANet leverages the advantages of the two different network models, namely, the classification network model based on a convolutional neural network (CNN) and the segmentation network model based on a fully convolutional network (FCN). The CNN reveals object hypotheses in the image <ref type="bibr" target="#b54">(Zhou et al., 2014)</ref>, yielding a cue on whether or not the input image contains camouflaged object(s). We thus aim to train the CNN model so that it classifies two classes: camouflaged image class and noncamouflaged image one. The FCN, on the other hand, provides us with the pixel-wise semantic information in the image. We consider the output of the FCN as the semantic information of different objects in the image. The two outputs from the two network streams are finally fused to produce a pixel-wisely accurate camouflage map which uniformly covers camouflaged objects. We note that our classification stream output, i.e., the probability (scalar), is multiplied (?) with each pixel value of 2D map produced by the segmentation stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Network Architecture</head><p>Segmentation Stream: The input and the output of the segmentation stream are both images, and, thus, any end-to-end FCN for segmentation can be employed. In the sense that we can employ any FCN for segmentation, ANet can be considered as a general network.</p><p>Though, as addressed above, methods for salient object detection alone may not work for accurate camouflaged object segmentation, effective compensation by the classification stream can be expected to boost up the performance in the ANet. Recent state-of-the-art methods for salient object detection have demonstrated their good performances. In addition, as we observed in <ref type="figure" target="#fig_2">Fig. 2</ref>, we have ambiguity in difference between camouflaged objects and salient objects. Based on these reasons, we consider a saliency model is suitable to use into ANet. We can easily swap the model with any other saliency model. We remark that it is also interesting to utilize saliency models in a similar but different domain such as camouflaged object segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification Stream:</head><p>We build the classification stream on top of convolution layers, which play as feature extraction module, of FCNs. In particular, we use three fully connected  layers with 4096, 4096, and 2 filters, respectively (cf. <ref type="figure" target="#fig_4">Fig. 5</ref> and <ref type="table" target="#tab_2">Table 1</ref>). We note that each of the first two fully connected layers is followed by a Rectified Linear Unit (ReLU) activation layer <ref type="bibr" target="#b17">(Krizhevsky et al., 2012)</ref>. The last layer is the soft-max layer. To train this stream, we use soft-max with the crossentropy loss. During the training process, we use dropout layers with a dropout rate of 50% after the first two fully connected layers to avoid over-fitting.</p><p>As seen above, our introduced ANet is conceptually simple.</p><p>Each stream in ANet has its own task and; thus, each stream is expected to be individually trained using data suitable for the task to enhance its ability. Then, the fusion of the two streams into one boosts up segmentation accuracy of camouflaged objects. Furthermore, ANet is flexible in the sense that we can employ any end-to-end FCN and easily switch it with another.</p><p>As we see in our experimental results, ANet is robust since it maintains good segmentation accuracy in both cases of with and without a camouflaged object in an input image. We also see its computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we first introduce evaluation datasets and evaluation criteria used in experiments. Then we describe implementing details as well as network training of our proposed ANet and other baselines. In experiments, we show the potential ability of transforming domain from salient object segmentation to camouflaged object segmentation. We next compare instances of our ANet with the FCN models fine-tuned on two evaluation datasets, to demonstrate that classifying camouflaged objects and non-camouflaged objects can boost up the camouflaged object segmentation. We also present the effi- ciency of our general ANet through short network training and fast running time. These results can be considered as the first baselines for the camouflaged object segmentation problem. Finally, we discuss the challenge of the proposed CAMO dataset to show rooms for further research on this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and Experimentation Setup</head><p>Any image in the CAMO dataset contains at least one camouflaged object. There is thus prior information that camouflaged objects are always present. In a realistic scenario, however, there is no guarantee that a camouflaged object is always present in an image. Therefore, we set up another dataset, called CAMO-COCO 5 , consisting of camouflaged object images and non-camouflaged object images. We used the entire images in the CAMO dataset for the camouflaged object images and collected additional 1250 images from the MS-COCO dataset <ref type="bibr" target="#b25">(Lin et al., 2014)</ref> for the non-camouflaged object images (cf. <ref type="figure">Fig. 6</ref>). We note that we created zero-mask ground-truth labels (all pixels have zero values) for the non-camouflaged object images. For each of the camouflaged image and noncamouflaged object image sets, we randomly chose 80% for training (1000 images) and used the remaining 20% for testing (250 images). <ref type="table" target="#tab_3">Table 2</ref> shows the number of images in the CAMO-COCO dataset used for experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation Criteria</head><p>We used the F-measure (F ? ) <ref type="bibr" target="#b0">(Achanta et al., 2009</ref>), Intersection Over Union (IOU) <ref type="bibr" target="#b28">(Long et al., 2015)</ref>, and Mean Absolute Error (MAE) as the metrics to evaluate obtained results. The 5 The link to the dataset will be available along with the publication of this paper.</p><p>first metric, F-measure, is a balanced measurement between precision and recall as follows:</p><formula xml:id="formula_0">F ? = 1 + ? 2 Precision ? Recall ? 2 ? Precision + Recall .</formula><p>(1)</p><p>Note that we set ? 2 = 0.3 as used in <ref type="bibr" target="#b0">(Achanta et al., 2009)</ref> to put an emphasis on precision. IOU is the area ratio of the overlapping against the union between the predicted camouflage map and the ground-truth map. Meanwhile, MAE is the average of the pixel-wise absolute differences between the predicted camouflage map and the ground-truth.</p><p>For MAE, we used the raw grayscale camouflage map. For the other metrics, we binarized the results depending on two contexts. In the first context, we assume that camouflaged objects are always present in every image like salient objects; we used an adaptive threshold <ref type="bibr" target="#b13">(Jia and Han, 2013</ref>) ? = ? + ? where ? and ? are the mean value and the standard deviation of the map, respectively. In the second context which is much closer to a real-world scenario, we assume that the existence of camouflaged objects is not guaranteed in each image; we used the fixed threshold ? = 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Implementation Details</head><p>To demonstrate the generality and flexibility of our proposed ANet, we employ various recent state-of-the-art FCNbased salient object segmentation models for the segmentation stream. They are DHS <ref type="bibr" target="#b26">(Liu and Han, 2016)</ref>, DSS <ref type="bibr" target="#b10">(Hou et al., 2017)</ref>, SRM <ref type="bibr" target="#b47">(Wang et al., 2017b)</ref>, and WSS <ref type="bibr" target="#b45">(Wang et al., 2017a)</ref>.</p><p>For each employed model, we train two streams sequentially. We first fine-tuned the segmentation stream using the CAMO dataset from the publicly available pre-trained model and then trained the classification stream using CAMO-COCO with fixed parameters in the segmentation stream. The final model is considered as our baseline for camouflaged object segmentation.</p><p>In both training steps, we set the size of each mini-batch to 2, and used the Stochastic Gradient Descent (SGD) optimization <ref type="bibr" target="#b35">(Rumelhart et al., 1988)</ref> with a moment ? = 0.9 and a weight decay of 0.0005. We trained the segmentation stream  for 10 epochs (corresponding to 5k iterations) with the learning rate of 10 ?4 and trained the classification stream for 3 epochs (3k iterations) with the learning rate of 10 ?6 . During the finetuning process, a simple data augmentation technique was used to avoid over-fitting. Images were rescaled to the resolution of the specific FCN employed in the segmentation stream and randomly flipped horizontally.</p><formula xml:id="formula_1">MAE ? F ? ? IOU ? F ? ? IOU ? MAE ? F ? ? IOU ? F ? ? IOU</formula><p>We note that for comparison, our employed FCN models (original) were trained on CAMO (or CAMO-COCO) dataset for 10 epochs with the learning rate of 10 ?4 and the other parameters were set similarly to our ANet. We also remark that we implemented our method in C/C++, using Caffe <ref type="bibr" target="#b14">(Jia et al., 2014)</ref> toolbox and conducted all the experiments on a computer with a Core i7 3.6 GHz processor, 32 GB of RAM, and two GTX 1080 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1.">Baseline Evaluation</head><p>We evaluated our model (baseline) on two datasets: CAMO (camouflaged object images only) and CAMO-COCO (both camouflaged and non-camouflaged object images) where only test images in each dataset were used for this evaluation. We employed an FCN (DHS <ref type="bibr" target="#b26">(Liu and Han, 2016)</ref>, DSS <ref type="bibr" target="#b10">(Hou et al., 2017)</ref>, SRM <ref type="bibr" target="#b47">(Wang et al., 2017b)</ref>, or WSS <ref type="bibr" target="#b45">(Wang et al., 2017a))</ref> as the segmentation stream. For comparison, we also evaluated each of used FCNs to see how it works for camouflaged object segmentation. Each was evaluated with three different models: the pre-trained models on saliency datasets (e.g.</p><p>MSRA <ref type="bibr" target="#b1">(Cheng et al., 2015)</ref>, DUT-OMRON <ref type="bibr" target="#b50">(Yang et al., 2013)</ref>, and DUTS <ref type="bibr" target="#b45">(Wang et al., 2017a)</ref>), the models that were finetuned with CAMO or CAMO-COCO (the model is specified by the word inside the parentheses). <ref type="figure" target="#fig_5">Figures 7, 8</ref>, and Table 3 illustrate the obtained results of our experiments. We note that ANet-DHS (baseline), for example, denotes our proposed model employing DHS as the segmentation stream. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2.">Performance of Camouflaged Object Segmentation</head><p>It is interesting for the saliency community to see the segmentation capabilities of saliency systems to segment camouflaged objects (which should actually not be possible due to different properties of those objects). Therefore, we show results of FCNs using pre-trained models for salient object segmentation in <ref type="table" target="#tab_4">Table 3</ref>. Although results are moderate (the worst results among compared methods for each FCN), it also illustrates the potential ability to transfer from salient object segmentation to camouflaged object segmentation due to the ambiguous border of these kinds of object. This shows the possibility of using existing saliency models to tackle camouflaged object segmentation problem. <ref type="table" target="#tab_4">Table 3</ref> shows that the fine-tuned FCN models achieve around 60+% accuracy in F ? and IOU (around 0.1 in MAE). Naturally, the model fine-tuned with the dataset used for testing performs better than the pre-trained model or the model fine-tuned with the other dataset. We also observe the model fine-tuned with CAMO-COCO performs better than the one fine-tuned with CAMO. This is because CAMO is a subset of CAMO-COCO. These mean that salient object segmentation methods are, to some extent, capable of segmenting even a nottarget object such as a camouflaged object. This may come from the ambiguity of the differences between salient objects and camouflaged objects. This, simultaneously, also supports the use of an FCN developed for salient object segmentation as the segmentation stream in our ANet.</p><p>We then draw our attention to the performances of ANet using an FCN. Since the segmentation stream of ANet is trained using CAMO and its output is multiplied by the output of the classification stream, the segmentation ability of ANet is limited by that of the employed FCN fine-tuned with CAMO. In other words, the performance of ANet on CAMO is expected to be comparable with that of the employed FCN (fine-tuned with CAMO), which is confirmed in    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3.">Performance of Camouflaged Object Classification</head><p>In order to evaluate the level of accuracy achieved by our classification stream, we compared the performance of our classification stream with that of the SVM-based classification using the Bag-of-Words model <ref type="bibr" target="#b3">(Fei-Fei and Perona, 2005)</ref> with SIFT features <ref type="bibr" target="#b29">(Lowe, 1999)</ref> (denoted by SVM-BoW) and</p><p>CNNs, including AlexNet <ref type="bibr" target="#b17">(Krizhevsky et al., 2012)</ref> and VGG-16 <ref type="bibr" target="#b39">(Simonyan and Zisserman, 2014b)</ref>. For CNNs, we finetuned them on our CAMO-COCO dataset from pre-trained models on the ImageNet dataset <ref type="bibr" target="#b36">(Russakovsky et al., 2015)</ref>.</p><p>The results are shown in <ref type="table" target="#tab_7">Table 4</ref>, indicating that the accuracy of our classification stream achieves around 90%, which outperforms SVM-BoW. ANet baseline models are also significantly better than both AlexNet and VGG-16. As a closer look, the classification stream achieves high accuracy on few training iterations (3 epochs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.4.">Computational Efficiency</head><p>We further evaluated the computational efficiency of all the baseline models. We also evaluated the average running time of the original FCN models employed in our ANet. The results are shown in <ref type="table" target="#tab_8">Table 5</ref>. We observe that stacking the classification stream does not incur much more processing time, indicating that the ANet is able to keep the running time computationally efficient. 5.5. Discussion</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1.">Failure Cases</head><p>The accuracy of our baselines on CAMO for camouflaged object segmentation is far lower than the state-of-the-art accuracy for salient object segmentation, which can achieve up to round 90% in F ? . This is mainly caused by the insufficient ability of the segmentation stream when we transfer domain from salient object segmentation to camouflaged object segmentation. <ref type="figure" target="#fig_8">Figure 9</ref> shows some failure segmentation results obtained by our used FCNs where input images involve challenging scenarios in CAMO (object appearance and background clutter in the first row; the shape complexity of insect legs in the second row; distraction in the third row).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2.">Joint Training</head><p>The classification stream is expected to boost the segmentation accuracy in ANet, however, fusing the two streams by the multiplication is still insufficient. Further effective fusion of the two streams should be explored to improve the segmentation accuracy.</p><p>For a two-stream network, jointly training two streams after training each stream separately is known as a strategy to improve the performance <ref type="bibr" target="#b12">(Jain et al., 2017)</ref>. Following this strategy, we jointly trained the ANet (baseline) using CAMO-COCO where we used 10 epochs (10k iterations) with the learning-rates of 10 ?4 (for the segmentation stream) and 10 ?6 (for the classification stream). Other parameters are set similarly to training two streams individually. The loss function for joint-training is defined as the summation of the loss of the classification stream and that of the segmentation stream: where? seg , y seg ,? cls , and y cls denote outputs and ground-truth labels of the segmentation stream and the classification stream, respectively. L cls is the soft-max loss of the classification stream and L seg is the specific loss of the corresponding FCN implemented in the segmentation stream. <ref type="table" target="#tab_9">Table 6</ref> illustrates the improvement of performances by introducing the joint-training to ANet. We see that differently from other two-stream network cases, jointly-training ANet does not lead to better performance for all methods. DHS and WSS reduce accuracy while DSS and SRM slightly increase accuracy (see <ref type="table" target="#tab_9">Table 6</ref>). This can be explained as follows. Each stream of ANet has its own task and thus using training data suitable for the task is mandatory to enhance its ability. During the joint-training of ANet, however, data used for training cannot be suitable for each of the tasks of the two streams, resulting in interfering with each other stream. This is also facilitated by the branch structure of ANet. Accordingly, joint-training potentially brings to ANet some collapse in improving training of each stream.</p><formula xml:id="formula_2">L = L seg (? seg , y seg ) + L cls (? cls , y cls ),<label>(2)</label></formula><p>Therefore, jointly training two streams may not bring gain to ANet due to opposite attributes of camouflaged objects and non-camouflaged objects. The two-step training approach is sufficient for ANet for the task of camouflaged object segmentation on the CAMO-COCO dataset. Further investigation on this issue is left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we addressed an interesting yet challenging problem of camouflaged object segmentation by providing a new image dataset of camouflaged object segmentation where each image is manually annotated with pixel-wise ground-truth.</p><p>We believe that our novel dataset will promote new advancements on camouflaged object segmentation. We also aim to explore camouflaged object segmentation on videos in the near future.</p><p>We proposed a simple and flexible end-to-end network, namely Anabranch Network, for camouflaged object segmentation where the classification stream and the segmentation stream are effectively combined to show the baseline performance. To show effectiveness of our proposed framework, which can boot the segmentation using classification, we applied it to different FCNs. Extensive experiments conducted on the newly built dataset demonstrate the superiority of the proposed network. In addition, our method is computationally efficient.</p><p>This paper focused on only regions, but the potential idea of utilizing classification into segmentation can be useful when applying for instance segmentation in appropriate ways. For examples, instance classification can be combined with instance segmentation, where each instance is classified whether it is camouflaged or salient. This is left for the future work. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>A few examples from our Camouflaged Object (CAMO) dataset with corresponding pixel-level annotations. Camouflaged objects attempt to conceal their texture into the background.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>proposed a camouflaged animal video dataset but the dataset has only 9 videos, and camouflaged animals really exist in a third of videos. The unpublished dataset proposed by Skurowski et.al. 4 has only 76</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>modified the FCN to combine pixel-wise and segment-wise feature maps for saliency value prediction. Wang et al. (Wang et al., 2017b) integrated a pyramid pooling module and a multi-stage refinement mechanism into an FCN for salient object segmentation. Wang et al. Ambiguous differences between camouflaged objects and salient objects. et al., 2017a) proposed a two-stage training method to compute saliency maps using image-level tags. Object proposal</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Some examples of challenging attributes and the attribute distribution over the CAMO dataset.humans in the real world, respectively. Camouflaged animals consist of amphibians, birds, insects, mammals, reptiles, and underwater animals in various environments, i.e., ground, underwater, desert, forest, mountain, and snow. Camouflaged human falls into soldiers on the battlefields and human body painting arts. The ratios for each category are shown inFig. 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Overview of our proposed Anabranch Network (ANet). The proposed network leverages the strength of both image classification and semantic segmentation tasks for camouflaged object segmentation.Fig. 6. Examples of non-camouflaged objects from MS-COCO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figures 7</head><label>7</label><figDesc>and 8 show that ANet yields better results than its employed original FCN (pre-trained) independently of the choice of the FCN. In particular, for non-camouflaged object images, the superiority of performances of ANet against its original FCN is distinguished. This suggests that the fusion with the classification stream indeed works effectively to boost the segmentation accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Visual comparison of our ANet employing different FCNs on natural camouflaged object images (CAMO dataset). From left to right, input image and ground-truth are followed by outputs obtained using ANet with DHS<ref type="bibr" target="#b26">(Liu and Han, 2016)</ref>, DHS (pre-trained), ANet with DSS<ref type="bibr" target="#b10">(Hou et al., 2017)</ref>, DSS (pre-trained), ANet with SRM<ref type="bibr" target="#b47">(Wang et al., 2017b)</ref>, SRM (pre-trained), ANet with WSS<ref type="bibr" target="#b45">(Wang et al., 2017a)</ref>, and WSS (pre-trained) in this order. The results obtained by ANet is surrounded with red rectangles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Visual comparison of our ANet employing different FCNs. From left to right, input image and ground-truth are followed by outputs obtained using ANet with DHS (Liu and Han, 2016), DHS (pre-trained), ANet with DSS (Hou et al., 2017), DSS (pre-trained), ANet with SRM (Wang et al., 2017b), SRM (pre-trained), ANet with WSS (Wang et al., 2017a), and WSS (pre-trained) in this order. The first eight rows are images with artificial camouflaged objects (CAMO dataset), and the last seven rows are images without camouflaged objects (COCO dataset). The results obtained by ANet is surrounded with red rectangles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Failure cases of FCNs on CAMO. Each case is shown in a horizontal triplet of images, from left to right: input image, ground-truth, and result by the best FCN (fine-tuned with CAMO) in this order.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>with the corresponding ground-truth label annotations. We focus on two categories, i.e., naturally camouflaged objects and artificially camouflaged objects, which usually correspond to animals and</figDesc><table><row><cell cols="2">Background clutter</cell><cell>57%</cell><cell></cell><cell>711</cell><cell></cell><cell></cell></row><row><cell cols="2">Shape complexity</cell><cell>40%</cell><cell></cell><cell>506</cell><cell></cell><cell></cell></row><row><cell>Small object</cell><cell></cell><cell>35%</cell><cell></cell><cell>438</cell><cell></cell><cell></cell></row><row><cell cols="2">Object occlusion</cell><cell>29%</cell><cell></cell><cell>359</cell><cell></cell><cell>1250</cell></row><row><cell cols="2">Multiple objects</cell><cell>8%</cell><cell></cell><cell>104</cell><cell></cell><cell></cell></row><row><cell>Distraction</cell><cell></cell><cell>3%</cell><cell></cell><cell>33</cell><cell></cell><cell></cell></row><row><cell>Object Appearance</cell><cell>Background Clutter</cell><cell>Shape Complexity</cell><cell>Small Object</cell><cell>Object Occlusion</cell><cell>Multiple Objects</cell><cell>Distraction</cell></row><row><cell></cell><cell cols="5">(a) Examples of challenging attributes</cell><cell></cell></row><row><cell>62%</cell><cell>57%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>40%</cell><cell>35%</cell><cell>29%</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8%</cell><cell>3%</cell></row><row><cell>Object</cell><cell>Background</cell><cell>Shape</cell><cell>Small</cell><cell>Object</cell><cell>Multiple</cell><cell>Distraction</cell></row><row><cell>appearance</cell><cell>clutter</cell><cell>complexity</cell><cell>object</cell><cell>occlusion</cell><cell>objects</cell><cell></cell></row><row><cell></cell><cell cols="5">(b) Attribute distribution over the CAMO dataset</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Architecture of the classification stream.</figDesc><table><row><cell cols="2">No Layer</cell><cell cols="2">Output Dropping Rate</cell></row><row><cell>1</cell><cell>Fully connected</cell><cell>2048</cell><cell></cell></row><row><cell>2</cell><cell>ReLU</cell><cell>2048</cell><cell></cell></row><row><cell>3</cell><cell>Dropout</cell><cell>2048</cell><cell>50%</cell></row><row><cell>4</cell><cell>Fully connected</cell><cell>2048</cell><cell></cell></row><row><cell>5</cell><cell>ReLU</cell><cell>2048</cell><cell></cell></row><row><cell>6</cell><cell>Dropout</cell><cell>2048</cell><cell>50%</cell></row><row><cell>7</cell><cell>Fully connected</cell><cell>2</cell><cell></cell></row><row><cell>8</cell><cell>Soft-max</cell><cell>2</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>CAMO-COCO dataset used in our experiments.</figDesc><table><row><cell></cell><cell>Training</cell><cell>Testing</cell><cell>Total</cell></row><row><cell>Camouflaged object images (CAMO)</cell><cell>1000</cell><cell>250</cell><cell>1250</cell></row><row><cell>Non-Camouflaged object images (MS-COCO)</cell><cell>1000</cell><cell>250</cell><cell>1250</cell></row><row><cell>Total</cell><cell>2000</cell><cell>500</cell><cell>2500</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Experimental results on two datasets: CAMO dataset (the left part), and CAMO-COCO dataset (the right part). The evaluation is based on F-measure<ref type="bibr" target="#b0">(Achanta et al., 2009</ref>) (the higher the better), IOU<ref type="bibr" target="#b28">(Long et al., 2015)</ref> (the higher the better), and MAE (the smaller the better). The 1st and 2nd places are shown in blue and red, respectively.</figDesc><table><row><cell>Dataset in test</cell><cell>CAMO</cell><cell>CAMO-COCO</cell></row><row><cell>Method</cell><cell>Adaptive Threshold Fixed Threshold</cell><cell>Adaptive Threshold Fixed Threshold</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>. When we evalu-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>The accuracy (%) of camouflaged object classification on the CAMO-COCO dataset.</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell></row><row><cell>SVM-BoW (Fei-Fei and Perona, 2005)</cell><cell>54.0</cell></row><row><cell>AlexNet (Krizhevsky et al., 2012)</cell><cell>76.0</cell></row><row><cell>VGG-16 (Simonyan and Zisserman, 2014b)</cell><cell>88.6</cell></row><row><cell>ANet-DHS</cell><cell>91.4</cell></row><row><cell>ANet-DSS</cell><cell>89.2</cell></row><row><cell>ANet-SRM</cell><cell>90.6</cell></row><row><cell>ANet-WSS</cell><cell>89.6</cell></row><row><cell cols="2">with CAMO have good performance for only CAMO but not</cell></row><row><cell>for CAMO-COCO (not for both).</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 .</head><label>5</label><figDesc>The average wall-clock time in millisecond for each image.</figDesc><table><row><cell>Method</cell><cell cols="2">FCN ANet (baseline)</cell></row><row><cell>DHS (Liu and Han, 2016)</cell><cell>63</cell><cell>73</cell></row><row><cell>DSS (Hou et al., 2017)</cell><cell>76</cell><cell>79</cell></row><row><cell>SRM (Wang et al., 2017b)</cell><cell>81</cell><cell>86</cell></row><row><cell>WSS (Wang et al., 2017a)</cell><cell>61</cell><cell>65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc>Performance of joint-training on CAMO dataset and CAMO-COCO dataset. The best results are shown in blue.</figDesc><table><row><cell>Dataset in test</cell><cell></cell><cell></cell><cell>CAMO</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CAMO-COCO</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell cols="4">Adaptive Threshold Fixed Threshold</cell><cell></cell><cell cols="4">Adaptive Threshold Fixed Threshold</cell></row><row><cell></cell><cell cols="2">MAE ? F ? ?</cell><cell>IOU ?</cell><cell>F ? ?</cell><cell>IOU ?</cell><cell cols="2">MAE ? F ? ?</cell><cell>IOU ?</cell><cell>F ? ?</cell><cell>IOU ?</cell></row><row><cell>ANet-DHS (baseline)</cell><cell>0.130</cell><cell>0.626</cell><cell>0.437</cell><cell>0.631</cell><cell>0.423</cell><cell>0.072</cell><cell>0.812</cell><cell>0.712</cell><cell>0.814</cell><cell>0.705</cell></row><row><cell>ANet-DHS (+joint training)</cell><cell>0.201</cell><cell>0.509</cell><cell>0.314</cell><cell>0.490</cell><cell>0.130</cell><cell>0.109</cell><cell>0.749</cell><cell>0.628</cell><cell>0.758</cell><cell>0.564</cell></row><row><cell>ANet-DSS (baseline)</cell><cell>0.132</cell><cell>0.587</cell><cell>0.404</cell><cell>0.607</cell><cell>0.390</cell><cell>0.067</cell><cell>0.795</cell><cell>0.701</cell><cell>0.804</cell><cell>0.694</cell></row><row><cell>ANet-DSS (+joint training)</cell><cell>0.126</cell><cell>0.644</cell><cell>0.441</cell><cell>0.651</cell><cell>0.417</cell><cell>0.064</cell><cell>0.822</cell><cell>0.719</cell><cell>0.825</cell><cell>0.707</cell></row><row><cell>ANet-SRM (baseline)</cell><cell>0.126</cell><cell>0.654</cell><cell>0.475</cell><cell>0.662</cell><cell>0.466</cell><cell>0.069</cell><cell>0.826</cell><cell>0.732</cell><cell>0.830</cell><cell>0.727</cell></row><row><cell>ANet-SRM (+joint training)</cell><cell>0.123</cell><cell>0.680</cell><cell>0.481</cell><cell>0.682</cell><cell>0.454</cell><cell>0.063</cell><cell>0.839</cell><cell>0.733</cell><cell>0.841</cell><cell>0.726</cell></row><row><cell>ANet-WSS (baseline)</cell><cell>0.140</cell><cell>0.661</cell><cell>0.459</cell><cell>0.643</cell><cell>0.407</cell><cell>0.078</cell><cell>0.826</cell><cell>0.710</cell><cell>0.820</cell><cell>0.697</cell></row><row><cell>ANet-WSS (+joint training)</cell><cell>0.145</cell><cell>0.626</cell><cell>0.394</cell><cell>0.628</cell><cell>0.324</cell><cell>0.074</cell><cell>0.813</cell><cell>0.692</cell><cell>0.816</cell><cell>0.662</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://sites.google.com/view/ltnghia/research/camo 3 https://sites.google.com/site/vantam/research</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://zgwisk.aei.polsl.pl/index.php/en/research/other-research/63animal-camouflage-analysis</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work is in part granted by University of Dayton SEED Grant (US), and by SOKENDAI Short-Stay Study Abroad Program (Japan). We also thank Vamshi Krishna Madaram and Zhe Huang (University of Dayton) for their support in the annotation of the CAMO dataset. We gratefully acknowledge NVIDIA for the support of GPU.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2014.2345401</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="569" to="582" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="409" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A bayesian hierarchical model for learning natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="524" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3468" to="3476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Foreground object segmentation for moving camera sequences based on foreground-background probabilistic models and prior probability maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bertolino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3312" to="3316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Texture segmentation by multiscale aggregation of filter responses and shape elements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="716" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust object co-segmentation using background prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1639" to="1651" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Advanced deeplearning techniques for salient and category-specific object detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="84" to="100" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fusionseg: Learning to combine motion and appearance for fully automatic segmention of generic objects in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Category-independent object-level saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1761" to="1768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An efficient content based image retrieval using color and texture of image subblocks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kavitha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Govardhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Engineering Science and Technology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Shifts in selective visual attention: Towards the underlying neural circuitry. Matters of Intelligence: Conceptual Structures in Cognitive Neuroscience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="page" from="115" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video salient object detection using spatiotemporal deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sugimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="5002" to="5015" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semantic instance meets salient object: Study on video semantic salient instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sugimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1779" to="1788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deeply supervised 3d recurrent fcn for salient object detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sugimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Instance-level salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5455" to="5463" />
		</imprint>
	</monogr>
	<note>Visual saliency based on multiscale deep features</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep contrast learning for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully convolutional instanceaware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dhsnet: Deep hierarchical saliency network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="678" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Foreground object detection using top-down information based on em framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="4204" to="4217" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">What makes a patch distinct?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Margolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1139" to="1146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Performance of decamouflaging through exploratory image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sengottuvelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Emerging Trends in Engineering and Technology</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Study on the camouflaged target detection method based on 3d convexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Modern Applied Science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">152</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">It&apos;s moving! a probabilistic model for causal motion segmentation in moving camera videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pia</forename><surname>Bideau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neurocomputing: Foundations of Research</title>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="page" from="696" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence PP</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ILSVRC</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Survey of object detection methods in camouflaged image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dhawale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IERI Procedia</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="351" to="357" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Robust outdoor human segmentation based on color-based statistical approach and edge combination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siricharoen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aramvith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Chalidabhongse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Siddhichai</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICGCS.2010.5543017</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Green Circuits and Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="463" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A new camouflage texture evaluation method based on wssim and nature image features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Technology</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3D convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep networks for saliency detection via local estimation and global search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3183" to="3192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Val Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A stagewise refinement model for detecting salient objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Camouflage performance analysis and evaluation framework based on features fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="4065" to="4082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3166" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Revisiting co-saliency detection: A novel approach based on two-stage multi-view spectral rotation co-18 clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3196" to="3209" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Detection of the mobile object with camouflage color under dynamic background based on optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Engineering</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="2201" to="2205" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Minimum barrier salient object detection at 80 fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mech</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1404" to="1412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Object detectors emerge in deep scene cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
