<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UNSUPERVISED META-LEARNING FOR FEW-SHOT IMAGE CLASSIFICATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siavash</forename><surname>Khodadadeh</surname></persName>
							<email>siavash.khodadadeh@knights.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ladislau</forename><surname>B?l?ni</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
							<email>shah@crcv.ucf.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Center for Research in Computer Vision</orgName>
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">UNSUPERVISED META-LEARNING FOR FEW-SHOT IMAGE CLASSIFICATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot or one-shot learning of classifiers requires a significant inductive bias towards the type of task to be learned. One way to acquire this is by meta-learning on tasks similar to the target task. In this paper, we propose UMTRA, an algorithm that performs unsupervised, model-agnostic meta-learning for classification tasks. The meta-learning step of UMTRA is performed on a flat collection of unlabeled images. While we assume that these images can be grouped into a diverse set of classes and are relevant to the target task, no explicit information about the classes or any labels are needed. UMTRA uses random sampling and augmentation to create synthetic training tasks for meta-learning phase. Labels are only needed at the final target task learning step, and they can be as little as one sample per class. On the Omniglot and Mini-Imagenet few-shot learning benchmarks, UMTRA outperforms every tested approach based on unsupervised learning of representations, while alternating for the best performance with the recent CACTUs algorithm. Compared to supervised model-agnostic metalearning approaches, UMTRA trades off some classification accuracy for a reduction in the required labels of several orders of magnitude.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Meta-learning or "learning-to-learn" approaches have been proposed in the neural networks literature since the 1980s <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b3">4]</ref>. The general idea is to prepare the network through several learning tasks T 1 . . . T n , in a meta-learning phase such that when presented with the target task T n+1 , the network will be ready to learn it as efficiently as possible.</p><p>Recently proposed model-agnostic meta-learning approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b30">31]</ref> can be applied to any differentiable network. When used for classification, the target learning phase consists of several gradient descent steps on a backpropagated supervised classification loss. Unfortunately, these approaches require the learning tasks T i to have the same supervised learning format as the target task. Acquiring labeled data for a large number of tasks is not only a problem of cost and convenience but also puts conceptual limits on the type of problems that can be solved through meta-learning. If we need to have labeled training data for tasks T 1 . . . T n in order to learn task T n+1 , this limits us to task types that are variations of tasks known and solved (at least by humans).</p><p>In this paper, we propose an algorithm called Unsupervised Meta-learning with Tasks constructed by Random sampling and Augmentation (UMTRA) that performs meta-learning of one-shot or few-shot classifiers in an unsupervised manner on an unlabeled dataset. Instead of starting from a collection of labeled tasks, {. . . T i . . .}, UMTRA starts with a collection of unlabeled data U = {. . . x i . . .}. We have only a set of relatively easy-to-satisfy requirements towards U: Its objects have to be drawn from the same distribution as the objects classified in the target task and it must have a set of classes significantly larger than the number of classes of the final classifier. Starting from this unlabeled dataset,  UMTRA uses statistical diversity properties and domain specific augmentations to generate the training and validation data for a collection of synthetic tasks, {. . . T i . . .}. These tasks are then used in the meta-learning process based on a modified classification variant of the MAML algorithm <ref type="bibr" target="#b10">[11]</ref>. <ref type="figure" target="#fig_0">Figure 1</ref> summarizes the differences between the original supervised MAML model and the process of generating synthetic tasks from unsupervised data in UMTRA.</p><p>The contributions of this paper can be summarized as follows:</p><p>? We describe a novel algorithm that allows unsupervised, model-agnostic meta-learning for few-shot classification by generating synthetic meta-learning data with artificial labels. ? From a theoretical point of view, we demonstrate a relationship between generalization error and the loss backpropagated from the validation set in MAML. Our intuition is that we can generate unsupervised validation tasks which can perform effectively if we are able to span the space of the classes by generating useful samples with augmentation. ? On all the Omniglot and Mini-Imagenet few-shot learning benchmarks, UMTRA outperforms every tested approach based on unsupervised learning of representations. It also achieves a significant percentage of the accuracy of the supervised MAML approach, while requiring vastly fewer labels. For instance, for 5-way 5-shot classification on the Omniglot dataset UMTRA obtains a 95.43% accuracy with only 25 labels, while supervised MAML obtains 98.83% with 24025. Compared with recent unsupervised meta-learning approaches building on top of stock MAML, UMTRA alternates for the best performance with the CACTUs algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Few-shot or one-shot learning of classifiers has significant practical applications. Unfortunately, the few-shot learning model is not a good fit to the traditional training approaches of deep neural networks, which work best with large amounts of data. In recent years, significant research targeted approaches to allow deep neural networks to work in few-shot learning settings. One possibility is to perform transfer learning, but it was found that the accuracy decreases if the target task diverges from the trained task. One solution to mitigate this is through the use of an adversarial loss <ref type="bibr" target="#b24">[25]</ref>.</p><p>A large class of approaches aim to enable few-shot learning by meta-learning -the general idea being that the metalearning prepares the network to learn from the small amount of training data available in the few-shot learning setting. Note that meta-learning can be also used in other computer vision applications, such as fast adaptation for tracking in video <ref type="bibr" target="#b32">[33]</ref>. The mechanisms through which meta-learning is implemented can be loosely classified in two groups. One class of approaches use a custom network architecture for encoding the information acquired during the meta-learning phase, for instance in fast weights <ref type="bibr" target="#b2">[3]</ref>, neural plasticity values <ref type="bibr" target="#b28">[29]</ref>, custom update rules <ref type="bibr" target="#b27">[28]</ref>, the state of temporal convolutions <ref type="bibr" target="#b29">[30]</ref> or in the memory of an LSTM <ref type="bibr" target="#b34">[35]</ref>. The advantage of this approach is that it allows us to fine-tune the architecture for the efficient encoding of the meta-learning information. A disadvantage, however, is that it constrains the type of network architectures we can use; innovations in network architectures do not automatically transfer into the meta-learning approach. In a custom network architecture meta-learning model, the target learning phase is not the customary network learning, as it needs to take advantage of the custom encoding.</p><p>A second, model-agnostic class of approaches aim to be usable for any differentiable network architecture. Examples of these algorithms are MAML <ref type="bibr" target="#b10">[11]</ref> or Reptile <ref type="bibr" target="#b30">[31]</ref>, where the aim is to encode the meta-learning in the weights of the network, such that the network performs the target learning phase with efficient gradients. Approaches that customize the learning rates <ref type="bibr" target="#b26">[27]</ref> during meta-training can also be grouped in this class. For this type of approaches, the target learning phase uses the well-established learning algorithms that would be used if learning from scratch (albeit it might use specific hyperparameter settings, such as higher learning rates). We need to point out, however, that the meta-learning phase uses custom algorithms in these approaches as well (although they might use the standard learning algorithm in the inner loop, such as in the case of MAML). A recent work similar in spirit to ours is the CACTUs unsupervised meta-learning model described in <ref type="bibr" target="#b14">[15]</ref>.</p><p>In this paper, we perform unsupervised meta-learning. Our approach generates tasks from unlabeled data which will help it to understand the structures of the relevant supervised tasks in the future. One should note that these relevant supervised tasks in the future do not have any intersection with the tasks which are used during the meta-learning. For instance, Wu et al. perform unsupervised learning by recognizing a certain internal structure between dataset classes <ref type="bibr" target="#b41">[42]</ref>. By learning this structure, the approach can be extended to semi-supervised learning. In addition, Pathak et al. propose a method which learns object features in an interesting unsupervised way by detecting movement patterns of segmented objects <ref type="bibr" target="#b33">[34]</ref>. These approaches are orthogonal to ours. We do not make assumptions that the unsupervised data shares classes with the target learning (in fact, we explicitly forbid it). Finally, <ref type="bibr" target="#b13">[14]</ref> define unsupervised metalearning in reinforcement learning context. The authors study how to generate tasks with synthetic reward functions (without supervision) such that when the policy network is meta trained on them, they can learn real tasks with manually defined reward functions (with supervision) much more quickly and with fewer samples.</p><p>3 The UMTRA algorithm</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>We consider the task of classifying samples x drawn from a domain X into classes y i ? Y = {C 1 , . . . , C N }. The classes are encoded as one-hot vectors of dimensionality N . We are interested in learning a classifier f ? that outputs a probability distribution over the classes. It is common to envision f as a deep neural network parameterized by ?, although this is not the only possible choice.</p><p>We package a certain supervised learning task, T , of type (N, K), that is with N classes of K training samples each, as follows. The training data will have the form (x i , y i ), where i = 1 . . . N ? K, x i ? X and y i ? Y , with exactly K samples for each value of y i . In the recent meta-learning literature, it is often assumed that the task T has K samples of each class for training and (separately), K samples for validation (x v j , y v j ). In supervised meta-learning, we have access to a collection of tasks T 1 . . . T n drawn from a specific distribution, with both supervised training and validation data. The meta-learning phase uses this collection of tasks, while the target learning uses a new task T with supervised learning data but no validation data. . . x i . . .} unlabeled dataset require :?, ?: step size hyperparameters require :A: augmentation function 1 randomly initialize ?; 2 while not done do</p><formula xml:id="formula_0">3 for i in 1 . . . N MB do 4 Sample N data points x 1 . . . x N from U; 5 T i ? {x 1 , . . . x N }; 6 end 7 foreach T i do 8 Generate training set D i = {(x 1 , 1), . . . , (x N , N )}; 9 ? i = ?; 10 for j in 1 . . . N U do 11 Evaluate ? ? i L Ti (f ? i ); 12</formula><p>Compute adapted parameters with gradient descent:</p><formula xml:id="formula_1">? i = ? i ? ?? ? i L Ti (f ? i ); 13 end 14</formula><p>Generate validation set for the meta-update</p><formula xml:id="formula_2">D i = {(A(x 1 ), 1), . . . , (A(x N ), N )} 15 end 16 Update ? ? ? ? ?? ? Ti L Ti (f ? i ) using each D i ; 17 end</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model</head><p>Unsupervised meta-learning retains the goal of meta-learning by preparing a learning system for the rapid learning of the target task T . However, instead of the collection of tasks T 1 . . . T n and their associated labeled training data, we only have an unlabeled dataset U = {. . . x i . . .}, with samples drawn from the same distribution as the target task. We assume that every element of this dataset is associated with a natural class C 1 . . . C c , ?x i ?j such that x i ? C j . We will assume that N c, that is, the number of natural classes in the unsupervised dataset is much higher than the number of classes in the target task. These requirements are much easier to satisfy than the construction of the tasks for supervised meta-learning -for instance, simply stripping the labels from datasets such as Omniglot and Mini-ImageNet satisfies them.</p><p>The pseudo-code of the UMTRA algorithm is described in Algorithm 1. In the following, we describe the various parts of the algorithm in detail. In order to be able to run the UMTRA algorithm on unsupervised data, we need to create tasks T i from the unsupervised data that can serve the same role as the meta-learning tasks serve in the full MAML algorithm. For such a task, we need to create both the training data D and the validation data D .</p><p>Creating the training data: In the original form of the MAML algorithm, the training data of the task T must have the form (x, y), and we need N ? K of them. The exact labels used during the meta-training step are not relevant, as they are discarded during the meta-training phase. They can be thus replaced with artificial labels, by setting them y ? {1, ...N }. It is however, important that the labels maintain class distinctions: if two data points have the same label, they should also have the same artificial labels, while if they have different labels, they should have different artificial labels.</p><p>The first difference between UMTRA and MAML is that during the meta-training phases, we always perform one-shot learning, with K = 1. Note that during the target learning phase we can still set values of K different from 1. The training data is created as the set</p><formula xml:id="formula_3">D i = {(x 1 , 1), . . . (x N , N )}, with x i sampled randomly from U.</formula><p>Let us see how this training data construction satisfy the class distinction conditions. The first condition is satisfied because there is only one sample for each label. The second condition is satisfied statistically by the fact that N c, where c is the total number of classes in the dataset. If the number of samples is significantly smaller than the number of classes, it is likely that all the samples will be drawn from different classes. If we assume that the samples are equally distributed among the classes (e.g. m samples for each class), the probability that all samples are in a different class is equal to</p><formula xml:id="formula_4">P = (c ? m) ? ((c ? 1) ? m)...((c ? N + 1) ? m) (c ? m) ? (c ? m ? 1)...(c ? m ? N + 1) = c! ? m N ? (c ? m ? N )! (c ? N )! ? (c ? m)!<label>(1)</label></formula><p>Translation + Zeroing Pixels Same Flip Grayscale Auto Augment Rotate To illustrate this, the probability for 5-way classification on the Omniglot dataset used with each of the 1200 characters is a separate class (c = 1200, N = 5) is 99.21%. For Mini-ImageNet (c = 64), the probability is 85.23%, while for the full ImageNet it would be about 99%.</p><p>Creating the validation data: For the MAML approach, the validation data of the meta-training tasks is actually training data in the outer loop. It is thus required that we create a validation dataset D i = {(x 1 , 1), . . . (x N , N )} for each task T i . Thus we need to create appropriate validation data for the synthetic task. A minimum requirement for the validation data is to be correctly labeled in the given context. This means that the synthetic numerical label should map in both cases to the same class in the unlabeled dataset: C such that</p><formula xml:id="formula_5">x i , x i ? C.</formula><p>In the original MAML model, these x i values are labeled examples part of the supervised dataset. In our case, picking such x i values is non-trivial, as we don't have access to the actual class. Instead, we propose to create such a sample by augmenting the sample used in the training data using an augmentation function x i = A(x i ) which is a hyperparameter of the UMTRA algorithm. A requirement towards the augmentation function is to maintain class membership x ? C ? A(x) ? C. We should aim to construct the augmentation function to verify this property for the given dataset U, based on what we know about the domain described by the dataset. However, as we do not have access to the classes, such a verification is not practically possible on a concrete dataset.</p><p>Another choice for the augmentation function A is to apply some kind of domain-specific change to the images or videos. Examples of these include setting some of the pixel values to zero in the image <ref type="figure" target="#fig_2">(Figure 2</ref>, left), or translating the pixels of the training image by some amount (eg. between -6 and 6).</p><p>The overall process of generating the training data from the unlabeled dataset in UMTRA and the differences from the supervised MAML approach is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Some theoretical considerations</head><p>While a full formal model of the learning ability of the UMTRA algorithm is beyond the scope of this paper, we can investigate some aspects of its behavior that shed light into why the algorithm is working, and why augmentation improves its performance. Let us denote our network with a parameterized function f ? . As we want to learn a few-shot classification task, T we are searching for the corresponding function f T , to which we do not have access. To learn this function, we use the training dataset,</p><formula xml:id="formula_6">D T = {(x i , y i )} n?k i=1 .</formula><p>For this particular task, we update our parameters (to ? ) to fit this task's training dataset. In other words, we want f ? to be a good approximation of f T . Finding ? such that,</p><formula xml:id="formula_7">? = argmin ? (xi,yi)?D T L(y i , f ? (x i ))</formula><p>is ill-defined because there are more than one solution for it. In meta-learning, we search for the ? value that gives us the minimum generalization error, the measure of how accurately an algorithm is able to predict outcome values for unseen data <ref type="bibr" target="#b0">[1]</ref>. We can estimate the generalization error based on sampled data points from the same task. Without loss of generality, let us consider a sampled data point (x 0 , y 0 ). We can estimate generalization error on this point as L(y 0 , f ? (x 0 )). In case of mean squared error, and by accepting irreducible error ? N (0, ?), we can decompose the expected generalization error as follows <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b11">12]</ref>:</p><formula xml:id="formula_8">E [L(y 0 , f ? (x 0 ))] = E[f ? (x 0 )] ? f T (x 0 ) 2 + E (f ? (x 0 )) 2 ? E [f ? (x 0 )] 2 + ? 2<label>(2)</label></formula><p>In this equation, when (x 0 , y 0 ) /</p><formula xml:id="formula_9">? D T we have E[(f ? (x 0 )) 2 ] ? E[f ? (x 0 )] 2<label>=</label></formula><p>0, which means that the estimation of the generalization error on these samples will be as unbiased as possible (only biased by ? 2 ). On the other hand, if (x 0 , y 0 ) ? D T , the estimation of the error is going to be highly biased. We conjecture that similar results will be observed for other loss functions as well with the estimate of the loss function being more biased if the samples are from the training data rather than outside it. As in the outer loop of MAML estimates the generalization error on a validation set for each task in a batch of tasks, it is important to keep the validation set separate from the training set, as this estimate will be eventually applied to the starter network.</p><p>In contrast, if we pick our validation set as points in D T , our algorithm is going to learn to minimize a biased estimation of the generalization error. Our experiments also show that if we choose the same data for train and test (A(x) = x), we will end up with an accuracy almost the same as training from scratch. UMTRA, however, tries to improve the estimation of generalization error with augmentation techniques. Our experiments show that by applying UMTRA with good choice of function for augmentation, we can achieve comparable results with supervised meta-learning algorithms.</p><p>In our supplementary material, we show that UMTRA is able to adapt very quickly with just few iterations to a new task. Last but not least, in comparison with CACTUs algorithm which applies advanced clustering algorithms such as DeepCluster <ref type="bibr" target="#b5">[6]</ref>, ACAI <ref type="bibr" target="#b4">[5]</ref>, and BiGAN <ref type="bibr" target="#b9">[10]</ref> to generate train and validation set for each task, our method does not require clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">UMTRA on the Omniglot dataset</head><p>Omniglot <ref type="bibr" target="#b22">[23]</ref> is a dataset of handwritten characters frequently used to compare few-shot learning algorithms. It comprises 1623 characters from 50 different alphabets. Every character in Omniglot has 20 different instances each was written by a different person. To allow comparisons with other published results, in our experiments we follow the experimental protocol described in <ref type="bibr" target="#b35">[36]</ref>: 1200 characters were used for training, 100 characters were used for validation and 323 characters were used for testing.</p><p>UMTRA, like the supervised MAML algorithm, is model-agnostic, that is, it does not impose conditions on the actual network architecture used in the learning. This does not, of course, mean that the algorithm performs identically for every network structure and dataset. In order to separate the performance of the architecture and the meta-learner, we run our experiments using an architecture originally proposed in <ref type="bibr" target="#b40">[41]</ref>. This classifier uses four 3 x 3 convolutional modules with 64 filters each, followed by batch normalization <ref type="bibr" target="#b15">[16]</ref>, a ReLU nonlinearity and 2 x 2 max-pooling. On the resulting feature embedding, the classifier is implemented as a fully connected layer followed by a softmax layer.</p><p>UMTRA has a relatively large hyperparameter space that includes the augmentation function. As pointed out in a recent study involving performance comparisons in semi-supervised systems <ref type="bibr" target="#b31">[32]</ref>, excessive tuning of hyperparameters can easily lead to an overestimation of the performance of an approach compared to simpler approaches. Thus, for the comparison in the remainder of this paper, we keep a relatively small budget for hyperparameter search: beyond basic sanity checks, we only tested 5-10 hyperparameter combinations per dataset, without specializing them to the N or K parameters of the target task. <ref type="table" target="#tab_1">Table 1</ref>, left, shows several choices for the augmentation function for the 5-way one-shot classification on Omniglot. Based on this table, in comparing with other approaches, we use an augmentation function consisting of randomly zeroed pixels and random shift.</p><p>In our experiments, we realized two of the most important hyperparameters in meta-learning are meta-batch size, N M B , and number of updates, N U . In table 2, we study the effects of these hyperparameters on the accuracy of the network for the randomly zeroed pixels and random shift augmentation. Based on this experiment, we decide to fix the meta-batch size to 25 and number of updates to 1.  In order to find out the relationship between the level of the augmentation and accuracy, we apply different levels of augmentation on images. If the generated samples are different from current observation but within the same class manifold, UMTRA performs well. The results of this experiment are shown in table 3.</p><p>The second consideration is what sort of baseline we should use when evaluating our approach on a few-shot learning task? Clearly, supervised meta-learning approaches such as an original MAML <ref type="bibr" target="#b10">[11]</ref> are expected to outperform our approach, as they use a labeled training set. A simple baseline is to use the same network architecture being trained from scratch with only the final few-shot labeled set. If our algorithm takes advantage of the unsupervised training set U, as expected, it should outperform this baseline.</p><p>A more competitive comparison can be made against networks that are first trained to obtain a favorable embedding using unsupervised learning on U, with the resulting embedding used on the few-shot learning task. These baselines are not meta-learning approaches, however, we can train them with the same target task training set as UMTRA. Similar to <ref type="bibr" target="#b14">[15]</ref>, we compare the following unsupervised pre-training approaches: ACAI <ref type="bibr" target="#b4">[5]</ref>, BiGAN <ref type="bibr" target="#b9">[10]</ref>, DeepCluster <ref type="bibr" target="#b5">[6]</ref> and InfoGAN <ref type="bibr" target="#b6">[7]</ref>. These up-to-date approaches cover a wide range of the recent advances in the area of unsupervised feature learning. Finally, we also compare against the CACTUs unsupervised meta-learning algorithm proposed in the <ref type="bibr" target="#b14">[15]</ref>, combined with MAML and ProtoNets <ref type="bibr" target="#b37">[38]</ref>. As a note, another unsupervised meta-learning approach related to UMTRA and CACTUs is AAL <ref type="bibr" target="#b1">[2]</ref>. However, as <ref type="bibr" target="#b1">[2]</ref> doesn't compare against stock MAML, the results are not directly comparable. <ref type="table" target="#tab_4">Table 4</ref>, columns three to six, shows the results of the experiments. For the UMTRA approach we trained for 6000 meta-iterations for the 5-way, and 36,000 meta-iterations for the 20-way classifications. Our approach, with the proposed hyperparameter settings outperforms, with large margins, training from scratch and the approaches based on unsupervised representation learning. UMTRA also outperforms, with a smaller margin, the CACTUs approach on all metrics, and in combination with both MAML and ProtoNets.</p><p>As expected, the supervised meta-learning baselines perform better than UMTRA. To put this value in perspective, we need to take into consideration the vast difference in the number of labels needed for these approaches. In 5-way one-shot classification, UMTRA obtains a 83.80% accuracy with only 5 labels, while supervised MAML obtains 94.46% but requires 24005 labels. For 5-way 5-shot classification UMTRA obtains a 95.43% accuracy with only 25 labels, while supervised MAML obtains 98.83% with 24025.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">UMTRA on the Mini-Imagenet dataset</head><p>The Mini-Imagenet dataset was introduced by <ref type="bibr" target="#b34">[35]</ref> as a subset of the ImageNet dataset <ref type="bibr" target="#b8">[9]</ref>, suitable as a benchmark for few-shot learning algorithms. The dataset is limited to 100 classes, each with 600 images. We divide our dataset into train, validation and test subsets according to the experimental protocol proposed by <ref type="bibr" target="#b40">[41]</ref>. The classifier network is similar to the one used in <ref type="bibr" target="#b10">[11]</ref>.</p><p>Since Mini-Imagenet is a dataset with larger images and more complex classes compared to Omniglot, we need to choose augmentation functions suitable to the model. We had investigated several simple choices involving random flips, shifts, rotation, and color changes. In addition to these hand-crafted algorithms, we also investigated the learned auto-augmentation method proposed in <ref type="bibr" target="#b7">[8]</ref>. <ref type="table" target="#tab_1">Table 1</ref>, right, shows the accuracy results for the tested augmentation functions. We found that auto-augmentation provided the best results, thus this approach was used in the remainder of the experiments. The last four columns of <ref type="table" target="#tab_4">Table 4</ref> lists the experimental results for few-shot classification learning on the Mini-Imagenet dataset. Similar to the Omniglot dataset, UMTRA performs better than learning from scratch and all the approaches that use unsupervised representation learning. It performs weaker than supervised meta-learning approaches that use labeled data. Compared to the various combinations involving the CACTUs unsupervised meta-learning algorithm, UMTRA performs better on 5-way one-shot classification, while it is outperformed by the CACTUs-MAML with DeepCluster combination for the 5, 20 and 50 shot classification.</p><p>A possible question might be raised whether the improvements we see are due to the meta-learning process or due to the augmentation enriching the few shot dataset. To investigate this, we performed several experiments on Omniglot and Mini-Imagenet by training the target tasks from scratch on the augmented target dataset. For 5-way, 1-shot learning on Omniglot the accuracy was: training from scratch 52.5%, training from scratch with augmentation 55.8%, UMTRA 83.8%. For MiniImagenet the numbers were: from scratch without augmentation 27.6%, from scratch with augmentation 28.8%, UMTRA 39.93%. We conclude that while augmentation does provide a (minor) improvement on the target training by itself, the majority of the improvement shown by UMTRA is due to the meta-learning process.</p><p>The results on Omniglot and Mini-Imagenet allow us to draw the preliminary conclusions that unsupervised metalearning approaches like UMTRA and CACTUs, which generate meta tasks T i from the unsupervised training data tend to outperform other approaches for a given unsupervised training set U. UMTRA and CACTUs use different, orthogonal approaches for building T . UMTRA uses the statistical likelihood of picking different classes for the training data of T i in case of K = 1 and large number of classes, and an augmentation function T for the validation data. CACTUs relies on an unsupervised clustering algorithm to provide a statistical likelihood of difference and sameness in the training and validation data of T i . Except in the case of UMTRA with A = 1, both approaches require domain specific knowledge.</p><p>The choice of the right augmentation function for UMTRA, the right clustering approach for CACTUs, and the other hyperparameters (for both approaches) have a strong impact on the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we described the UMTRA algorithm for few-shot and one-shot learning of classifiers. UMTRA performs meta-learning on an unlabeled dataset in an unsupervised fashion, without putting any constraint on the classifier network architecture. Experimental studies over the few-shot learning image benchmarks Omniglot and Mini-Imagenet show that UMTRA outperforms learning-from-scratch approaches and approaches based on unsupervised representation learning. It alternated in obtaining by best result with the recently proposed CACTUs algorithm that takes a different approach to unsupervised meta-learning by applying clustering on an unlabeled dataset. The statistical sampling and augmentation performed by UMTRA can be seen as a cheaper alternative to the dataset-wide clustering performed by CACTUs.</p><p>The results also open the possibility that these approaches might be orthogonal, and in combination might yield an even better performance. For all experiments, UMTRA performed worse than the equivalent supervised meta-learning approach -but requiring 3-4 orders of magnitude less labeled data. The supplemental material shows that UMTRA is not limited to image classification but it can be applied to other tasks as well, such as video classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material for Unsupervised Meta-Learning for Few-Shot Image Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evolution of accuracy during training</head><p>In these series of experiments we study the evolution of the accuracy obtained after a specific number of gradient training steps during the target learning phase. The results for Omniglot are shown in <ref type="figure" target="#fig_4">Figure 3</ref> (with K=1), while those for Mini-Imagenet in <ref type="figure">Figure 4</ref> with K values of 1, 5 and 20. For both datasets, we compare learning from scratch, UMTRA and supervised MAML. As expected, both MAML and UMTRA reach their accuracy plateau very quickly during target training, while learning from scratch takes a larger number of training steps. Further training does not appear to provide any benefit for either approach. The results are averaged among 1000 tasks. This demonstrates that UMTRA has the capacity to learn to adapt to novel tasks by just looking at unlabeled data and generating tasks from that dataset in an unsupervised manner. An interesting phenomena happens with K = 5 and K = 20 values for Mini-Imagenet: the accuracy curve of UMTRA dips after the first iteration, and it takes several iterations to recover. We conjecture that this is a result of the fact that UMTRA sets K = 1 during meta-learning, thus the resulting network is best optimized to learn from one sample per class.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Representations</head><p>To compare generalization of training from scratch, UMTRA and supervised MAML, we visualize the activations of the last hidden layer of the network on Omniglot dataset by t-SNE. We compare all of the methods on the same target training task which is constructed by sampling five characters from test data and selecting one image from each character class randomly. Each character has 20 different instances. <ref type="figure">Figure 5</ref> shows the t-SNE visualization of raw pixel values of these 100 images. Instances which are sampled for the one-shot learning task are connected to each other by dotted lines. <ref type="figure">Figure 6</ref> shows the visualization of the last hidden layer activations for the same task. UMTRA as well as MAML can adapt quickly to a feature space which has a better generalization than training from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video Domain</head><p>In this section, we show how the UMTRA can be applied to video action recognition, a domain significantly more complex and data intensive than the one used in the few-shot learning benchmarks such as Omniglot and Mini-Imagenet. To the best of our knowledge, we are the first to apply meta-learning to video action recognition. We perform our comparisons using one of the standard video action recognition datasets, UCF-101 . UCF-101 includes 101 action classes divided into five types: Human-Object Interaction, Body-Motion Only, Human Human Interaction, Playing Musical Instruments and Sports. The dataset is composed of snippets of Youtube videos. Many videos have poor lighting, cluttered background and severe camera motion. As the classifier on which to apply the meta-learning process, we use a 3D convolution network, C3D .</p><p>Performing unsupervised meta-learning on video data, requires several adjustments to the UMTRA workflow, with regards to the initialization of the classifier, the split between meta-learning data and testing data, and the augmentation function.</p><p>First, networks of the complexity of C3D cannot be learned from scratch using the limited amount of data available in few-shot learning. In the video action recognition research, it is common practice to start with a network that had been pre-trained on a large dataset, such as Sports-1M dataset , an approach we also use in all our experiments.  Second, we make the choice to use two different datasets for the meta-learning phase (Kinetics) and for the few-shot learning and evaluation (UCF-101 ). This gives us a larger dataset for training since Kinetics contains 400 actions, but it introduces an additional challenge of domain-shift: the network is pre-trained on Sports-1M, meta-trained on Kinetics and few-shot trained on UCF-101. This approach, however, closely resembles the practical setup when we need to do few-shot learning on a novel domain. When using the Kinetics dataset, we limit it to 20 instances per class.</p><p>For the augmentation function A, working in the video domain opens a new possibility, of creating an augmented sample by choosing a temporally shifted video fragment from the same video. In other words, we can use self supervision in video domain: The augmentation is to sample another part of the same video clip. <ref type="figure">Figure 7</ref> shows some samples of these augmentations. In our experiments, we have experimented both with UMTRA (using a Kinetics dataset stripped from labels), and supervised meta-learning (retaining the labels on Kinetics). This supervised meta-learning experiment is also significant because, to the best of our knowledge, meta-learning has never been applied to human action recognition from videos.   In our evaluation, we perform 30 different experiments. At each experiment we sample 5 classes from UCF-101, perform the one-shot learning, and evaluate the classifier on all the examples for the 5 classes from UCF-101. As the number of samples per class are not the same for all classes, in <ref type="table" target="#tab_6">Table 5</ref> we report both the accuracy and F1-score.</p><p>The results allow us to draw several conclusions. The relative accuracy ranking between training from scratch, pretraining and unsupervised meta-learning and supervised meta-learning remained unchanged. Supervised meta-learning had proven feasible for one-shot classifier training for video action recognition. UMTRA performs better than other approaches that use unsupervised data. Finally, we found that the domain shift from Kinetics to UCF-101 was successful.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The process of creation of the training and validation data of the meta-training task T . (top) Supervised MAML: We start from a dataset where the samples are labeled with their class. The training data is created by sampling N distinct classes C Li , and choosing a random sample x i from each. The validation data is created by choosing a different sample x i from the same class. (bottom) UMTRA: We start from a dataset of unlabeled data. The training data is created by randomly choosing N samples x i from the dataset. The validation data is created by applying the augmentation function A to each sample from the training data. For both MAML and UMTRA, artificial temporary labels 1, 2 . . . N are used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 :</head><label>1</label><figDesc>Unsupervised Meta-learning with Tasks constructed by Random sampling and Augmentation (UMTRA) require :N : class-count, N MB : meta-batch size, N U : no. of updates require :U = {.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Augmentation techniques on Omniglot (left) and Mini-Imagenet (right). Top row: Original images in training data. Bottom: augmented images for the validation set, transformed with an augmentation function A. Auto Augment [8] applies augmentations from a learned policy based on combinations of translation, rotation, or shearing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>The accuracy curves during the target training task on the Omniglot dataset for K = 1. The band around lines denotes a 95% confidence interval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>The accuracy curves during the target training task on the Mini-Imagenet dataset. Accuracy curves are shown for K = 1 (Top left), K = 5 (Top right), and K = 20 (Bottom). The band around lines denotes a 95% confidence interval. t-SNE on the Omniglot raw pixel values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Visualization of the last hidden layer activation values by t-SNE on the Omniglot dataset before target task training (Left), and after target task training (Right). Visualized features are shown for training from scratch (Top), UMTRA (Middle), and MAML (Bottom). Each class is shown by a different color and shape. From each class one instance is used for target task training. Training instances are denoted by larger and lighter symbols and are connected to each other by dotted lines ? ? ? Example of the training data and the augmentation function A for video. The training data x is a 16 frame segment starting from a random time in the video sample (Here we show three frames of a sample at each column). The validation data x = A(x) is also a 16 frame segment, starting from a different, randomly selected time from the same video sample.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>:1811.11819v2 [cs.CV] 7 Nov 2019</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Supervised MAML</cell><cell></cell><cell></cell></row><row><cell>C 1</cell><cell>C L 1</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>...</cell><cell>n</cell><cell>Model</cell></row><row><cell>C 2 C 3 . . .</cell><cell>. C L 2 . . C L 3</cell><cell cols="5">1 ? 1 ? 2 ? 3 2 3 ... N ... ? n</cell><cell>Updated Model ?</cell><cell>Update model parameters based on outer loss</cell></row><row><cell>C 1200</cell><cell>C L N</cell><cell>1</cell><cell>2</cell><cell cols="3">3 ... N</cell><cell>?</cell></row><row><cell cols="2">Sample N classes</cell><cell cols="5">Sample 2 data points from each class</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">UMTRA</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>...</cell><cell>n</cell><cell>Model</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell cols="3">3 ... N</cell><cell>?</cell><cell>Update</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>model</cell></row><row><cell></cell><cell></cell><cell>? 1 ( )</cell><cell>? 2 ( )</cell><cell cols="3">? 3 ( ) ( ) ( ) ... ? n</cell><cell>Updated Model</cell><cell>parameters based on outer loss</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell cols="3">3 ... N</cell><cell>?</cell></row><row><cell></cell><cell cols="2">Sample N data points</cell><cell cols="3">? i = ( i )</cell><cell></cell><cell></cell></row></table><note>33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.arXiv</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The influence of augmentation function on the accuracy of UMTRA for 5-way one-shot classification on the (Left: Omniglot dataset, Right: Mini-Imagenet dataset). For all cases, we use meta-batch size N MB = 4 and number of updates N U = 5, except the ones with best hyperparameters.</figDesc><table><row><cell>Augmentation Function A</cell><cell>Accuracy</cell><cell>Augmentation Function A</cell><cell>Accuracy</cell></row><row><cell>Training from scratch A = 1</cell><cell>52.50 52.93</cell><cell>Training from scratch A = 1</cell><cell>24.17 26.49</cell></row><row><cell>A = randomly zeroed pixels</cell><cell>56.23</cell><cell>A = Shift + random flip</cell><cell>30.16</cell></row><row><cell>A = randomly zeroed pixels</cell><cell>67.00</cell><cell>A = Shift + random flip + ran-</cell><cell>32.80</cell></row><row><cell>(with best hyperparameters)</cell><cell></cell><cell>domly change to grayscale</cell><cell></cell></row><row><cell>A = randomly zeroed pixels</cell><cell>83.80</cell><cell>A = Shift + random flip + ran-</cell><cell>35.09</cell></row><row><cell>+ random shift (with best</cell><cell></cell><cell>dom rotation + color distortions</cell><cell></cell></row><row><cell>hyperparameters)</cell><cell></cell><cell>A = Auto Augment [8]</cell><cell>39.93</cell></row><row><cell>Supervised MAML</cell><cell>98.7</cell><cell>Supervised MAML</cell><cell>46.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The effect of hyperparameters meta-batch size, N M B , and number of updates, N U on accuracy. Omniglot 5-way one shot. 79.04 80.72 81.60 82.72 83.80 5 76.08 76.68 77.20 79.56 81.12 83.32 10 79.20 79.24 80.92 80.68 83.52 83.26</figDesc><table><row><cell># Updates</cell><cell>NMB</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>25</cell></row><row><cell>1</cell><cell></cell><cell>67.08</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The effect of the augmentation level on UMTRA's accuracy on the Omniglot dataset. In all of the experiments we use random pixel zeroing with meta-batch size N MB = 25 and number of updates N U = 1.</figDesc><table><row><cell>Translation Range (Pixels)</cell><cell>0</cell><cell>0-3</cell><cell>3-6</cell><cell>0-6</cell><cell>6-9</cell><cell>9-12</cell><cell>0-9</cell></row><row><cell>Accuracy %</cell><cell cols="5">67.0 82.8 80.4 83.8 79.8</cell><cell>77</cell><cell>80.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Accuracy in % of N-way K-shot (N,K) learning methods on the Omniglot and Mini-Imagenet datasets. The ACAI / DC label means ACAI Clustering on Omniglot and DeepCluster on Mini-Imagenet. The source of non-UMTRA values is<ref type="bibr" target="#b14">[15]</ref>.</figDesc><table><row><cell>Omniglot</cell><cell>Mini-Imagenet</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Accuracy and F1-Score for a 5-way, one-shot classifier trained and evaluated on classes sampled from UCF-101. All training (even for "training from scratch"), employ a C3D network pre-trained on Sports-1M. For all approaches, none of the UCF-101 classes was seen during pre-or meta-learning.</figDesc><table><row><cell>Algorithm</cell><cell>Test Accuracy / F1-Score</cell></row><row><cell>Training from scratch</cell><cell>29.30 / 20.48</cell></row><row><cell>Pre-trained on Kinetics</cell><cell>45.51 / 42.49</cell></row><row><cell>UMTRA on unlabeled Kinetics (ours)</cell><cell>60.33 / 58.47</cell></row><row><cell>Supervised MAML on Kinetics</cell><cell>71.08 / 69.44</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: This research is based upon work supported in parts by the National Science Foundation under Grant numbers IIS-1409823 and IIS-1741431 and Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA R&amp;D Contract No. D17PC00345. The views, findings, opinions, and conclusions or recommendations contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the NSF, ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malik</forename><surname>Yaser S Abu-Mostafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsuan-Tien</forename><surname>Magdon-Ismail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>AMLBook</publisher>
			<biblScope unit="volume">4</biblScope>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Assume, augment and learn: Unsupervised few-shot meta-learning via random labels and data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antreas</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09884</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using fast weights to attend to the recent past</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>of Advances in Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4331" to="4339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning a synaptic learning rule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><surname>Cloutier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
		<respStmt>
			<orgName>Universit? de Montr?al, D?partement d&apos;informatique et de recherche op?rationnelle</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Understanding and improving interpolation in autoencoders via an adversarial regularizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.07543</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>of Advances in Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<title level="m">Learning augmentation policies from data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09782</idno>
		<title level="m">Adversarial feature learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Int&apos;l Conf. on Machine Leanring (ICML)</title>
		<meeting>of Int&apos;l Conf. on Machine Leanring (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The elements of statistical learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer series in statistics</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Regularization theory and neural networks architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Girosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="219" to="269" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unsupervised meta-learning for reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Eysenbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.04640</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Unsupervised learning via meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02334</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Objects2action: Classifying and localizing actions without any video example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int&apos;l Conf. on computer vision</title>
		<meeting>of the IEEE Int&apos;l Conf. on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4588" to="4596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">An introduction to statistical learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gareth</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">112</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 22nd ACM Int&apos;lal Conf. on Multimedia</title>
		<meeting>of the 22nd ACM Int&apos;lal Conf. on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">One shot learning of simple visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brenden</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Annual Meeting of the Cognitive Science Society</title>
		<meeting>of the Annual Meeting of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Int&apos;lal Conf. on Computer Vision (ICCV)</title>
		<meeting>of Int&apos;lal Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Label efficient learning of transferable representations across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li F Fei-Fei</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>of Advances in Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="165" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Online learning of a memory for learning rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kappler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Int&apos;l Conf. on Robotics and Automation (ICRA)</title>
		<meeting>of IEEE Int&apos;l Conf. on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2425" to="2432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00222</idno>
		<title level="m">Learning unsupervised learning rules</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Miconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth O</forename><surname>Stanley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02464</idno>
		<title level="m">Differentiable plasticity: training plastic neural networks with backpropagation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A simple neural attentive meta-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03141</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<title level="m">Reptile: a scalable metalearning algorithm</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Realistic evaluation of deep semi-supervised learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekin</forename><surname>Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3235" to="3246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Meta-tracker: Fast and robust online adaptation for visual object trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunbyung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="569" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning features by watching objects move</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition(CVPR)</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition(CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2701" to="2710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Int&apos;l Conf. on Learning Representations (ICLR)</title>
		<meeting>of Int&apos;l Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Int&apos;lal Conf. on Machine Learning (ICML)</title>
		<meeting>of Int&apos;lal Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook. PhD thesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
		<respStmt>
			<orgName>Technische Universit?t M?nchen</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>of Advances in Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int&apos;l Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE Int&apos;l Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>of Advances in Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition(CVPR)</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition(CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03530</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
