<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><forename type="middle">?</forename><surname>Bin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><forename type="middle">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
						</author>
						<title level="a" type="main">Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a new Vision Transformer (ViT) architecture Multi-Scale Vision Longformer, which significantly enhances the ViT of [12] for encoding highresolution images using two techniques. The first is the multi-scale model structure, which provides image encodings at multiple scales with manageable computational cost. The second is the attention mechanism of Vision Longformer, which is a variant of Longformer [3], originally developed for natural language processing, and achieves a linear complexity w.r.t. the number of input tokens. A comprehensive empirical study shows that the new ViT significantly outperforms several strong baselines, including the existing ViT models and their ResNet counterparts, and the Pyramid Vision Transformer from a concurrent work [47], on a range of vision tasks, including image classification, object detection, and segmentation. The models and source code are released at https://github.com/ microsoft/vision-longformer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Vision Transformer (ViT) <ref type="bibr" target="#b11">[12]</ref> has shown promising results on image classification tasks for its strong capability of long range context modeling. But its quadratic increase of both computational and memory complexity hinders its application on many vision tasks that require highresolution feature maps computed on high-resolution images <ref type="bibr" target="#b0">1</ref> , like object detection <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b23">24]</ref>, segmentation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b5">6]</ref>, and human pose estimation <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b36">37]</ref>. Vision-language tasks, like VQA, image captioning, and image-text retrieval, also benefit from high-resolution feature maps <ref type="bibr" target="#b15">[16,</ref><ref type="bibr">53]</ref>, which are extracted with pre-trained CNN models. Developing a vision Transformer that can process high-resolution feature maps is a critical step toward the goal of unifying the model ? Microsoft Corporation ? indicates equal contributions. <ref type="bibr" target="#b0">1</ref> In this paper, encoding a high-resolution image means generating high-resolution feature maps for high-resolution images. architecture of vision and language modalities and improving multi-modal representation learning.</p><p>In this paper, we propose a new vision Transformer architecture Multi-Scale Vision Longformer, which significantly enhances the baseline ViT <ref type="bibr" target="#b11">[12]</ref> for encoding highresolution images using two techniques: <ref type="bibr" target="#b0">(1)</ref> the multi-scale model structure, and (2) the attention mechanism of Vision Longformer.</p><p>Models with multi-scale (pyramid, hierarchical) structure provide a comprehensive encoding of an image at multiple scales, while keeping the computation and memory complexity manageable. Deep convolutional networks are born with such multi-scale structure, which however is not true for the conventional ViT architecture. To obtain a multi-scale vision Transformer, we stack multiple (e.g., four) vision Transformers (ViT stages) sequentially. The first ViT stage operates on a high-resolution feature map but has a small hidden dimension. As we go to later ViT stages, the feature map resolution reduces while the hidden dimension increases. The resolution reduction is achieved by performing patching embedding at each ViT stage. In our experiments, we find that with the same number of model parameters and the same model FLOPs, the multiscale ViT achieves a significantly better accuracy than the vanilla ViT on image classification task. The results show that the multi-scale structure not only improves the computation and memory efficiency, but also boosts the classification performance. The proposed multi-scale ViT has the same network structure as conventional (multi-scale) CNN models such as ResNet <ref type="bibr" target="#b13">[14]</ref>, and can serve as a replaceand-plug-in choice for almost all ResNet applications. In this paper, we demonstrate this plausible property in image classification, object detection and instance segmentation.</p><p>The multi-scale structure alone is not sufficient to scale up ViT to process high-resolution images and feature maps, due to the quadratic increase of the computation and memory complexity with respect to the number of tokens in the self-attention layers. Compared to natural language tasks where data is 1-D, this problem is more severe in vision tasks where the increase in complexity is quartic (fourth or-der) with the increase of image resolution. For example, the computational complexity of a 4? higher resolution multihead self attention (MSA) layer (hidden dimension reduced by 4, i.e., 4H ? 4W ? D 4 ) equals to that of 64 layers in the original size (i.e., H ? W ? D). To address this challenge, we develop a 2-D version of Longformer <ref type="bibr">[3]</ref>, called Vision Longformer, to achieve a linear complexity w.r.t. the number of tokens (quadratic w.r.t. resolution). Our experiments show that compared to the baseline ViT, Vision Longformer shows no performance drop while significantly reduces the computational and memory cost in encoding images. The result indicates that the "local attention + global memory" structure in Vision Longformer is a desirable inductive bias for vision Transformers. We also compare Vision Longformer with other efficient attention mechanisms. The result again validates its superior performance on both image classification and object detection tasks.</p><p>The main contributions of this paper are two-fold: (1) We propose a new vision Transformer that uses the multiscale model structure and the attention mechanism of 2-D Longformer for efficient high-resolution image encoding. <ref type="bibr" target="#b1">(2)</ref> We perform a comprehensive empirical study to show that the proposed ViT significantly outperforms strong baselines, including previous ViT models, their ResNet counterparts, and a model from a concurrent work, on image classification, object detection and segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The Vision Transformer (ViT) <ref type="bibr" target="#b11">[12]</ref> applies a standard Transformer, originally developed for natural language processing (NLP), for image encoding by treating an image as a word sequence, i.e., splitting an image into patches (words) and using the linear embeddings of these patches as an input sequence. ViT has shown to outperform convolution neural network (CNN) models such as the ResNet <ref type="bibr" target="#b13">[14]</ref>, achieving state-of-the-art performance on multiple image classification benchmarks, where training data is sufficient. DeiT <ref type="bibr" target="#b42">[43]</ref> is another computer vision model that leverages Transformer. It uses a teacher-student strategy specific to Transformers to improve data efficiency in training. Thus, compared to ViT, it requires much less training data and computing resources to produce state-of-the-art image classification results. In addition to image classification, Transformers have also been applied to other compute vision tasks, including object detection <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b10">11]</ref>, segmentation <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b47">48]</ref>, image enhancement <ref type="bibr">[5,</ref><ref type="bibr" target="#b49">50]</ref>, image generation <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b6">7]</ref>, video processing <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b56">57]</ref>, and vision-language tasks <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Developing an efficient attention mechanism for highresolution image encoding is the focus of this work. Our model is inspired by the efficient attention mechanisms developed for Transformers, most of which are for NLP tasks. These mechanisms can be grouped into four categories. The first is the sparse attention mechanism, including contentindependent sparsity <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b14">15]</ref> and content-dependent sparsity <ref type="bibr">[18,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b54">55]</ref>. Axial Transformer <ref type="bibr" target="#b14">[15]</ref> and Image Transformer <ref type="bibr" target="#b29">[30]</ref> are among few sparsity-based efficient attentions that are developed for image generation. The second is the memory-based mechanism, including Compressive Transformers <ref type="bibr" target="#b31">[32]</ref> and Set Transformer <ref type="bibr" target="#b19">[20]</ref>. These models use some extra global tokens as static memory and allow all the other tokens to attend only to those global tokens. The third is the low-rank based mechanism. For example the Linformer <ref type="bibr" target="#b45">[46]</ref> projects the input key-value pairs into a smaller chunk, and performs cross-attention between the queries and the projected key-value pairs. The fourth is the (generalized) kernel-based mechanism, including Performer <ref type="bibr" target="#b9">[10]</ref> and Linear Transformers <ref type="bibr" target="#b16">[17]</ref>. Many models utilize hybrid attention mechanisms. For example, Longformer[3], BigBird <ref type="bibr" target="#b50">[51]</ref> and ETC <ref type="bibr" target="#b0">[1]</ref> combine the sparsity and memory mechanisms; Synthesizers <ref type="bibr" target="#b38">[39]</ref> combines the sparsity and low-rank mechanisms. Readers may refer to <ref type="bibr" target="#b41">[42]</ref> and <ref type="bibr" target="#b40">[41]</ref> for a comprehensive survey and benchmarks, respectively.</p><p>In this paper, we developed a 2-D version of Longformer[3], called Vision Longformer, which utilizes both the sparsity and memory mechanisms. Its conv-like sparsity mechanism is conceptually similar to the sparsity mechanism used in the Image Transformer <ref type="bibr" target="#b29">[30]</ref>.</p><p>The multi-scale vision Transformer architecture is another technique we use in our proposed high-resolution Vision Longformer. The hierarchical Transformers <ref type="bibr" target="#b28">[29]</ref> for NLP contain two stages, with the first stage processing overlapping segments and the second stage using the embeddings of the CLS tokens from all segments as input. In our proposed Vision Longformer, size reduction is performed by the patch embedding at the beginning of each stage, by merging all tokens in a patch from previous stage into a single token at the current stage. We typically use 4 stages for our model since we have empirically verified that using 4 stages is better than using 2 or 3 stages, especially for object detection tasks. Informer <ref type="bibr" target="#b54">[55]</ref> takes a similar stacked multistage approach to encoding long sequences, where the size reduction between stages is achieved by max-pooling.</p><p>Pyramid Vision Transformer (PVT) <ref type="bibr" target="#b46">[47]</ref>, Swin Transformer <ref type="bibr" target="#b25">[26]</ref> and HanoNet <ref type="bibr" target="#b43">[44]</ref> are concurrent works of ours. All these works use a multi-scale architecture where multiple (slightly modified) ViTs are stacked. The authors of PVT propose the spatial-reduction attention (SRA) to alleviate the cost increase in self-attention layers. However, the computation and memory complexity of PVT still increases quartically w.r.t. resolution (with a much smaller constant). Swin Transformer <ref type="bibr" target="#b25">[26]</ref> and HanoNet <ref type="bibr" target="#b43">[44]</ref> utilizes similar local attention mechanism as our Vision Longformer, but from different perspectives and implementations. . An E-ViT (a ? n / p) module is a ViT encoder with an efficient attention mechanism a, n efficient transformer blocks, input patch size p. We add a LayerNorm after the patch embedding. We add ng extra global tokens, as a form of global memory, and simply throw them away when going to the next stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multi-Scale Stacked Vision Transformers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-Scale Model Architecture</head><p>Efficient ViT (E-ViT). As shown in <ref type="figure" target="#fig_0">Figure 1</ref> (Bottom), we improve the encoding efficiency of vision Transformer by making the following modifications to the vanilla ViT. The modified ViT is referred to as Efficient ViT (E-ViT).</p><p>1. We add a Layer Normalization (LayerNorm) after the patch embedding.</p><p>2. We define a number of global tokens, including the CLS token. Correspondingly, the tokens associated with image and feature patches are referred to as local tokens afterwards.</p><p>3. We replace the vanilla full self-attention with an efficient attention mechanism, denoted by a, which will be described in detail in Sections 3.2 and 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>We use either an Absolute 2-D Positional Embedding (APE for short, separately encoding x and y coordinates and concatenating them) or a Relative Positional Bias (RPB for short) to replace the original absolute 1-D positional embedding.</p><p>Except for attention a, E-ViT has the following architecture parameters inherited from the vanilla ViT : input patch size p, number of attention blocks n, hidden dimension d and number of heads h, denoted as E-ViT(a ? n/p ; h, d, n g ). Using the full attention mechanism (i.e., a = full) and one global token (i.e., the CLS token with n g = 1), the deficient E-ViT(full ? 12/16 ; h, d, 1) models still achieve better Im-ageNet classification performance than the baseline ViT for both tiny (h = 3, d = 192) and small (h = 6, d = 384) model sizes, as shown in <ref type="table" target="#tab_1">Table 2</ref>. The performance gain is attributed to the added LayerNorm, as we show in the Supplementary.</p><p>Mathematically, an E-ViT(a ? n/p ; h, d, n g ) encoding module can be written as:</p><formula xml:id="formula_0">z 0 = [x 1 g ; . . . ; x ng g ; LN (x 1 p E); . . . ; LN (x n l p E)] + E ops ,<label>(1)</label></formula><formula xml:id="formula_1">z k = M SA a (LN (z k?1 )) + z k?1 , k = 1, .., n (2) z k = M LP (LN (z k )) + z k , k = 1, .., n,<label>(3)</label></formula><p>where LN is the added Layer Normalization after the patch embedding E, M SA a is the multi-head self-attention with attention type a, and M LP is the feed-forward block in a standard Transformer. When the absolute 2-D positional embedding is used, E ops ? R (n l +ng)?d contains the 2-D positional embedding of n l local tokens and the 1-D positional embedding of n g global tokens. When the relative positional bias is used, E ops = 0 and the per-head relative positional bias is directly added to the attention scores in the M SA a modules, as in Equation <ref type="formula" target="#formula_3">(4)</ref>.</p><p>Stack multiple E-ViT modules as multi-scale vision Transformers. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> (Top), a multiscale Vision Transformer is built by stacking multiple E-ViT modules (or stages). In what follows, we describe several design choices we have made when building the multiscale ViT.</p><p>What are the patch size and hidden dimension at each stage? As required in object detection and human pose estimation, for models with 4-scale feature maps, the first feature map needs to down-sample the image by 4 and thus stage 1 can be written as E-ViT(a 1 ? n 1 /4 ; h 1 , d 1 , n g,1 ).</p><p>We typically use only one attention block, i.e., n 1 = 1.</p><p>The first stage generates the highest-resolution feature map, which consumes lots of memory, as shown in <ref type="table" target="#tab_1">Table 2</ref>. We also construct several 3-stage models, whose first stage patch size is 8. For later stages, the patch sizes are set to 2, which downsizes the feature map resolution by 2. Following the practice in ResNet, we increase the hidden dimension twice when downsizing the feature map resolution by 2. We list a few representative model configurations in <ref type="table">Table 1</ref>. Different attention types (a) have different choices of number of global tokens n g . But they share the same model configurations. Thus we do not specify a and n g in <ref type="table">Table 1</ref>. How to connect global tokens between consecutive stages? The choice varies at different stages and among different tasks. For the tasks in this paper, e.g., classification, object detection, instance segmentation, we simply discard the global tokens and only reshape the local tokens as the input for next stage. In this choice, global tokens only plays a role of an efficient way to globally communicate between distant local tokens, or can be viewed as a form of global memory. These global tokens are useful in vision-language tasks, in which the text tokens serve as the global tokens and will be shared across stages. Should we use the average-pooled layer-normed features or the LayerNormed CLS token's feature for image classification? The choice makes no difference for flat models. But the average-pooled feature performs better than the CLS feature for multi-scale models, especially for the multi-scale models with only one attention block in the last stage (including all models in <ref type="table">Table 1</ref>). Please refer to the Supplementary for an ablation study. As reported in <ref type="table" target="#tab_1">Table 2</ref>, the multi-scale models outperform the flat models even in low-resolution classification tasks, demonstrating the importance of multi-scale structure. However, the full self-attention mechanism suffers from the quartic computation/memory complexity w.r.t. the resolution of feature maps, as shown in <ref type="table" target="#tab_1">Table 2</ref>. Thus, it is Right: the Low-rank based attention mechanism. Without "local?local" attentions in Vision Longformer, we get the Global Former. With a linear layer as the projection, we get Linformer <ref type="bibr" target="#b45">[46]</ref>. With a conv layer with equal kernel size and stride, we get Spatial Reduction Attention (SRA) <ref type="bibr" target="#b46">[47]</ref>.</p><p>impossible to train 4-stage multi-scale ViTs with full attention using the same setting (batch size and hardware) used for DeiT training. , and ImageNet accuracy with image size 224. "Full-2,9,1-APE" stands for a 3-stage multiscale ViT with a = full attention, with 2,9,1 number of attention blocks in each stage, respectively, and with Absolute 2-D Positional Embedding (APE). Since all our multi-scale models use average-pooled feature from the last stage for classification, we report Top-1 accuracy of "E-ViT(full/16)-APE" both with the CLS feature (first) and with the averagepooled feature (second). The multi-scale models consistently outperform the flat models, but the memory usage of full attention quickly blows up when only one high-resolution block is introduced. The Vision Longformer ("ViL-") saves FLOPs and memory, without performance drop. Using relative positional bias ("ViL-***-RPB") further improves the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model #Params FLOPs Memory</head><formula xml:id="formula_2">Top-1 (M) (G) (M) (%)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Vision Longformer: A "Local Attention + Global Memory" Mechanism</head><p>We propose to use the "local attention + global memory" efficient mechanism, as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref> (Left), to reduce the computational and memory cost in the E-ViT module. The 2-D Vision Longformer is an extension of the 1-D Longformer [3] originally developed for NLP tasks. We add n g global tokens (including the CLS token) that are allowed to attend to all tokens, serving as global memory. Local tokens are allowed to attend to only global tokens and their local 2-D neighbors within a window size. After all, there are four components in this "local attention + global memory" mechanism, namely global-to-global, local-to-global, global-to-local, and local-to-local, as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref> (Left). In Equation <ref type="formula">(2)</ref>, a Multi-head Self-Attention (MSA) block with the Vision Longformer attention mechanism is denoted as M SA ViL , i.e., a = ViL in Equation <ref type="formula">(2)</ref>. Relative positional bias for Vision Longformer. Following <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b25">26]</ref>, we add a relative positional bias B to each head when computing the attention score:</p><formula xml:id="formula_3">Attention(Q, K, V ) = SoftMax(QK T / ? d + B)V,<label>(4)</label></formula><p>where Q, K, V are the query, key and value matrices and d is the query/key dimension. This relative positional bias makes Vision Longformer translational invariant, which is a desired property for vision models. We observe significant improvements over the absolute 2-D positional embedding, as shown in <ref type="table" target="#tab_1">Table 2</ref> for ImageNet classification and Section 4.4 for COCO object detection. Theoretical complexity. Given the numbers of global and local tokens, denoted by n g and n l respectively, and local attention window size w, the memory complexity of the M SA ViL block is O(n g (n g + n l ) + n l w 2 ). Although <ref type="bibr">[3]</ref> points out that separating the attention parameters for global and local tokens is useful, we do not observe obvious gain in our experiments and thus simply let them share the same set of attention parameters. We empirically set the window size w to 15 for all E-ViT stages, which makes our model comparable with the global attention window size 14 of ViT/16 acted on 224 ? 224 images. With such a window size, only attentions in the first two stages (in 4-stage multiscale ViTs) are local. The attentions in the later two stages are equivalent 2 to full attention. In our experiments, we find that it is sufficient to use only one global token (n g = 1) for ImageNet classification problems. So, the effective memory complexity of the M SA ViL block is O((15 2 + 1)n l ), which is linear w.r.t. the number of tokens.</p><p>Superior performance in ImageNet classification. Results in <ref type="table" target="#tab_1">Table 2</ref> show that in comparison with the full atten-tion models, the proposed multi-scale Vision Longformer achieves a similar or even better performance, while saving significant memory and computation cost. The memory saving is significant for feature maps with resolution 56?56 (i.e., the feature maps in the first stage of a 4-stage multiscale model). The savings are even more significant for higher resolution feature maps. This makes Vision Longformer scalable to high-resolution vision tasks, such as object detection and segmentation. When equipped with relative positional bias, Vision Longformer outperforms the full attention models with absolute positional embedding. This indicates that the "local attention + global memory" mechanism is a good inductive bias for vision Transformers. Three implementations of Vision Longformer and its random-shifting training strategy. Vision Longformer is conceptually similar to conv-like local attention. We have implemented Vision Longformer in three ways: (1) using Pytorch's unfold function (nn.unfold or tensor.unfold), (2) using a customized CUDA kernel and (3) using a sliding chunk approach. The unfold implementation is simple but very slow, i.e., 24 times slower than full attention on 40 ? 40 ? 768 feature map. The implementation using the customized CUDA kernel is about 20% faster than the full attention in the same setting, while achieving the theoretical memory complexity. The sliding-chunk approach is the fastest, which is 60% faster than the full attention with a cost of consuming slightly more memory than the theoretical complexity. With the sliding chunk implementation, we also propose a random-shifting training strategy for Vision Longformer, which further improves the training speed and memory consumption during training. Please refer to the Supplementary for details of these implementations and the random-shifting training strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Other Efficient Attention Mechanisms</head><p>We compare Vision Longformer with the following alternative choices of efficient attention methods. We put detailed descriptions of these methods and their experimental setup in the Supplementary. Pure global memory (a = global). In Vision Longformer, see <ref type="figure" target="#fig_1">Figure 2</ref> (Left), if we remove the local-to-local attention, then we obtain the pure global memory attention mechanism (called Global Attention hereafter). Its memory complexity is O(n g (n g + n l )), which is also linear w.r.t. n l . However, for this pure global memory attention, n g has to be much larger than 1. We gradually increase n g (by 2 each time) and its performance gets nearly saturated at 128. Therefore, n g = 128 is the default for this Global attention. Linformer <ref type="bibr" target="#b45">[46]</ref> (a = LIN) projects the n l ? d dimensional keys and values to K ? d dimensions using additional projection layers, where K n l . Then the n l queries only attend to these projected K key-value pairs. The memory complexity of Linformer is O(Kn l ). We gradually increase K (by 2 each time) and its performance gets nearly saturated at 256. Therefore, K = 256 is the default for this Linformer attention, which turns out to be the same with the recommended value. Notice that Linformer's projection layer (of dimension K ?n l ) is specific to the current n l , and cannot be transferred to higher-resolution tasks that have a different n l . Spatial Reduction Attention (SRA) <ref type="bibr" target="#b46">[47]</ref> (a = SRA) is similar to Linformer, but uses a convolution layer with kernel size R and stride R to project the key-value pairs, hence resulting in n l /R 2 compressed key-value pairs. Therefore, The memory complexity of SRA is O(n 2 l /R 2 ), which is still quadratic w.r.t. n l but with a much smaller constant 1/R 2 . When transferring the ImageNet-pretrained SRAmodels to high-resolution tasks, SRA still suffers from the quartic computation/memory blow-up w.r.t. the feature map resolution. Pyramid Vision Transformer <ref type="bibr" target="#b46">[47]</ref> uses this SRA to build multi-scale vision transformer backbones, with different spatial reduction ratios (R 1 = 8, R 2 = 4, R 3 = 2, R 4 = 1) for each stage. With this PVT's setting, the key and value feature maps at all stages are essentially with resolution H/32 ? W/32. Performer <ref type="bibr" target="#b9">[10]</ref> (a = performer) uses random kernels to approximate the Softmax computation in MSA, and achieves a linear computation/memory complexity with respect to n l and the number of random features. We use the default 256 orthogonal random features (OR) for Performer, and provide other details in the Supplementary. Compare Vision Longformer with other attention mechanisms. On the ImageNet classification task in <ref type="table">Table 3</ref>, all efficient attention mechanisms above show a large performance gap from Vision Longformer. Linformer performs very competitively. Global attention and Performer have a similar performance with the DeiT model (72.2 for tiny and 79.8 for small). We use spatial reduction ratios 16, 8, 4, 2 from stage1 to stage4 for the multi-scale SRA model, which is different from the reduction ratios 8, 4, 2, 1 in PVT <ref type="bibr" target="#b46">[47]</ref>. This more aggressive spatial reduction makes the classification performance worse in <ref type="table">Table 3</ref>, but makes the memory cost manageable when transfer to detection tasks for input image size 8000 ? 1333. For a more complete comparison of these models, including model parameters, FLOPs and memory usage, please refer to the Supplementary. Why is Longformer better? One possible reason is that the conv-like sparsity is a good inductive bias for vision transformers, compared with other attention mechanisms. This is supported by the visualization of the attention maps from pretrained DeiT models <ref type="bibr" target="#b42">[43]</ref>. Another explanation is that Vision Longformer keeps the key and value feature maps high resolution. However, low resolution-based attention mechanims like Linformer and SRA and pure global attention lose the high-resolution information in the key and value feature maps. Mixed attention mechanisms (Partial X-former) for classification tasks. For classification tasks with 224 ? 224 image size as input, the feature map size at Stage3 in multiscale ViTs is 14 ? 14. This is the same as the feature map size in ViT and DeiT, and best suits for full attention. A natural choice is to use efficient attention in the first two stages (with high-resolution feature map but with small number of blocks) and to use full attention in the last two stages. Multiscale ViTs with this mixed attention mechanisms are called "Parital X-former". We also report these Partial X-formers' performance in <ref type="table">Table 3</ref>. All these Partial X-formers perform well on ImageNet classification, with very little (even no) gap between Full Attention and Vision Longformer. These Partial X-forms achieve very good accuracy-efficiency performance for low-resolution classification tasks. We do not have "Partial ViL" for classification because ViL's window size is 15, and thus its attention mechanism in the last two stages is equivalent to the full attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Transfer to High-resolution Vision Tasks</head><p>Similar to the transfer-ability of ImageNet-pretrained CNN weights to downstream high-resolution tasks, such as object detection and segmentation, multi-scale Vision Longformer pretrained on ImageNet can be transferred to such high-resolution tasks, as we will show in Section 4.3.</p><p>However, Linformer is not transferable because the weights of the linear projection layer is specific to a resolution. The Partial X-formers and Multi-scale ViT with full attention are not transferable due to its prohibitively large memory usage after transferred to high-resolution tasks. In <ref type="table">Table 8</ref>, we also show the superior performance of Vision Longformer over other attention mechanisms, on the object detection and segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we show the final performance of Multiscale Vision Longformer (short for ViL) on ImageNet classification in Section 4.1 &amp; 4.2 and downstream highresolution detection tasks in Section 4.3. We follow the DeiT training configuration for ImageNet classification training, and use the standard "?1" and "?3+MS" training schedules with the "AdamW" optimizer for detection tasks. We refer to the Supplementary for detailed experimental settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ImageNet Classification</head><p>Following DeiT <ref type="bibr" target="#b42">[43]</ref> and PVT <ref type="bibr" target="#b46">[47]</ref>, we build multi-scale ViLs with four different sizes, i.e., tiny, small, medium and base. The detailed model configuration is specified in Table 1. We train multi-scale ViLs purely on ImageNet1K, following the setting in DeiT <ref type="bibr" target="#b42">[43]</ref>.</p><p>In <ref type="table" target="#tab_3">Table 4</ref>, we report our results and compare with ResNets <ref type="bibr" target="#b13">[14]</ref>, ViT <ref type="bibr" target="#b11">[12]</ref>, DeiT <ref type="bibr" target="#b42">[43]</ref> and PVT <ref type="bibr" target="#b46">[47]</ref>. Our models outperform other models in the same scale by a large margin. We again confirm that the relative positional bias (RPB) outperforms the absolute 2-D positional embedding (APE) on Vision Longformer. When compared with Swin Transformers <ref type="bibr" target="#b25">[26]</ref>, our models still performs better with fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">ImageNet-21K pretraining and ImageNet-1K finetuning</head><p>When trained purely on ImageNet-1K, the performance gain from ViL-Medium to ViL-Base is very marginal. This is consistent with the observation in ViT <ref type="bibr" target="#b11">[12]</ref>: large pure transformer based models can be trained well only when training data is sufficient.</p><p>Therefore, we conducted experiments in which ViL-Medium/Base models are first pre-trained on ImageNet-21k with image size 224 2 and finetuned on ImageNet-1K with image size 384 2 . For ViT models on image size 384 2 , there are in total 24 ? 24 tokens with full attention. For ViL models on image size 384 2 , we set the window sizes to be <ref type="bibr" target="#b12">(13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr">25,</ref><ref type="bibr">25)</ref> from Stage1 to Stage4. Therefore, in the last two stages, the ViL models' attention is still equivalent to full attention. As shown in In <ref type="table">Table 5</ref>, the performance gets significantly boosted after ImageNet-21K pretraining for both ViL medium and base models. We want to point out that the performance of ViL-Medium model has surpassed that of ViT-Base/16, ViT-Large/16 and BiT-152x4-M, in the ImageNet-21K pretraining setting. The performance of ViL-Base models are even better. This shows the superior performance and parameter efficiency of ViL models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Detection Tasks</head><p>We apply our ViL to two representative object detection pipelines including RetinaNet <ref type="bibr" target="#b23">[24]</ref> and Mask-RCNN <ref type="bibr" target="#b12">[13]</ref>. We follow the conventional setting to use our Vision Longformer as the backbone to generate feature maps for both detection pipelines. Similar to <ref type="bibr" target="#b46">[47]</ref>, we extract the features from all four scales and then feed them to the detection and/or instance segmentation head. To adapt the learned relative positional bias to the higher image resolution in detection, we perform bilinear interpolation on it prior to the training. In our experiments, all models are evaluated on COCO dataset <ref type="bibr">[25]</ref>, with 118k images for training and 5k images for evaluation. We report the results for both 1? and 3?+MS training schedules, and compare them with two backbone architectures: ResNet <ref type="bibr" target="#b13">[14]</ref> and PVT <ref type="bibr" target="#b12">[13]</ref>.</p><p>As shown in <ref type="table">Table 6</ref>, our ViL achieves significantly better performance than the ResNet and PVT architecture. The improvements are uniform over all model sizes (tiny, small, medium, base) and over all object scales (AP S , AP M , AP L ). The improvement is so large that ViL-Tiny with "3x+MS" schedule already outperforms the ResNeXt101-64x4d and the PVT-Large models. A similar trend is observed with the Mask R-CNN pipeline. As shown in Table 7, our ViL backbone significantly surpasses ResNet and PVT baselines on both object detection and instance segmentation. When compared with the concurrent Swin Transformer <ref type="bibr" target="#b25">[26]</ref>, our model also outperforms it with fewer parameter and FLOPs. More specifically, our ViL-Small achieves 47.1 AP b with 45M parameters, while Swin-Tiny achieves 46.0 AP b with 48M parameters. These consistent and significant improvements with both RetinaNet and Mask R-CNN demonstrate the promise of our proposed ViL when using it as the image encoder for high-resolution dense object detection tasks. Compare with other efficient attention mechanisms. Similar to Sec 4.4, we study SRA <ref type="bibr" target="#b46">[47]</ref>, Global Transformer and Performer and their corresponding partial version with Mask R-CNN pipeline (trained with the 1? schedule). As we can see in <ref type="table">Table 8</ref>, when efficient attention mechanisms are used in all stages, ViL achieves much better performance than the other three mechanisms. Specifically, our ViL achieves 42.9 AP b while the other three are all around 36.0 AP b . When efficient attention mechanisms are only used in the first two stages (Par-Xformer), the gaps between different mechanisms shrink to around 1.0 point while our ViL still outperform all others. Moreover, the ViL model outperforms the partial models of all other attention mechanisms and has a very small gap (0.4 AP b ) from the Partial-ViL model. These results show that the "local attention + global memory" mechanism in Vision Longformer can retain the good performance of the full attention mechanism in ViT, and that it is a clear better choice than other efficient attention mechanisms for high-resolution vision tasks. The effects of window size and number of global tokens are not obvious in ImageNet classification, as long as the last two stages use full attention. For different window sizes in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21]</ref> and different number of global tokens in [0, 1, 2, 4, 8], the final top-1 accuracy differs by at most 0.2 for ViL-Small models. Meanwhile, their effects are significant in high-resolution tasks, where ViL models use local attention in all stages. In <ref type="figure" target="#fig_2">Figure 3</ref>, we report their effects in COCO object detection with Mask R-CNN. We notice that the window size plays a crucial role and the default window size 15 gives the best performance. Smaller window sizes lead to serious performance drop. As shown in Figure 3  <ref type="table" target="#tab_3">Table 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study for Detection Tasks</head><p>(Right), as long as there is one global token, adding more global tokens does not improve the performance any more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we have presented a new Vision Transformer (ViT) architecture Multi-Scale Vision Longformer to address the computational and memory efficiency that prevents the vanilla ViT model from applying to vision tasks requiring high-resolution feature maps. We mainly developed two techniques: 1) a multi-scale model structure designed for Transformers to provide image encoding at multiple scales with manageable computational cost, and 2) an efficient 2-D attention mechanism of Vision Longformer for achieving a linear complexity w.r.t. the number of input tokens. The architecture design and the efficient attention mechanism are validated with comprehensive ablation studies. Our experimental results show that the new ViT architecture effectively addresses the computational and memory efficiency problem and outperforms several strong baselines on image classification and object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Model configurations</head><p>We listed the model configuration of all models used in this paper in <ref type="table">Table 9</ref>. We do not specify the attention mechanism here, because the model configuration is the same for all attention mechanisms and the attention-specific parameters are specified in <ref type="table">Table 15</ref>.  <ref type="table">Table 9</ref>. Model architecture for multi-scale stacked ViTs. Architecture parameters for each E-ViT module E-ViT(a ? n/p ; h, d): number of attention blocks n, input patch size p, number of heads h and hidden dimension d. See the meaning of these parameters in <ref type="figure" target="#fig_0">Figure 1 (Bottom)</ref>. <ref type="table">Table 10</ref> summarizes our training setups for our different models. For the ImageNet classification task, our setting mainly follow that in DeiT <ref type="bibr" target="#b42">[43]</ref>. For example, we do not use dropout but use random path. We use all data augmentations in DeiT <ref type="bibr" target="#b42">[43]</ref>, except that we apply Repeated Augmentation only on Medium and Base models. When fine-tuning from a ImageNet-21K pretrained checkpoint, we mainly follow the practice of ViT <ref type="bibr" target="#b11">[12]</ref>, train on image size 384 ? 384, use SGD with momentum 0.9, use no weight decay, and use only random cropping for data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Experimental settings</head><p>For COCO object detection/segmentation tasks, we follow the standard "1?" and "3 ? +MS" schedules. We only change the optimizer from SGD to AdamW and search for good initial learning rate and weight decay. For the "1?" schedule, the input image scale is fixed to be (800, 1333) for the min and max sizes, respectively. For the "3 ? +MS" schedule, the input image is randomly resized to have min size in {640, 672, 704, 736, 768, 800}. We found that there is obvious over-fitting in Training ViL-Medium and ViL-Base models on COCO, mainly because that these two models are relatively large but they are only pretrained on Im-ageNet. Therefore, we are taking the best checkpoint (one epoch per checkpoint) along the training trajectory to report the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. More experimental results</head><p>B.1. Ablation study on the architecture design of multi-scale Vision Longformer</p><p>In this section, we present two ablation studies on the model architecture of multi-scale Vision Longformer. Ablation of the effects of LayerNorm and 2-D positional embedding in the patch embedding. In <ref type="table" target="#tab_1">Table 2</ref>, we show that our flat model E-ViT(full ? 12/16), which only differs from the standard ViT/DeiT model by an newly-added LayerNorm after the patch embedding and the 2-D positional embedding, has better performance than the standard ViT/DeiT model. In <ref type="table">Table 11</ref>, we show that this better performance comes from the newly-added LayerNorm. Feature from the CLS token or from average pooling? As shown in <ref type="table" target="#tab_1">Table 12</ref>, for ViL models that has only one attention block in the last stage (ViL 1-2-8-1), the average pooled feature from all tokens works better than the feature of the CLS token. However, when there are more than 2 attention blocks in the last stage (ViL 1-1-8-2), the difference between these two features disappears. The ViL 1-1-8-2 model has better performance than the ViL 1-2-8-1 model because it has more trainable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. A comprehensive comparison of different attention mechanisms on ImageNet classification</head><p>We compare different attention mechanisms with different model sizes and architectures in <ref type="table" target="#tab_3">Table 13 and Table 14</ref>.</p><p>In <ref type="table">Table 13</ref>, we show their performance on ImageNet-1K classification problem, measured by Top-1 accuracy. In <ref type="table" target="#tab_3">Table 14</ref>, we show their number of parameters and FLOPs. We would like to comment that FLOPs is just a theoretical estimation of computation complexity, and it may not fit well the space/time cost in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementations and Efficiency of Vision Longformer In Practice</head><p>There is a trivial implementation of the conv-like sliding window attention, in which we compute the full quadratic attention and then mask out non-neighbor tokens. This approach suffers from the quadratic complexity w.r.t. number of tokens (quartic w.r.t. feature map size), and is impractical for real use, as shown by the blue curve in <ref type="figure" target="#fig_3">Figure 5</ref>. We  only use it to verify the correctness of our other implementations. <ref type="figure">Figure 4</ref>. The sliding-chunk implementation of Vision Longformer. This implementation (Right) lets one token attends to more tokens than the exact conv-like local attention (Left). Our sliding-chunk implementation has the choice to be exactly the same with the conv-like local attention (Left), by masking out tokens that should not be attended to. For chunks on the boundaries, our implementation supports both no padding and cyclic padding.</p><p>We have implemented Vision Longformer in three ways:</p><p>1. Using Pytorch's unfold function. We have two subversions: one using nn.functional.unfold (denoted as "unfold/nn.F") and the other using tensor.unfold (denoted as "unfold/tensor"). As shown in <ref type="figure" target="#fig_3">Figure 5</ref>, the "unfold/tensor" version (red solid line) is more efficient both in time and memory than the "unfold/nn.F" version (red dotted line). However, both of them are even slower and use more memory than the full atten-tion! 2. Using a customized CUDA kernel, denoted as "cuda kernel". We make use of the TVM, like what has done in Longformer [3], to write a customized CUDA kernel for Vision Longformer. As shown in <ref type="figure" target="#fig_3">Figure 5</ref>, the "cuda kernel" (green line) achieves the theoretical optimal memory usage. Its time complexity is also reduced to linear w.r.t. number of tokens (quadratic w.r.t. feature map size). However, since it's not making use of the highly optimized matrix multiplication libraries in CUDA, it's speed is still slow in practice.</p><p>3. Using a sliding chunk approach, illustrated in <ref type="figure">Figure 4</ref>. For this sliding chunk approach, we have two subversions: one using Pytorch's autograd to compute backward step (denoted as "SCw/Autograd") and the other writing a customized torch.autograd.Function with hand-written backward function (denoted as "SCw/Handgrad"). Both sub versions of this sliding chunk approach are fully implemented with Pytorch functions and thus make use of highly optimized matrix multiplication libraries in CUDA. As shown in <ref type="figure" target="#fig_3">Figure 5</ref>, both of them are faster than the "cuda kernel" implementation.</p><p>In the sliding chunk approach, to achieve a conv-like local attention mechanism with window size 2w + 1, we split the feature map into chunks with size w ? w. Each chunk only attends to itself and its 8 neighbor chunks. The Pytorch Autograd will save 9 copies of the feature map (9 nodes in the computing graph) for automatic back-propagation, which is not time/memory efficient. The "SCw/Handgrad" version defines a customized torch.autograd.Function with hand-written backward function, which greatly saves the memory usage and also speeds up the algorithm, as shown in <ref type="figure" target="#fig_3">Figure 5</ref>. We would like to point out that the memory usage of the "SCw/Handgrad" version is nearly optimal (very close to that of the "cuda kernel"). Similar speed-memory trade-off with different implementations of local attention mechanism has been observed in the 1-D Longformer [3], too; see <ref type="figure" target="#fig_0">Figure 1</ref> in <ref type="bibr">[3]</ref>. We would like to point out that Image Transformer <ref type="bibr" target="#b29">[30]</ref> has an implementation of of 2-D conv-like local attention mechanism, which is similar to our "SCw/Autograd" version. The Image Transformer <ref type="bibr" target="#b29">[30]</ref> applies it to the image generation task.</p><p>This sliding-chunk implementation <ref type="figure">(Figure 4</ref> Right) lets one token attends to more tokens than the exact conv-like local attention <ref type="figure">(Figure 4</ref> Left). Our sliding-chunk implementation has the choice to be 1. exactly the same with the conv-like local attention (Left), by masking out tokens that should not be attended to, 2. sliding chunk without padding, in which the chunks on the boundary have less chunks to attend to, 3. sliding chunk with cyclic padding, in which the chunks on the boundary still attend to 9 chunks with cyclic padded chunks.</p><p>Since these three masking methods only differ by the attention masks to mask out invalid tokens, their speed and <ref type="figure">Figure 6</ref>. Compare of three masking methods of our "SCw/Handgrad" implementation of conv-like local attention: exact conv-like sliding window attention, sliding chunk attention without padding for boundary chunks, and sliding chunk attention with cyclic padding for boundary chunks. The are nearly the same in terms of running time (including forward and backward) and memory usage. The window size is 17 and thus chunk size is 8. memory usage are nearly the same, as shown in <ref type="figure">Figure 6</ref>. For ImageNet classification, we observe no obvious difference in top1 accuracy between "exact sliding window" and "sliding chunk without padding", while "sliding chunk with cyclic padding" performs slightly worse most of the time. For object detection, we observe that "sliding chunk without padding" performs consistently better than "exact sliding window", as shown in <ref type="figure">Figure 8</ref>. Therefore, we make "sliding chunk without padding" as the default making method for Vision Longformer, although it sacrifices some translational invariance compared with "exact sliding window". <ref type="bibr">Figure 8</ref>. "Sliding chunk without padding" performs consistently better than "exact sliding window" for object detection with Mask R-CNN. All use the same ImageNet1K pre-trained checkpoint (ViL-Small-RPB in <ref type="table" target="#tab_3">Table 4</ref>).</p><p>In <ref type="figure" target="#fig_4">Figure 7</ref>, we show the running time (including forward and backward) and memory usage of our "SCw/Handgrad" implementation of conv-like local attention (sliding chunk attention without padding mode) with different window sizes. We can see that the speed is not sensitive to the window size for small window sizes (? 17) and the memory usage monotonically increases.</p><p>Finally, both the "unfold/nn.F" and the "cuda kernel" implementations support dilated conv-like attention. The customized CUDA kernel is even more flexible to support different dilations for different heads. The sliding-chunk implementation does not support this dilated conv-like attention. In this paper, we always use the sliding-chunk implementation due to its superior speed and nearly optimal memory complexity.</p><p>In <ref type="figure" target="#fig_3">Figure 5</ref>, 6 and 7, the evaluation is performed on a single multi-head self-attention module (MSA) with the conv-like local attention mechanism, instead of on the full multi-scale Vision Longformer. With this evaluation, we can clearly see the difference among different implementations of the conv-like local attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Random-shifting strategy to improve training efficiency</head><p>We propose the random-shifting training strategy for Vision Longformer, to further accelerate the training speed of Vision Longformer. More specifically, instead of attending to all 8 neighbor patches, one patch can only attend to itself and one random neighbor patch during training. To achieve <ref type="figure">Figure 9</ref>. Illustration of the 8 modes in the random-shifting training strategy. For mode i (1 &lt;= i &lt;= 8), the query chunk (dark brown) attends to itself and the i'th neighbor chunk. <ref type="figure" target="#fig_0">Figure 10</ref>. The random-shifting strategy does not harm the model performance (Left), an accelerates the Vision Longformer training significantly (Right). When zooming in, the performance of "random-shift and switch at 75%" is slightly better than the "Sliding-chunk attention with 8 neighbor chunks". this, we define 10 modes of the sliding-chunk local attention:</p><p>? 0 (default): attend to itself and all 8 neighbor chunks,</p><p>? -1 : only attend to itself chunk, ? i (1 &lt;= i &lt;= 8) : attend to itself chunk and the i'th neighbor chunk.</p><p>The ordering of the 8 neighbor patches is visualized in <ref type="figure">Figure 9</ref>. During training, we can randomly sample one mode from 1 to 8 and perform the corresponding random-shifting attention. We switch from the random-shifting mode to the default 8-neighbor mode after x% training iterations, and this switch time x% is a hyper-parameter with default value 75%. This switch, can be seen as fine-tuning, is necessary to mitigate the difference of model's behavior during training and inference. As shown in <ref type="figure" target="#fig_0">Figure 10</ref>, this random-shifting training strategy accelerates the Vision Longformer training significantly, while not harming the final model performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>A Multi-scale vision Transformers (bottom) by stacking 4 E-ViT modules (Top)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Left: the Vision Longformer attention mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Effects of window size (Left) and number of global tokens (Right) in Vision Longformer for object detection with Mask R-CNN. All use the same ImageNet1K pre-trained checkpoint (ViL-Small-RPB in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Compare of running time (including forward and backward) and memory usage of different implementations of the convlike attention in Vision Longformer. All of these implementations shown in the figures are mathematically equivalent, doing the exact conv-like sliding window attention with window size 17.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Running time (including forward and backward) and memory usage of our "SCw/Handgrad" implementation of convlike local attention (sliding chunk attention without padding mode) with different window sizes. The speed is not sensitive to the window size for small window sizes (? 17) and the memory usage monotonically increases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>DeiT-Small / 16 [43]</cell><cell>22.1</cell><cell>4.6</cell><cell>67.1</cell><cell>79.9</cell></row><row><cell>E-ViT(full/16)-APE</cell><cell>22.1</cell><cell>4.6</cell><cell>67.1</cell><cell>80.4/80.7</cell></row><row><cell>Full-1,10,1-APE</cell><cell>27.58</cell><cell>4.84</cell><cell>78.5</cell><cell>81.7</cell></row><row><cell>Full-2,9,1-APE</cell><cell>26.25</cell><cell>5.05</cell><cell>93.8</cell><cell>81.7</cell></row><row><cell>Full-1,1,9,1-APE</cell><cell>25.96</cell><cell>6.74</cell><cell>472.9</cell><cell>81.9</cell></row><row><cell>Full-1,2,8,1-APE</cell><cell>24.63</cell><cell>6.95</cell><cell>488.3</cell><cell>81.9</cell></row><row><cell>ViL-1,10,1-APE</cell><cell>27.58</cell><cell>4.67</cell><cell>73.0</cell><cell>81.6</cell></row><row><cell>ViL-2,9,1-APE</cell><cell>26.25</cell><cell>4.71</cell><cell>81.4</cell><cell>81.8</cell></row><row><cell>ViL-1,1,9,1-APE</cell><cell>25.96</cell><cell>4.82</cell><cell>108.5</cell><cell>81.8</cell></row><row><cell>ViL-1,2,8,1-APE</cell><cell>24.63</cell><cell>4.86</cell><cell>116.8</cell><cell>82.0</cell></row><row><cell>ViL-1,10,1-RPB</cell><cell>27.61</cell><cell>4.67</cell><cell>78.8</cell><cell>81.9</cell></row><row><cell>ViL-2,9,1-RPB</cell><cell>26.28</cell><cell>4.71</cell><cell>88.7</cell><cell>82.3</cell></row><row><cell>ViL-1,1,9,1-RPB</cell><cell>25.98</cell><cell>4.82</cell><cell>121.8</cell><cell>82.2</cell></row><row><cell>ViL-1,2,8,1-RPB</cell><cell>24.65</cell><cell>4.86</cell><cell>131.6</cell><cell>82.4</cell></row></table><note>. Flat vs Multi-scale Models: Number of paramers, FLOPS, memory per image (with Pytorch Automatic Mixed Precision en- abled)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>Model</cell><cell cols="2">#Params (M) GFLOPs</cell><cell>Top-1 (%)</cell></row><row><cell>R18</cell><cell>11.7</cell><cell>1.8</cell><cell>69.8</cell></row><row><cell>DeiT-Tiny/16[43]</cell><cell>5.7</cell><cell>1.3</cell><cell>72.2</cell></row><row><cell>PVT-Tiny[47]</cell><cell>13.2</cell><cell>1.9</cell><cell>75.1</cell></row><row><cell>ViL-Tiny-APE</cell><cell>6.7</cell><cell>1.3</cell><cell>76.3</cell></row><row><cell>ViL-Tiny-RPB</cell><cell>6.7</cell><cell>1.3</cell><cell>76.7</cell></row><row><cell>R50</cell><cell>25.6</cell><cell>4.1</cell><cell>78.5</cell></row><row><cell>DeiT-Small/16[43]</cell><cell>22.1</cell><cell>4.6</cell><cell>79.9</cell></row><row><cell>PVT-Small[47]</cell><cell>24.5</cell><cell>3.8</cell><cell>79.8</cell></row><row><cell>Swin-Tiny[26]</cell><cell>28</cell><cell>4.5</cell><cell>81.2</cell></row><row><cell>ViL-Small-APE</cell><cell>24.6</cell><cell>4.9</cell><cell>82.0</cell></row><row><cell>ViL-Small-RPB</cell><cell>24.6</cell><cell>4.9</cell><cell>82.4</cell></row><row><cell>R101</cell><cell>44.7</cell><cell>7.9</cell><cell>79.8</cell></row><row><cell>PVT-Medium[47]</cell><cell>44.2</cell><cell>6.7</cell><cell>81.2</cell></row><row><cell>Swin-Small[26]</cell><cell>50</cell><cell>8.7</cell><cell>83.2</cell></row><row><cell>ViL-Medium-APE</cell><cell>39.7</cell><cell>8.7</cell><cell>83.3</cell></row><row><cell>ViL-Medium-RPB</cell><cell>39.7</cell><cell>8.7</cell><cell>83.5</cell></row><row><cell>X101-64x4d</cell><cell>83.5</cell><cell>15.6</cell><cell>81.5</cell></row><row><cell>ViT-Base/16[12]</cell><cell>86.6</cell><cell>17.6</cell><cell>77.9</cell></row><row><cell>DeiT-Base/16[43]</cell><cell>86.6</cell><cell>17.6</cell><cell>81.8</cell></row><row><cell>PVT-Large[47]</cell><cell>61.4</cell><cell>9.8</cell><cell>81.7</cell></row><row><cell>Swin-Base[26]</cell><cell>88</cell><cell>15.4</cell><cell>83.5</cell></row><row><cell>ViL-Base-APE</cell><cell>55.7</cell><cell>13.4</cell><cell>83.2</cell></row><row><cell>ViL-Base-RPB</cell><cell>55.7</cell><cell>13.4</cell><cell>83.7</cell></row></table><note>. Number of paramers, FLOPS and ImageNet accuracy. Trained on ImageNet-1K with image size 224. Our ViL models are highlighted with gray background.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .Table 7 .</head><label>67</label><figDesc>AP AP 50 AP 75 AP S AP M AP L AP AP 50 AP 75 AP S AP M AP L Object detection performance on the COCO val2017 with RetinaNet. The FLOPs (G) are measured at resolution 800 ? 1333, and</figDesc><table><row><cell>Model</cell><cell>#Params (M)</cell><cell cols="4">No IN-21K GFLOPs Top-1 GFLOPs Top-1 After IN-21K</cell></row><row><cell>ViT-Base/16[12]</cell><cell>86.6</cell><cell>17.6</cell><cell>77.9</cell><cell>49.3</cell><cell>84.0</cell></row><row><cell>ViT-Large/16[12]</cell><cell>307</cell><cell>61.6</cell><cell>76.5</cell><cell>191.1</cell><cell>85.2</cell></row><row><cell>BiT-152x4-M[19]</cell><cell>928</cell><cell>182</cell><cell>81.3</cell><cell>837</cell><cell>85.4</cell></row><row><cell>Swin-Base[26]</cell><cell>88</cell><cell>15.4</cell><cell>83.5</cell><cell>47.1</cell><cell>86.4</cell></row><row><cell>ViL-Medium-RPB</cell><cell>39.7</cell><cell>8.7</cell><cell>83.5</cell><cell>28.4</cell><cell>85.7</cell></row><row><cell>ViL-Base-RPB</cell><cell>55.7</cell><cell>13.4</cell><cell>83.7</cell><cell>43.7</cell><cell>86.2</cell></row><row><cell cols="6">Table 5. Trained purely on ImageNet-1K with image size 224 (No</cell></row><row><cell cols="6">IN-21K). Pretained on ImageNet-21K with image size 224 and</cell></row></table><note>Finetuned on ImageNet-1K with image size 384 (After IN-21K), except BiT-M [19] fine-tuned with image size 480. Our ViL mod- els are highlighted with gray background.FLOPs for PVT architecture are not available. Our ViL-Tiny and ViL-Small models are pre-trained on ImageNet-1K, our ViL-Medium and ViL-Base models are pre-trained on ImageNet-21k. ViL results are highlighted with gray background.Object detection and instance segmentation performance on the COCO val2017 with Mask R-CNN. The FLOPs (G) are measured at resolution 800 ? 1333, and FLOPs for PVT architecture are not available. Our ViL-Tiny and ViL-Small models are pre-trained on ImageNet-1K, our ViL-Medium and ViL-Base models are pre-trained on ImageNet-21k. ViL results are highlighted with gray background.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 .Table 11 .</head><label>1011</label><figDesc>Hyperparameters for training. We use MsViT to represent the multi-scale vision transformers with different kinds of attention mechanisms, including our Vision Longformer (ViL). For the experiments trained on COCO, MsViT is combined with the Retinanet or Mask R-CNN. The training configs for Retinanet or Mask R-CNN are the same, and we still use MsViT for their unified short name. We do not apply gradient clipping for all ImageNet classification training and apply gradient clipping at global norm 1 for COCO object detection/segmentation. We use AdamW for all our experiments, except that we use SGD with momentum 0.9 for the ImageNet-384 fine-tuning experiments. Ablation of the effects of LayerNorm and 2-D positional embedding in the patch embedding of the E-ViT module, with ImageNet Top-1 accuracy. The improvement over DeiT<ref type="bibr" target="#b42">[43]</ref> comes from the added LayerNorm. The 2-D positional embedding is mainly for saving parameters for high-resolution feature maps. The column names of "CLS" and "Ave Pool" indicate how the image feature is obtained for the linear classification head.</figDesc><table><row><cell>Model</cell><cell cols="4">Tiny CLS Ave Pool CLS Ave Pool Small</cell></row><row><cell cols="2">DeiT/16[43] 72.2</cell><cell>-</cell><cell>79.8</cell><cell>-</cell></row><row><cell cols="2">+Layernorm 72.91</cell><cell>73.36</cell><cell>80.33</cell><cell>80.32</cell></row><row><cell>+2D Pos</cell><cell>73.21</cell><cell>73.09</cell><cell>80.44</cell><cell>80.75</cell></row><row><cell>Model</cell><cell cols="4">Tiny CLS Ave Pool CLS Ave Pool Small</cell></row><row><cell cols="2">ViL-1,2,8,1-APE 75.72</cell><cell>75.98</cell><cell>81.65</cell><cell>81.99</cell></row><row><cell cols="2">ViL-1,1,8,2-APE 76.18</cell><cell>76.25</cell><cell>82.12</cell><cell>82.08</cell></row><row><cell cols="5">Table 12. For ViL models that has only one attention block in the</cell></row><row><cell cols="5">last stage (ViL 1-2-8-1), the average pooled feature from all tokens</cell></row><row><cell cols="5">works better than the feature of the CLS token. When there are</cell></row><row><cell cols="5">more than 2 attention blocks in the last stage (ViL 1-1-8-2), the</cell></row><row><cell cols="4">difference between these two features disappears.</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Equivalent in our sliding chunks implementation, which is our default choice.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>lized in this work</head><p>In this paper, we compare Vision Longformer with the following alternative choices of efficient attention methods. Pure global memory (a = global). In Vision Longformer, see <ref type="figure">Figure 2</ref> (Left), if we remove the local-to-local attention, then we obtain the pure global memory attention mechanism (called Global Attention hereafter). Its memory complexity is O(n g (n g + n l )), which is also linear w.r.t. n l . However, for this pure global memory attention, n g has to be much larger than 1. In practice, we set different numbers of global tokens for different stages, as shown in <ref type="table">Table 15</ref>, with more global tokens in the first 2 stages and less in the last 2 stages. This setting makes the memory/computation complexity comparable with other attention mechanisms under the same model size. Linformer <ref type="bibr" target="#b45">[46]</ref> (a = LIN) projects the n l ? d dimensional keys and values to K ? d dimensions using additional projection layers, where K n l . Then the n l queries only attend to these projected K key-value pairs. The memory complexity of Linformer is O(Kn l ). We gradually increase K (by 2 each time) and its performance gets nearly saturated at 256. Therefore, K = 256 is our choice for this Linformer attention, which turns out to be the same with the recommended value. Notice that Linformer's projection layer (of dimension K ?n l ) is specific to the current n l , and cannot be transferred to higher-resolution tasks that have a different n l . It is possible to transfer Linformer's weight by resizing feature maps of a different size to the original feature map size that Linformer is trained with and then applying the Linformer's projection. We do not explore this choice in this work. Spatial Reduction Attention (SRA) <ref type="bibr" target="#b46">[47]</ref> (a = SRA) is similar to Linformer, but uses a convolution layer with kernel size R and stride R to project the key-value pairs, hence resulting in n l /R 2 compressed key-value pairs. Therefore, The memory complexity of SRA is O(n 2 l /R 2 ), which is still quadratic w.r.t. n l but with a much smaller constant 1/R 2 . When transferring the ImageNet-pretrained SRAmodels to high-resolution tasks, SRA still suffers from the quartic computation/memory blow-up w.r.t. the feature map resolution. Pyramid Vision Transformer <ref type="bibr" target="#b46">[47]</ref> uses this SRA to build multi-scale vision transformer backbones, with different spatial reduction ratios (R 1 = 8, R 2 = 4, R 3 = 2, R 4 = 1) for each stage. With this PVT's setting, the key and value feature maps at all stages are essentially with resolution H 32 ? W 32 . This choice is able to scale up to image resolution 600?1000, but the memory usage is much larger than ResNet counterparts for 800 ? 1333.</p><p>In this paper, we benchmarked the performance of SRA/32 with SR ratios R 1 = 8, R 2 = 4, R 3 = 2, R 4 = 1 (same as PVT <ref type="bibr" target="#b46">[47]</ref>) and SRA/64 with SR ratios R 1 = 16, R 2 = 8, R 3 = 4, R 4 = 2 (two times more downsizing from that in PVT <ref type="bibr" target="#b46">[47]</ref>), as shown in <ref type="table">Table 15</ref>. The SRA/64 setting makes the memory usage comparable with other efficient attention mechanisms under the same model size, but introduces more parameters due to doubling the kernel size of the convolutional projection layer. Performer <ref type="bibr" target="#b9">[10]</ref> (a = performer) uses random kernel approximations to approximate the Softmax computation in MSA, and achieves a linear computation/memory complexity with respect to n l and the number of random features K. We use the default K = 256 orthogonal random features (OR) for Performer. The memory/space complexity of performer is O(Kd + n l d + Kn l ) while its computation/time complexity is O(Kn l d). For the time complexity, we ignore the complexity of generating the orthogonal random features, which in practice cannot be ignored during training. We refer to Section B.3 in <ref type="bibr" target="#b9">[10]</ref> for a detailed discussion of theoretical computation/memory complexity of Performer.</p><p>One important technique in training Performer is to redraw the random features during training. In our Ima-geNet classification training, we adopt a heuristic adaptive redrawing schedule: redraw every 1 + 5T iterations in Epoch T (T = 0, 1, ..., 299). In our COCO object detection/segmentation training, the Performer is initialized from ImageNet pretrained checkpoint and thus there is no need to redraw very frequently in the initial training stage.Therefore, we redraw the random features every 1000 iterations in COCO object detection/segmentation training. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Etc: Encoding long and structured data in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Sanghai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08483</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unilmv2: Pseudo-masked language models for unified language model pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhao</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="642" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00364</idno>
		<title level="m">Pre-trained image processing transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>PMLR, 2020. 2</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="1691" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11740</idno>
		<title level="m">Uniter: Learning universal image-text representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Rethinking attention with performers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Up-detr: Unsupervised pre-training for object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yugeng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junying</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09094</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Axial attention in multidimensional transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12180</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">In defense of grid features for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10267" to="10276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
	<note>Nikolaos Pappas, and Fran?ois Fleuret</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11370</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Set transformer: A framework for attention-based permutation-invariant neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungtaek</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3744" to="3753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Unicoder-VL: A universal encoder for vision and language by cross-modal pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuejian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.06066</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Visualbert: A simple and performant baseline for vision and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Oscar: Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Vil-BERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hierarchical transformers for long document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pappagari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zelasko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Villalba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Carmiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="838" to="844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Blockwise selfattention for long document understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02972</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Compressive transformers for long-range sequence modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Jack W Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05507</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Taghi</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vl-Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08530</idno>
		<title level="m">Pre-training of generic visual-linguistic representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">LXMERT: Learning cross-modality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Synthesizer: Rethinking self-attention in transformer models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Che</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00743</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sparse sinkhorn attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<idno>PMLR, 2020. 2</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="9438" to="9447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Long range arena: A benchmark for efficient transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.04006</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06732</idno>
		<title level="m">Efficient transformers: A survey</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Scaling local self-attention for parameter efficient visual backbones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.12731</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Max-deeplab: End-to-end panoptic segmentation with mask transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00759</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belinda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">End-toend video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoshan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.14503</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning texture transformer network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5791" to="5800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinava</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.14062</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning joint spatial-temporal transformations for video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="528" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Vinvl: Revisiting visual representations in vision-language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">End-to-end object detection with adaptive clustering transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09315</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Informer: Beyond efficient transformer for long sequence time-series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieqi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wancai</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07436</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Unified vision-language pretraining for image captioning and VQA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">End-to-end dense video captioning with masked transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Tiny-4stage / 4 means that the model has comparable size with DeiT-Tiny, has 4 stages and uses patch size 4x4 in the initial pixel space. 1-2-8-1 means that the model contains 4 stages, each stage has 1/2/8/1 MSA-FFN blocks, respectively. *Partial* means that the last two stages, which contain most of the attention blocks, still use full attention. Vision Longformer does not have *Partial* version because its window size is set as 15 (comparable with the ViT(DeiT)/16 feature map size 14), and its attention mechanism in the last two stages is equivalent to full attention. * indicates that the training batch size is 256 (with learning rate linearly scaled down)</title>
	</analytic>
	<monogr>
		<title level="m">Table 13. Overall comparison in ImageNet top-1 accuracy, with input size 224</title>
		<imprint/>
	</monogr>
	<note>different from all other experiments with batch size 1024 in this table</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Tiny-4stage / 4 means that the model has comparable size with DeiT-Tiny, has 4 stages and uses patch size 4x4 in the initial pixel space. 1-2-8-1 means that the model contains 4 stages, each stage has 1/2/8/1 MSA-FFN blocks, respectively. *Partial* means that the last two stages, which contain most of the attention blocks, still use full attention. Vision Longformer does not have *Partial* version because its window size is set as 15 (comparable with the ViT(DeiT)/16 feature map size 14</title>
	</analytic>
	<monogr>
		<title level="m">Overall comparison in number of parameters (M) and GFLOPs, with input size 224</title>
		<imprint/>
	</monogr>
	<note>Table 14. and its attention mechanism in the last two stages is equivalent to full attention</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
