<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Geometry Sharing Network for 3D Point Cloud Classification and Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingye</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">SIAT-SenseTime Joint Lab</orgName>
								<orgName type="department" key="dep2">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">ShenZhen Key Lab of Computer Vision and Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhou</surname></persName>
							<email>zp.zhou@siat.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">SIAT-SenseTime Joint Lab</orgName>
								<orgName type="department" key="dep2">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">ShenZhen Key Lab of Computer Vision and Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
							<email>yu.qiao@siat.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">SIAT-SenseTime Joint Lab</orgName>
								<orgName type="department" key="dep2">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">ShenZhen Key Lab of Computer Vision and Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">SIAT Branch</orgName>
								<orgName type="department" key="dep2">Shenzhen Institute of Artificial Intelligence and Robotics for Society</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Geometry Sharing Network for 3D Point Cloud Classification and Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In spite of the recent progresses on classifying 3D point cloud with deep CNNs, large geometric transformations like rotation and translation remain challenging problem and harm the final classification performance. To address this challenge, we propose Geometry Sharing Network (GS-Net) which effectively learns point descriptors with holistic context to enhance the robustness to geometric transformations. Compared with previous 3D point CNNs which perform convolution on nearby points, GS-Net can aggregate point features in a more global way. Specially, GS-Net consists of Geometry Similarity Connection (GSC) modules which exploit Eigen-Graph to group distant points with similar and relevant geometric information, and aggregate features from nearest neighbors in both Euclidean space and Eigenvalue space. This design allows GS-Net to efficiently capture both local and holistic geometric features such as symmetry, curvature, convexity and connectivity. Theoretically, we show the nearest neighbors of each point in Eigenvalue space are invariant to rotation and translation. We conduct extensive experiments on public datasets, ModelNet40, ShapeNet Part. Experiments demonstrate that GS-Net achieves the state-of-the-art performances on major datasets, 93.3% on ModelNet40, and are more robust to geometric transformations. Code is released on https://github.com</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Analysis and classification of 3D point cloud is an important problem in computer vision and graphics, due to its wide applications in robot manipulation <ref type="bibr" target="#b16">(Rusu et al. 2008</ref>), autonomous driving <ref type="bibr" target="#b15">(Qi et al. 2018)</ref> etc. The challenge of this problem comes from several aspects. Firstly, the point cloud are sparsely sampled from 3D surfaces in an irregular and off-order way. Secondly, the point cloud usually undergoes large geometric transformations and deformations. It is important to achieve robustness to transformation and permutation for analyzing and classifying 3D point cloud.</p><p>Large research efforts have been devoted to solving the above problems. One research direction <ref type="bibr" target="#b19">(Su et al. 2015</ref>; <ref type="figure">Figure 1</ref>: Visualization of Eigen-Graph (Best view in color and zoom in). Given a red anchor point, traditional convolutions for 3D points operate on a local region as shown in the red circle. To explore the geometry of point cloud such as symmetry, curvature, convexity and connectivity, we use Eigen-Graph to group neighbors in both Euclidean space and Eigenvalue space. The anchor point's neighbors in Euclidean space are colored blue and its neighbors in Eigenvalue space are colored green, thus we construct the Eigen-Graph of the anchor point and its neighbors. Obviously, green points provide more information about the geometry of the whole point cloud. It shows that our method indeed associates the anchor points with points having similar local geometry, even though these points are far away from each other in Euclidean space. <ref type="bibr" target="#b12">Maturana and Scherer 2015;</ref><ref type="bibr">Wu et al. 2015)</ref> aims to represent the irregular 3D point cloud using regular data, in that way they can use classical convolution neural network to process the regular data. Two of the most common regular representations are voxels and multi-view images. However, both these representations have limitations. Dense voxels representation is inefficient due to the sparsity of input point clouds, while multi-view images may lose 3D structures of points and cause occlusion problem.</p><p>Another direction focuses on designing convolution operations for irregular points, which are inspired by the prominent success of CNNs on regular grid data, such as audio and images. PointNet <ref type="bibr" target="#b13">(Qi et al. 2017a</ref>) learns a spatial encoding of each point directly on Euclidean space and aggre-gates all individual point features by max pooling to obtain a global point cloud signature. Max pooling as a symmetric operation can obtain permutation invariance. By its design, PointNet does not capture local geometry directly which is indispensable to the description of 3D shape. Other works <ref type="bibr" target="#b14">(Qi et al. 2017b;</ref><ref type="bibr" target="#b24">Xu et al. 2018</ref>) mainly utilize group operations (k-nearest neighbors group or ball region group) to identify local points for convolution. But these group operations only focus on local neighborhood region in Euclidean space. Despite of the discreteness and irregularity of point cloud, these operations mainly account the local structures of each point and are not efficient to capture the holistic geometric information from distant information. The holistic geometry not only provides discriminative cues for classification but also help to achieve robustness to transformation. In addition, points with similar geometric structures can be far away from each other in Euclidean space. Previous works mentioned above largely neglect the geometric relationships among these distant points.</p><p>Inspired by the above analysis, this paper proposes Geometry Sharing Network (GS-Net) which aggregates features in both Euclidean space and Eigenvalue space. GS-Net exploits Eigen-Graph to calculate structure tensor for measuring local geometric properties of input points, which further allows us to identify points with similar local structures but located distant in Euclidean space. We prove that these structure tensors are invariant to transformations and yield rich local structural information. As shown in <ref type="figure">Figure 1</ref>, given an anchor point for convolution, GS-Net identifies a group of neighbor points from Euclidean space and also another group of points with similar local structures (Eigen-graph features). Then the convolutions are performed for both groups to capture local and holistic geometric representation separately. The convolutional features from both groups are integrated for classification or segmentation. We conduct extensive experiments to examine the proposed methods. Our method achieves the state-of-the-art performance on Model-Net40 for classification (93.3%) and shows more robustness to geometric transformations than previous methods.</p><p>The main contributions of this paper are summarized as follows.</p><p>? We propose a novel Geometry Similarity Connection (GSC) module which exploits Eigen-Graph to group distant points with similar and relevant geometric information and aggregate features from neighbors in both Euclidean space and Eigenvalue space which can capture local and holistic geometric information more efficiently.</p><p>? We introduce 3D structure tensor and Eigen-Graph to capture the geometric features of points. Theoretically, we prove these features are invariant to translation and rotation.</p><p>? Our GS-Net achieves the state-of-the-art performances on major datasets, ModelNet40, ShapeNet Part. Moreover, GSC module can be integrated into different existing pipelines for point cloud analysis.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deep Learning on Point Cloud Analysis</head><p>Deep neural networks have enjoyed remarkable success for various vision tasks, however it remains challenging to apply CNNs to domains lacking a regular structure such as 3D point cloud. These challenges include: (1) local and holistic geometric information representation;</p><p>(2) permutation invariance;</p><p>(3) rotation and translation invariance. However, not all networks can address these problems absolutely.</p><p>PointNet <ref type="bibr" target="#b13">(Qi et al. 2017a)</ref> and <ref type="bibr">DeepSet (Zaheer et al. 2017</ref>) are pioneering architectures that directly process point cloud. The basic idea is to learn a spatial encoding of each point and then aggregate all individual point features to a holistic signature. But by this design, relations between points are not sufficiently captured. To remedy this, Point-Net++ <ref type="bibr" target="#b14">(Qi et al. 2017b</ref>) partitions point cloud into overlapping local regions by the distance metric of the underlying space and extracts local features capturing fine geometric structures from neighbors, but it still only considers every point in its local region independently. In our method, we address this issue by defining a convolution block that group the features from the neighbors in Euclidean space and Eigenvalue space.</p><p>DGCNN  captures local geometric structure while maintaining permutation invariance and reconstructs the k-nn graph using nearest neighbors in the features space produced by each layer. Different with DGCNN, our method does not use dynamic strategy, we apply Eigen-Decomposition to choose the nearest neighbors and share the local features with distant points with similar geometric information.</p><p>( <ref type="bibr" target="#b20">Thomas et al. 2018a</ref>) directly uses eigenvalues and fuctions of eigenvalues as features in deep-learning setting. <ref type="bibr" target="#b10">(Landrieu and Simonovsky 2018)</ref> adds eigenvalues to its shape descriptors. Our method aggregates features from nearest neighbors in Eigenvalue space in order to capture holistic geometric information. And we also uses eigenvalue as features in our network settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Classical Geometric Representation</head><p>The local geometry of point cloud is estimated by the distribution of points in the neighborhood. <ref type="bibr" target="#b1">(Demantke et al. 2011)</ref> propose a method which aims at finding the optimal neighborhood radius for each point, working directly and exclusively in the 3D domain, without relying on surface descriptors or structures. Firstly, they compute three dimensionality features for each point, between predefined minimal and maximal neighborhood scale. The three dimensionality features (a 1D , a 2D , a 3D ) (Demantke et al. 2011) are computed exhaustively, at each point and for each accepted neighborhood scale from local covariance matrix. Various geometrical features can be derived from the eigenvalues of the covariance matrix. a 1D , a 2D , a 3D describe linear, planar, and scatter respectively. In our GS-Net, we use operations on eigenvalues to improve the robustness of rotation and translation in GS-Net. Moreover, the Eigen-Graph (Sec 3.2) enhances the representation of local geometry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Rotation Invariance for Point Cloud Analysis</head><p>In comparison to permutation invariance, rotation invariance is a more challenging problem. Previous works has dealt with issues of invariance or equivarance under particular input transformations. PointNet <ref type="bibr" target="#b13">(Qi et al. 2017a</ref>) and Point-Net++ <ref type="bibr" target="#b14">(Qi et al. 2017b</ref>) guarantee the permutation invariance by a symmetric pooling operator and PointNet employs a complex and computationally intensive spatial transformer network to learn 3D alignment, PCPNet <ref type="bibr" target="#b5">(Guerrero et al. 2018</ref>) also uses a learned transformer block, but these networks (including <ref type="bibr" target="#b24">Xu et al. 2018)</ref>) do not include rotation invariance. <ref type="bibr" target="#b20">(Thomas et al. 2018b</ref>) upgrades the existing neural network with rotation invariance property, a special convolutional operation is designed as a basis block in the network. But it causes the loss of information as there is no bijection between R 3 and 2-dimensional sphere. In our method, Eigen-Graph addresses the rotation invariance naturally with Eigen-Decomposition of 3D structure tensor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>As shown in <ref type="figure">Figure 2</ref>, we consider a S-dimensional point cloud with N points, denoted by X = {x 1 , . . . , x N } ? R S . Usually, each point of point cloud contains 3D coordinates</p><formula xml:id="formula_0">x i = x 1 i , x 2 i , x 3 i , which means that S = 3;</formula><p>it is also possible to include other coordinates representing RGB information, normal vectors, and so on. In our network architecture, we use hierarchical structure to learn local and holistic features of point cloud. On each level, we use Geometry Similarity Connection(GSC) module (Sec 3.2) to capture abundant local geometric information of each point and share geometric features with distant points. After that, we adopt the FPS algorithm to down-sample the points and the features (Sec 3.4). Low-level features represent the local geometric information, while high-level features provide semantic information.</p><p>As for classification task, instead of using only the last level's features as the encoder's output ), we concatenate all levels' features together and extract the holistic features by global max pooling and global average pooling. The concatenation of all levels' features aims to fuse the features from different levels and the pooling operator urges to capture the most effective features for classification. Then we handle the holistic features by fully-connected layers with integrated dropout <ref type="bibr" target="#b18">(Srivastava et al. 2014)</ref> to calculate the probability for each category. The cross-entropy loss is used for training.</p><p>As for segmentation task, our segmentation network has an encoder which is the same as the classification network's. We need to interpolate the features on each level of the encoder module and then concatenate them. Inspired by <ref type="bibr" target="#b14">(Qi et al. 2017b</ref>), we also concatenate repeated one-hot category label to the features before MLP <ref type="bibr" target="#b6">(Hornik 1991)</ref>. This mechanism is designed to apply the category supervision to the point-wise segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Geometry Similarity Connection Module</head><p>This subsection describes the Geometry Similarity Connection (GSC) module in GS-Net. It is illustrated in <ref type="figure" target="#fig_0">Figure 3</ref> and 4. The structure of Eigen-Graph is shown in <ref type="figure">Figure 1</ref>.</p><p>Eigen-Graph. As shown in <ref type="figure" target="#fig_1">Figure 4</ref>, we use knearest neighbors search (KNN) algorithm to get k 1nearest neighbors of each point</p><formula xml:id="formula_1">x i in Euclidean space. Let x i1 , . . . , x i k 1 be k 1 -nearest neighbors of x i . Let M = x i1 ? x i , . . . , x i k 1 ? x i , where x ij (1 ? j ? k 1 ) belongs to k 1 -nearest neighbors of x i in Euclidean space.</formula><p>We define the 3D structure tensor as C = M M T , even if the ground truth (surface) is locally flat, noise points cause unflatness of point cloud sampled from the surface. As long as the neighbor region of the given point is not flat, C is a symmetric positive definite matrix. We have the decomposition C = R?R T , where R is a rotation matrix and ? is a diagonal and positive definite matrix, known as eigenvectors and eigenvalues matrices respectively. The positive eigenvalues ? ? R 3 are ordered so that ? 1 ? ? 2 ? ? 3 &gt; 0. At each point x i , we get the 3D structure tensor and denote the eigenvalues at point</p><formula xml:id="formula_2">x i by ? 1 i , ? 2 i , ? 3 i (1 ? i ? N )</formula><p>. We use L 2 norm to calculate the distances between different points.</p><formula xml:id="formula_3">Distance(x i , x j ) = ? i ? ? j L 2<label>(1)</label></formula><p>We choose the indices of k 2 -nearest neighbors of each point according to Eigen Matrix whose element is</p><formula xml:id="formula_4">D ij = ? i ? ? j L 2 .</formula><p>GroupLayer. Now we have k 1 -nearest neighbors' indices in Euclidean space and k 2 -nearest neighbors' indices in Eigenvalue space. As we have presented in <ref type="figure" target="#fig_0">Figure 3</ref>, we denote the input features of level l by F l = {f l 1 , f l 2 , ..., f l N }. For convenience, we omit the superscript l. In GroupLayer, let f i 1 1 , . . . , f i 1 k 1 be k 1 -nearest neighbors' features of point x i , and let f i 2 1 , . . . , f i 2 k 2 be k 2 -nearest neighbors' features of point x i . We group the neighbor features as follows:</p><formula xml:id="formula_5">f k1 i = j:(i,j)?E (f j ? f i , f j ) , j ? i 1 1 , . . . , i 1 k1</formula><p>(2)</p><formula xml:id="formula_6">f k2 i = p:(i,p)?E (f p ? f i , f p ) , p ? i 2 1 , . . . , i 2 k2<label>(3)</label></formula><p>where means concatenation. Then we concatenate f k1 i with f k2 i as the features at each point:</p><formula xml:id="formula_7">f i = f k1 i , f k2 i<label>(4)</label></formula><p>In the first GSC module shown in <ref type="figure">Figure 2</ref>, the input features are the coordinates X and the eigenvalues E of points. We group coordinates using k 1 -nearest neighbors and group eigenvalues using k 2 -nearest neighbors. In the other GSC modules, we use the previous level's output as the input features and group features in both Euclidean space and Eigenvalue space. <ref type="figure">Figure 2</ref>: GS-Net architecture for classification and segmentation (Best view in color and zoom in). In encoder, we use hierarchical structure to learn the features on each level. The network takes N points as input. We use Geometry Similarity Connection (GSC) module to capture abundant local geometry features of each point which can be shared with distant points. After that, we down-sample the points and features to the next level. For classification, we concatenate down-sampled features from each level and pool the holistic features to a 1D global descriptor, which is used to generate classification scores. The segmentation network concatenate the interpolated features of each levels and then calculate each point's scores. The color from red to blue denotes the features' response, while the size of points denotes the features' channels number on different levels.</p><p>MLP and MaxPooing. In GSC module, we calculate features at each point from GroupLayer and implement the multilayer perception (MLP) <ref type="bibr" target="#b6">(Hornik 1991)</ref>, then we use Max-Pool in neighbor domains to get the features of each point:</p><formula xml:id="formula_8">f i = M axP ool(M LP (f i ))</formula><p>(5) And the output of GSC module is denoted by F = {f 1 , f 2 , ..., f N }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Rotation and Translation Invariance</head><p>In this subsection we give some theoretical analysis about rotation and translation invariant robustness of our method. As we have mentioned in Sec 3.2, the 3D structure tensor is C = M M T . We denote the eigenvalues of C as (? 1 , ? 2 , ? 3 ) and the corresponding eigenvectors are (v 1 , v 2 , v 3 ). Thus we have the following equation:</p><formula xml:id="formula_9">Cv q = ? q v q , 1 ? q ? 3<label>(6)</label></formula><p>The way we get 3D structure tensor guarantees that 3D structure tensor of each point is invariant to translation. Let R be an arbitrary rotation matrix in 3D Euclidean space. After applying rotation matrix to point cloud, we get the new 3D structure tensor C = RM (RM ) T . We can get the following equations:</p><formula xml:id="formula_10">R T R = RR T = I<label>(7)</label></formula><p>C Rv q = RM M T R T Rv q = RM M T v q = RCv q = ? q Rv q (8) From equations above, we know that (? 1 , ? 2 , ? 3 ) are also the eigenvalues of 3D structure tensor C . So the eigenvalues of each point is invariant to rotation and translation which ensure the indices of k 2 -nearest neighbors of each point are invariant (illustrated in <ref type="figure" target="#fig_2">Figure 5</ref>). This mechanism improves the robustness of our model to rotation and translation. The empirical experiment results also demonstrate what we have proved theoretically (Sec 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Complements of the Architecture</head><p>Hierarchical Feature Learning. Our method follows the design where the hierarchical structure <ref type="bibr" target="#b14">(Qi et al. 2017b</ref>) is composed of a set of abstract layers. By this way, we can enlarge receptive field of each point progressively along the hierarchy. As shown in <ref type="figure">Figure 2</ref>, the hierarchical structure is composed of three abstract levels. An abstract level l takes N l ? 3 points matrix and N l ? C l features matrix as input. The output are N l+1 ? 3 points matrix and N l+1 ? C l+1 features matrix. We use FPS algorithm to down-sample the points and features at 3 levels (1024-512-256 points in classification network).</p><p>Feature Interpolation for Segmentation Task. In segmentation task, to obtain the feature map which has the same number of points as the original input, we must interpolate  features from the coarsest scale to the original scale <ref type="bibr" target="#b14">(Qi et al. 2017b</ref>). The l-th features interpolation level takes N l ?C l decoder features matrix as input, let X l and X 1 be the spatial points set with N l ? 3 and N 1 ? 3 coordinates. To obtain the features of 1-st level, we simply find three nearest neighbors of X 1 in X l and then calculate the weighted sum of their features. The combination weights are acquired according to the neighbors' normalized spatial distances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we conduct comprehensive experiments to evaluate our GS-Net. In Sec 4.1, we evaluate our GS-Net for point cloud analysis on classification task and segmentation task. In Sec 4.2, we compare the rotation robustness of GS-Net with state-of-the-art methods.  Part Segmentation on ShapeNet Part. Part segmentation task is a challenging task for fine-gained shape analysis. We evaluate our method for this task on ShapeNet Part benchmark <ref type="bibr" target="#b26">(Yi et al. 2016)</ref>. ShapeNet Part consists of 16,880 models from 16 shape categories and 50 different parts in total, with 14,006 models for training and 2,874 models for testing split. Each point cloud is annotated with 2 to 6 parts. We choose mIoU as the evaluation metric which is averaged across all classes and instances. The results are summarized in <ref type="table">Table 1</ref>. The input consists of coordinates and normals. Our method can effectively deal with point Method Input Accuracy Pointwise-CNN <ref type="bibr" target="#b7">(Hua, Tran, and Yeung 2017)</ref> 1k points 86.1 ECC(Simonovsky and Komodakis 2017) 1k points 87.1 PointNet <ref type="bibr" target="#b13">(Qi et al. 2017a)</ref> 1k points 89.2 SCN <ref type="bibr" target="#b22">(Xie et al. 2018a)</ref> 1k points 90.0 Flex-Conv <ref type="bibr" target="#b4">(Groh, Wieschollek, and Lensch 2018)</ref> 1k points 90.2 PointNet++ <ref type="bibr" target="#b14">(Qi et al. 2017b)</ref> 1k points 90.7 SO-Net <ref type="bibr" target="#b11">(Li, Chen, and Hee Lee 2018)</ref> 2k points 90.9 KCNet <ref type="bibr" target="#b17">(Shen et al. 2018)</ref> 1k points 91.0 MRTNet <ref type="bibr" target="#b3">(Gadelha, Rui, and Maji 2018)</ref> 1k points 91.2 Spec-GCN <ref type="bibr" target="#b1">(Chu, Samari, and Siddiqi 2018)</ref> 1k points 91.5 PAT(FPS+GSS) <ref type="bibr" target="#b25">(Yang et al. 2019)</ref> 1k points 91.7 Kd-Net <ref type="bibr" target="#b9">(Klokov and Lempitsky 2017)</ref> 1k points 91.8 SpiderCNN <ref type="bibr" target="#b24">(Xu et al. 2018)</ref> 1k points 92.2 DGCNN  1k points 92.2 PCNN <ref type="bibr" target="#b0">(Atzmon, Maron, and Lipman 2018)</ref> 1k points 92.3 Ours 1k points 92.9 Ours(k=32) 2k points 93.3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Point Cloud Analysis</head><p>PointNet++ <ref type="bibr" target="#b14">(Qi et al. 2017b)</ref> 5k points+nor 91.9 Spec-GCN <ref type="bibr" target="#b1">(Chu, Samari, and Siddiqi 2018)</ref> 1k points+nor 91.8 SpiderCNN <ref type="bibr" target="#b24">(Xu et al. 2018)</ref> 1k points+nor 92.4 <ref type="table">Table 2</ref>: Classification results (%) on ModelNet40 dataset. "nor" denotes the normal of point cloud.</p><p>clouds with geometric characteristic such as symmetrical structure. <ref type="figure" target="#fig_3">Figure 6</ref> shows some segmentation examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison of Rotation Robustness</head><p>We compare GS-Net with the state-of-the-art approaches on ModelNet40 classification for rotation-robustness evaluation. The results are summarized in <ref type="table" target="#tab_3">Table 3</ref> with four comparisons: (1) both training set and test set are augmented by random angle rotation for z axis(z/z); (2) training set with random angle rotation for z axis and test set with random angles rotation for all three axes (x,y,z) (z/s); (3) both training set and test set are augmented by random angles rotation for all three axes (s/s); (4) only test set with random angles rotation for all three axes (0/s). <ref type="table" target="#tab_3">Table 3</ref> consists of two groups of approaches. The first group consists of four approaches: DGCNN , Point <ref type="bibr" target="#b13">(Qi et al. 2017a</ref>), Point++ <ref type="bibr" target="#b14">(Qi et al. 2017b)</ref> and SpiderCNN <ref type="bibr" target="#b24">(Xu et al. 2018)</ref>, while the second group is our approach with different settings. Different from our model shown in <ref type="figure">Figure 2</ref>, last one of second group only use eigenvalues as the input features without any coordinates information and it achieves the best performances of comparison <ref type="formula">(2)</ref> and <ref type="formula" target="#formula_7">(4)</ref>. While our original model achieves the best performance of comparison (3) and get a comparable result with DGCNN of comparison (1). These comparisons aim to validate the eigenvalues of each point is invariant to rotation and can improve robustness of our method to rotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis of GS-Net</head><p>In Sec 5.1, we perform the ablation analysis of GS-Net. We discuss the effectiveness of architecture design and input features. Sec 5.2 is the complexity comparison of GS-Net and existing methods. Sec 5.3 shows that Eigen-Graph efficiently capture local and holistic geometric features such as   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablation Analysis</head><p>Analysis of Architecture Design. We analyze the effectiveness of our method's components on ModelNet40 benchmark for classification task. The results are summarized in <ref type="table" target="#tab_5">Table 4</ref>. All experiments in the ablation study are conducted using k = 20 nearest neighbors. Input Features. The input features directly affect the representation of local geometry and relations between points, thus how to define the input features is an worth exploring issue. In order to find the most suitable feature combination, we experiment with six settings, whose results are summarized in <ref type="table" target="#tab_6">Table 5</ref>. As can be seen, using only coordinates, the accuracy can also reach 92.5%; Inspire by <ref type="bibr" target="#b23">(Xie et al. 2018b)</ref> we use only shape context as the input feature of the points and the result can reach 91.9%; using the differences of coordinates, the result can reach 92.6%; with the combination of coordinates and their differences, the result improves to 92.7%; then we add eigenvalues of points and their differences to the input features, it gets an accuracy of 92.8%; on this basis, we add 3D Euclidean distance of points and their neighbors, it obtains the accuracy of 92.9%; however, with    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Complexity Analysis</head><p>We evaluate the model complexity in terms of model size and forward time in <ref type="table">Table 6</ref>. The forward time is recorded with a batch size of 8 on a single GTX 1080 GPU, which is the same hardware environment of the comparison models. These models are implemented by Pytorch. As illustrated, our method has the competitive performance with great parameter-efficiency and acceptable speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Visualization of GS-Net</head><p>As shown in <ref type="figure">Figure 7</ref>, we visualize the Eigen-Graph of the anchor points (red) from three point clouds. The blue points in the first row represent the nearest neighbors in Euclidean space, while the green points in the second row indicate the Input features Channels Acc. x j 3 92.5 ? j 3 85.2 s i 8 91.9 x j ? x i 3 92.6 x j ? x i , x j 6 92.7 x j ? x i , x j , ? j ? ? i , ? j 12 92.8 x j ? x i , x j , ? j ? ? i , ? j , d ij 13 92.9 x j ? x i , x j , ? j ? ? i , ? j , v j ? v i , v j 30 92.4  <ref type="table">Table 6</ref>: Complexity analysis of GS-Net in classification.</p><p>nearest neighbors in Eigenvalue space. As can be seen, the green points have similar local geometry with the anchor point. Moreover, the Eigen-Graph is rotation invariant, as <ref type="figure" target="#fig_2">Figure 5</ref> shows, nearest neighbors in Eigenvalue space can not be influenced by rotations and translations of the point cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We develop Geometry Sharing Net (GS-Net) for point cloud analysis. The core to GS-Net is GSC module, which can share the similar geometric information with distant points and can be integrated into different existing pipelines for point cloud analysis. Moreover, the Eigen-Graph of GSC module improves the rotation and translation robustness fundamentally. Experiments have shown that GS-Net achieves the state-of-the-art performance and has robustness to geometric transformations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Geometry Similarity Connection module (Best view in color and zoom in). k 1 denotes the number of nearest neighbors in Euclidean space and k 2 means number of nearest neighbors in Eigenvalue space. The input are points and features on level-l. In GSCM, Eigen-Graph is designed to compute the indices of neighbors in Euclidean space and Eigenvalue space. The GroupLayer shares the local geometric features according to the indices of k 2 -nearest neighbors in Eigenvalue space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Eigen-Graph module(Best view in color and zoom in). It first gets the structure tensor of the input points, then use Eigen-Decomposition to compute the eigenvalues which are used to get the indices of the nearest neighbors in Eigenvalue space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of rotation and translation invariance. We visualize the anchor point(red)'s neighbors (green) in Eigenvalue space. The green points are not influenced by the rotation and translation of the input points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Segmentation examples on ShapeNet Part.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FPS</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>74.6 74.3 70.3 88.6 73.5 90.2 87.2 81.0 94.9 57.4 86.7 78.1 51.8 69.9 80.3 PointNet(Qi et al. 2017a) 83.7 83.4 78.7 82.5 74.9 89.6 73.0 91.5 85.9 80.8 95.3 65.2 93.0 81.2 57.9 72.8 80.6 SCN(Xie et al. 2018a) 84.6 83.8 80.8 83.5 79.3 90.5 69.8 91.7 86.5 82.9 96.0 69.2 93.8 82.5 62.9 74.4 80.8 SO-Net(Li, Chen, and Hee Lee 2018) 84.6 81.9 83.5 84.8 78.1 90.8 72.2 90.1 83.6 82.3 95.2 69.3 94.2 80.0 51.6 72.1 82.6 KCNet(Shen et al. 2018) 84.7 82.8 81.5 86.4 77.6 90.3 76.8 91.0 87.0 84.5 95.5 69.2 94.4 81.6 60.1 75.2 81.3 RS-Net(Huang, Wang, and Neumann 2018) 84.9 82.7 86.4 84.1 78.2 90.4 69.3 91.4 87.0 83.5 95.4 66.0 92.6 81.8 56.1 75.8 82.2</figDesc><table><row><cell>method</cell><cell cols="2">instance aero bag</cell><cell>cap</cell><cell>car</cell><cell>chair ear</cell><cell>guitar knife lamp lap</cell><cell>motor mug pistol rocket skate table</cell></row><row><cell></cell><cell>m-IOU</cell><cell></cell><cell></cell><cell></cell><cell>ph</cell><cell>top</cell><cell>board</cell></row><row><cell cols="8">Kd-Net(Klokov and Lempitsky 2017) 80.1 PointNet++(Qi et al. 2017b) 82.3 85.1 82.4 79.0 87.7 77.3 90.8 71.8 91.0 85.9 83.7 95.3 71.6 94.1 81.3 58.7 76.4 82.6</cell></row><row><cell>DGCNN(Wang et al. 2018)</cell><cell>85.1</cell><cell cols="6">84.2 83.7 84.4 77.1 90.9 78.5 91.5 87.3 82.9 96.0 67.8 93.3 82.6 59.7 75.5 82.0</cell></row><row><cell>SpiderCNN(Xu et al. 2018)</cell><cell>85.3</cell><cell cols="6">83.5 81.0 87.2 77.5 90.7 76.8 91.1 87.3 83.3 95.8 70.2 93.5 82.7 59.7 75.8 82.8</cell></row><row><cell>Ours</cell><cell>85.3</cell><cell cols="6">82.9 84.3 88.6 78.4 89.7 78.3 91.7 86.7 81.2 95.6 72.8 94.7 83.1 62.3 81.5 83.8</cell></row><row><cell></cell><cell cols="6">Table 1: Segmentation results on ShapeNet Part dataset.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparisons of rotation robustness on ModelNet40 classification. ? i means the eigenvalues of the anchor point and ? j denotes the eigenvalues of its neighbors.</figDesc><table /><note>symmetry and connectivity.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: ablation study of architecture design (%). 'FPS' in-</cell></row><row><cell>dicates whether to use FPS down-sample strategy in the clas-</cell></row><row><cell>sification network. 'EU' denotes that our method groups the</cell></row><row><cell>neighbors in Euclidean space, while 'EI' denotes that our</cell></row><row><cell>method groups the neighbors in Eigenvalue space.</cell></row><row><cell>the addition of eigenvectors, it can not perform as well as</cell></row><row><cell>other settings.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>The result (%) of six intuitive input features. i denotes index of anchor point and j denotes its neighbors' indices (x: coordinates,x:shape context, ?: eigenvalues, v: eigenvectors, d: Euclidean distance).Figure 7: Visualization of the anchor points(red)' neighbors in Euclidean space and Eigenvalue space. The neighbors in Euclidean space are colored blue and the neighbors in Eigenvalue space are colored green. The green points have similar local geometry with the corresponding red point.</figDesc><table><row><cell>Method</cell><cell>Model</cell><cell>Forward</cell><cell>Accuracy</cell></row><row><cell></cell><cell>Size(MB)</cell><cell>Time(ms)</cell><cell>(%)</cell></row><row><cell>PointNet(Qi et al. 2017a)</cell><cell>13.4</cell><cell>30</cell><cell>89.2</cell></row><row><cell>PointNet++(Qi et al. 2017b)</cell><cell>7.0</cell><cell>603</cell><cell>91.9</cell></row><row><cell>DGCNN(Wang et al. 2018)</cell><cell>7.2</cell><cell>73</cell><cell>92.2</cell></row><row><cell>Ours</cell><cell>6.0</cell><cell>126</cell><cell>92.9</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head><p>This work is partially supported by the National Key </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Point convolutional neural networks by extension operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maron</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lipman ; Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Local spectral graph convolution for point set feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samari</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Samari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Siddiqi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Demantke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mallet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vallet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Dimensionality based scale selection in 3d lidar point clouds</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gvcnn: Group-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="264" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multiresolution tree networks for 3d point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Gadelha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gadelha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Flex-convolution (million-scale pointcloud learning beyond grid-worlds)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieschollek</forename><surname>Groh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Groh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wieschollek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Lensch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pcpnet learning local shape properties from raw point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Guerrero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="75" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Approximation capabilities of multilayer feedforward networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="257" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tran</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Yeung ; Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Yeung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recurrent slice networks for 3d segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2626" to="2635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Escape from cells: Deep kd-networks for the recognition of 3d point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="863" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
	<note>Landrieu and Simonovsky</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">So-net: Self-organizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee</forename><forename type="middle">;</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9397" to="9406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards 3d point cloud based object maps for household environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Rusu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="927" to="941" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mining point cloud local structures by kernel correlation and graph pooling</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
	<note>Simonovsky and Komodakis. Dynamic edge-conditioned filters in convolutional neural networks on graphs</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic classification of 3d point clouds with multiscale spherical neighborhoods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="390" to="398" />
		</imprint>
	</monogr>
	<note>International Conference on 3D Vision (3DV)</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<idno type="arXiv">arXiv:1801.07829</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision &amp; Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Dynamic graph cnn for learning on point clouds</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attentional shapecontextnet for point cloud recognition</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4606" to="4615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attentional shapecontextnet for point cloud recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spidercnn: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modeling point clouds with self-attention and gumbel subset sampling</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3323" to="3332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3d shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">210</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Deep sets</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
