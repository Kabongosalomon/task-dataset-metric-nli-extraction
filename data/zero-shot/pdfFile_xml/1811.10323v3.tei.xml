<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Universal Semi-Supervised Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tarun</forename><surname>Kalluri</surname></persName>
							<email>tarun.05.kalluri@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Visual Information Technology</orgName>
								<orgName type="institution">IIIT Hyderabad</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Varma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Visual Information Technology</orgName>
								<orgName type="institution">IIIT Hyderabad</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Visual Information Technology</orgName>
								<orgName type="institution">IIIT Hyderabad</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Universal Semi-Supervised Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, the need for semantic segmentation has arisen across several different applications and environments. However, the expense and redundancy of annotation often limits the quantity of labels available for training in any domain, while deployment is easier if a single model works well across domains. In this paper, we pose the novel problem of universal semi-supervised semantic segmentation and propose a solution framework, to meet the dual needs of lower annotation and deployment costs. In contrast to counterpoints such as fine tuning, joint training or unsupervised domain adaptation, universal semi-supervised segmentation ensures that across all domains: (i) a single model is deployed, (ii) unlabeled data is used, (iii) performance is improved, (iv) only a few labels are needed and (v) label spaces may differ. To address this, we minimize supervised as well as within and cross-domain unsupervised losses, introducing a novel feature alignment objective based on pixel-aware entropy regularization for the latter. We demonstrate quantitative advantages over other approaches on several combinations of segmentation datasets across different geographies (Germany, England, India) and environments (outdoors, indoors), as well as qualitative insights on the aligned representations 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation is the task of pixel level classification of an image into a predefined set of categories. Stateof-the-art semantic segmentation architectures <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8]</ref> pretrain deep networks for a classification task on datasets like ImageNet <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b53">54]</ref> and then fine-tune on finely annotated labeled examples <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b64">65]</ref>. The availability of such large-scale labeled datasets has been crucial to achieve high accuracies for semantic segmentation in applications ranging from natural scene understanding <ref type="bibr" target="#b17">[18]</ref> to medical imaging <ref type="bibr" target="#b51">[52]</ref>. However, performance often suffers even in the presence of <ref type="bibr" target="#b0">1</ref> Code available at https://github.com/tarun005/USSS ICCV19  <ref type="figure" target="#fig_1">Figure 1</ref>: Proposed universal segmentation model can be jointly trained across datasets with different label spaces, making use of the large amounts of unlabeled data available. Traditional transfer learning based approaches typically require training separate models for each domain. a minor domain shift. For example, a segmentation model trained on a driving dataset from a specific geographic location may not generalize to a new city due to differences in weather, lighting or traffic density. Further, a segmentation model trained on traffic scenes for outdoor navigation may not be applicable for an indoor robot.</p><p>While such domain shift is a challenge for any machine learning problem, it is particularly exacerbated for segmentation where human annotation is highly prohibitive and redundant for different locations and tasks. Thus, there is a growing interest towards learning segmentation representations that may be shared across domains. A prominent line of work addresses this through unsupervised domain adaptation from a labeled source to an unlabeled target domain <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b5">6]</ref>. But there remain limitations. For instance, unsupervised domain adaptation usually does not leverage target domain data to improve source performance. Further, it is designed for the restrictive setting of large-scale labeled source domain and unlabeled target domain. While some applications such as self-driving have large-scale annotated datasets for particular source domains (for example synthetic datasets like Synthia <ref type="bibr" target="#b52">[53]</ref>), the vast majority of applications only have limited data in any domain. Finally, most of the above works assume that the target label set matches with the source one, which is often not the case in practice. For example, road scene segmentation across different countries, or segmentation across outdoor and indoor scenes, have domain-specific label sets.</p><p>In this paper, we propose and address the novel problem of universal semi-supervised semantic segmentation as a practical setting for many real-world applications. It seeks to aggregate knowledge from several different domains during training, each of which has few labeled examples but several unlabeled examples. The goal is to simultaneously limit training cost through reduced annotations and deployment cost by obtaining a single model to be used across domains. Label spaces may be partially or fully nonoverlapping across domains. While fine-tuning a source model on a small amount of target data is a possible counterpoint, it usually requires plentiful source labels and necessitates deployment of a separate model in every domain due to catastrophic forgetting <ref type="bibr" target="#b39">[40]</ref>. Another option is joint training, which does yield a unified model across domains, but does not leverage unlabeled data available in each domain. Our semi-supervised universal segmentation approach leverages both limited labeled and larger-scale unlabeled data in every domain, to obtain a single model that performs well across domains. <ref type="table" target="#tab_2">Table 1</ref> presents the advantage of the proposed semi-supervised universal segmentation over some of the existing approaches.</p><p>In particular, we use the labeled examples in each domain to supervise the universal model, akin to multi-tasking <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b29">30]</ref>, albeit with limited labels. We simultaneously make use of the large number of unlabeled examples to align pixel level deep feature representations from multiple domains using entropy regularization based objective functions. Entropy regularization uses unsupervised examples and helps in encouraging low density separation between the feature representations and improve the confidence of predictions. Moreover models trained on one domain typically result in noisy predictions and high entropy output maps when deployed in a different domain, and the proposed cross dataset entropy minimization encourages refined prediction maps across datasets. We calculate the similarity score vector between the encoder outputs at each pixel and the label embeddings (computed from class prototypes <ref type="bibr" target="#b58">[59]</ref>), and minimize the entropy of this discrete distribution to positively align similar examples between the labeled and the unlabeled images. We do this unsupervised alignment both within domain, as well as across domains.</p><p>We believe such within and cross-domain alignment is  fruitful even with non-overlapping label spaces, particularly so for semantic segmentation, since label definitions often encode relationships that may positively reinforce performance in each domain. For instance, two road scene datasets such as Cityscapes <ref type="bibr" target="#b11">[12]</ref> and IDD <ref type="bibr" target="#b64">[65]</ref> might have different labels, but share similar label hierarchies. Even an outdoor dataset like Cityscapes and an indoor one like SUN <ref type="bibr" target="#b59">[60]</ref> may have label relationships, for example, between horizontal (road, floor) and vertical (building, wall) classes. Similar observations have been made for multi-task training <ref type="bibr" target="#b70">[71]</ref>.</p><p>We posit that our pixel wise entropy-based objective discovers such alignments to improve over joint training, as demonstrated quantitatively and qualitatively in our experiments. Specifically, our experiments lend insights across various notions of domain gaps. With Cityscapes <ref type="bibr" target="#b11">[12]</ref> as one of domains (road scenes in Germany), we derive universal models with respect to CamVid (roads in England) <ref type="bibr" target="#b3">[4]</ref>, IDD (roads in India) <ref type="bibr" target="#b64">[65]</ref> and SUN (indoor rooms) <ref type="bibr" target="#b59">[60]</ref>. In each case, our semi-supervised universal model improves over fine-tuning and joint training, with visualizations of the learned feature representations demonstrating conceptually meaningful alignments. We use dilated residual networks in our experiments <ref type="bibr" target="#b69">[70]</ref>, but the framework is equally applicable to any of the existing deep encoder-decoder architectures for semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Contributions</head><p>? We propose a universal segmentation framework to train a single joint model on multiple domains with disparate label spaces to improve performance on each domain. This framework adds no extra parameters or significant overhead during inference compared to existing methods for deep semantic segmentation. ? We introduce a pixel-level entropy regularization scheme to train semantic segmentation architectures using datasets with few labeled examples and larger quantities of unlabeled examples (Section 3). ? We demonstrate the effectiveness of our alignment over a wide variety of indoor <ref type="bibr" target="#b59">[60]</ref> and outdoor <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b3">4]</ref> segmentation datasets with various degrees of label overlaps. We also compare our results with other semi-supervised approaches, based on adversarial losses, giving improved results (Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Semantic Segmentation Semantic segmentation in computer vision is the task of assigning semantic labels to each pixel of an image. Most of the state of the art models for semantic segmentation <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b50">51]</ref> have been possible largely due to breakthroughs in deep learning that have pushed the boundaries of performance in image classification <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> and related tasks. The pioneering work in <ref type="bibr" target="#b34">[35]</ref> proposes an end-to-end trainable network for semantic segmentation by replacing the fully connected layers of pretrained AlexNet <ref type="bibr" target="#b31">[32]</ref> and VGG Net <ref type="bibr" target="#b57">[58]</ref> with fully convolutional layers that aggregate spatial information across various resolutions. Noh et al. <ref type="bibr" target="#b43">[44]</ref> use transpose convolutions to build a learnable decoder module, while DeepLab network <ref type="bibr" target="#b7">[8]</ref> uses artrous convolutions along with artrous spatial pyramid pooling for better aggregation of spatial features. Segmentation architectures based on dilated convolutions <ref type="bibr" target="#b68">[69]</ref> for real time semantic segmentation have also been proposed <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b50">51]</ref>.</p><p>Semi Supervised Learning Most of the existing semantic segmentation architectures require large scale annotation of labeled data for achieving good results. To address this limitation, various semi supervised learning methods have been proposed in <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b66">67]</ref>, which make use of easily available large scale unsupervised or weakly supervised data during training. While these approaches deliver competitive results when trained and deployed on a specific dataset, the need for learning efficient segmentation models transferable across domains and environments having limited training data remains.</p><p>Transfer Learning and Domain Adaptation Transfer learning <ref type="bibr" target="#b67">[68]</ref> involves transferring deep feature representations learned in one domain or task to another domain or task where labeled data availability is low. Previous works demonstrate transfer learning capabilities between related tasks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b48">49]</ref> or even completely different tasks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b34">35]</ref>. Unsupervised domain adaptation is a related paradigm which leverages labeled data from a source domain to learn a classifier for a new unsupervised target domain in the presence of a domain shift. Various generative and discriminative domain adaptation methods have been proposed for classification tasks in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b4">5]</ref> and for semantic scene segmentation in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b72">73]</ref>. Most of these works in domain adaptation assume equal source and target dataset label spaces or a subset target label space, which is not the most general case for real world applications. To address this limitation with the domain adaptation approaches, we propose a method similar to <ref type="bibr" target="#b36">[37]</ref> which works in the extreme case of non-intersecting label spaces. Moreover, pixel level adaptation based methods are typically focused on using knowledge from a large labeled source domain (eg. Synthia <ref type="bibr" target="#b52">[53]</ref>) to improve performance on a specific target domain, while we propose a joint training framework to train a single model that delivers good performance on both the domains.</p><p>Universal Segmentation Multitask learning <ref type="bibr" target="#b6">[7]</ref> is shown to improve performance for many tasks that share useful relationships between them in computer vision <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b70">71]</ref>, natural language processing <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b29">30]</ref> and speech recognition <ref type="bibr" target="#b55">[56]</ref>. Universal Segmentation builds on this idea by training a single joint model that is useful across multiple semantic segmentation domains with possibly different label spaces to make use of transferable representations at lower levels of the network. Liang et al. <ref type="bibr" target="#b33">[34]</ref> first propose the idea of universal segmentation by performing dynamic propagation through a label hierarchy graph constructed from an external knowledge source like WordNet. We propose an alternative method to perform universal segmentation without the need for any outside knowledge source or additional model parameters during inference, and instead make efficient use of the large set of unlabeled examples in each of the domains for unsupervised feature alignment. Following the success of metric learning based approaches in tasks such as fine grained classification <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref>, latent hierarchy learning <ref type="bibr" target="#b54">[55]</ref> and zero-shot prediction <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b32">33]</ref>, we use pixel level class prototypes <ref type="bibr" target="#b58">[59]</ref> for performing semantic transfer across domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Description</head><p>In this section, we explain the framework used to train a single model across different segmentation datasets with possibly disparate label spaces using a novel pixel aware entropy regularization objective.</p><p>We have d datasets</p><formula xml:id="formula_0">{D (i) } d i=1</formula><p>, each of which has few labeled examples and many unlabeled examples. The labeled images and corresponding labels from D (i) are denoted by</p><formula xml:id="formula_1">{X (i) l , Y (i) } N (i) l i=1 , where Y (i) ? Y i , and N (i) l is the num- ber of labeled examples. The unlabeled images are repre- sented by {X (i) u } N (i) u i=1 , and N (i) u is the number of unlabeled examples. We work with domains with very few labeled examples (N (i) u N (i) l )</formula><p>, and consider the general case of non-intersecting label spaces, so that Y p = Y q for any p, q. The label spaces might still have a partial overlap between them, which is common in the case of segmentation datasets. For ease of notation, we consider the special case of two datasets {D <ref type="bibr" target="#b0">(1)</ref> , D (2) }, but similar idea can be applied for the case of multiple datasets as well.</p><p>The proposed universal segmentation model is summarized in <ref type="figure">Figure 2</ref>. Deep semantic segmentation architectures generally consist of an encoder module which aggregates the classwise centroids</p><formula xml:id="formula_2">E n c o d e r M o d u l e F ( . ) D e c o d e r ( . ) G 1 D e c o d e r ( . ) G 2 ( . ) L s u p W 1 H 1 C 1 W 2 C 2 ( . ) L u n s u p Entropy Module H 2 Domain A Domain B Domain A Domain B { } X ( 2 ) U { } X ( 1 ) U { , } X ( 1 ) L Y ( 1 ) { , } X ( 2 ) L Y ( 2 ) Labeled set Unlabeled Set ? ( . )</formula><p>Entropy Similarity <ref type="figure">Figure 2</ref>: Different modules in the proposed universal semantic segmentation framework. {X spatial information across various resolutions and a decoder module that consists of a classifier and an up sampler to enable pixel wise predictions at a resolution that matches the input. In order to enable joint training with multiple datasets, we modify this encoder decoder architecture by having a shared encoder module F and different decoder layers G 1 (.), G 2 (.) for prediction in different label spaces. For a labeled input image x l , the pixel wise predictions are denoted b? y (k) = G k (F(x l )) for k = 1, 2 which, along with the labeled annotations, gives us the supervised loss. To make use of the semantic information from the unlabeled examples X u , we propose an entropy regularization module E. This entropy module takes as input the output of the encoder F(.) to give pixel wise representation outputs in an embedding space. The entropy of the similarity score vector of these embedding representations with the label embeddings results in the unsupervised loss term. Each of these loss terms is explained in detail in the following sections.</p><formula xml:id="formula_3">(1) l , Y (1) } , {X (2) l , Y (2) }</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised Loss</head><p>The supervised loss is the softmax cross entropy loss between the predicted segmentation mask? and the corresponding pixel wise ground truth segmentation masks for all labeled examples. Specifically, for the samples from dataset k,</p><formula xml:id="formula_4">L (k) S = 1 N (k) l xi?D (k) ? k (y i , G k (F (x i ))) ,<label>(1)</label></formula><p>where ? k is the softmax cross entropy loss function over the label space Y k , which is averaged over all the pixels of the segmentation map. L Entropy Module The large number of unsupervised images available provides us with rich information regarding the visual similarity between the domains and the label structure, which the existing methods on adversarial based semi supervised segmentation <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b27">28]</ref> or universal segmentation <ref type="bibr" target="#b33">[34]</ref> do not exploit. To address this limitation, we propose using entropy regularization to transfer the information from labeled images to the unlabeled images, as well as among the unlabeled images between the datasets. Entropy regularization is proved to encourage low density separation between the clusters in the feature space <ref type="bibr" target="#b19">[20]</ref>, hence resulting in high confidence predictions and smooth output maps for semi supervised learning. A crucial difference between some previous works which use entropy regularization for semi supervised learning <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b65">66]</ref> and ours is that we perform entropy regularization in a separate embedding space using an entropy module E, unlike the other works which apply this objective directly in the softmax output space. This embedding approach helps in achieving semantic transfer between datasets with disparate label sets, hence aiding in closely aligning the visually similar pixel level features calculated from the segmentation network from both the datasets.</p><p>The entropy module is explained in <ref type="figure">Figure 3</ref>, and works similar to the decoder module in a segmentation architecture. Firstly, we project the encoder outputs from the segmentation network from both datasets into a common d dimensional embedding space R d , and upsample this output map to match the size of the input. Then, a similarity metric ?, which operates on each pixel, is used to calculate the similarity score of the embedding representations with each of the d <ref type="figure">Figure 3</ref>: In addition to a traditional decoder layer that outputs predictions in the respective label spaces R c , we also have an entropy module E(.) that first maps the features of both the domains into a common embedding space R d , and then calculates similarity scores with the label embeddings of respective datasets.</p><p>dimensional label embeddings using the equation</p><formula xml:id="formula_5">[v ij ] k = ? E F x (i) u , c (j) k ?k ? {|Y j |},<label>(2)</label></formula><p>where</p><formula xml:id="formula_6">x (i) u is an image from the i th unlabeled set, c (j)</formula><p>k ? R d is the label embedding corresponding to the k th label from the j th dataset and [v ij ] ? R |Yj | . When i = j, the scores correspond to the similarity scores within a dataset, and when i = j, they provide the cross dataset similarity scores. The label embeddings are just the prototype features calculated using the labeled data. They are pre computed and kept fixed over the course of training the network, since we found that the limited supervised data was not sufficient to jointly train a universal segmentation model as well as fine tune the label embeddings. More details on calculating label embeddings is presented in the supplementary section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised Loss</head><p>We have two parts for the unsupervised entropy loss. The first part, the cross dataset entropy loss, is obtained by minimizing the entropy of the cross dataset similarity vectors.</p><formula xml:id="formula_7">L U S,c = H(?([v 12 ])) + H(?([v 21 ])),<label>(3)</label></formula><p>where H(.) is the entropy measure of a discrete distribution, ?(.) is the softmax operator and the similarity vector [v] is from Eq <ref type="bibr" target="#b1">(2)</ref>. Minimizing L U S,c makes the probability distribution peaky over a single label from a dataset, which helps in label side semantic transfer across datasets and hence improving the overall prediction certainty of the network. In addition, we also have a within dataset entropy loss given by</p><formula xml:id="formula_8">L U S,w = H(?([v 11 ])) + H(?([v 22 ]))<label>(4)</label></formula><p>which aligns the unlabeled examples within the same domain.</p><p>The total loss L T is the sum of the supervised loss from Eq (1), and the unsupervised losses from Eq <ref type="formula" target="#formula_7">(3)</ref> and Eq (4), written as</p><formula xml:id="formula_9">L T = L S (X (1) l , Y (1) , X (2) l , Y (2) ) + ? ? L U S,c (X (1) u , X (2) u ) +? ? L U S,w (X (1) u , X (2) u )<label>(5)</label></formula><p>where ? and ? are a hyper parameters that control the influence of the unsupervised loss in the total loss.</p><p>Inference For a query image q (k) from dataset k during test time, the output? (k) = G k (F(q (k) )) gives us the segmentation map over the label set Y k and the pixel wise label predictions. This adds no computation overhead or extra parameters to our approach during inference compared to existing deep semantic segmentation approaches. We note that although we calculate feature and label embeddings in our method and metric based inference schemes like nearest neighbor search might enable prediction in a label set agnostic manner, calculating pixel wise nearest neighbors using existing methods can prove very slow and costly for images with high resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>We provide the performance results of the proposed approach on a wide variety of real world datasets used in autonomous driving as well as indoor segmentation settings. We show the superiority of the our method over the existing baselines (Section 4.2), demonstrate improvement upon the state of the art semi-supervised approaches (Section 4.3), and also show the results on cross domain datasets (Section 4.4). Using only a fraction of the labeled data available, we show competitive results on these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training Details</head><p>Datasets We show the results of our approach on large scale urban driving datasets from various domains like Cityscapes <ref type="bibr" target="#b11">[12]</ref> (CS), CamVid <ref type="bibr" target="#b3">[4]</ref> (CVD) and Indian Driving Dataset (IDD) <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b64">65]</ref>.</p><p>Cityscapes <ref type="bibr" target="#b11">[12]</ref> is a standard autonomous driving dataset consisting of 2975 training images collected from various cities across Europe finely annotated with 19 classes. CamVid <ref type="bibr" target="#b3">[4]</ref> dataset contains 367 training, 101 validation and 233 testing images taken from video sequences finely labeled with 32 classes, although we use the more popular 11 class version from <ref type="bibr" target="#b2">[3]</ref>. We also demonstrate results on IDD <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b64">65]</ref> dataset, which is an in-the-wild dataset for autonomous navigation in unconstrained environments. It consists of 6993 training and 981 validation images finely annotated with 26 classes collected from 182 drive sequences on Indian roads, taken in highly varying weather and environment conditions. This is a challenging driving dataset   since it contains images taken from largely unstructured environments. While these autonomous driving datasets typically offer many challenges, there is still limited variation with respect to the classes, object orientation or camera angles. Therefore, we also use SUN RGB-D <ref type="bibr" target="#b59">[60]</ref> dataset for indoor segmentation, which contains 5285 training images along with 5050 validation images finely annotated with 37 labels consisting of regular household objects like chair, table, desk, pillow etc. We report results on the 13 class version used in <ref type="bibr" target="#b20">[21]</ref>, and use only the RGB information for our universal training and ignore the depth information provided.</p><p>Architecture Although the proposed framework is readily applicable to any state-of-the art encoder-decoder semantic segmentation framework, we use the openly available PyTorch implementation of dilated residual network <ref type="bibr" target="#b69">[70]</ref> owing to its low latency in autonomous driving applications. We take the embedding dimension d to be 128, and use dot product for the pixel level similarity metric ?(.) as it can be implemented as a 1 ? 1 convolution on most of the modern deep learning packages. More details for each experimental setting is presented in the supplementary section.</p><p>Evaluation Metric We use the mean IoU (Intersection over Union) as the performance analysis metric. The IoU for each class is given by  where TP , FP , FN are the true positive, false positive and false negative pixels respectively, and mIoU is the mean of IoUs over all the classes. mIoUs are calculated separately for all datasets in a universal model. All the mIoU values reported are on the publicly available validation sets for the CS, IDD and SUN-RGB datasets, and on the test set for the CamVid dataset.</p><formula xml:id="formula_10">IoU = TP TP+FP+FN ,<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Studies</head><p>We perform the following ablation studies in our experiments to provide insights into the various components of the proposed objective function. (i) Train on source: We train a semantic segmentation network using only the limited training data available on one dataset, and provide results when tested on both the datasets. Since the label spaces do not directly overlap, we finetune a different classifier (decoder) for both the datasets and keep the feature extractor (encoder) as the same. (ii) Univ-basic: To study the effect of the unsupervised losses, we put ?, ? = 0 and perform training using only the supervised loss term from Eq (1) and no entropy module at all. This is similar to plain joint training using the supervised data from each domain. (iii) Univ-cross: To study the effect of the cross dataset loss term from Eq (3), we conduct experiments by adding ? = 1 to the loss term. (iv) Univ-full: This is the proposed model, including all the supervised and unsupervised loss terms. We use ?, ? = 1 in  the loss function in Eq <ref type="bibr" target="#b4">(5)</ref>. The best model is defined as the model having the highest average mIoU across the datasets. Although many works on domain adaptation also provide results on Cityscapes dataset, we note that we cannot directly compare our result against them, since the problem setting is very different. While most of the domain adaptation approaches use large scale synthetic datasets as source dataset to improve performance on a specific target domain, we train our models on multiple resource constrained real world datasets directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cityscapes + CamVid</head><p>The results for training a universal model on Cityscapes and CamVid datasets is given in <ref type="table" target="#tab_5">Table 3</ref>. For a setting of N=100 which corresponds to using 100 labeled examples from each domain, the proposed method gives the best mIoU value of 41.03% on Cityscapes and 54.62% on CamVid clearly outperforming the baseline approaches. Moreover, the universal segmentation method using the proposed unsupervised losses also performs better than using only supervised losses, which demonstrates the advantage of having unsupervised entropy regularization in domains with few labeled data and lots of unlabeled data.</p><p>Another observation from <ref type="table" target="#tab_5">Table 3</ref> is that for N=100, a model trained only on Cityscapes suffers a performance drop of 13.5% mIoU when tested on the CamVid dataset compared to a model trained on Camvid alone. Similarly, the performance drop in the case of Cityscapes is 18% mIoU for a model trained on Camvid. Therefore, it is evident that models trained on one dataset, like Cityscapes do not always perform well when deployed on a different dataset, like CamVid, due to domain shift and result in noisy predictions and poor output maps. This further brings out the necessity of training a single model which performs well on both the domains by using an entropy regularization based semantic transfer objective.</p><p>In the case of semantic segmentation datasets, very low values of N offers challenges like limited representation for many of the smaller labels, but we notice that the proposed model for N=50 still manages to perform consistently better on both the datasets.</p><p>Comparison of class-wise mIoUs of the universal segmentation approach for N=100 with CS+CamVid is given in <ref type="table" target="#tab_4">Table 2</ref>. Entropy regularization clearly boosts performance in 9 of the 11 classes on the CamVid dataset, and for 10 out of 19 classes on the Cityscapes dataset. More importantly, it is the smaller classes like pole, traffic sign, pedestrian and fence which benefit greatly from universal training on both the Cityscapes and CamVid datasets, in spite of using only a small fraction of the labeled examples from these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IDD + Cityscapes</head><p>This combination is a chosen for validating the universal segmentation approach as the images are from widely dissimilar domains in terms of geography, weather conditions as well as traffic setup, and the datasets together capture the wide variety of road scenes one might encounter while training autonomous driving datasets for vision based navigation. The results for universal semantic segmentation using IDD and Cityscapes (CS) are shown in <ref type="table" target="#tab_9">Table 5</ref>. Using 100 training examples from each domain, the proposed univ-full model gives an mIoU of 36.48% on Cityscapes (CS) and 27.45% on IDD using a Resnet-18 backbone, performing better than the univ-basic method on the average mIoU.</p><p>Similar to the CS+CamVid case, the features trained on Cityscapes dataset do not transfer directly to IDD, and shows a performance drop of 12% mIoU, demonstrating the necessity of learning universal representations for large scale datasets as well.</p><p>Furthermore, as an extreme case, we show the utility of the proposed approach even in the case of large number of labeled examples. We choose N=1500, which is a challenging setting since the number of supervised examples are already sufficient to train a joint model. However, from <ref type="table" target="#tab_9">Table 5</ref>, the Resnet-50 based universal model still provides advantage over joint training method, which proves that adding unsupervised examples always helps the training, and more unsupervised examples can be added to these datasets to push the state of the art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with state-of-the art</head><p>In addition to demonstrating the superiority of the proposed method over the baseline approaches, we also compare some of the existing semi supervised semantic segmentation works (which are targeted towards single dataset) with ours in <ref type="table" target="#tab_7">Table 4</ref>, for similar amounts of labeled training data. Our model which uses dilated residual network gives competitive results on Cityscapes validation set when compared to <ref type="bibr" target="#b27">[28]</ref> which uses a more complex DeepLab-V2 architecture. Similarly, without using any unsupervised video images unlike <ref type="bibr" target="#b60">[61]</ref>, we show superior results on the CamVid test set compared to them, in spite of the fact that our model is trained to perform well on multiple datasets at once. Most of the previous works optimize adversarial losses, and our results prove that entropy minimization is better suited for   semi supervised approaches where limited supervision is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Cross Domain Experiment</head><p>A useful advantage of the universal segmentation model is its ability to perform knowledge transfer between datasets used in completely different settings, due to its effectiveness in exploiting useful visual relationships. We demonstrate this effect in the case of joint training between Cityscapes, which is a driving dataset with road scenes used for autonomous navigation and SUN RGB-D, which is an indoor segmentation dataset with household objects used for high-level scene understanding.</p><p>The label sets in Cityscapes and SUN-RGBD dataset are completely different (non overlapping), so the simple joint training techniques generally give poor results. However, from <ref type="table" target="#tab_11">Table 6</ref>, our model outperforms the baselines and provides a good joint model across the domains making use of the unlabeled examples. We also compare our work against SceneNet <ref type="bibr" target="#b40">[41]</ref>, which uses large scale synthetic examples with RGB and depth data for pre-training, as well as all of the 5.3k available labeled examples for training. Using only 28% of the training data from the SUN-RGB dataset, and limited supervision from Cityscapes instead of synthetic examples, we achieve upto 88% of the mIoU reported in <ref type="bibr" target="#b40">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Feature Visualization</head><p>A more intuitive understanding of the feature alignment performed by our universal model is obtained from the tSNE embeddings <ref type="bibr" target="#b37">[38]</ref> of the visual features. The pixel wise output of the encoder module is used to plot the tSNE of selected labels in <ref type="figure" target="#fig_2">Figure 4</ref>. For the universal training between CS and CVD in <ref type="figure" target="#fig_2">Figures 4a and 4b</ref>, we can observe that classes like Building-CS and Building-CVD, as well as Sidewalk-CS and Pavement-CVD align with each other better when trained using a universal segmentation objective. For the universal training between CS and SUN from <ref type="figure" target="#fig_2">Figure 4c</ref> and <ref type="figure" target="#fig_2">Figure 4d</ref>, labels with similar visual attributes such as Road and Floor align close to each other in spite of the label sets themselves being completely non overlapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we demonstrate a simple and effective way to perform universal semi-supervised semantic segmentation. We train a joint model using the few labeled examples and large amounts of unlabeled examples from each domain by an entropy regularization based semantic transfer objective. We show this approach to be useful in better alignment of the visual features corresponding to different domains. We demonstrate superior performance of the proposed approach when compared to supervised training or joint training based methods over a wide variety of segmentation datasets with varying degree of label overlap. We hope that our work would address the growing concern in the deep learning community over the difficulty involved in collection of large number of labeled examples for dense prediction tasks such as semantic segmentation. We also believe that other computer vision tasks like object detection and instance aware segmentation can benefit greatly from the ideas discussed in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training Details</head><p>We give details of the parameters used for training the universal segmentation models in various settings. We use the openly available PyTorch implementation of dilated residual network <ref type="bibr" target="#b69">[70]</ref>, with encoders designed using , ResNet-50 (drn-d-54) as well as ResNet-101 (drn-d-105) architectures. The decoder consists of a 1x1 convolution layer followed by a bilinear upsampling layer. We train every model on 2 Nvidia GeForce GTX 1080 GPUs for 200 epochs. During training, we use a crop size of 512x512 for Cityscapes and IDD datasets, 360x480 for the Camvid dataset and 480x640 for the SUN-RGB dataset. Validation mIoUs are reported on the standard resolutions from the dataset. We employ SGD learning algorithm with an initial learning rate of 0.001 and a momentum of 0.9, along with a poly learning rate schedule with a power of 0.9 <ref type="bibr" target="#b7">[8]</ref>. We use a batch size of 10, and take the embedding dimension to be 128. The default values for ? and ? are taken to be 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Label Embeddings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Calculating the label embeddings</head><p>In this section, we describe the method used to obtain the vector representations for the labels. For each dataset separately, we train an end-to-end segmentation network from scratch using only the limited training data available in that dataset. We use this trained segmentation network to calculate the encoder outputs of the training data at each pixel. Typically, the size of the output dimension of the encoder at each pixel (512 for a ResNet encoder) is not equal to the dimension of the label embeddings (d=128, in our case). So we first apply a dimensionality reduction technique like PCA to reduce the dimension of the outputs to match the dimension of the label embeddings d, and then calculate the class wise centroids to obtain the label embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Updating the label embeddings</head><p>In our original experiments, we fixed the pretrained label embeddings over the phase of training the universal model. Here, we present a method to jointly train the segmentation model as well as the label embeddings. We initialize the embeddings with the values computed from the pretrained networks, and make use of the following exponentially weighted average rule to update the centroids at the t th time step.</p><formula xml:id="formula_11">c (k) t = ? ? c (k) t?1 + (1 ? ?) ? ? L (F t (x (k) u )).<label>(7)</label></formula><p>In Eq <ref type="bibr" target="#b6">(7)</ref>, c (k) t?1 denotes the centroids at the (t?1) th time step, F t is the state of the encoder module at the t th time step and ? L calculates the class wise centroids. ? is the update factor, where a value of ? = 1 implies that the centroids are not updated from their initial state, and a value of ? = 0 means that the centroids are calculated afresh at each update. We  make an update to the centroids after every mini-batch of the original training data. From <ref type="table" target="#tab_13">Table 7</ref>, experiments with ? = 0.5 suggests that jointly training the network as well as updating the label embeddings reduces the performance compared to having fixed label embeddings. We believe that this is primarily due to having insufficient training data for jointly updating embeddings as well as the network weights, although this merits a deeper investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Multiple Label Embeddings</head><p>Many labels in a segmentation dataset often appear in more than one visual form or modalities. For example, road class can appear as dry road, wet road, shady road etc., or a class labeled as building can come in different structures and sizes. To better capture the multiple modalities involved in the visual information of the label, we propose using multiple embeddings for each label instead of a single mean centroid. This is analogous to polysemy in vocabulary, where many words can have multiple meanings and can occur in different contexts, and context specific word vector representations are used to capture this behavior. To calculate the multiple label embeddings, we perform K-means clustering of the pixel level encoder feature representations calculated from networks pretrained on the limited supervised data, and calculate similarity scores with all the multiple label embeddings. <ref type="table" target="#tab_13">Table 7</ref> shows that using K=3 embeddings per label gives an advantage over using 1 embedding per label, so apparently some amount of over segmentation helps. However, further increasing K to 5 hurts the performance, as not all the labels benefit from having multiple modalities per label. So, an interesting future direction can be to examine optimum number of embeddings per label.</p><p>Particularly, from <ref type="table" target="#tab_15">Table 8</ref>, it is evident that classes like Road, Building, Person etc. benefit largely from having multiple embeddings per label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Choice of label embeddings</head><p>In our work, we chose the pixel level class prototypes to be the label embeddings. We believe that this helps in better capturing visual information from the images compared to other approaches like Word2Vec <ref type="bibr" target="#b41">[42]</ref>. To this end, we provide results of our approach replacing the prototype label embeddings with word vectors of the labels, by using the publicly available 128 dimensional word vectors for the labels from the Cityscapes and CamVid datasets.</p><p>From <ref type="table" target="#tab_13">Table 7</ref>, having class prototypes as label embeddings, which are computed from the labeled data, performs better than using Word2vec based embeddings, which capture semantics of the word meaning rather than the visual appearance of the label. The performance improvement is more evident in case of N=100, which demonstrates that in presence of sufficient labeled data, class prototypes are better suited as label embeddings than word vector representations. Similar observations have been made in <ref type="bibr" target="#b58">[59]</ref> as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Direct Softmax Entropy Regularization</head><p>Entropy regularization is used to enhance the confidence of predictions made on unlabeled samples. In the case of deep neural networks, applying this directly to the softmax scores will make the predictions confident by simply increasing the weights of the last layer. So, we follow an approach where we calculate similarity between normalized label prototypes and encoder embeddings through our entropy module. Direct SER result from <ref type="table" target="#tab_13">Table 7</ref> further demonstrates the fact that applying SER (softmax entropy regularization) directly to our network shows inferior performance compared to the proposed entropy module based approach.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>are the set of labeled examples and X (1) u , X (2) u are the set of unlabeled examples. The entropy module uses the unlabeled examples to perform alignment of pixel wise features from multiple domains by calculating pixel wise similarity with the labels, and minimizing the entropy of this discrete distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 1 )</head><label>1</label><figDesc>S and L<ref type="bibr" target="#b1">(2)</ref> S together comprise the supervised loss term L S .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>tSNE visualizations of the encoder output representations for majority classes from CS, CVD and SUN datasets. Plots (a) and (b) are for the Univ-basic and Univ-full model from CS-CVD datasets. Observe that the feature embeddings for large classes like CS:Building-CVD:Building, CS:SideWalk-CVD:Pavement, CS:Sky-CVD:Sky align a lot better with universal model. Plots (c) and (d) are for the Univ-basic and Univ-full model from CS-SUN datasets, and labels with similar visual features like CS:Road -SUN:Floor show better feature alignment. Best viewed in color and zoom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison of Universal Semi-Supervised Segmentation against existing methods.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>.70 16.58 29.84 22.31 33.74 83.88 32.89 82.07 52.67 21.57 81.11 19.01 3.87 0.0 19.64 49.01 40.97 Univ-basic 87.00 44.54 77.77 10.21 11.07 25.54 14.51 25.82 80.72 22.40 78.19 49.00 19.64 75.35 1.86 0.25 10.98 24.01 33.73 26.16 38.71 82.30 36.39 81.61 54.38 20.48 81.71 2.37 22.79 3.85 1.31 46.23 41.03</figDesc><table><row><cell>Method</cell><cell>Road</cell><cell>SideWalk</cell><cell cols="2">Building</cell><cell>Wall</cell><cell>Fence</cell><cell>Pole</cell><cell cols="2">Traff. lt. Traff. Sgn.</cell><cell>Veg.</cell><cell>Train</cell><cell>Sky</cell><cell>Person</cell><cell>Rider</cell><cell>Car</cell><cell>Truck</cell><cell>Bus</cell><cell>Train</cell><cell>MotorCyc.</cell><cell>Bicycle</cell><cell>mIoU</cell></row><row><cell>CS only</cell><cell>91.76</cell><cell>54.78</cell><cell>80.02</cell><cell></cell><cell cols="15">38.83</cell><cell>41.08</cell><cell>36.04</cell></row><row><cell>Univ-full</cell><cell>92.18</cell><cell>51.29</cell><cell>80.07</cell><cell></cell><cell>0.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell>Sky</cell><cell></cell><cell></cell><cell>Buil.</cell><cell cols="2">Pole</cell><cell>Road</cell><cell cols="2">Pave.</cell><cell>Tree</cell><cell></cell><cell>Sign</cell><cell cols="2">Fence</cell><cell>Car</cell><cell></cell><cell>Ped.</cell><cell>Bicy.</cell><cell>mIoU</cell></row><row><cell cols="6">Camvid only 85.58 75.15</cell><cell cols="2">8.17</cell><cell cols="8">84.86 52.34 69.68 27.11 20.48</cell><cell>73.1</cell><cell></cell><cell cols="2">24.36 29.42</cell><cell>50.02</cell></row><row><cell cols="2">Univ-basic</cell><cell cols="4">87.04 76.67</cell><cell cols="2">9.56</cell><cell>83.5</cell><cell cols="5">51.35 70.07 27.75</cell><cell>22.6</cell><cell></cell><cell cols="4">73.22 33.94 35.25</cell><cell>51.9</cell></row><row><cell cols="2">Univ-full</cell><cell cols="2">86.3</cell><cell cols="5">77.23 17.13 84.99</cell><cell cols="11">53.35 70.57 31.99 32.45 72.94 36.61 37.22</cell><cell>54.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Class-wise IoU values for the 19 classes in Cityscapes dataset and 11 classes in the CamVid dataset with various ablations of universal semantic segmentation models, for N=100 on Resnet-18. Note the improvement of our method (Univ-full) for smaller classes like pole and sign on Cityscapes and CamVid datasets. L c , L w ) 34.01 53.23 43.62 41.03 54.62 47.83</figDesc><table><row><cell>Method</cell><cell></cell><cell>N=50</cell><cell></cell><cell>N=100</cell></row><row><cell></cell><cell>CS</cell><cell>CVD Avg.</cell><cell>CS</cell><cell>CVD Avg.</cell></row><row><cell>Train on CS</cell><cell cols="2">33.33 32.92 33.13</cell><cell cols="2">40.97 36.52 38.75</cell></row><row><cell>Train on CVD</cell><cell cols="2">19.47 42.81 31.14</cell><cell cols="2">22.20 50.02 36.11</cell></row><row><cell>Univ-basic (L s )</cell><cell cols="2">32.82 48.56 40.69</cell><cell cols="2">36.04 51.90 43.97</cell></row><row><cell>Univ-cross (+ L c )</cell><cell cols="2">33.86 52.57 43.22</cell><cell cols="2">37.82 49.31 43.57</cell></row><row><cell>Univ-full (+</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>mIoU values for universal segmentation using Cityscapes (CS) and CamVid (CVD) datasets with a Resnet-18 backbone. N is the number of supervised examples available from each dataset. Bold entries have the highest average mIoU across the datasets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Comparison of our approach with other semi-supervised approaches on the Resnet-101 backbone and CS+CVD dataset. Our approach (Univ-full) results in a single model across datasets unlike the previous semi-supervised approaches and deliver com- petitive performance on both the datasets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Train on CS 40.97 14.64 27.81 64.23 32.50 48.37 Train on IDD 25.05 26.53 25.79 46.32 55.01 50.67</figDesc><table><row><cell>Method</cell><cell></cell><cell>N=100 (Resnet-18)</cell><cell></cell><cell></cell><cell>N=1500 (Resnet-50)</cell></row><row><cell></cell><cell>CS</cell><cell>IDD</cell><cell>Avg.</cell><cell>CS</cell><cell>IDD</cell><cell>Avg.</cell></row><row><cell>Univ-basic</cell><cell cols="3">37.94 25.21 31.58</cell><cell cols="3">63.55 53.21 58.38</cell></row><row><cell>Univ-full</cell><cell cols="3">36.48 27.45 31.97</cell><cell cols="3">64.12 55.14 59.63</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Universal segmentation results using IDD and CS datasets. Our approach (Univ-full) performs better across Resnet-18 and Resnet-50 CNN backbones.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>mIoU values for universal segmentation across different task datasets with Resnet-50 backbone. While Cityscapes is an autonomous driving dataset, SUN dataset is mainly used for indoor segmentation. This demonstrates the effectiveness of universal segmentation even across diverse environments.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>= 0.5] 33.28 48.7 40.99 33.51 49.49 41.50 Ours[Word2Vec] 33.48 53.19 43.34 36.18 52.72 44.45</figDesc><table><row><cell>Method</cell><cell></cell><cell>N=50</cell><cell></cell><cell>N=100</cell></row><row><cell></cell><cell>CS</cell><cell>CVD Avg.</cell><cell>CS</cell><cell>CVD Avg.</cell></row><row><cell>Ours[? Ours[K=1]</cell><cell cols="2">34.01 53.23 43.62</cell><cell cols="2">41.03 54.62 47.83</cell></row><row><cell>Ours[K=3]</cell><cell cols="2">35.23 52.38 43.81</cell><cell cols="2">41.82 54.96 48.39</cell></row><row><cell>Ours[K=5]</cell><cell cols="2">33.76 52.77 43.27</cell><cell cols="2">40.08 55.02 47.55</cell></row><row><cell>Direct SER</cell><cell cols="2">21.36 35.24 28.30</cell><cell cols="2">23.7 30.67 27.19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Extension ofTable 3from the original paper. ? is the update factor during training, and the default value is 1. K is the number of embeddings per label. Ours[Word2Vec] uses word vectors as label embeddings. The model gives best performance for K=3, ? = 1 while using prototype embeddings.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>51.29 80.07 00.00 24.01 33.73 26.16 38.71 82.30 36.39 81.61 54.38 20.48 81.71 02.37 22.79 03.85 01.31 46.23 41.03 K=3 93.10 56.82 80.48 00.03 17.84 32.49 24.07 33.51 82.52 38.52 80.12 53.22 15.35 81.34 07.79 20.79 04.18 22.57 49.90 41.82 K=5 89.58 48.50 81.21 14.55 07.89 27.77 22.72 33.95 84.36 34.33 80.52 54.39 22.51 81.52 02.41 09.43 07.28 10.40 48.18 40.08 17.13 84.99 53.35 70.57 31.99 32.45 72.94 36.61 37.22 54.61 K=3 87.67 78.51 16.37 84.84 53.18 73.33 34.02 27.71 74.42 40.36 34.24</figDesc><table><row><cell>Road</cell><cell>SideWalk</cell><cell>Building</cell><cell>Wall</cell><cell>Fence</cell><cell>Pole</cell><cell>Traffic light</cell><cell>Traffic Sign</cell><cell>Vegetation</cell><cell>Train</cell><cell>Sky</cell><cell>Person</cell><cell>Rider</cell><cell>Car</cell><cell>Truck</cell><cell>Bus</cell><cell>Train</cell><cell>MotorCycle</cell><cell>Bicycle</cell><cell>mIoU</cell></row><row><cell>K=1 92.18 Method</cell><cell>Sky</cell><cell>Buil.</cell><cell></cell><cell>Pole</cell><cell></cell><cell>Road</cell><cell>Pave.</cell><cell></cell><cell>Tree</cell><cell>Sign</cell><cell></cell><cell>Fence</cell><cell></cell><cell>Car</cell><cell>Ped.</cell><cell></cell><cell>Bicy.</cell><cell></cell><cell>mIoU</cell></row><row><cell>K=1</cell><cell>86.3</cell><cell cols="18">77.23 54.96</cell></row><row><cell>K=5</cell><cell cols="4">86.07 76.39 15.25</cell><cell></cell><cell>87.87</cell><cell>63.6</cell><cell></cell><cell>70.95</cell><cell cols="2">32.6</cell><cell>34.6</cell><cell></cell><cell cols="2">76.63 30.42</cell><cell></cell><cell>30.9</cell><cell></cell><cell>55.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 :</head><label>8</label><figDesc>Class-wise IoU values for the 19 classes in Cityscapes dataset and 11 classes in the CamVid dataset for different K, for N=100 on Resnet-18.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Label-embedding for attribute-based classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="819" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluation of output embeddings for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2927" to="2936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00561</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic object classes in video: A highdefinition ground truth database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Partial adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijia</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Autodial: Automatic domain alignment layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">Maria</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel Rota</forename><surname>Bulo</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Road: Reality oriented adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7892" to="7901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">No more discrimination: Cross city adaptation of road scene segmenters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-Cheng</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1310.1531</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Domainadversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A survey on deep learning techniques for image and video semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Orts-Escolano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergiu</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Villena-Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Martinez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Garcia-Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="65" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Understanding real world indoor scenes with synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Viorica Patraucean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4077" to="4085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">CyCADA: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Fcns in the wild: Pixel-level adversarial and constraint-based adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02649</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Decoupled deep neural network for semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1495" to="1503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Conditional generative adversarial network for structured domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixiang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1335" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adversarial learning for semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan-Ting</forename><surname>Liou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Anoop Namboodiri, Manmohan Chandrakar, and Srikumar Ramalingam. AutoNUE workshop and challenge at ECCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbumani</forename><surname>Cv Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Subramanian</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">One model to learn them all</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05137</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attribute-based classification for zero-shot visual object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Christoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="453" to="465" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dynamic-structured semantic propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Label efficient learning of transferable representations acrosss domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li F Fei-Fei</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="165" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">The natural language decathlon: Multitask learning as question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08730</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychology of learning and motivation</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1989" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scenenet rgb-d: Can 5m synthetic images beat generic imagenet pre-training on indoor segmentation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mccormac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2678" to="2687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Image to image translation for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zak</forename><surname>Murez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungnam</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4500" to="4509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1717" to="1724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1742" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pinheiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8004" to="8013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Erfnet: Efficient residual factorized convnet for realtime semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Lvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The SYNTHIA Dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Class2Str: End to end latent hierarchy learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Multi-task learning in deep neural networks for improved phoneme recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasha</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Droppo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6965" to="6969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Semi supervised semantic segmentation using generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasim</forename><surname>Souly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Concetto</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.10349</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4068" to="4076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">IDD: A dataset for exploring problems of autonomous navigation in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbumani</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himalaya</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12833</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Revisiting dilated convolution: A simple approach for weakly-and semisupervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7268" to="7277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A survey of transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Taghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingding</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Dilated residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Taskonomy: Disentangling task transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">A curriculum domain adaptation approach to the semantic segmentation of urban scenes. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Foroosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
