<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CURVATURE-GUIDED DYNAMIC SCALE NETWORKS FOR MULTI-VIEW STEREO</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khang</forename><surname>Truong Giang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<postCode>34141</postCode>
									<settlement>Daejeon</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soohwan</forename><surname>Song</surname></persName>
							<email>soohwansong@etri.re.kr</email>
							<affiliation key="aff1">
								<orgName type="department">Intelligent Robotics Research Division</orgName>
								<orgName type="institution">ETRI</orgName>
								<address>
									<postCode>34129</postCode>
									<settlement>Daejeon</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungho</forename><surname>Jo</surname></persName>
							<email>shjo@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<postCode>34141</postCode>
									<settlement>Daejeon</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CURVATURE-GUIDED DYNAMIC SCALE NETWORKS FOR MULTI-VIEW STEREO</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-view stereo (MVS) is a crucial task for precise 3D reconstruction. Most recent studies tried to improve the performance of matching cost volume in MVS by designing aggregated 3D cost volumes and their regularization. This paper focuses on learning a robust feature extraction network to enhance the performance of matching costs without heavy computation in the other steps. In particular, we present a dynamic scale feature extraction network, namely, CDSFNet. It is composed of multiple novel convolution layers, each of which can select a proper patch scale for each pixel guided by the normal curvature of the image surface. As a result, CDFSNet can estimate the optimal patch scales to learn discriminative features for accurate matching computation between reference and source images. By combining the robust extracted features with an appropriate cost formulation strategy, our resulting MVS architecture can estimate depth maps more precisely. Extensive experiments showed that the proposed method outperforms other methods on complex outdoor scenes. It significantly improves the completeness of reconstructed models. As a result, the method can process higher resolution inputs within faster run-time and lower memory than other MVS methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A challenging problem in multi-view stereo (MVS) is accurately estimating dense correspondences across a collection of high-resolution images. Many MVS studies have tried to solve ill-posed MVS problems such as matching ambiguity and high computational complexity. In such problems, learning-based MVS methods <ref type="bibr" target="#b23">Luo et al., 2019;</ref><ref type="bibr" target="#b4">Chen et al., 2020a;</ref><ref type="bibr" target="#b32">Xu &amp; Tao, 2020a;</ref><ref type="bibr" target="#b36">Xue et al., 2019)</ref> usually outperform traditional methods <ref type="bibr" target="#b10">(Galliani et al., 2015;</ref><ref type="bibr" target="#b26">Schonberger &amp; Frahm, 2016)</ref>. The learning-based methods are generally composed of three steps, including feature extraction, cost volume formulation, and cost volume regularization. Most studies made an effort to improve the performance of cost formulation <ref type="bibr" target="#b23">(Luo et al., 2019;</ref><ref type="bibr" target="#b34">Xu &amp; Tao, 2020c;</ref><ref type="bibr" target="#b40">Yi et al., 2020)</ref> or cost regularization <ref type="bibr" target="#b23">(Luo et al., 2019;</ref><ref type="bibr" target="#b29">Wang et al., 2021)</ref>. The visibility information <ref type="bibr" target="#b34">Xu &amp; Tao, 2020c)</ref> or attention techniques <ref type="bibr" target="#b40">(Yi et al., 2020;</ref> are considered for cost aggregation and feature matching, respectively. Furthermore, some studies employed a hybrid 3D UNet structure <ref type="bibr" target="#b23">(Luo et al., 2019;</ref><ref type="bibr" target="#b28">Sormann et al., 2020)</ref> for cost volume regularization. While these approaches significantly improve MVS performances, their feature extraction networks have some drawbacks that degrade the quality of 3D reconstruction. First, they observed a restricted size of receptive fields from the fixed size of convolutional kernels. This leads to difficulty in learning a robust pixel-level representation when an object's scale varies extremely in images. As a result, the extracted features cause the low quality of matching costs, especially when the difference of camera poses between reference and source images is large. Second, MVS networks are often trained on low-resolution images because of limited memory and restricted computation time. Therefore, the existing feature extraction methods utilizing fixed-scale feature representation on deep networks could not generalize to high-resolution images in prediction; the MVS performance is downgraded on a high-resolution image set.</p><p>To address these issues, we propose a new feature extraction network that can be adapted to various object scales and image resolutions. We refer the proposed network as curvature-guided dynamic Published as a conference paper at ICLR 2022 (a) Illustration of scale selection for each pixel in reference image from our proposed CDSConv (b) CDSConv layer <ref type="figure">Figure 1</ref>: The results extracted by the proposed CDSConv and overall pipeline of CDSConv scale feature network (CDSFNet). The proposed network is composed of novel convolution layers, curvature-guided dynamic scale convolution (CDSConv), which select a suitable patch scale for each pixel to learn robust representation. This selection is achieved by computing the normal curvature of the image surface at several candidate scales and then analyzing the computed outputs via a classification network. The pixel-level patch scales estimated from the proposed network are dynamic with respect to textures, scale of objects, and epipolar geometry. Therefore, it trains more discriminative features than existing networks <ref type="bibr" target="#b11">(Gu et al., 2020;</ref><ref type="bibr" target="#b6">Cheng et al., 2020;</ref><ref type="bibr" target="#b37">Yang et al., 2020;</ref> for accurate matching cost computation. <ref type="figure">Fig. 1a</ref> illustrates the dynamic pixel-level patch scales estimated from CDSConv where the window size is ranged from 7?7 to 17?17. As shown in the figure, CDSConv predicts the adaptive patch scales where the scale is small in thin structure or texture-rich regions and large in the texture-less regions. Moreover, the scale is dynamic according to image resolution. CDSConv generally produces small scale values in a low-resolution image and large scale values in a high-resolution image.</p><p>We also formulate a new MVS framework, CDS-MVSNet, which aims at improving the quality of MVS reconstruction while decreasing computation time and memory consumption. Similar to previous works <ref type="bibr" target="#b11">(Gu et al., 2020;</ref><ref type="bibr" target="#b6">Cheng et al., 2020;</ref><ref type="bibr" target="#b37">Yang et al., 2020;</ref><ref type="bibr" target="#b29">Wang et al., 2021)</ref>, CDS-MVSNet is composed of a cascade network structure to estimate high-resolution depth maps in a coarse-tofine manner. For each cascade stage, it formulates a 3D cost volume based on the output features of CDSFNet, which reduces the matching ambiguity by using proper pixel's scale. For instance, consider a 3D point on the scene that needs to be reconstructed, CDSFNet can capture the same context information in both reference and source views for this point to extract the matchable features. Furthermore, CDS-MVSNet applies visibility-based cost aggregation to improve the performance of stereo matching. The pixel-wise visibility is estimated from the curvature information that encodes the matching ability of extracted features implicitly. Therefore, CDS-MVSNet performs accurate stereo matching and cost aggregation even on low-resolution images. Finally, the proposed network can generate high quality 3D models only by processing half-resolution images. This approach can significantly reduce the computation time and memory consumption of MVS reconstruction.</p><p>The contribution of this paper is summarized as follows:</p><p>? We present the CDSConv that learns dynamic scale features guided by the normal curvature of image surface. This operation is implemented by approximating surface normal curvatures in several candidate scales and choosing a proper scale via a classification network.</p><p>? We propose a new feature extraction network, CDSFNet, which is composed of multiple CDSConv layers for learning robust representation at pixel level. CDFSNet estimates the optimal pixel's scale to learn features. The scale is selected adaptively with respect to structures, textures, and epipolar constraints.</p><p>? We present CDS-MVSNet for MVS depth estimation. CDS-MVSNet performs accurate stereo matching and cost aggregation by handling the ambiguity and visibility in the image matching process. It also significantly reduces the run-time and memory consumption by processing the half-resolution images while maintaining the reconstruction quality.</p><p>? We verify the effectiveness of our method on two benchmark datasets; DTU <ref type="bibr" target="#b0">(Aanaes et al., 2016)</ref> and Tanks &amp; Temples <ref type="bibr" target="#b19">(Knapitsch et al., 2017)</ref>. The results demonstrate that the proposed feature learning method boosts the performance of MVS reconstruction.</p><p>Multi-view Stereo. Traditional MVS studies can be divided into three categories: volumetric, point cloud-based, and depth map-based methods <ref type="bibr" target="#b9">(Furukawa &amp; Hern?ndez, 2015)</ref>. Comparatively, depth map-based methods are more concise and flexible, they estimate depth maps for all reference images and then perform depth fusion to achieving a 3D model <ref type="bibr" target="#b10">(Galliani et al., 2015;</ref><ref type="bibr" target="#b27">Sch?nberger et al., 2016;</ref><ref type="bibr" target="#b31">Xu &amp; Tao, 2019)</ref>. The popular MVS methods in this category are usually based on Patch-Match Stereo algorithm <ref type="bibr" target="#b1">(Barnes et al., 2009)</ref>. Recently, many studies have applied a learning-based approach, which has achieved significant performance improvements over traditional methods. Several volumetric methods <ref type="bibr" target="#b18">(Kar et al., 2017;</ref><ref type="bibr" target="#b16">Ji et al., 2017)</ref> employed 3D CNNs to explicitly predict global 3D surfaces. <ref type="bibr" target="#b38">Yao et al. (2018)</ref> proposed a depth-map-based method, MVSNet, which is an end-to-end trainable architecture with three steps. However, these methods still confronted the matching ambiguity problem and could not process high-resolution images due to limited memory. Many methods based on the three-step architecture of MVSNet are proposed to address these issues. Several studies <ref type="bibr" target="#b11">(Gu et al., 2020;</ref><ref type="bibr" target="#b6">Cheng et al., 2020;</ref><ref type="bibr" target="#b37">Yang et al., 2020;</ref><ref type="bibr" target="#b29">Wang et al., 2021;</ref> applied a coarse-to-fine framework which estimates the depths through multiple stages to reduce the computational complexity. They used the estimated depth of each stage to adjust the depth hypothesis range for cost formulation in the next stage. To enhance the matching cost volume, other methods introduced a novel strategy for cost volume formulation. <ref type="bibr" target="#b23">Luo et al. (2019)</ref> proposed a patch-wise feature matching instead of pixel-wise matching to compute the cost. <ref type="bibr" target="#b40">Yi et al. (2020)</ref> and  applied an attention technique to improve feature matching. ; <ref type="bibr" target="#b34">Xu &amp; Tao (2020c)</ref> estimated visibility information to score the view weights for cost aggregation.</p><p>All these methods <ref type="bibr" target="#b11">(Gu et al., 2020;</ref><ref type="bibr" target="#b6">Cheng et al., 2020;</ref><ref type="bibr" target="#b37">Yang et al., 2020;</ref><ref type="bibr" target="#b29">Wang et al., 2021;</ref><ref type="bibr" target="#b41">Yu &amp; Gao, 2020</ref>) had a trade-off between computation and performance; the methods with high performance usually require expensive computations. Also, they only applied standard CNNs with static scale representation for feature extraction. In this work, we aim to learn dynamic scale representation for more accurate feature matching. We also estimate pixel-wise visibility information to remove noises and wrong matching pixels in cost aggregation. These goals are achieved by analyzing the normal curvature of the image surface.</p><p>Recently,  proposed an MVS method that considers the normal curvature information for feature extraction. This method handles ambiguity in the patch-match stereo method <ref type="bibr" target="#b1">(Barnes et al., 2009)</ref>. It selects a patch scale for each pixel by thresholding the normal curvatures computed in several candidate scales. Although the method effectively improves the quality of patch matching, the use of its handcrafted features limits the overall MVS performance. In contrast, our method does not require extracting handcrafted features and determining the threshold for scale selection. Furthermore, CDSConv operates not only on the color information of images but also on the high dimensional features. Multiple CDSConv operations can be stacked to form a very deep architecture for robust feature extraction in MVS.</p><p>Multi-scale features. Due to large-scale variations of objects and textures in complex scenes, multiscale representations are adapted for accurate and effective dense pixel-level prediction in many computer vision tasks. For the MVS task, Feature Pyramid Network (FPN) <ref type="bibr" target="#b20">(Lin et al., 2017;</ref><ref type="bibr" target="#b11">Gu et al., 2020;</ref><ref type="bibr" target="#b6">Cheng et al., 2020;</ref>) is a common approach for multi-scale feature extraction. It is designed as the UNet-liked architecture <ref type="bibr" target="#b25">(Ronneberger et al., 2015)</ref> to fuse features from different scales into a single one. However, the multi-scale of FPN is achieved at an image level through down-upsampling operations. It cannot capture the large-scale variation of objects at a pixel level. Several studies in image segmentation can learn pixel-level multi-scale features by using multiple kernels with different sizes <ref type="bibr" target="#b13">(He et al., 2019;</ref><ref type="bibr" target="#b22">Liu et al., 2016;</ref><ref type="bibr" target="#b2">Chen et al., 2017;</ref><ref type="bibr" target="#b44">Zhao et al., 2017)</ref>. However, these methods prefer a large receptive field for segmentation tasks and usually ignore the details of objects essential for MVS tasks. In contrast to the aforementioned methods <ref type="bibr" target="#b11">(Gu et al., 2020;</ref><ref type="bibr" target="#b6">Cheng et al., 2020;</ref><ref type="bibr" target="#b13">He et al., 2019)</ref>, our proposed CDSConv can select an optimal scale for each pixel to produce a robust representation and enhance the matching uncertainty. Moreover, our feature extraction CDSFNet can cover denser scales by utilizing the power of a deep CNN.</p><p>Dynamic filtering. Instead of observing static receptive fields, several studies modified standard convolution to choose the adaptive receptive field for each pixel <ref type="bibr" target="#b17">Jia et al., 2016;</ref><ref type="bibr" target="#b12">Han et al., 2018)</ref>. SAC  was proposed to modify the fixed-size receptive field by learning position-adaptive scale coefficients. Deformable ConvNets <ref type="bibr" target="#b7">(Dai et al., 2017;</ref><ref type="bibr" target="#b45">Zhu et al., 2019)</ref> learned offsets for each pixel in a regular sampling grid of standard convolution to enlarge the sampling field with an arbitrary form that can discover geometric-invariant features. Recently, <ref type="bibr" target="#b5">Chen et al. (2020b)</ref> proposed a dynamic convolution composed of multiple kernels with the same size to increase model complexity without increasing the network depth or width. All these existing works learn dynamic filters directly from the input images. They are suitable for segmentation or recognition tasks. For MVS task, they ignore the epipolar constraint between reference and source images. In contrast, our dynamic filtering is guided by normal curvature computed in the direction of epipolar line, which can measure the matching capacity between reference and source image patches. To the best of our knowledge, this is the first work that introduces dynamic filters to MVS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED FEATURE EXTRACTION NETWORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">NORMAL CURVATURE</head><p>Normal curvature is used to estimate how much a surface is curved at a specific point along a particular direction <ref type="bibr" target="#b8">(Do Carmo, 2016)</ref>. Let p(X, ?) be a patch centered at pixel X and having a scale ? which is proportional to the window size. L(X, I src ) is denoted as the epipolar line on the reference image I ref , which passes through X and is related to a source image I src . To reduce the matching ambiguity between p in I ref and the correct patch p in I src , a proper scale ? should be chosen when the corresponding patch p(X, ?) is less linearly correlated to its surrounding patches along the epipolar line L. From the scale space theory <ref type="bibr" target="#b21">(Lindeberg, 1994)</ref>, the patch p(X, ?) can be considered as a point on the image surface represented at the scale ?. Therefore, the scale ? is optimal for stereo matching if the image surface represented in that scale is curved significantly at the point X along the epipolar line direction . This requires to compute the normal curvature for the patch p(X, ?) along the direction ? = [u, v] T of the epipolar line L. The formula can be expressed in terms of the values of the first and second fundamental forms of the image surface:</p><formula xml:id="formula_0">curv ? (X, ?) = F M II F M I = 1 1 + I 2 x + I 2 y ? I xx I xy I xy I yy ? T ? 1 + I 2 x I x I y I x I y 1 + I 2 y ? T ,<label>(1)</label></formula><p>where I x , I y , I xx , I xy , I yy are the first-order and second-order derivatives of the image I along the x-axis and y-axis, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CURVATURE-GUIDED DYNAMIC SCALE CONVOLUTION</head><p>This section describes the novel convolutional module CDSConv to extract dynamic scale features. Given a set of K convolutional kernels with different size {C 1 , C 2 , . . . , C K } corresponding to K candidate scales {? 1 , ? 2 , . . . , ? K }, CDSConv aims to select a proper scale for each pixel X. Hence, it can produce a robust output feature F out from the input F in based on the selected scale. <ref type="figure">Fig. 1b</ref> shows our overall pipeline of CDSConv including two steps. First, CDSConv estimates approximately normal curvatures at K candidate scales. Second, it performs a scale selection step to output the optimal scale from K estimated curvatures. This selection is implemented by a classification network composed of two convolutional blocks and a Softmax activation for the output.</p><p>Learnable normal curvature. The formula of normal curvature of a patch centered at X along the direction of epipolar line ? = [u, v] T in Eq. 1 can be re-written as follows</p><formula xml:id="formula_1">curv ? (X, ?) = u 2 I xx (X, ?) + 2uvI xy (X, ?) + v 2 I yy (X, ?) 1 + I 2 x (X, ?) + I 2 y (X, ?) 1 + (uI x (X, ?) + vI y (X, ?)) 2<label>(2)</label></formula><p>where I(X, ?) = I(X) * G(X, ?) is the image intensity of pixel X in the image scale ?; it is determined by convolving I with a Gaussian kernel G(X, ?) with the window size/scale ? (Lindeberg, 1994). The derivatives I x , I y , I xx , I xy , and I yy can be computed by convolution between the original image I and the derivatives of Gaussian kernel G(X, ?)</p><formula xml:id="formula_2">? i+j ?x i ?y j I(X, ?) = I(X) * ? i+j ?x i ?y j G(X, ?)<label>(3)</label></formula><p>where * is the convolution operator. There are two main drawbacks when embedding the normal curvature in Eq. 2 into a deep neural network. First, the computation is heavy because of five convolution operations for computing the derivatives I x , I y , I xx , I xy , and I yy . Second, using Eq. 2 to compute curvature is infeasible when the pixel X is a latent feature F in (X) instead of the image intensity I(X). For these reasons, we derive an approximate form of the normal curvature, which reduces the computational cost and can handle the high-dimensional feature input.</p><p>To address the heavy computation issue, we notice that the curvature curv ? and derivatives I x i y j are proportional to Gaussian kernel G, following Eq. 2 and Eq. 3. Moreover, we use a classification network to select the patch scale automatically from the curvature inputs. Therefore, we can perform normalization for the curvatures by rescaling the kernel G. We restrict the gaussian kernel in a small range, i.e., G(X, ?) 1. As a result, the derivatives I x , I y , I xx , I xy , and I yy are also much smaller than 1. We can approximate the denominator in Eq. 2 by 1. Eq. 2 can be re-defined as</p><formula xml:id="formula_3">curv ? (X, ?) ? u 2 I xx (X, ?) + 2uvI xy (X, ?) + v 2 I yy (X, ?)<label>(4)</label></formula><p>where curv ? (X, ?), I xx , I xy and I yy is much smaller than 1, which implies that their values should be restricted in a small range. The computation in Eq. 4 is reduced by haft compared to Eq. 2.</p><p>To make Eq. 4 work with the high-dimensional image feature F in (X, ?), we propose to use learnable kernels instead of using fixed derivatives of Gaussian kernel. In particular, for each scale ?, we introduce three learnable convolution kernels K xx ? , K xy ? , K yy ? to replace G xx , G xy , and G yy respectively. These kernels adapt to the input features to approximately compute the second-order derivatives of the image surface. They are trained implicitly by backpropagation when training the end-to-end networks. Weights of these kernels need to be restricted to a small value K (.) ? 1 because of the assumption about the Gaussian kernel. We enforce this constraint by adding a regularization term to the loss in Section 4.2.</p><p>In summary, we propose learnable normal curvature having the formula as follows (written in the matrix form)</p><formula xml:id="formula_4">curv ? (X, ?) = ? F in * K xx ? F in * K xy ? F in * K xy ? F in * K yy ? ? T (5) where K (.)</formula><p>? s are the learnable kernels and K (.) ? ? 0, F in is the input feature.</p><p>Scale selection. After obtaining K normal curvatures {curv ?1 , curv ?2 , . . . , curv ? K } in K candidate scales, this step selects a proper scale ? (.) for each pixel X and then outputs the feature F out (X) from the corresponding convolution kernel C <ref type="bibr">(.)</ref> . The proper scale is selected by analyzing the curvatures estimated in K candidate scales. It is observed that a small scale cannot capture enough context information to learn discriminative feature, especially in low-textured regions. Meanwhile, a large scale smooths local structure in rich texture regions. MARMVS  chose the proper scale by searching the normal curvature with a threshold T = 0.01. However, using of fixed threshold T prevents generalization on various scenes and structures. Instead, we propose a classification strategy that automatically selects the proper scale from K curvature inputs. This is achieved by using a lightweight CNN with two convolutional blocks. For each pixel X, the CNN outputs one-hot vector [w 1 , w 2 , . . . , w K ] for scale selection by applying a Softmax with small temperature parameter, which is a differentiable approximation to argmax operator <ref type="bibr" target="#b15">(Jang et al., 2017)</ref>.</p><p>Finally, the feature output F out is produced from the feature input F in with K candidate kernels {C 1 , C 2 , . . . , C K } by using weighted sum</p><formula xml:id="formula_5">F out = w 1 (F in * C 1 ) + w 2 (F in * C 2 ) + ... + w K (F in * C K )<label>(6)</label></formula><p>where * is the convolution operator. Also, we can extract the normal curvature corresponding to the selected scale by</p><formula xml:id="formula_6">N C est = w 1 curv ?1 + w 2 curv ?2 + ... + w K curv ? K<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">CURVATURE-GUIDED DYNAMIC SCALE FEATURE NETWORK</head><p>This section introduces the Curvature-guided dynamic scale feature network, CDSFNet, which is used as feature extraction step for our MVS framework. CDSFNet is composed of multiple CD-SConv layers instead of standard convolution layers. By expanding the searching scale-space,  <ref type="figure">Figure a</ref> shows the scale maps when images is captured at the far view and close-up view. <ref type="figure">Figure b</ref> shows the results when the difference of camera poses is small (left) and large (right).</p><p>CDSFNet can select the optimal scale for each pixel to learn robust representation that reduces matching ambiguity.</p><p>Architecture. CDSFNet is an Unet-liked architecture <ref type="bibr" target="#b25">(Ronneberger et al., 2015)</ref>. It is organized into three levels of spatial resolution l ? {0, 1, 2} to adapt the coarse-to-fine MVS framework. Given the inputs including image I and its estimated epipole e, the network outputs three features for three levels</p><formula xml:id="formula_7">{F (0) , F (1) , F (2) } and three estimated normal curvatures {N C est,(0) , N C est,(1) , N C est,(2) }.</formula><p>Understanding CDSFNet. The main goal of CDSFNet is to select pixel-level scale for robust feature extraction. The scale changes dynamically, depending on the detail of image objects and the epipolar constraint, i.e. the relative camera pose between reference and source images. Therefore, our method is more appropriate for MVS than the other dynamic scale networks <ref type="bibr" target="#b13">(He et al., 2019;</ref><ref type="bibr" target="#b17">Jia et al., 2016)</ref> proposed in image segmentation task.</p><p>To have a low computational complexity, the number of candidate scales in CDSConv layer should be small, we only use 2 or 3 candidates. However, the searching scale-space is expanded profoundly when multiple CDSConv layers are stacked in CDSFNet. <ref type="figure" target="#fig_0">Fig. 2</ref> show the patch scale or window size for each pixel estimated roughly from the first two layers of CDSFNet. For each reference and source views, we draw the reference and source scale maps respectively. There are two main advantages of CDSFNet shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. First, the estimated scales in rich texture, thin structure, and near-edge regions are often small while those in untextured regions are large. Second, the scales are changed with respect to viewpoints ( <ref type="figure" target="#fig_0">Fig. 2a</ref>) and relative camera pose between reference and source views <ref type="figure" target="#fig_0">(Fig. 2b</ref>). In <ref type="figure" target="#fig_0">Fig. 2a</ref>, the closer viewpoint is, the larger scales are estimated. In <ref type="figure" target="#fig_0">Fig. 2b</ref>, the reference and source scale maps are similar when the difference of camera poses is not large. Otherwise, when the difference is large, the scale map of reference view is changed to adapt the source view, which was marked in the red circle. This is a superiority of CDSFNet because it estimates scale based on epipolar constraint between reference and source views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CDS-MVSNET</head><p>In this section, we describe the proposed MVS network referred to as CDS-MVSNet, which predicts a depth map D ref from the reference I 0 and N source images {I i } N i=1 . We adopted the cascade structure of CasMVSNet <ref type="bibr" target="#b11">(Gu et al., 2020)</ref> as the baseline structure of CDS-MVSNet. The network is composed of multiple cascade stages to predict the depth maps in a coarse-to-fine manner. Each stage estimates a depth through three steps: feature extraction, cost volume formulation, cost volume regularization &amp; depth regression. CDS-MVSNet formulates a 3D cost volume based on the output features of CDSFNet. The features of CDSFNet effectively reduce the matching ambiguity by considering the proper pixel scales. Due to the robustness of our feature extraction and cost formulation, CDS-MVSNet can produce more accurate depths in coarser stages compared to other cascade MVS methods <ref type="bibr" target="#b11">(Gu et al., 2020;</ref><ref type="bibr" target="#b6">Cheng et al., 2020;</ref><ref type="bibr" target="#b37">Yang et al., 2020;</ref><ref type="bibr" target="#b29">Wang et al., 2021)</ref>. There is even little difference in the accuracy of the depths estimated at half resolution and full resolution. Therefore, differently from CasMVSNet, CDS-MVSNet only computes the depths of half-resolution images using three-step MVS computation and then upsamples the depths to the original resolution in the final stage. The upsampled depths at the original resolution can be refined by a 2D CNN to obtain the final depths. This approach can drastically reduce the computation time and GPU memory requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DETAILS OF ARCHITECTURE</head><p>Robust Feature Extraction. Given the reference image I 0 and N source images {I 1 , I 2 , . . . , I N }, CDSFNet extracts a robust feature pair (F 0,i , F i ) i=1...N for each image pair (I 0 , I i ) i=1...N . Note that the feature of the reference image F 0,i varies according to each source image i due to the guidance of epipolar geometry in normal curvature estimation. This is our superiority compared to the state-of-the-art MVS networks which learn an identical feature for the reference image.</p><p>Cost Volume Formulation. We first compute two-view matching costs and then aggregate them into a single cost. The two-view matching cost is computed per sampled depth hypothesis to form cost volume <ref type="bibr" target="#b38">(Yao et al., 2018;</ref><ref type="bibr" target="#b11">Gu et al., 2020;</ref><ref type="bibr" target="#b29">Wang et al., 2021)</ref>. Given a sampled depth hypothesis d j (j = 1. . . D) along with all camera parameters, we warp the feature maps of source image</p><formula xml:id="formula_8">{F i } N i=1 into this depth plane. Let F w i (d j )</formula><p>be the warped feature map of source image i at the depth hypothesis d j . The two-view matching cost V i (d j ) between the reference image I 0 and source image I i at the depth hypothesis d j is determined as</p><formula xml:id="formula_9">V i (d j ) = F 0,i , F w i (d j ) ,</formula><p>where ., . is the pixel-wise inner product for measuring the similarity of two feature maps.</p><p>Next, we perform cost aggregation over the computed two-view cost volumes. Inspired from visibility-awareness for cost aggregation <ref type="bibr" target="#b34">Xu &amp; Tao, 2020c)</ref>, we employ pixelwise view weight prediction. However, instead of predicting directly from two-view cost volume <ref type="bibr" target="#b34">Xu &amp; Tao, 2020c;</ref><ref type="bibr" target="#b40">Yi et al., 2020)</ref>, we additionally utilize the normal curvature maps, N C est , estimated by CDSFNet in Eq. 7. The normal curvature can implicitly provide information about level details of surface. For instance, a pixel with small normal curvature indicates that it belongs to a large untextured region. In this case, its feature is unmatchable and needs to be eliminated from cost aggregation to reduce noises for cost regularization step.</p><p>In summary, the estimated normal curvature encodes implicitly the matching ability of learned feature. Let N C est i be the estimated normal curvature of the reference view related to source view i, and H i be the entropy computed from two-view matching cost volume V i . We use a simple 2D CNN denoted as V is(.) to predict a view weight map W i from two inputs N C est i and H i . Finally, the aggregated cost V is computed by weighted mean.</p><formula xml:id="formula_10">H i = ? D j=1 V i (d j ) log V i (d j ), W i = V is(N C est i , H i ), V (d j ) = N i=1 W i V i (d j ) N i=1 W i</formula><p>Cost volume regularization &amp; Depth regression. Following most recent learning-based MVS methods, a 3D CNN is applied to obtain a depth probability volume P from the aggregated matching cost volume V . The depth is then regressed from P by D est = D j=1 d j P (d j )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">LOSS FUNCTION</head><p>We propose a feature loss for training CDSFNet effectively. Given a ground truth depth D gt of the reference image, we warp the feature of source images into this depth map and then compute the matching cost V (D gt ). We label this map as positive, hence defines its classification label as c = 1.</p><p>To generate the matching costs with negative labels c = 0, we randomly sample N d neighboring depths D neg around the ground truth D gt and then compute the matching costs V (D neg ) for these depths. Finally, binary cross entropy is used to define the feature loss. However, we need to add regularization terms to restrict the convolutional weights of CDSFNet, w CDSF , and estimated normal curvature N C est to a small range as mentioned in Section 3.2. Therefore, the final feature loss is defined as:</p><formula xml:id="formula_11">L f eat = 1 M X log sig(V (D gt ))(1 ? sig(V (D neg (X))) + ? 1 w CDSF 2 + ? 2 M N C est 2 (8)</formula><p>where sig(.) is the sigmoid function, M is total number of pixels X, ? 1 = 0.01 and ? 2 = 0.1 are the regularized hyperparameters.</p><p>Similar to most learning-based MVS methods <ref type="bibr" target="#b38">(Yao et al., 2018;</ref><ref type="bibr" target="#b11">Gu et al., 2020)</ref>, the depth loss L depth is defined by L1 loss between the predicted depth and the ground truth depth. Because our method includes 4 stages, there are four depth loss {L (l) depth } 3 l=0 . Finally, the total loss L total is defined as a sum of two mentioned losses L total = 5L f eat + 3 l=0 L (l) depth  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">BENCHMARK PERFORMANCE</head><p>Results for DTU dataset. We predict the depth at the highest resolution 1536 ? 1152. The performance of the proposed method was compared with state-of-the-art MVS methods, including conventional and learning-based methods, using the DTU dataset. However, we especially notice the learning-based methods including CasMVSNet, UCSNet, CVP-MVSNet, and Vis-MVSNet, which adopted the cascade structure for MVS depth estimation. Similar to us, these methods used the same 3D CNN cost regularization. Therefore, we can validate our feature extraction and cost formulation by comparing with them. <ref type="table" target="#tab_0">Table 1</ref> presents the quantitative results achieved by the methods on the DTU evaluation dataset. We calculated the three standard error metrics <ref type="bibr" target="#b0">(Aanaes et al., 2016)</ref> given on the official site of DTU: accuracy, completeness, and overall error. Following the same setups with most of the baselines, we trained our model on the DTU training set and then used this model for evaluation. Moreover, we also provided the results which produced by training on both DTU and BlendedMVS simultaneously. In both cases, our method exhibited the best performance in terms of completeness and overall error. We also achieved good results for both accuracy and completeness, while the other methods often produced a large trade-off between these two metrics. For example, Gipuma had a high accuracy at 0.283 but it produced very low completeness, 0.873. In contrast, PatchMatchNet was with a high completeness (0.277) and very low accuracy (0.427). Moreover, our method outperformed all similar methods using the cascade structure. This indicates that our feature representation and cost aggregation guided by normal curvature can remarkably improve MVS depth prediction remarkably.</p><p>To validate the dynamic scale property of our CDSFNet, we measure depth precision on different image resolutions. Depth precision is the average percentage of depths with errors lower than a defined threshold <ref type="bibr" target="#b11">(Gu et al., 2020)</ref>. Here we computed precision with the thresholds of 2mm and 4mm. We compared to CasMVSNet, PatchMatchNet, UCSNet, and CVP-MVSNet, which applied the cascade structure to predict high-resolution depths. Note that all methods are trained on the datasets with image-resolution of 640 ? 512. <ref type="table" target="#tab_1">Table 2</ref> presents the quantitative results for the depth estimated on five image resolutions. The baseline methods degraded the performance when the image-resolution was increased while our method could maintain a stable performance at any image resolution. The reason is that our feature extraction CDSFNet can utilize dynamic scales for learning features. The dynamic scale adapts to various image-resolutions and object scales in image. Generalization on Tanks &amp; Temples Dataset. To verify the generalization capability of our CDS-MVSNet, we also provided two evaluations similar to the evaluation of DTU. First, we fine-tuned the pre-trained model on DTU by using BlendedMS dataset. Second, we used directly the model trained on both datasets for evaluation. The reconstructed point clouds were submitted to the benchmark website of Tanks &amp; Temples <ref type="bibr" target="#b19">(Knapitsch et al., 2017)</ref> to receive the F-score, which is a combination of precision and recall of the reconstructed 3D model. <ref type="table" target="#tab_2">Table 3</ref> lists the quantitative results of our method and other state-of-the-art methods. As shown in <ref type="table" target="#tab_2">Table 3</ref>, our method achieved the best mean Fscores for both evaluations, 60.82 and 61.58, respectively. Our method outperformed both the state-of-the-art traditional methods such as ACMM, ACMP and the learning-based methods including AttMVS and Vis-MVSNet. The qualitative results of all scenes are shown in detail in the supplemental materials. Compared to CasMVSNet and UCSNet, which are most similar to our method except the feature extraction and cost formulation, we obtained better mean F-score with high margins. This demonstrates the effectiveness of our feature extraction CDSFNet and cost aggregation strategy on complex outdoor scenes.</p><p>Run-time and Memory. This section evaluates the memory consumption and run-time compared to several state-of-the-art learning-based methods that achieve competing performance and employ the cascade structure similar to us: CasMVSNet, UCS-Net, CVP-MVSNet, and FastMVSNet. <ref type="figure">Fig. 3</ref> shows the memory and run-time of all methods measured on DTU evaluation set with different image resolutions. We observe that the run-time and memory of all methods are more required when the image-resolution increases higher. However, we notice that the memory consumption of our method was dropped from the resolution of 80% while our run-time was slightly increased. This indicates that our method can release a significant amount of GPU memory by only sacrificing a small amount of run-time. Our method achieves significantly faster run-time and lower required memory at the very high image-resolution compared to the other baselines. For example, at a resolution of 1536 ? 1152 (92.2%), the memory consumption is reduced by 54.0%, 34.1%, and 54.0% compared to CasMVSNet, UCSNet, and CVP-MVSNet respectively. The corresponding comparison results of run-time are 43.2%, 37.8%, and 74.9%. Finally, we conclude that our method is more efficient in memory consumption and run-time than most state-of-the-art learning-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We propose a dynamic scale network CDSFNet for MVS task. CDSFNet can extract the discriminative features to improve the matching ability by analyzing the normal curvature of the image surface. We then design an efficient MVS architecture, CDS-MVSNet, which achieves state-of-the-art performance on various outdoor scenes where X = [x, y] is a pixel coordinate and I(x, y) is the color information of image. The unit normal vector of surface s at point X is determined as</p><formula xml:id="formula_12">n = s x ? s y s x ? s y = 1 1 + I 2 x + I 2 y [?I x , ?I y , 1],</formula><p>where s x and s y are the first partial derviatives with respect to x-axis and y-axis, ? is the crossproduct operator. Then, the first and second fundamental form of surface s at point X can be written by Note that "?" is the dot-product operation between two vectors. Finally, the normal curvature of s at point X along the direction ? = [u, v] T of epipolar line is defined as</p><formula xml:id="formula_13">F I = s x .s x s x .</formula><formula xml:id="formula_14">curv ? (X, ?) = F M II F M I = 1 1 + I 2 x + I 2 y ? I xx I xy I xy I yy ? T ? 1 + I 2 x I x I y I x I y 1 + I 2 y ? T = 1 1 + I 2 x + I 2 y u 2 I xx + 2uvI xy + v 2 I yy u 2 (1 + I 2 x ) + 2uvI x I y + v 2 (1 + I 2 y = 1 1 + I 2 x + I 2 y u 2 I xx + 2uvI xy + v 2 I yy u 2 + v 2 + (uI x + vI y ) 2</formula><p>Because ? = [u, v] T is an unit vector, u 2 + v 2 = 1. Therefore, curv ? (X, ?) = u 2 I xx + 2uvI xy + v 2 I yy</p><formula xml:id="formula_15">1 + I 2 x + I 2 y 1 + (uI x + vI y ) 2</formula><p>This formula can be applied to an image patch p with a scale ? centered at X. Following the scale space theory <ref type="bibr" target="#b21">(Lindeberg, 1994)</ref>, the patch p can be regarded as a pixel when the image I is represented at the scale ?. Let I(X, ?) be the image intensity at the scale ?. I(X, ?) can be computed from I(X) by using a Gaussian kernel with size ?, I(X, ?) = I(X) * G(X, ?)</p><p>Therefore, we can finally write the normal curvature computed for X at the scale ? as</p><formula xml:id="formula_16">curv ? (X, ?) = u 2 I xx (X, ?) + 2uvI xy (X, ?) + v 2 I yy (X, ?) 1 + I 2 x (X, ?) + I 2 y (X, ?) 1 + (uI x (X, ?) + vI y (X, ?)) 2 ,</formula><p>which was Eq. 2 mentioned in Section 3.2 A.2 DETAIL OF MVS ARCHITECTURE <ref type="figure">Fig. 4</ref> depicts the multi-stage architecture of CDS-MVSNet. First, CDS-MVSNet uses the feature extraction CDSFNet to extract outputs at three resolutions { 1 8 , 1 4 , 1 2 } corresponding to the first three stages {0, 1, 2}. For each of these stages l ? {0, 1, 2}, CDS-MVSNet predicts the depth at the corresponding image-resolution W 2 3?l ? H 2 3?l by using three-step MVS as described in section 4.1. CDS-MVSNet also uses an independent 3D CNN for each stage to regularize 3D cost volume. The estimated depth of current stage is upsampled and used to reduce the depth hypothesis range of 3D cost volume in the next stage. For the last stage l = 3, CDS-MVSNet outputs the full-resolution depths by performing depth refinement from the upsampled depth and the color image. <ref type="figure">Figure 4</ref>: CDS-MVSNet architecture. For the first three stages l ? {0, 1, 2}, the estimated depth of current stage is used to reduce the depth hypothesis range of a cost volume in the next stage. For the last stage l = 3, we perform depth refinement from the estimated depth of previous stage and the color information CDSFNet Given a reference image I 0 and N source images {I 1 , I 2 , . . . , I N } with corresponding camera parameters {Q 0 , . . . , Q N }, we can compute an epipole pair {(e 0,i , e i )} N i=1 for each reference-source image pair {(I 0 , I i )} N i=1 based on the epipolar geometry. We use the epipole to determine the epipolar line for each pixel. Therefore, given the inputs including image I and epipole e, CDSFNet extracts three feature maps corresponding to the three levels {0, 1, 2} of CDS-MVSNet. <ref type="figure" target="#fig_1">Fig. 5</ref> describes the Unet-liked architecture of CDSFNet. <ref type="figure" target="#fig_2">Fig. 6 and 7</ref> show the results of dynamic scale estimation and normal curvature estimation respectively. Depth Refinement. Instead of using three-step MVS depth estimation as described for the finest resolution level (stage 3), we directly up-sample the depth estimated in previous stage (from resolution W 2 ? H 2 to W ? H) and refine the upsampled depth with the RGB image. Following MSG-Net <ref type="bibr" target="#b14">(Hui et al., 2016)</ref> and PatchMatchNet <ref type="bibr" target="#b29">(Wang et al., 2021)</ref>, we implement a depth residual network to perform refinement. The network uses a 2D CNN to output a residual from the upsampled depth. This residual depth is then added to the up-sampled depth, to get the refined depth map D ref . <ref type="figure">Figure 6</ref>: Illustrations for dynamic scale estimation according to epipolar constraint between reference and source views. Consider a pixel of reference view marked in red color, its epipolar line, marked in yellow, varies with each source view. Therefore, the normal curvatures estimated at that pixel are different, this leads to the different estimated scale in the reference view when it is matched to a source view. For each pixel X, the normal curvature computed at X is large when X belongs to a rich texture region. In this case, the estimated scale for X is small as shown in <ref type="figure">Fig. 6</ref> to preserve the local detail of the scenes. Otherwise, the curvature is small when X belongs to an untextured region; therefore, a large scale is chosen to increase the completeness. These figures demonstrate that the normal curvature can provide guidance to choose the optimal scale to improve the matching ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 IMPLEMENTATION DETAILS</head><p>Training. We applied the same image-resolution 640?512 in both DTU and BlendedMVS datasets. The preprocessing of these datasets for view selection strategy and ground-truth depth maps was provided by <ref type="bibr" target="#b38">Yao et al. (2018)</ref>. For CDS-MVSNet, we set the number of input views to 3, the number of depth hypothesis planes to {48, 32, 8} with the corresponding depth interval scales {4, 2, 1}. To train CDFSNet effectively, we used the feature loss as mentioned in Section 4.2. Besides, we implemented the scale selection step of CDSConv by first initializing the Softmax temperature by 1. This hyperparameter was then decreased over training time until it reached 0.01 to produce the sparse output for scale selection. The entire network CDS-MVSNet was trained with 30 epochs, using the SGD optimizer with an initial learning rate of 0.01, and on two NVIDIA Titan V GPU with a batch size of 6. We provided two experimental scenarios. First, we trained the model on DTU training set and then evaluated on DTU test set. This pre-trained model is then fine-tuned on BlendedMVS dataset for 15 epochs with a learning rate of 10 ?4 to evaluate on Tanks &amp; Temples. Second, we trained a single model on both DTU and BlendedMVS datasets and use this model for evaluation. This training strategy can improve the performance as indicated in <ref type="table" target="#tab_0">Table 1 and 3.</ref> Point cloud generation. To reconstruct the final point cloud, we filtered out the low confidence depths based on the probability maps estimated from the depth probability volumes of all stages . Similar to <ref type="bibr" target="#b11">(Gu et al., 2020;</ref>, we then integrated all the depth maps based on the fusion method proposed in <ref type="bibr" target="#b10">(Galliani et al., 2015;</ref><ref type="bibr" target="#b38">Yao et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 ABLATION STUDY</head><p>Effectiveness of learnable curvature. We demonstrate the advantage of our proposed learnable curvature in comparison to the original curvature . We replace the curvature estimation in Eq.5 with Eq.1 and then re-train the MVS network on the DTU dataset with identical setups. <ref type="table">Table 4</ref> presents four model variants denoted as A, B, C, and D. Each variant implements a specific version of curvature (original or learnable) and then uses the visibility aggregation with or without the curvature prior. We consider a comparison of the two groups, models A and B versus models C <ref type="figure">Figure 8</ref>: A qualitative comparison on DTU evaluation set <ref type="bibr" target="#b0">(Aanaes et al., 2016)</ref> between the learnable and original curvature. We visualize the curvature maps at the scale ? = 3 (kernel size is 3) extracted from the first four layers of CDSFNet. The first and second layers output the curvature maps at the full resolution, the third and fourth layers extract the maps at the half-resolution. The red circles denote the effect of attention mechanism, which is benefited from the learnable kernels K as presented in Section 3.2 <ref type="table">Table 4</ref>: Comparison between our learnable curvature and the original curvature . All models are trained on a subset of DTU training set, using the same setups and hyperparameters. The evaluation of depth map and pointcloud is collected on DTU validation and test set, respectively. and D. To evaluate the depth maps, we measure the percentage depth precision with the thresholds {2, 4}mm and Mean Absolute Error (MAE). To evaluate the reconstructed models, we use the accuracy, completeness, and overall metrics. We also provide the runtime and memory consumption to evaluate the computational efficiency. <ref type="table">Table 4</ref> indicates that the models with learnable curvature (C and D) achieve a better performance than the original curvature (A and B) for all evaluation metrics. The learnable curvature has a low computation cost and boosts the quality of depth and reconstruction significantly. The original curvature degrades the performance because it cannot handle the high-dimensional features. The original method treats each feature channel equally and averages all feature channels when computing the derivatives by Gaussian filters. Therefore, it may produce a high curvature estimation even though the neighboring feature vectors are visually similar to each other, as shown in <ref type="figure">Fig. 8</ref>. <ref type="figure">Fig. 8</ref> shows the normal curvatures estimated by the original method  and our learnable method. The curvature maps are extracted from the first four CDSConv layers of CDSFNet. As shown in <ref type="figure">Fig. 8</ref>, the original method provides a ground truth estimation only in the first layer when the input is a color image (i.e., the estimated curvatures are small in untextured regions while large in near-edge or rich-textured regions). However, it produces noisy estimation when the inputs are the feature maps in the subsequent layers, especially in the third and fourth layers where the curvatures do not reflect the edge or texture information. On the other hand, although the learnable curvature approximates the ground truth (the original curvature in the first layer), it consistently preserves the properties of normal curvature concerning the untextured and rich-textured regions in every layer. We further notice that our learning-based estimation takes advantage of the attention mechanism, as shown in red circles. It focuses on the rich texture region to improve the completeness of reconstruction, which is illustrated in <ref type="figure">Fig. 9</ref> in Appendix A.5. <ref type="figure">Fig. 8</ref> also implies that the estimated curvature tends to be higher at the deeper layers because it encodes a larger patch scale.</p><p>Curvature-guided visibility aggregation. We proposed a pixel-wise visibility prediction method for cost aggregation in Section 4.1. Previous works generally predicted the visibility weights only from the two-view cost volume <ref type="bibr" target="#b34">Xu &amp; Tao, 2020c)</ref>. Compared to these works,  our method additionally considers the curvature prior estimated by CDSFNet. To evaluate the effectiveness of curvature prior to visibility prediction, we compared the performance of our method with the baseline method . To implement the baseline method, we removed the curvature input from our visibility prediction network and only used the entropy information of two view cost volumes. We compared the models using curvature prior (Models B and D) with those without using it (Models A and C). <ref type="table">Table 4</ref> shows the experimental results. As shown in <ref type="table">Table 4</ref>, Model B had the worst performance in terms of overall. Because Model B produced the noisy curvature, this noisy estimation degraded the performance of Model B compared to Model A. Model D remarkably improves the depth estimation performances remarkably and achieves the similar performance of reconstruction compared to Model C without increasing runtime and memory consumption. These results demonstrate the effectiveness of the proposed learnable curvature and curvature-guided visibility prediction in the MVS process.</p><p>Analysis of CDSFNet and the cascade structure. This section first analyzes the effectiveness of proposed CDSFNet when increasing the number of candidate scales. To measure explicitly the matchability of features extracted by CDSFNet, we remove the cost-volume regularization step in the MVS architecture and then regress the depth directly from the aggregated 3D cost volume. <ref type="table" target="#tab_5">Table 5</ref> presents the results when increasing the number candidate scales N c in each CDSConv layer. When N c is equal to 1, CDSFNet is considered as Feature Pyramid Network (FPN), which is applied to most of the state-of-the-art learning-based methods <ref type="bibr" target="#b11">(Gu et al., 2020;</ref>. As shown in <ref type="table" target="#tab_5">Table 5</ref>, CDSFNet achieved better performance when using more candidate scales. However, it suffered from heavy computation. Therefore, we chose the design with two or three scales for each CDSConv (CDSFNet architecture as shown in <ref type="figure" target="#fig_1">Fig. 5</ref>), to guarantee both the performance of reconstruction and computational efficiency.</p><p>Next, we investigate the improvement of depth estimation over the cascade stages. <ref type="table" target="#tab_6">Table 6</ref> provides the accuracy of depth estimation over all stages of our CDS-MVSNet. The first three stages mainly contribute to the final quality of estimated depth. The last stage l = 3 is used for depth upsampling; it outputs the full-resolution depth while maintaining the same depth accuracy compared to the previous stage. Suppose this stage is implemented by the three-step MVS depth estimation pipeline mentioned in Section 4.1, the entire MVS network suffers from a considerable computation and cannot fit into a limited GPU memory. Therefore, we only apply the depth refinement with a 2D CNN to the last stage for efficiency in time and memory.</p><p>A.5 QUALITATIVE RESULTS ON DTU AND TANKS &amp; TEMPLES <ref type="figure">Figure 9</ref>: Qualitative results on DTU evaluation dataset. Our method achieved the high completeness of reconstructed models. The visualization of reconstructed models for DTU evaluation set are shown in the top <ref type="figure">figure.</ref> A comparison with the other state-of-the-art methods is shown in the bottom figure.   and Vis-MVSNet  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Dynamic scale according to different captured viewpoints and relative camera pose between reference and source views.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>The Unet-liked architecture of CDSFNet. It is organized into three levels: quater-resolution, haftresolution, and full-resolution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of the normal curvature maps estimated from our feature extraction CDSFNet by Eq. 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 10 :</head><label>10</label><figDesc>Qualitative results on the intermediate set of Tanks &amp; Temples. Our method achieved the high completeness of reconstruction for complex outdoor scenes. The bottom figure shows the error visualization for Recall scores compared to AttMVS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Quantitative results on the DTU evaluation dataset (lower is better).</figDesc><table><row><cell>Methods</cell><cell cols="3">Mean error distance (mm) Acc. Comp. Overall</cell></row><row><cell>Gipuma (Galliani et al., 2015)</cell><cell>0.283</cell><cell>0.873</cell><cell>0.578</cell></row><row><cell>Colmap (Sch?nberger et al., 2016)</cell><cell>0.400</cell><cell>0.664</cell><cell>0.532</cell></row><row><cell>P-MVSNet ? (Luo et al., 2019)</cell><cell>0.406</cell><cell>0.434</cell><cell>0.420</cell></row><row><cell>Fast-MVSNet ? (Yu &amp; Gao, 2020)</cell><cell>0.336</cell><cell>0.403</cell><cell>0.370</cell></row><row><cell>Vis-MVSNet ? (Zhang et al., 2020)</cell><cell>0.369</cell><cell>0.361</cell><cell>0.365</cell></row><row><cell>AttMVS  ? (Luo et al., 2020)</cell><cell>0.383</cell><cell>0.329</cell><cell>0.356</cell></row><row><cell>CasMVSNet ? (Gu et al., 2020)</cell><cell>0.325</cell><cell>0.385</cell><cell>0.355</cell></row><row><cell>UCSNet ? (Cheng et al., 2020)</cell><cell>0.338</cell><cell>0.349</cell><cell>0.344</cell></row><row><cell>PatchMatchNet ? (Wang et al., 2021)</cell><cell>0.427</cell><cell>0.277</cell><cell>0.352</cell></row><row><cell>PVA-MVSNet ? (Yi et al., 2020)</cell><cell>0.379</cell><cell>0.336</cell><cell>0.357</cell></row><row><cell>CVP-MVSNet ? (Yang et al., 2020)</cell><cell>0.296</cell><cell>0.406</cell><cell>0.351</cell></row><row><cell>BP-MVSNet  ? (Sormann et al., 2020)</cell><cell>0.333</cell><cell>0.320</cell><cell>0.327</cell></row><row><cell>Ours ? (DTU only)</cell><cell>0.352</cell><cell>0.280</cell><cell>0.316</cell></row><row><cell>Ours (DTU+BlendedMVS)</cell><cell>0.351</cell><cell>0.278</cell><cell>0.315</cell></row><row><cell cols="4">traditional method, ? trained on training set of DTU,  ? trained on a modified version of DTU</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Evaluation of estimated depth with different image resolutions on DTU (higher is better).</figDesc><table><row><cell></cell><cell>Methods</cell><cell>832 x</cell><cell cols="3">Resolutions (width x height) 1152 x 1280 x 1408 x</cell><cell>1536 x</cell></row><row><cell></cell><cell></cell><cell>576</cell><cell>832</cell><cell>960</cell><cell>1088</cell><cell>1152</cell></row><row><cell>Prec. 2mm (%)</cell><cell cols="2">CasMVSNet PatchMatchNet 69.52 76.54 UCSNet 74.11 CVP-MVSNet 66.80 Ours 72.71</cell><cell>75.35 71.23 73.83 69.75 75.04</cell><cell>74.01 71.37 72.95 70.71 75.09</cell><cell>72.22 71.20 71.58 71.03 74.91</cell><cell>70.92 70.99 70.49 71.16 74.64</cell></row><row><cell>Prec. 4mm (%)</cell><cell cols="2">CasMVSNet PatchMatchNet 78.31 82.11 UCSNet 82.18 CVP-MVSNet 75.28 Ours 80.86</cell><cell>80.47 78.32 80.10 77.41 81.38</cell><cell>79.02 78.03 78.59 78.18 80.99</cell><cell>77.32 77.67 76.91 78.68 80.48</cell><cell>76.15 77.34 75.61 78.76 80.02</cell></row></table><note>The datasets used in our evaluation are DTU (Aanaes et al., 2016), BlendedMVS (Yao et al., 2020), and Tanks &amp; Temples (Knapitsch et al., 2017). Due to the simple camera trajectory of all scenes in DTU, we additionally utilize the BlendedMVS dataset with diverse camera trajectories for training. Tanks &amp; Temples is provided as a set of video sequences in realistic environments. We train the model on the training sets of DTU and BlendedMVS, then evaluate the testing set of DTU and intermediate set of Tanks &amp; Temples. Our source code is available at https://github.com/ TruongKhang/cds-mvsnet.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance comparisons (F-score) of various reconstruction algorithms on the intermediate sequences of the Tanks &amp; Temples benchmark. The higher value of F-score implies better reconstruction results. Our method achieves the best performance in terms of mean F-score. ? only training on the training set of DTU, ? training on DTU and then fine-tuning on BlendedMVS, ? training on a modified version of DTU</figDesc><table><row><cell>Method</cell><cell>Mean</cell><cell>Family</cell><cell>Francis</cell><cell>Horse</cell><cell cols="2">Lighthouse M60</cell><cell cols="3">Panther Playground Train</cell></row><row><cell>ACMM (Xu &amp; Tao, 2019)</cell><cell>57.27</cell><cell>69.24</cell><cell>51.45</cell><cell>46.97</cell><cell>63.20</cell><cell>55.07</cell><cell>57.64</cell><cell>60.08</cell><cell>54.48</cell></row><row><cell>ACMP (Xu &amp; Tao, 2020b)</cell><cell>58.41</cell><cell>70.30</cell><cell>54.06</cell><cell>54.11</cell><cell>61.65</cell><cell>54.16</cell><cell>57.60</cell><cell>58.12</cell><cell>57.25</cell></row><row><cell>Fast-MVSNet ? (Yu &amp; Gao, 2020)</cell><cell>47.39</cell><cell>65.18</cell><cell>39.59</cell><cell>34.98</cell><cell>47.81</cell><cell>49.16</cell><cell>46.20</cell><cell>53.27</cell><cell>42.91</cell></row><row><cell>Vis-MVSNet  ? (Zhang et al., 2020)</cell><cell>60.03</cell><cell>77.40</cell><cell>60.23</cell><cell>47.07</cell><cell>63.44</cell><cell>62.21</cell><cell>57.28</cell><cell>60.54</cell><cell>52.07</cell></row><row><cell>AttMVS ? (Luo et al., 2020)</cell><cell>60.05</cell><cell>73.90</cell><cell>62.58</cell><cell>44.08</cell><cell>64.88</cell><cell>56.08</cell><cell>59.39</cell><cell>63.42</cell><cell>56.06</cell></row><row><cell>CasMVSNet ? (Gu et al., 2020)</cell><cell>56.84</cell><cell>76.37</cell><cell>58.45</cell><cell>46.26</cell><cell>55.81</cell><cell>56.11</cell><cell>54.06</cell><cell>58.18</cell><cell>49.51</cell></row><row><cell>UCSNet ? (Cheng et al., 2020)</cell><cell>54.83</cell><cell>76.09</cell><cell>53.16</cell><cell>43.03</cell><cell>54.00</cell><cell>55.60</cell><cell>51.49</cell><cell>57.38</cell><cell>47.89</cell></row><row><cell>PVA-MVSNet ? (Yi et al., 2020)</cell><cell>54.46</cell><cell>69.36</cell><cell>46.80</cell><cell>46.01</cell><cell>55.74</cell><cell>57.23</cell><cell>54.75</cell><cell>56.70</cell><cell>49.06</cell></row><row><cell>CVP-MVSNet ? (Yang et al., 2020)</cell><cell>54.03</cell><cell>76.50</cell><cell>47.74</cell><cell>36.34</cell><cell>55.12</cell><cell>57.28</cell><cell>54.28</cell><cell>57.43</cell><cell>47.54</cell></row><row><cell>BP-MVSNet  ? (Sormann et al., 2020)</cell><cell>57.60</cell><cell>77.31</cell><cell>60.90</cell><cell>47.89</cell><cell>58.26</cell><cell>56.00</cell><cell>51.54</cell><cell>58.47</cell><cell>50.41</cell></row><row><cell>Ours  ? (fine-tuning on BlendedMVS)</cell><cell>60.82</cell><cell>78.17</cell><cell>61.74</cell><cell>53.12</cell><cell>60.25</cell><cell>61.91</cell><cell>58.45</cell><cell>62.35</cell><cell>50.58</cell></row><row><cell>Ours (DTU+BlendedMVS)</cell><cell>61.58</cell><cell>78.85</cell><cell>63.17</cell><cell>53.04</cell><cell>61.34</cell><cell>62.63</cell><cell>59.06</cell><cell>62.28</cell><cell>52.30</cell></row><row><cell>traditional method,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Figure 3: Relating GPU memory and run-time to the input resolution on DTU's evaluation set. The original image resolution is 1600 ? 1200 (100%). Here we evaluate at the highest resolution 1536 ? 1152 (95.2%).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Impact of the number of candidate scales in CD-SConv layer. The models are trained on DTU training set at the resolution 320 ? 256, without 3D cost regularization. The statistics below are collected on DTU validation set. FPN) 40.87 55.95 66.72 21.41 0.114 2 44.46 58.10 67.89 18.89 0.134 3 45.27 59.14 69.17 17.46 0.155 4 46.77 60.63 70.61 16.10 0.195</figDesc><table><row><cell>#scales/layer (N c )</cell><cell>Prec. 2mm</cell><cell>Prec. 4mm</cell><cell>Prec. 8mm</cell><cell>MAE (mm)</cell><cell>Time (s)</cell></row><row><cell>1 (CDSFNet ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>The accuracy of estimated depth over different stages in our CDS-MVSNet. The entire MVS network is trained on DTU training set. The trained model is used to evaluate on the DTU test set at the resolution 1536 ? 1152</figDesc><table><row><cell>Stages</cell><cell>Prec. 1mm</cell><cell>Prec. 2mm</cell><cell>Prec. 4mm</cell><cell>MAE (mm)</cell></row><row><cell>0</cell><cell>39.99</cell><cell>61.89</cell><cell>76.35</cell><cell>11.43</cell></row><row><cell>1</cell><cell>58.01</cell><cell>72.92</cell><cell>80.21</cell><cell>10.80</cell></row><row><cell>2</cell><cell>63.97</cell><cell>75.43</cell><cell>80.92</cell><cell>10.67</cell></row><row><cell>3</cell><cell>64.26</cell><cell>75.52</cell><cell>80.94</cell><cell>10.62</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by the National Research Foundation of Korea (NRF) funded by the Ministry of Education under Grant 2016R1D1A1B01013573.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Large-scale data for multiple-view stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Aanaes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Rasmus Ramsb?l Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Engin</forename><surname>Vogiatzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders Bjorholm</forename><surname>Tola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="168" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Patchmatch: A randomized correspondence algorithm for structural image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Point-based multi-view stereo network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1538" to="1547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Visibility-aware point-based multi-view stereo network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="3695" to="3708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamic convolution: Attention over convolution kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11030" to="11039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep stereo using adaptive thin volume representation with uncertainty awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zexiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><forename type="middle">Erran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2524" to="2534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Differential geometry of curves and surfaces: revised and updated second edition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manfredo P Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carmo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Courier</title>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Dover Publications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multi-view stereo: A tutorial. Foundations and Trends? in Computer Graphics and Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Hern?ndez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Massively parallel multiview stereopsis by surface normal diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvano</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="873" to="881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cascade cost volume for high-resolution multi-view stereo and stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuozhuo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feitong</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2495" to="2504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Face recognition with contrastive convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunrui</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuzhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="118" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic multi-scale filters for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongying</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3562" to="3572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Depth map super-resolution by deep multi-scale guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="353" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rkE3y85ee" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Surfacenet: An end-to-end 3d neural network for multiview stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2307" to="2315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="667" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning a multi-view stereo machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>H?ne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05375</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tanks and temples: Benchmarking large-scale scene reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arno</forename><surname>Knapitsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scale-space theory: A basic tool for analyzing structures at different scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of applied statistics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="225" to="270" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">P-mvsnet: Learning patch-wise matching confidence aggregation for multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haipeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10452" to="10461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention-aware multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuesong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1590" to="1599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computerassisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Structure-from-motion revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Schonberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4104" to="4113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pixelwise view selection for unstructured multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enliang</forename><surname>Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="501" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bp-mvsnet: Belief-propagation-layers for multi-view-stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Sormann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Kn?belreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattia</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedrich</forename><surname>Fraundorfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="394" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Patchmatchnet: Learned multi-view patchmatch stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangjinhua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvano</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Speciale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14194" to="14203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dynamic filtering with large sampling field for convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandrajit</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="185" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-scale geometric consistency guided multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5483" to="5492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning inverse depth regression for multi-view stereo with correlation cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12508" to="12515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Planar prior assisted patchmatch multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12516" to="12523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Pvsnet: Pixelwise visibility-aware multi-view stereo network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.07714</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Marmvs: Matching ambiguity reduced multiple view stereo for efficient large scale scene reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiguang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunan</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5981" to="5990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mvscrf: Learning multi-view stereo with conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youze</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiansheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weitao</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4312" to="4321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cost volume pyramid based depth inference for multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaomiao</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4877" to="4886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mvsnet: Depth inference for unstructured multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="767" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Blendedmvs: A large-scale dataset for generalized multi-view stereo networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufan</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1790" to="1799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pyramid multi-view stereo net with self-adaptive view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhuang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runze</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="766" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fast-mvsnet: Sparse-to-dense multi-view stereo with learned propagation and gauss-newton refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1949" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Visibility-aware multi-view stereo network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scale-adaptive convolutions for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jintao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2031" to="2039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9308" to="9316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">The image surface can be represented by s(x, y) =</title>
		<editor>Xu et al.</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>First, we re-derive Eq. 1 proposed by. x, y, I(x, y)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
