<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking Keypoint Representations: Modeling Keypoints and Poses as Objects for Multi-Person Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Mcnally</surname></persName>
							<email>wmcnally@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Systems Design Engineering</orgName>
								<orgName type="department" key="dep2">Waterloo Artificial Intelligence Institute</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Rethinking Keypoint Representations: Modeling Keypoints and Poses as Objects for Multi-Person Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Human Pose Estimation</term>
					<term>Object Detection</term>
					<term>YOLO</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>0000?0002?7187?7147] , Kanav Vats [0000?0002?9515?7259] , Alexander Wong [0000?0002?5295?2797] , and John McPhee [0000?0003?3908?9519]</p><p>Abstract. In keypoint estimation tasks such as human pose estimation, heatmap-based regression is the dominant approach despite possessing notable drawbacks: heatmaps intrinsically suffer from quantization error and require excessive computation to generate and post-process. Motivated to find a more efficient solution, we propose to model individual keypoints and sets of spatially related keypoints (i.e., poses) as objects within a dense single-stage anchor-based detection framework. Hence, we call our method KAPAO (pronounced "Ka-Pow"), for Keypoints And Poses As Objects. KAPAO is applied to the problem of single-stage multi-person human pose estimation by simultaneously detecting human pose and keypoint objects and fusing the detections to exploit the strengths of both object representations. In experiments we observe that KAPAO is faster and more accurate than previous methods, which suffer greatly from heatmap post-processing. The accuracy-speed trade-off is especially favourable in the practical setting when not using test-time augmentation. Source code: https://github.com/wmcnally/kapao.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Keypoint estimation is a computer vision task that involves localizing points of interest in images. It has emerged as one of the most highly researched topics in the computer vision literature <ref type="bibr">[1, 9, 11, 15, 18, 19, 37-40, 47, 49, 55, 60, 63, 67, 70]</ref>. The most common method for estimating keypoint locations involves generating target fields, referred to as heatmaps, that center 2D Gaussians on the target keypoint coordinates. Deep convolutional neural networks <ref type="bibr">[26]</ref> are then used to regress the target heatmaps on the input images, and keypoint predictions are made via the arguments of the maxima of the predicted heatmaps <ref type="bibr" target="#b34">[57]</ref>.</p><p>While strong empirical results have positioned heatmap regression as the de facto standard method for detecting and localizing keypoints <ref type="bibr">[3, 5-7, 12, 24, 35, 37, 43, 54, 57, 66, 68]</ref>, there are several known drawbacks. First, these methods suffer from quantization error: the precision of a keypoint prediction is inherently  <ref type="bibr" target="#b11">[12]</ref>, HigherHRNet <ref type="bibr" target="#b6">[7]</ref>, HigherHR-Net + SWAHR <ref type="bibr">[35]</ref>, and CenterGroup <ref type="bibr" target="#b2">[3]</ref> without test-time augmentation (TTA), i.e., excluding multi-scale testing and horizontal flipping. The raw data are provided in <ref type="table" target="#tab_1">Table 1</ref>. The circle size is proportional to the number of model parameters.</p><p>limited by the spatial resolution of the output heatmap. Larger heatmaps are therefore advantageous, but require additional upsampling operations and costly processing at higher resolution <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr">35,</ref><ref type="bibr">37]</ref>. Even when large heatmaps are used, special post-processing steps are required to refine keypoint predictions, slowing down inference <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr">35,</ref><ref type="bibr">43]</ref>. Second, when two keypoints of the same type (i.e., class) appear in close proximity to one another, the overlapping heatmap signals may be mistaken for a single keypoint. Indeed, this is a common failure case <ref type="bibr" target="#b4">[5]</ref>. For these reasons, researchers have started to investigate alternative, heatmap-free keypoint detection methods <ref type="bibr">[27,</ref><ref type="bibr">29,</ref><ref type="bibr">30,</ref><ref type="bibr">38,</ref><ref type="bibr" target="#b44">67]</ref>.</p><p>In this paper, we introduce a new heatmap-free keypoint detection method and apply it to single-stage multi-person human pose estimation. Our method builds on recent research showing how keypoints can be modeled as objects within a dense anchor-based detection framework by representing keypoints at the center of small keypoint bounding boxes <ref type="bibr">[38]</ref>. In preliminary experimentation with human pose estimation, we found that this keypoint detection approach works well for human keypoints that are characterized by local image features (e.g., the eyes), but the same approach is less effective at detecting human keypoints that require a more global understanding (e.g., the hips). We therefore introduce a new pose object representation to help detect sets of keypoints that are spatially related. Furthermore, we detect keypoint objects and pose objects simultaneously and fuse the results using a simple matching algorithm to exploit the benefits of both object representations. By virtue of detecting pose objects, we unify person detection and keypoint estimation and provide a highly efficient single-stage approach to multi-person human pose estimation.</p><p>As a result of not using heatmaps, KAPAO compares favourably against recent single-stage human pose estimation models in terms of accuracy and inference speed, especially when not using test-time augmentation (TTA), which represents how such models are deployed in practice. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, KAPAO achieves an AP of 70.6 on the Microsoft COCO Keypoints validation set without TTA while having an average latency of 54.4 ms (forward pass + post-processing time). Compared to the state-of-the-art single-stage model High-erHRNet + SWAHR <ref type="bibr">[35]</ref>, KAPAO is 5.1? faster and 3.3 AP more accurate when not using TTA. Compared to CenterGroup <ref type="bibr" target="#b2">[3]</ref>, KAPAO is 3.1? faster and 1.5 AP more accurate. The contributions of this work are summarized as follows:</p><p>-A new pose object representation is proposed that extends the conventional object representation by including a set of keypoints associated with the object. -A new approach to single-stage human pose estimation is developed by simultaneously detecting keypoint objects and pose objects and fusing the detections. The proposed heatmap-free method is significantly faster and more accurate than state-of-the-art heatmap-based methods when not using TTA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Heatmap-free keypoint detection. DeepPose <ref type="bibr" target="#b35">[58]</ref> regressed keypoint coordinates directly from images using a cascade of deep neural networks that iteratively refined the keypoint predictions. Shortly thereafter, Tompson et al. <ref type="bibr" target="#b34">[57]</ref> introduced the notion of keypoint heatmaps, which have since remained prevalent in human pose estimation <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr">24,</ref><ref type="bibr">37,</ref><ref type="bibr">43,</ref><ref type="bibr" target="#b31">54,</ref><ref type="bibr" target="#b42">65,</ref><ref type="bibr" target="#b43">66,</ref><ref type="bibr">68</ref>] and other keypoint detection applications <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b36">59,</ref><ref type="bibr" target="#b40">63]</ref>. Remarking the computational inefficiencies associated with generating heatmaps, Li et al.</p><p>[30] disentangled the horizontal and vertical keypoint coordinates such that each coordinate was represented using a one-hot encoded vector. This saved computation and permitted an expansion of the output resolution, thereby reducing the effects of quantization error and eliminating the need for refinement post-processing. Li et al.</p><p>[27] introduced the residual log-likelihood (RLE), a novel loss function for direct keypoint regression based on normalizing flows <ref type="bibr" target="#b30">[53]</ref>. Direct keypoint regression has also been attempted using Transformers <ref type="bibr">[29]</ref>. Outside the realm of human pose estimation, Xu et al. <ref type="bibr" target="#b44">[67]</ref> regressed anchor templates of facial keypoints and aggregated them to achieve state-of-the-art accuracy in facial alignment. In sports analytics, McNally et al.</p><p>[38] encountered the issue of overlapping heatmap signals in the development of an automatic scoring system for darts and therefore opted to model keypoints as objects using small square bounding boxes. This keypoint representation proved to be highly effective and serves as the inspiration for this work. Single-stage human pose estimation. Single-stage human pose estimation methods predict the poses of every person in an image using a single forward pass <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr">25,</ref><ref type="bibr">42,</ref><ref type="bibr">44]</ref>. In contrast, two-stage methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr">24,</ref><ref type="bibr">27,</ref><ref type="bibr">37,</ref><ref type="bibr" target="#b23">46,</ref><ref type="bibr" target="#b31">54,</ref><ref type="bibr" target="#b43">66]</ref> first detect the people in an image using an off-the-shelf person detector (e.g., Faster R-CNN <ref type="bibr" target="#b29">[52]</ref>, YOLOv3 <ref type="bibr" target="#b28">[51]</ref>, etc.) and then estimate poses for each detection. Single-stage methods are generally less accurate, but usually perform better in crowded scenes <ref type="bibr">[28]</ref> and are often preferred because of their simplicity and efficiency, which becomes particularly favourable as the number of people in the image increases. Single-stage approaches vary more in their design compared to two-stage approaches. For instance, they may: (i) detect all the keypoints in an image and perform a bottom-up grouping into human poses <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr">25,</ref><ref type="bibr">35,</ref><ref type="bibr">42,</ref><ref type="bibr" target="#b25">48]</ref>; (ii) extend object detectors to unify person detection and keypoint estimation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr">36,</ref><ref type="bibr" target="#b41">64,</ref><ref type="bibr">70]</ref>; or (iii) use alternative keypoint/pose representations (e.g., predicting root keypoints and relative displacements <ref type="bibr" target="#b11">[12,</ref><ref type="bibr">44,</ref><ref type="bibr" target="#b22">45]</ref>). We briefly summarize the most recent state-of-the-art single-stage methods below.</p><p>Cheng et al. <ref type="bibr" target="#b6">[7]</ref> repurposed HRNet <ref type="bibr" target="#b31">[54]</ref> for bottom-up human pose estimation by adding a transpose convolution to double the output heatmap resolution (HigherHRNet) and using associative embeddings [42] for keypoint grouping. They also implemented multi-resolution training to address the scale variation problem. Geng et al. <ref type="bibr" target="#b11">[12]</ref> predicted person center heatmaps and 2K offset maps representing offset vectors for the K keypoints of a pose candidate centered on each pixel using an HRNet backbone. They also disentangled the keypoint regression (DEKR) using separate regression heads and adaptive convolutions. Luo et al.</p><p>[35] used HigherHRNet as a base and proposed scale and weight adaptive heatmap regression (SWAHR), which scaled the ground-truth heatmap Gaussian variances based on the person scale and balanced the foreground/background loss weighting. Their modifications provided significant accuracy improvements over HigherHRNet and comparable performance to many two-stage methods. Again using HigherHRNet as a base, Bras? et al. <ref type="bibr" target="#b2">[3]</ref> proposed CenterGroup to match keypoints to person centers using a fully differentiable self-attention module that was trained end-to-end together with the keypoint detector. Notably, all of the aforementioned methods suffer from costly heatmap post-processing and as such, their inference speeds leave much to be desired.</p><p>Extending object detectors for human pose estimation. There is significant overlap between the tasks of object detection and human pose estimation. For instance, He et al. <ref type="bibr" target="#b13">[14]</ref> used the Mask R-CNN instance segmentation model for human pose estimation by predicting keypoints using one-hot masks. Wei et al. adapted the FCOS <ref type="bibr" target="#b33">[56]</ref> object detector with FCPose using dynamic filters <ref type="bibr" target="#b20">[21]</ref>. While these methods based on object detectors provide good efficiency, their accuracies have not competed with state-of-the-art heatmap-based methods. Our work is most similar to Point-Set Anchors <ref type="bibr" target="#b41">[64]</ref>; however, our method does not require defining data-dependent pose anchors. Moreover, we simultaneously detect individual keypoints and poses and fuse the detections to improve the accuracy of our final pose predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">KAPAO: Keypoints and Poses as Objects</head><p>KAPAO uses a dense detection network to simultaneously predict a set of keypoint objects {? k ?? k } and a set of pose objects {? p ?? p }, collectivel? O =? k ? p . We introduce the concept behind each object type and the relevant notation below. All units are assumed to be in pixels unless stated otherwise.</p><p>A keypoint object O k is an adaptation of the conventional object representation in which the coordinates of a keypoint are represented at the center</p><formula xml:id="formula_0">(b x , b y ) of a small bounding box b with equal width b w and height b h : b = (b x , b y , b w , b h ).</formula><p>The hyperparameter b s controls the keypoint bounding box size (i.e., b s = b w = b h ). There are K classes of keypoint objects, one for each type in the dataset <ref type="bibr">[38]</ref>.</p><p>Generally speaking, a pose object O p is considered to be an extension of the conventional object representation that additionally includes a set of keypoints associated with the object. While we expect pose objects to be useful in related tasks such as facial and object landmark detection <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b44">67]</ref>, they are applied herein to human pose estimation via detection of human pose objects, comprising a bounding box of class "person," and a set of keypoints z = {(x k , y k )} K k=1 that coincide with anatomical landmarks.</p><p>Both object representations possess unique advantages. Keypoint objects are specialized for the detection of individual keypoints that are characterized by strong local features. Examples of such keypoints that are common in human pose estimation include the eyes, ears and nose. However, keypoint objects carry no information regarding the concept of a person or pose. If used on their own for multi-person human pose estimation, a bottom-up grouping method would be required to parse the detected keypoints into human poses. In contrast, pose objects are better suited for localizing keypoints with weak local features as they enable the network to learn the spatial relationships within a set of keypoints. Moreover, they can be leveraged for multi-person human pose estimation directly without the need for bottom-up keypoint grouping.</p><p>Recognizing that keypoint objects exist in a subspace of a pose objects, the KAPAO network was designed to simultaneously detect both object types with minimal computational overhead using a single shared network head. During inference, the more precise keypoint object detections are fused with the human pose detections using a simple tolerance-based matching algorithm that improves the accuracy of the human pose predictions without sacrificing any significant amount of inference speed. The following sections provide details on the network architecture, the loss function used to train the network, and inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architectural Details</head><p>A diagram of the KAPAO pipeline is provided in <ref type="figure">Figure 2</ref> (1)</p><formula xml:id="formula_1">? ? 8 ? 16 ? 32 ? 64 NMS(O p ) NMS(O k ) ?(O p ' , O k ' ) I G ? ? ? ? ? ? ? ? ? (G,G) ? P ? Fig. 2.</formula><p>KAPAO uses a dense detection network N trained using the multi-task loss L to map an RGB image I to a set of output grids? containing the predicted pose objects? p and keypoint objects? k . Non-maximum suppression (NMS) is used to obtain candidate detections? p? and? k? , which are fused together using a matching algorithm ? to obtain the final human pose predictionsP. The Na and No dimensions in? are not shown for clarity.</p><p>N a is the number of anchor channels and N o is the number of output channels for each object. N is a YOLO-style feature extractor that makes extensive use of Cross-Stage-Partial (CSP) bottlenecks <ref type="bibr" target="#b39">[62]</ref> within a feature pyramid [31] macroarchitecture. To provide flexibility for different speed requirements, three sizes of KAPAO models were trained (i.e., KAPAO-S/M/L) by scaling the number of layers and channels in N . Due to the nature of strided convolutions, the features in an output grid cell G s i,j are conditioned on the image patch I p = I si:s(i+1),sj:s(j+1) . Therefore, if the center of a target object (b x , b y ) is situated in I p , the output grid cell? s i,j is responsible for detecting it. The receptive field of an output grid increases with s, so smaller output grids are better suited for detecting larger objects.</p><p>The output grid cells? s i,j contain N a anchor channels corresponding to anchor boxes A s = {(A wa , A ha )} Na a=1 . A target object O is assigned to an anchor channel via tolerance-based matching of the object and anchor box sizes. This provides redundancy such that the grid cells? s i,j can detect multiple objects and enables specialization for different object sizes and shapes. Additional detection redundancy is provided by also allowing the neighbouring grid cells? s i?1,j and G s i,j?1 to detect an object in I p [23, <ref type="bibr" target="#b38">61]</ref>. The N o output channels of? s i,j,a contain the properties of a predicted ob-ject?, including the objectnessp o (the probability that an object exists),</p><formula xml:id="formula_2">the intermediate bounding boxest ? = (t ? x ,t ? y ,t ? w ,t ? h ), the object class score? c = (? 1 , ...,? K+1 ), and the intermediate keypointsv ? = {(v ? xk ,v ? yk )} K k=1</formula><p>for the human pose objects. Hence, N o = 3K + 6.</p><p>Following <ref type="bibr">[23,</ref><ref type="bibr" target="#b38">61]</ref>, an object's intermediate bounding boxt is predicted in the grid coordinates and relative to the grid cell origin (i, j) using:</p><formula xml:id="formula_3">t x = 2?(t ? x ) ? 0.5t y = 2?(t ? y ) ? 0.5 (2) t w = A w s (2?(t ? w )) 2t h = A h s (2?(t ? h )) 2 . (3) t x t x v y1 v x1 t h t y t h t w t y t w grid cell target human pose object y = p o , t x , t y , t w , t h , c 1 , ... , c K+1 , v x1 , v y1 , ... , v xK , v yK keypoint object y = 1, 0.2, 0.7, 5.5, 8.8, 1, 0, ..., 0, -0.4, 3.7, ..., 0.6, -4.1 p o t x t y t w t h c 1 c 2 c K+1 v x1 v y1 v xK v yK y = 1, 0.3, 0.7, 0.4, 0.4, 0, 0, ..., 1, ..., 0, ?, ?, ... ?, ? p o t x t y t w t h c 1 c 2 c 15 c K+1 v x1 v y1 v xK v yK (left knee) (right ankle)</formula><p>no object y = 0, ?, ?, ..., ?, ? This detection strategy is extended to the keypoints of a pose object. A pose object's intermediate keypointsv are predicted in the grid coordinates and relative to the grid cell origin (i, j) using:</p><formula xml:id="formula_4">p o t x t y v xK v yK (nose)</formula><formula xml:id="formula_5">v xk = A w s (4?(v ? xk ) ? 2)v yk = A h s (4?(v ? yk ) ? 2).<label>(4)</label></formula><p>The sigmoid function ? facilitates learning by constraining the ranges of the object properties (e.g.,v xk andv yk are constrained to ?2 Aw s and ?2 A h s , respectively). To learnt andv, losses are applied in the grid space. Sample targets t and v are shown in <ref type="figure" target="#fig_3">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Loss Function</head><p>A target set of grids G is constructed and a multi-task loss L(?, G) is applied to learn the objectnessp o (L obj ), the intermediate bounding boxest (L box ), the class scores? (L cls ), and the intermediate pose object keypointsv (L kps ). The loss components are computed for a single image as follows:</p><formula xml:id="formula_6">L obj = s ? s n(G s ) G s BCE(p o , p o ? IoU(t, t))<label>(5)</label></formula><formula xml:id="formula_7">L box = s 1 n(O ? G s ) O?G s 1 ? IoU(t, t)<label>(6)</label></formula><formula xml:id="formula_8">L cls = s 1 n(O ? G s ) O?G s BCE(?, c) (7) L kps = s 1 n(O p ? G s ) O p ?G s K k=1 ?(? k &gt; 0)||v k ? v k || 2<label>(8)</label></formula><p>where ? s is the grid weighting, BCE is the binary cross-entropy, IoU is the complete intersection over union (CIoU) <ref type="bibr">[69]</ref>, and ? k are the keypoint visibility flags. When G s i,j,a represents a target object O, the target objectness p o = 1 is multiplied by the IoU to promote specialization amongst the anchor channel predictions <ref type="bibr" target="#b27">[50]</ref>. When G s i,j,a is not a target object, p o = 0. In practice, the losses are applied over a batch of images using batched grids. The total loss L is the weighted summation of the loss components scaled by the batch size N b :</p><formula xml:id="formula_9">L = N b (? obj L obj + ? box L box + ? cls L cls + ? kps L kps ).<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inference</head><p>The predicted intermediate bounding boxest and keypointsv are mapped back to the original image coordinates using the following transformation:</p><formula xml:id="formula_10">b = s(t + [i, j, 0, 0])? k = s(v k + [i, j]).<label>(10)</label></formula><p>G s i,j,a represents a positive pose object detection? p if its confidencep o ? max(?) is greater than a threshold ? cp and arg max(?) = 1. Similarly,? s i,j,a represents a positive keypoint object detection? k ifp o ? max(?) &gt; ? ck and arg max(?) &gt; 1, where the keypoint object class is arg max(?)?1. To remove redundant detections and obtain the candidate pose objects? p? and the candidate keypoint object? O k? , the sets of positive pose object detections? p and positive keypoint object detections? p are filtered using non-maximum suppression (NMS) applied to the pose object bounding boxes with the IoU thresholds ? bp and ? bk :</p><formula xml:id="formula_11">O p? = NMS(? p , ? bp )? k? = NMS(? k , ? bk ).<label>(11)</label></formula><p>It is noted that ? ck and ? bk are scalar thresholds used for all keypoint object classes. Finally, the human pose predictionsP = {P i ? R K?3 } for i ? {1...n(? p? )} are obtained by fusing the candidate keypoint objects with the candidate pose objects using a distance tolerance ? f d . To promote correct matches of keypoint objects to poses, the keypoint objects are only fused to pose objects with confidencep o ? max(?) &gt; ? f c :</p><formula xml:id="formula_12">P = ?(? p? ,? k? , ? f d , ? f c ).<label>(12)</label></formula><p>The keypoint object fusion function ? is defined in Algorithm 1, where the following notation is used to index an object's properties:x =? x (e.g., a pose object's keypoints? are referenced as? p z ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Limitations</head><p>A limitation of KAPAO is that pose objects do not include individual keypoint confidences, so the human pose predictions typically contain a sparse set of keypoint confidencesP i [:, 3] populated by the fused keypoint objects (see Algorithm 1 for details). If desired, a complete set of keypoint confidences can be induced by only using keypoint objects, which is realized when ? ck ? 0. Another limitation is that training requires a considerable amount of time and GPU memory due to the large input size used. KAPAO-S, M, and L, respectively. Validation was performed after every epoch, saving the model weights that provided the highest validation AP.</p><formula xml:id="formula_13">Algorithm 1: Keypoint object fusion (?) Input:? p? ,? k? , ? f d , ? f c Output:P if n(? p? ) &gt; 0 then P ? {0K?3 | ? {1...n(? p? )}} // initialize poses ? ? {0 | ? {1...n(? p? )}} // initialize pose confidences for (i,? p ) ? enumerate(? p? ) do ?i =? p po ? max(? p c ) for k ? {1...K} d? Pi[k] ? (? p x k ,? p y k , 0) // assign pose object keypoint? P * ? {Pi ?P | ?i &gt; ? f c } // poses above confidence threshold if n(P * ) &gt; 0 ? n(? k? ) &gt; 0 then for? k ?? k? do k ? arg max(? k c ) ? 1 // keypoint index C k ?? k po max(? k c ) // keypoint object confidence di ? ||P * i [k, [1, 2]] ? (? k bx ,? k by )||2 // distances m ? arg min(d) // match index if dm &lt; ? f d ?P * m [k, 3] &lt; C k then P * m [k] = (? k bx ,? k by , C k ) //</formula><p>Testing. The six inference parameters (? cp , ? ck , ? bp , ? bk , ? f d , and ? f c ) were manually tuned on the validation set using a coarse grid search to maximize accuracy. The results were not overly sensitive to the inference parameter values. When using TTA, the input image was scaled by factors of 0.8, 1, and 1.2, and the unscaled image was horizontally flipped. During post-processing, the multiscale detections were concatenated before running NMS. When not using TTA, rectangular input images were used (i.e., 1280 px on the longest side), which marginally reduced the accuracy but increased the inference speed.</p><p>Results. FLOPs are not only a poor indication of inference speed <ref type="bibr" target="#b7">[8]</ref>, but they are also only computed for the forward pass of the network and thus do not provide an indication of the amount of computation required for post-processing.</p><p>Due to expensive heatmap refinement, the post-processing times of High-erHRNet, HigherHRNet + SWAHR, and DEKR are at least an order of magnitude greater than KAPAO-L when not using TTA. The post-processing time of KAPAO depends less on the input size so it only increases by approximately 1 ms when using TTA. Conversely, HigherHRNet and HigherHRNet + SWAHR generate and refine large heatmaps during multi-scale testing and therefore require more than two orders of magnitude more post-processing time than KAPAO-L.</p><p>CenterGroup requires significantly less post-processing time than HigherHR-Net and DEKR because it skips heatmap refinement and directly encodes pose center and keypoint heatmaps as embeddings that are fed to an attention-based grouping module. When not using TTA, CenterGroup-W48 provides an improvement of 2.5 AP over HigherHRNet-W48 and has a better accuracy-speed tradeoff. Still, KAPAO-L is 3.1? faster than CenterGroup-W48 and 1.5 AP more accurate due to its efficient network architecture and near cost-free post-processing. When using TTA, KAPAO-L is 1.7 AP less accurate than CenterGroup-W48, but 4.9? faster. KAPAO-L also achieves state-of-the-art AR, which is indicative of better detection rates.</p><p>We suspect that KAPAO is more accurate without TTA compared to previous methods because it uses larger input images; however, we emphasize that KAPAO consumes larger input sizes while still being faster than previous methods due to its well-designed network architecture and efficient post-processing. For the same reason, TTA (multi-scale testing in particular) doesn't provide as much of a benefit; input sizes &gt;1280 are less effective due to the dataset images being limited to 640 px.</p><p>In <ref type="table" target="#tab_2">Table 2</ref>, the accuracy of KAPAO is compared to single-stage and twostage methods on test-dev. KAPAO-L achieves state-of-the-art AR and falls within 1.7 AP of the best performing single-stage method HigherHRNet-W48 + SWAHR while being 7.4? faster. Notably, KAPAO-L is more accurate than the early two-stage methods G-RMI <ref type="bibr" target="#b23">[46]</ref> and RMPE <ref type="bibr" target="#b9">[10]</ref> and popular single-stage methods like OpenPose <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, Associative Embeddings [42], and PersonLab <ref type="bibr" target="#b22">[45]</ref>. Compared to other single-stage methods that extend object detectors for human pose estimation (Mask R-CNN <ref type="bibr" target="#b13">[14]</ref>, CenterNet [70], Point-Set Anchors <ref type="bibr" target="#b41">[64]</ref>, and FCPose [36]), KAPAO-L is considerably more accurate. Among all the singlestage methods, KAPAO-L achieves state-of-the-art AP at an OKS threshold of 0.50, which is indicative of better detection rates but less precise keypoint localization. This is an area to explore in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Lat. (ms) AP AP .50 AP .75 AP M AP L AR G-RMI <ref type="bibr">[</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">CrowdPose</head><p>KAPAO was trained on the trainval split with 12k images and was evaluated on the 8k images in test. The same training and inference settings as on COCO were used except the models were trained for 300 epochs and no validation was performed during training. The final model weights were used for testing. <ref type="table" target="#tab_3">Table 3</ref> compares the accuracy of KAPAO against state-of-the-art methods. It was found that KAPAO excels in the presence of occlusion, achieving competitive results across all metrics compared to previous single-stage methods and state-of-theart accuracy for AP .50 . The proficiency of KAPAO in crowded scenes is clear when analyzing AP E , AP M , and AP H : KAPAO-L and DEKR-W48 <ref type="bibr" target="#b11">[12]</ref> perform equally on images with easy Crowd Index (less occlusion), but KAPAO-L is 1.1 AP more accurate for both medium and hard Crowd Indices (more occlusion). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>The influence of the keypoint bounding box size b s , one of KAPAO's important hyperparameters, was empirically analyzed. Five KAPAO-S models were trained on COCO train2017 for 50 epochs using normalized keypoint bounding box sizes b s /max(w, h) ? {0.01, 0.025, 0.05, 0.075, 0.1}. The validation AP is plotted in <ref type="figure">Figure 4</ref> (left is fast; the added post-processing time per image is ? 1.7 ms on COCO and ? 4.5 ms on CrowdPose. Relative to the time required for the forward pass of the network (see <ref type="table" target="#tab_1">Table 1</ref>), these are small increases. The fusion of keypoint objects by class is also studied. <ref type="figure">Figure 4</ref> (right) plots the fusion rates for each keypoint type for KAPAO-S with no TTA on COCO val2017. The fusion rate is equal to the number of fused keypoint objects divided by the number of keypoints of that type in the dataset. Because the number of human pose predictions is generally greater than the actual number of person instances in the dataset, the fusion rate can be greater than 1. As originally hypothesized, keypoints that are characterized by distinct local image features (e.g., the eyes, ears, and nose) have higher fusion rates as they are detected more precisely as keypoint objects than as pose objects. Conversely, keypoints that require a more global understanding (e.g., the hips) are better detected using pose objects, as evidenced by lower fusion rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper presents KAPAO, a heatmap-free keypoint estimation method based on modeling keypoints and poses as objects. KAPAO is effectively applied to the problem of single-stage multi-person human pose estimation by detecting human pose objects. Moreover, fusing jointly detected keypoint objects improves the accuracy of the predicted human poses with minimal computational overhead. When not using test-time augmentation, KAPAO is significantly faster and more accurate than previous single-stage methods, which are impeded greatly by heatmap post-processing and bottom-up keypoint grouping. Moreover, KAPAO performs well in the presence of heavy occlusion as evidenced by competitive results on CrowdPose. 68. Yang, S., Quan, Z., Nie, M., Yang, W.: Transpose: Keypoint localization via transformer. In: ICCV (2021) 69. Zheng, Z., Wang, P., Liu, W., Li, J., Ye, R., Ren, D.: Distance-IoU loss: Faster and better learning for bounding box regression. In: AAAI (2020) 70. Zhou, X., Wang, D., Kr?henb?hl, P.: Objects as points. arXiv preprint arXiv:1904.07850 (2019)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Hyperparameters</head><p>For convenience, the KAPAO hyperparameters used to generate the results in this paper are provided in <ref type="table" target="#tab_6">Table 5</ref> in the order they appear in the text. Other hyperparameters not referenced in the text (e.g., augmentation settings) are included in the code. Many of the hyperparameters are inherited from [23], where an evolutionary algorithm was used to search for optimal values for object detection on COCO. Some hyperparameters, such as the keypoint bounding box size b s and the keypoint loss weight ? kps , were manually tuned using a small grid search. The influence of b s is studied in Section 4.3. Relative to KAPAO-L, the number of layers and channels in KAPAO-M were scaled by 2/3 and 3/4, respectively. Similarly, the number of layers and channels in KAPAO-S were scaled by 1/3 and 1/2, respectively.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Influence of input size on accuracy and speed</head><p>The trade-off between accuracy and inference speed was investigated for various input sizes. The AP on COCO val2017 was computed without TTA for max(w, h) ? {640, 768, 896, 1024, 1152, 1280}. and KAPAO-L, using an input size of 1024 reduced the accuracy marginally but also reduced the latency by ?30%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Video Inference Demos</head><p>The source code includes five video inference demos. The first four demos run inference on RGB video clips sourced from YouTube to demonstrate the practical use of KAPAO under various inference settings. The final demo demonstrates the generalization of KAPAO by running inference on a depth video converted to RGB. All reported inference speeds include all processing (i.e., including image loading, resizing, inference, graphics plotting, etc.).</p><p>Shuffle. KAPAO runs fastest on low resolution video with few people in the frame. This demo runs KAPAO-S on a single-person 480p dance video using an input size of 1024. The inference speed is ?9.5 FPS on a workstation CPU (Intel Core i7-8700K), and ?65 FPS on the TITAN Xp GPU. A screenshot of the inference is provided in <ref type="figure" target="#fig_8">Figure 6</ref> (top-left).</p><p>Flash Mob. KAPAO-S was run on a 720p flash mob dance video using an input size of 1280. A screenshot of the inference is shown in <ref type="figure" target="#fig_8">Figure 6</ref> (top-right). On a workstation housing a TITAN Xp GPU, the inference speed was ?35 FPS. Squash. KAPAO-S was run on a 1080p slow-motion squash video using an input size of 1280. A simple player tracking algorithm was implemented based on the frame-to-frame pose differences. The inference speed was ?22 FPS on the TITAN Xp GPU. A screenshot is provided in <ref type="figure" target="#fig_8">Figure 6</ref> (bottom-right).</p><p>Depth Videos. Finally, the robustness and generalization capabilities of KA-PAO are demonstrated by running inference with KAPAO-S on depth videos obtained from a fencing action recognition dataset <ref type="bibr">[A1]</ref>. The depth information was converted to RGB format in a 480p video. A screenshot from the inference video, which ran at ?60 FPS on the TITAN Xp GPU, is displayed in <ref type="figure" target="#fig_9">Figure 7</ref>. Despite the marked difference in appearance between the depth images and the KAPAO training images, human poses were still detected with high confidence.</p><p>This interesting test result can be attributed to pose object representation learning, where spatial relations between human keypoints are learned using largescale features and high-level context (e.g., like the edges making up the human shape). This is further supported by the fact that fewer keypoint objects were detected in the depth images. Zhu et al. <ref type="bibr">[A2]</ref> use KAPAO to extract 2D pose information from depth videos and predict fine-grained footwork in fencing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Error Analysis</head><p>While the AP and AR metrics are robust and perceptually meaningful (i.e., algorithms with higher AP/AR are generally more accurate), they can hide the underlying causes of error and are not sufficient for truly understanding an algorithm's behaviour. For example, the results presented in the previous section showed that KAPAO consistently provides higher AR than previous single-stage methods and higher AP at lower OKS thresholds (e.g., AP .50 ). The exact cause for this result cannot be understood through analysis of the AP/AR metrics alone, but a potential explanation is that KAPAO provides more precise person/pose detection (i.e., more true positives and fewer false positives and false negatives), but less precise keypoint localization. To investigate this further, this section provides a more in-depth analysis of the error using the error taxonomy and analysis code provided by Ronchi and Perona [A3]. Ronchi and Perona propose an error taxonomy for multi-person human pose estimation on COCO that includes four error categories: Background False Positives, False Negatives, Scoring, and Localization. Scoring errors are due to suboptimal confidence score assignment; they occur when two detections are in the proximity of a ground-truth annotation and the one with the highest confidence has the lowest OKS. Localization errors are due to poor keypoint localization within a detected instance; they are further categorized into four types: Jitter : small localization error (0.5 ? exp(?d 2 i /2s 2 k 2 i ) &lt; 0.85); Miss: large localization error (exp(?d 2 i /2s 2 k 2 i ) &lt; 0.5); Inversion: left-right keypoint flipping within an instance; and Swap: keypoint swapping between instances. The reader may refer to [A3] for a more detailed description of the error classifications. <ref type="figure">Figure 8</ref> plots the precision-recall curves for HigherHRNet-W48 <ref type="bibr" target="#b6">[7]</ref>, HigherHRNet-W48 + SWAHR [35], DEKR-W48 <ref type="bibr" target="#b11">[12]</ref>, CenterGroup-W48 <ref type="bibr" target="#b2">[3]</ref>, and KAPAO-L at an OKS threshold of 0.85 using the results without TTA. Recalling that AP ? is equal to the area under the precision-recall curve generated at OKS = ?, the coloured regions in <ref type="figure">Figure 8</ref> reflect the theoretical improvement in AP .85 as a result of sequentially rectifying the aforementioned error types.</p><p>KAPAO-L provides the highest original AP .85 (represented by the white region). Furthermore, KAPAO-L is less prone to Swap and Inversion errors, which can be attributed to the pose object representation that models cohesive pose instances and eliminates the need for bottom-up keypoint grouping algorithms that are prone to such errors. This is further supported by DEKR having lower Swap error than the other three methods, which all perform bottom-up keypoint grouping. Interestingly, DEKR has the most room for improvement in assigning optimal confidence scores, which could be the motivation behind using a model-  <ref type="bibr" target="#b11">[12]</ref>, CenterGroup-W48 <ref type="bibr" target="#b2">[3]</ref>, and KAPAO-L for an OKS threshold of 0.85 (without TTA). Plots generated using the coco-analyze toolbox [A3]. AP .85 values in legends given in decimals as opposed to percent.</p><p>agnostic scoring regression network to improve the AP in the original paper (not included in these results). As previously hypothesized, KAPAO has more Jitter error than some of the other methods as a result of having less precise keypoint localization. Conversely, KAPAO-L provides better detection as shown by less improvement after correcting the false positive and false negative errors. It is speculated that since KAPAO was designed using an object detection network as its backbone, it is more optimized for person/pose object detection and less optimized for keypoint localization compared to the other single-stage human pose estimation methods. Rebalancing KAPAO to focus more on keypoint localization is thus a recommended area for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Qualitative Comparisons</head><p>KAPAO-L predictions from COCO val2017 are qualitatively compared with CenterGroup-W48 without TTA. To systematically review cases where KAPAO-L performs better than CenterGroup-W48, and vice versa, the maximum OKS was found for each ground-truth instance (OKS max ) using the top-20 scoring pose detections for each model. For each validation image, the difference between the summations of the OKS maxima was computed:</p><formula xml:id="formula_14">?OKS = ?OKS KAP AO?L max ? ?OKS CenterGroup?W 48 max<label>(13)</label></formula><p>? OKS is positive for images where KAPAO-L performs better than CenterGroup-W48. Conversely, ? OKS is negative for images where CenterGroup-W48 performs better than KAPAO-L. To plot partial poses using KAPAO-L, the keypoint object confidence threshold ? ck was lowered to 0.01 to promote the fusion of keypoint objects and increase the frequency of keypoint confidences in the predicted posesP (see Section 3.4 for details). The ? ck of 0.01 lowered the AP from 70.4 to 70.1. It also increased the post-processing time from approximately 3 ms to 5 ms per image due to the increased number of keypoint objects that are fused in Algorithm 1. Using these inference settings, KAPAO-L is still 1.0 AP more accurate and 3.0? faster than CenterGroup-W48 on val2017.</p><p>It is observed that extreme values of ? OKS are associated with crowded images (&gt; 20 people) containing a limited number of annotations (&lt; 20 annotations). For these images, ?OKS max is contingent on whether the ground-truth instances are predicted by the top-20 scoring detections and therefore an element of chance is involved. <ref type="figure" target="#fig_10">Figure 9</ref> illustrates such a scenario. The top-left image shows the ground-truth pose annotations (white); the top-right image shows the top-20 scoring CenterGroup-W48 detections (orange); the bottom-left image shows the top-20 scoring KAPAO-L detections (green); and the bottom-right image shows the same KAPAO-L detections but only plots the fused keypoint objects (light green). The top-20 KAPAO-L detections contain 8 of the 10 groundtruth instances whereas the top-20 CenterGroup-W48 predictions contain 6. As a result, ? OKS = 2.06. Because all the COCO keypoint metrics are computed using the 20 top-scoring detections, false negatives are artificially inflated while true positives are artificially deflated. While it is perplexing that the COCO metrics possess an element of randomness, it is conceivable that over many images the randomness averages out and does not favour one model over another. Moreover, the COCO dataset is sparsely populated with crowded images so these rare cases likely have a negligible influence on AP/AR. The implications of only using 20 detections on datasets like CrowdPose may be more severe and worth investigating, however. To avoid the aforementioned issues with comparing OKS in crowded scenes, the following comparisons consist of images where OKS max &gt; 0.5 for all groundtruth instances using both models. <ref type="figure" target="#fig_0">Figure 10</ref> shows an example where KAPAO-L performs better than CenterGroup-W48 (? OKS = +0.68). The keypoint grouping module of CenterGroup-W48 severely mixes up the keypoint identities (swap error). Swap error is a common failure case for CenterGroup but an uncommon failure case for KAPAO due to its detection of holistic pose objects (quantitative errors provided in <ref type="figure">Figure 8</ref>). <ref type="figure" target="#fig_0">Figure 11</ref> shows the image with the lowest ? OKS (?0.66). For three of the ground-truth instances situated near the top of the frame, KAPAO predicts the locations of the nose, eyes, and ears significantly lower than the ground-truth locations, resulting in lower OKS for these poses. These errors are the result of the keypoint object bounding boxes being cut-off by the edge of the frame such that the center of the keypoint object bounding box no longer coincides with the actual keypoint locations. These errors could be rectified with relatively simple alterations to the inference code in future work. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Accuracy vs. Inference Speed: KAPAO compared to state-of-the-art single-stage multi-person human pose estimation methods DEKR</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc><ref type="bibr" target="#b41">[64]</ref> proposed Point-Set Anchors, which adapted the RetinaNet[32] object detector using pose anchors instead of bounding box anchors. Zhou et al. [70] modeled objects using heatmap-based center points with CenterNet and represented poses as a 2K-dimensional property of the center point. Mao et al. [36]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>. It uses a deep convolutional neural network N to map an RGB input image I ? R h?w?3 to a set of four output grids? = {? s | s ? {8, 16, 32, 64}} containing the object prediction? O, where? s ? R h s ? w s ?Na?No : N (I) =?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Sample targets for training, including a human pose object (blue), keypoint object (red), and no object (green). The "?" values are not used in the loss computation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>https://doi.org/10.5281/zenodo.4679653 24. Khirodkar, R., Chari, V., Agrawal, A., Tyagi, A.: Multi-hypothesis pose networks: Rethinking top-down pose estimation. In: ICCV (2021) 25. Kreiss, S., Bertoni, L., Alahi, A.: Pifpaf: Composite fields for human pose estimation. In: CVPR (2019) 26. LeCun, Y., Bengio, Y., et al.: Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks 3361(10) (1995) 27. Li, J., Bian, S., Zeng, A., Wang, C., Pang, B., Liu, W., Lu, C.: Human pose regression with residual log-likelihood estimation. In: ICCV (2021) 28. Li, J., Wang, C., Zhu, H., Mao, Y., Fang, H.S., Lu, C.: Crowdpose: Efficient crowded scenes pose estimation and a new benchmark. In: CVPR (2019) 29. Li, K., Wang, S., Zhang, X., Xu, Y., Xu, W., Tu, Z.: Pose recognition with cascade transformers. In: CVPR (2021) 30. Li, Y., Yang, S., Zhang, S., Wang, Z., Yang, W., Xia, S.T., Zhou, E.: Is 2d heatmap representation even necessary for human pose estimation? arXiv preprint arXiv:2107.03332 (2021) 31. Lin, T.Y., Doll?r, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature pyramid networks for object detection. In: CVPR (2017) 32. Lin, T.Y., Goyal, P., Girshick, R., He, K., Doll?r, P.: Focal loss for dense object detection. In: ICCV (2017) 33. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll?r, P., Zitnick, C.L.: Microsoft COCO: Common objects in context. In: ECCV (2014) 34. Loshchilov, I., Hutter, F.: SGDR: Stochastic gradient descent with warm restarts. In: ICLR (2017) 35. Luo, Z., Wang, Z., Huang, Y., Wang, L., Tan, T., Zhou, E.: Rethinking the heatmap regression for bottom-up human pose estimation. In: CVPR (2021) 36. Mao, W., Tian, Z., Wang, X., Shen, C.: Fcpose: Fully convolutional multi-person pose estimation with dynamic instance-aware convolutions. In: CVPR (2021) 37. McNally, W., Vats, K., Wong, A., McPhee, J.: EvoPose2D: Pushing the boundaries of 2d human pose estimation using accelerated neuroevolution with weight transfer. IEEE Access (2021) 38. McNally, W., Walters, P., Vats, K., Wong, A., McPhee, J.: DeepDarts: Modeling keypoints as objects for automatic scorekeeping in darts using a single camera. In: CVPRW (2021) 39. McNally, W., Wong, A., McPhee, J.: Action recognition using deep convolutional neural networks and compressed spatio-temporal pose encodings. Journal of Computational Vision and Imaging Systems 4(1), 3-3 (2018) 40. McNally, W., Wong, A., McPhee, J.: STAR-Net: Action recognition using spatiotemporal activation reprojection. In: CRV (2019) 41. Nesterov, Y.: A method for solving the convex programming problem with convergence rate o(1/k2). Proceedings of the USSR Academy of Sciences 269, 543-547 (1983) 42. Newell, A., Huang, Z., Deng, J.: Associative embedding: End-to-end learning for joint detection and grouping. In: NeurIPS (2017) 43. Newell, A., Yang, K., Deng, J.: Stacked hourglass networks for human pose estimation. In: ECCV (2016) 44. Nie, X., Feng, J., Zhang, J., Yan, S.: Single-stage multi-person pose machines. In: ICCV (2019)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>G 64 ?</head><label>64</label><figDesc>{4.0, 1.0, 0.25, 0.06} objectness loss weight ? obj 0.7 ? (w/640) 2 ? 3/n(s) bounding box loss weight ? box 0.05 ? 3/n(s) class loss weight ? cls 0.3 ? (K + 1)/80 ? 3/n(s) pose object keypoints loss weight ? kps 0.025 ? 3/n(s) batch sizes for KAPAO-S, M, and L N b 128, 72, 48 pose, keypoint obj. conf. thresholds ?cp, ? ck 0.001, 0.2 pose, keypoint obj. IoU thresholds ? bp , ? bk 0.65, 0.25 maximum fusion distance (px) ? f d 50 pose obj. conf. threshold for fusion ? f c 0.3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 Fig. 5 .</head><label>55</label><figDesc>plots the results for each model. For all three models, reducing the input size to 1152 had a negligible effect on the accuracy but provided a meaningful latency reduction. For KAPAO-M Accuracy-speed trade-off for input sizes ranging from 640 to 1280. Evaluated on COCO val2017 using a batch size of 1 on a TITAN Xp GPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Red Light, Green Light. This demo runs KAPAO-L on a 480p video clip from the TV show Squid Game using an input size of 1024. For this demo, incomplete poses were plotted using keypoint objects only by setting ? ck = 0.01 (see Section 3.4 for details). A screenshot of the inference is provided inFigure 6 (bottom-left). The GPU inference speed varied between 15 and 30 FPS depending on the number of people in the frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>KAPAO video inference demo screenshots. Top-left: shuffling demo. Top-right: flash mob demo. Bottom-left: red light, green light demo. Bottom-right: squash demo. All video clips were sourced from YouTube.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 .</head><label>7</label><figDesc>Pose objects generalize well and can even be detected in depth video. Shown here is a screenshot of KAPAO-S running inference on a depth video obtained from a fencing action recognition dataset [A1, A2].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>Qualitative comparison between KAPAO-L and CenterGroup-W48 (COCO image 24021, ? OKS = 2.06). Top-left: ground-truth. Top-right: top-20 scoring Cen-terGroup predictions. Bottom-left: top-20 scoring KAPAO predictions (all keypoints). Bottom-right: top-20 scoring KAPAO-L predictions (fused keypoint objects).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 .Fig. 11 .</head><label>1011</label><figDesc>Qualitative comparison between KAPAO-L and CenterGroup-W48 (COCO image 49759, ? OKS = +0.68). Top-left: ground-truth. Top-right: top-20 scoring Cen-terGroup predictions. Bottom-left: top-20 scoring KAPAO predictions (all keypoints). Bottom-right: top-20 scoring KAPAO-L predictions (fused keypoint objects). A3. Ronchi, M.R., Perona, P.: Benchmarking and error diagnosis in multi-instance pose estimation. In: ICCV (2017) Qualitative comparison between KAPAO-L and CenterGroup-W48 (COCO image 326248, ? OKS = ?0.66). Top-left: ground-truth. Top-right: top-20 scoring Cen-terGroup predictions. Bottom-left: top-20 scoring KAPAO predictions (all keypoints). Bottom-right: top-20 scoring KAPAO-L predictions (fused keypoint objects).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>AR detection metrics based on Object Keypoint Similarity [33] and compare against state-of-the-art methods. All hyperparameters are provided in the source code.4.1 Microsoft COCO KeypointsTraining. KAPAO-S/M/L were all trained for 500 epochs on COCO train2017 using stochastic gradient descent with Nesterov momentum [41], weight decay, and a learning rate decayed over a single cosine cycle [34] with a 3-epoch warmup period<ref type="bibr" target="#b12">[13]</ref>. The input images were resized and padded to 1280?1280, keeping the original aspect ratio. Data augmentation used during training included mosaic<ref type="bibr" target="#b1">[2]</ref>, HSV color-space perturbations, horizontal flipping, translations, and scaling. Many of the training hyperparameters were inherited from [23, 61], including the anchor boxes A and the loss weights w, ? obj , ? box , and ? cls . Others, including the keypoint bounding box size b s and the keypoint loss weight ? kps , were manually tuned using a small grid search. The models were trained on four V100 GPUs with 32 GB memory each using batch sizes of 128, 72, and 48 for</figDesc><table><row><cell>assign keypoint object to pose We evaluate KAPAO on two multi-person human pose estimation datasets: elseP = ? // empty set 4 Experiments COCO Keypoints [33] (K = 17) and CrowdPose [28] (K = 14). We report the TTA Input Size(s) Params (M) FP (ms) PP (ms) Lat. (ms) AP AR HigherHRNet-W32 [7] N 512 28.6 46.1 50.1 96.2 63.6 69.0 + SWAHR [35] N 512 28.6 45.1 86.6 132 64.7 70.3 HigherHRNet-W32 [7] N 640 28.6 52.4 71.4 124 64.9 70.3 HigherHRNet-W48 [7] N 640 63.8 75.4 59.2 135 66.6 71.5 + SWAHR [35] N 640 63.8 86.3 194 280 67.3 73.0 DEKR-W32 [12] N 512 29.6 62.6 34.9 97.5 62.4 69.6 DEKR-W48 [12] N 640 65.7 109 45.8 155 66.3 73.2 CenterGroup-W32 [3] N 512 30.3 98.9 16.0 115 66.9 71.6 CenterGroup-W48 [3] N 640 65.5 155 14.5 170 69.1 74.0 KAPAO-S N 1280 12.6 14.7 2.80 17.5 63.0 70.2 KAPAO-M N 1280 35.8 30.7 2.88 33.5 68.5 75.5 KAPAO-L N 1280 77.0 51.3 3.07 54.4 70.6 77.4 HigherHRNet-W32 [7] Y 256, 512, 1024 28.6 365 372 737 69.9 74.3 + SWAHR [35] Y 256, 512, 1024 28.6 389 491 880 71.3 75.9 HigherHRNet-W32 [7] Y 320, 640, 1280 28.6 431 447 878 70.6 75.0 HigherHRNet-W48 [7] Y 320, 640, 1280 63.8 643 436 1080 72.1 76.1 + SWAHR [35] Y 320, 640, 1280 63.8 809 781 1590 73.0 77.6 DEKR-W32 [12] Y 256, 512, 1024 29.6 552 137 689 70.5 76.2 DEKR-W48 [12] Y 320, 640, 1280 65.7 1010 157 1170 72.1 77.8 CenterGroup-W32 [3] Y 256, 512, 1024 30.3 473 13.8 487 71.9 76.1 CenterGroup-W48 [3] Y 320, 640, 1280 65.5 1050 11.8 1060 73.3 77.6 standard AP/Method KAPAO-S</cell></row></table><note>Y 1024, 1280, 1536 12.6 61.5 3.70 65.2 64.4 71.5 KAPAO-M Y 1024, 1280, 1536 35.8 126 3.67 130 69.9 76.8 KAPAO-L Y 1024, 1280, 1536 77.0 211 3.70 215 71.6 78.5 Table 1. Accuracy and speed comparison with state-of-the-art single-stage human pose estimation models on COCO val2017, including the forward pass (FP) and post- processing (PP). Latencies (Lat.) averaged over val2017 using a batch size of 1 on a TITAN Xp GPU.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>compares the accuracy, forward pass (FP) time, and postprocessing (PP) time of KAPAO with state-of-the-art single-stage methods High-erHRNet<ref type="bibr" target="#b6">[7]</ref>, HigherHRNet + SWAHR [35], DEKR<ref type="bibr" target="#b11">[12]</ref>, and CenterGroup<ref type="bibr" target="#b2">[3]</ref> on val2017. Two test settings were considered: (1) without any test-time augmen-tation (using a single forward pass of the network), and (2) with multi-scale and horizontal flipping test-time augmentation (TTA). It is noted that with the exception of CenterGroup, no inference speeds were reported in the original works. Rather, FLOPs were used as an indirect measure of computational efficiency.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>46]  ?</cell><cell>-</cell><cell cols="6">64.9 85.5 71.3 62.3 70.0 69.7</cell></row><row><cell>RMPE [10]  ?</cell><cell>-</cell><cell cols="6">61.8 83.7 69.8 58.6 67.6 -</cell></row><row><cell>CPN [6]  ?</cell><cell>-</cell><cell cols="6">72.1 91.4 80.0 68.7 77.2 78.5</cell></row><row><cell>SimpleBaseline [66]  ?</cell><cell>-</cell><cell cols="6">73.7 91.9 81.1 70.3 80.0 79.0</cell></row><row><cell>HRNet-W48 [54]  ?</cell><cell>-</cell><cell cols="6">75.5 92.5 83.3 71.9 81.5 80.5</cell></row><row><cell>EvoPose2D-L [37]  ?</cell><cell>-</cell><cell cols="6">75.7 91.9 83.1 72.2 81.5 81.7</cell></row><row><cell>MIPNet [24]  ?</cell><cell>-</cell><cell cols="2">75.7 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RLE [27]  ?</cell><cell>-</cell><cell cols="6">75.7 92.3 82.9 72.3 81.3 -</cell></row><row><cell>OpenPose [4, 5]</cell><cell>74*</cell><cell cols="6">61.8 84.9 67.5 57.1 68.2 66.5</cell></row><row><cell>Mask R-CNN [14]</cell><cell>-</cell><cell cols="6">63.1 87.3 68.7 57.8 71.4 -</cell></row><row><cell>Associative Embeddings [42]</cell><cell>-</cell><cell cols="6">65.5 86.8 72.3 60.6 72.6 70.2</cell></row><row><cell>PersonLab [45]</cell><cell>-</cell><cell cols="6">68.7 89.0 75.4 64.1 75.5 75.4</cell></row><row><cell>SPM [44]</cell><cell>-</cell><cell cols="6">66.9 88.5 72.9 62.6 73.1 -</cell></row><row><cell>PifPaf [25]</cell><cell>-</cell><cell>66.7</cell><cell>-</cell><cell>-</cell><cell cols="3">62.4 72.9 -</cell></row><row><cell>HGG [22]</cell><cell>-</cell><cell cols="6">67.6 85.1 73.7 62.7 74.6 71.3</cell></row><row><cell>CenterNet [70]</cell><cell>-</cell><cell cols="6">63.0 86.8 69.6 58.9 70.4 -</cell></row><row><cell>Point-Set Anchors [64]</cell><cell>-</cell><cell cols="6">68.7 89.9 76.3 64.8 75.3 -</cell></row><row><cell>HigherHRNet-W48 [7]</cell><cell>1080</cell><cell cols="6">70.5 89.3 77.2 66.6 75.8 74.9</cell></row><row><cell>+ SWAHR [35]</cell><cell cols="7">1590 72.0 90.7 78.8 67.8 77.7 -</cell></row><row><cell>FCPose [36]</cell><cell>93*</cell><cell cols="6">65.6 87.9 72.6 62.1 72.3 -</cell></row><row><cell>DEKR-W48 [12]</cell><cell>1170</cell><cell cols="6">71.0 89.2 78.0 67.1 76.9 76.7</cell></row><row><cell>CenterGroup-W48 [3]</cell><cell>1060</cell><cell cols="6">71.4 90.5 78.1 67.2 77.5 -</cell></row><row><cell>KAPAO-S</cell><cell>65.2</cell><cell cols="6">63.8 88.4 70.4 58.6 71.7 71.2</cell></row><row><cell>KAPAO-M</cell><cell>130</cell><cell cols="6">68.8 90.5 76.5 64.3 76.0 76.3</cell></row><row><cell>KAPAO-L</cell><cell>215</cell><cell cols="6">70.3 91.2 77.8 66.3 76.8 77.7</cell></row></table><note>. Accuracy comparison with two-stage ( ?) and single-stage methods on COCO test-dev. Best results reported (i.e., including TTA). DEKR results use a model- agnostic rescoring network [12]. Latencies (Lat.) taken from Table 1. *Latencies re- ported in original papers [4, 36] and measured using an NVIDIA GTX 1080Ti GPU.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>MethodLat. (ms) AP AP .50 AP .75 AP E AP M AP H</figDesc><table><row><cell></cell><cell></cell><cell>Mask R-CNN [14]</cell><cell>-</cell><cell cols="9">57.2 83.5 60.3 69.4 57.9 45.8</cell><cell></cell></row><row><cell></cell><cell></cell><cell>AlphaPose [10]  ?</cell><cell>-</cell><cell cols="10">61.0 81.3 66.0 71.2 61.4 51.1</cell></row><row><cell></cell><cell></cell><cell>SimpleBaseline [66]  ?</cell><cell>-</cell><cell cols="9">60.8 81.4 65.7 71.4 61.2 51.2</cell><cell></cell></row><row><cell></cell><cell></cell><cell>SPPE [28]</cell><cell>-</cell><cell cols="9">66.0 84.2 71.5 75.5 66.3 57.4</cell><cell></cell></row><row><cell></cell><cell></cell><cell>MIPNet [24]  ?</cell><cell>-</cell><cell cols="2">70.0 -</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell></row><row><cell></cell><cell></cell><cell>OpenPose [5]</cell><cell>74*</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell cols="5">62.7 48.7 32.3</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">HigherHRNet-W48 [7] 1080</cell><cell cols="9">67.6 87.4 72.6 75.8 68.1 58.9</cell><cell></cell></row><row><cell></cell><cell></cell><cell>DEKR-W48 [12]</cell><cell>1170</cell><cell cols="9">68.0 85.5 73.4 76.6 68.8 58.4</cell><cell></cell></row><row><cell></cell><cell></cell><cell>CenterGroup-W48 [3]</cell><cell cols="11">1060 70.0 88.9 75.7 77.3 70.8 63.2</cell></row><row><cell></cell><cell></cell><cell>KAPAO-S</cell><cell>65.2</cell><cell cols="9">63.8 87.7 69.4 72.1 64.8 53.2</cell><cell></cell></row><row><cell></cell><cell></cell><cell>KAPAO-M</cell><cell>130</cell><cell cols="9">67.1 88.8 73.4 75.2 68.1 56.9</cell><cell></cell></row><row><cell></cell><cell></cell><cell>KAPAO-L</cell><cell>215</cell><cell cols="9">68.9 89.4 75.6 76.6 69.9 59.5</cell><cell></cell></row><row><cell>COCO Validation AP</cell><cell>50.0 52.5 55.0 57.5</cell><cell cols="2">0.010 0.025 Norm. Keypoint Bounding Box Size bs/max(w, h) 0.050 0.075 0.100</cell><cell>Fusion Rate</cell><cell>0.0 0.5 1.0</cell><cell>ears</cell><cell>eyes</cell><cell>nose</cell><cell>shoulders</cell><cell>elbows</cell><cell>ankles</cell><cell>knees</cell><cell>wrists</cell><cell>hips</cell></row></table><note>. Comparison with single-stage and two-stage ( ?) methods on CrowdPose test, including TTA. DEKR results use a model-agnostic rescoring network [12]. HigherHR- Net + SWAHR [35] not included due to issues reproducing the results reported in the paper using the source code. Latencies (Lat.) taken from Table 1. *Latency reported in original paper [4] and measured using NVIDIA GTX 1080Ti GPU on COCO.Fig. 4. Left: The influence of keypoint object bounding box size on learning. Each KAPAO-S model was trained for 50 epochs. Right: Keypoint object fusion rates for each keypoint type. Evaluated on COCO val2017 using KAPAO-S without TTA.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>). The results are consistent with the prior work of McNally et al. [38]: b s /max(w, h) &lt; 2.5% destabilizes training leading to poor accuracy, and optimal b s /max(w, h) is observed around 5% (used for the experiments in previous section). In contrast to McNally et al., the accuracy in this study degrades quickly for b s /max(w, h) &gt; 5%. It is hypothesized that large b s in this application interferes with pose object learning.The accuracy improvements resulting from fusing the keypoint objects with the pose objects are provided inTable 4. Keypoint object fusion adds no less than 1.0 AP and over 3.0 AP in some cases. Moreover, keypoint object fusion</figDesc><table><row><cell>Method</cell><cell>TTA</cell><cell>? Lat. (ms) / ?AP (COCO val2017)</cell><cell>? Lat. (ms) / ?AP (CrowdPose test)</cell></row><row><cell cols="2">KAPAO-S N</cell><cell>+1.2 / +2.4</cell><cell>+3.3 / +2.9</cell></row><row><cell cols="2">KAPAO-M N</cell><cell>+1.2 / +1.1</cell><cell>+3.5 / +3.2</cell></row><row><cell cols="2">KAPAO-L N</cell><cell>+1.7 / +1.2</cell><cell>+4.2 / +1.0</cell></row><row><cell cols="2">KAPAO-S Y</cell><cell>+1.7 / +2.8</cell><cell>+3.9 / +3.2</cell></row><row><cell cols="2">KAPAO-M Y</cell><cell>+1.6 / +1.5</cell><cell>+4.4 / +3.5</cell></row><row><cell cols="2">KAPAO-L Y</cell><cell>+1.4 / +1.1</cell><cell>+4.5 / +1.0</cell></row><row><cell cols="4">Table 4. Accuracy improvement when fusing keypoint object detections with human</cell></row><row><cell cols="4">pose detections. Latencies averaged over each dataset using a batch size of 1 on a</cell></row><row><cell>TITAN Xp GPU.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>, G 16 G 32 , and</figDesc><table><row><cell>Hyperparameter Description</cell><cell>Symbol</cell><cell>Value(s)</cell></row><row><cell>output grid scales</cell><cell>s</cell><cell>{8, 16, 32, 64}</cell></row><row><cell>keypoint object bounding box size (px)</cell><cell>bs</cell><cell>64</cell></row><row><cell>input image height, width (px)</cell><cell>h, w</cell><cell>1280, 1280</cell></row><row><cell>G 8 anchor boxes (width, height) (px)</cell><cell>A 8</cell><cell>{(19, 27), (44, 40), (38, 94)}</cell></row><row><cell>G 16 anchor boxes (width, height) (px)</cell><cell>A 16</cell><cell>{(96, 68), (86, 152), (180, 137)}</cell></row><row><cell>G 32 anchor boxes (width, height) (px)</cell><cell cols="2">A 32 {(140, 301), (303, 264), (238, 542)}</cell></row><row><cell>G 64 anchor boxes (width, height) (px)</cell><cell cols="2">A 64 {(436, 615), (739, 380), (925, 792)}</cell></row><row><cell>loss weights for G 8</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>The hyperparameters used in the KAPAO experiments. n(s) is the number of output grids.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was supported in part by Compute Canada, the Canada Research Chairs Program, the Natural Sciences and Engineering Research Council of Canada, a Microsoft Azure Grant, and an NVIDIA Hardware Grant.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Posetrack: A benchmark for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Yolov4: Optimal speed and accuracy of object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y M</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10934</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The center of attention: Center-keypoint grouping via attention for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bras?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Openpose: realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08008</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">HigherHRNet: Scale-aware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">RepVGG: Making VGGstyle convnets great again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Style aggregated network for facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">RMPE: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Actor-transformers for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gavrilyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sanford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Javan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bottom-up human pose estimation via disentangled keypoint regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<editor>Mask R-CNN</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Awr: Adaptive weighting regression for 3d hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">DeeperCut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Multi-person pose estimation with local joint-to-person associations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Hand pose estimation via latent 2.5 d heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Breuel Juergen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Unsupervised learning of object landmarks through conditional image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Joint learning of semantic alignment and object landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<title level="m">Dynamic filter networks. NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Differentiable hierarchical graph grouping for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, partbased, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Towards accurate multi-person pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">DeepCut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Efficient online multi-person 2d pose tracking with recurrent spatio-temporal affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Raaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Discovery of latent 3d keypoints via end-to-end geometric reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">FCOS: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">DeepPose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Puck-Net: Estimating hockey puck location from broadcast video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mcnally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dulhanty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Clausi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zelek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Ttnet: Real-time temporal and spatial video analysis of table tennis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Voeikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Falaleev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Baikulov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.08036</idno>
		<title level="m">Scaled-YOLOv4: Scaling cross stage partial network</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Cspnet: A new backbone that can enhance learning capability of cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Yeh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Adaptive wing loss for robust face alignment via heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fuxin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Point-set anchors for object detection, instance segmentation and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<title level="m">Convolutional pose machines</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">AnchorFace: An anchor-based facial landmark detector across large poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Classification of basic footwork in fencing using accelerometer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Malawski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kwolek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing: Algorithms, Architectures, Arrangements, and Applications</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">FenceNet: Fine-grained Footwork Prediction in Fencing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcphee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note>Submitted to CVSports</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
