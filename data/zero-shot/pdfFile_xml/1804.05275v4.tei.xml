<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Horizontal Pyramid Matching for Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fu</surname></persName>
							<email>yangfu2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">IFP</orgName>
								<orgName type="institution">UIUC</orgName>
								<address>
									<settlement>Beckman</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
							<email>yunchao@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">IFP</orgName>
								<orgName type="institution">UIUC</orgName>
								<address>
									<settlement>Beckman</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqian</forename><surname>Zhou</surname></persName>
							<email>yuqian2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">IFP</orgName>
								<orgName type="institution">UIUC</orgName>
								<address>
									<settlement>Beckman</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
							<email>shihonghui3@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">IFP</orgName>
								<orgName type="institution">UIUC</orgName>
								<address>
									<settlement>Beckman</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Stevens Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
							<email>xinchao.wang@stevens.edu</email>
							<affiliation key="aff3">
								<orgName type="department">CloudWalk Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Yao</surname></persName>
							<email>yaozhiqiang@cloudwalk.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
							<email>t-huang1@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">IFP</orgName>
								<orgName type="institution">UIUC</orgName>
								<address>
									<settlement>Beckman</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Horizontal Pyramid Matching for Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the remarkable recent progress, person reidentification (Re-ID) approaches are still suffering from the failure cases where the discriminative body parts are missing. To mitigate such cases, we propose a simple yet effective Horizontal Pyramid Matching (HPM) approach to fully exploit various partial information of a given person, so that correct person candidates can be still identified even even some key parts are missing. Within the HPM, we make the following contributions to produce a more robust feature representation for the Re-ID task: 1) we learn to classify using partial feature representations at different horizontal pyramid scales, which successfully enhance the discriminative capabilities of various person parts; 2) we exploit average and max pooling strategies to account for person-specific discriminative information in a global-local manner. To validate the effectiveness of the proposed HPM, extensive experiments are conducted on three popular benchmarks, including Market-1501, DukeMTMC-ReID and CUHK03. In particular, we achieve mAP scores of 83.1%, 74.5% and 59.7% on these benchmarks, which are the new state-of-the-arts. Our code is available on Github .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Person re-identification (Re-ID) aims at re-identifying a query person from a collection of images, which are taken by multiple cameras across time. It is challenging to learning robust feature representations <ref type="bibr" target="#b17">(Yang et al. 2009</ref>) for each person because of large variations of human attributes like poses, gaits, clothes, as well as the environmental settings like illumination, complex background, and occlusions. To address the complexities of visual cues, deep-based approaches <ref type="bibr" target="#b0">(Ahmed, Jones, and Marks 2015;</ref><ref type="bibr" target="#b11">Hermans, Beyer, and Leibe 2017;</ref><ref type="bibr" target="#b8">Ding et al. 2015;</ref>) provide promising solutions. However, these approaches only take advantage of global person features, which turns out to be sensitive to the missing key parts.</p><p>To relieve such issues, many recent approaches have been focusing on learning partial discriminative feature representations. These methods usually take advantage of both global features like body size and local ones like cloths logo, to enhance the robustness of the Re-ID methods. They can be categorized into three types based on the local-region generation scheme. In the first type, prior knowledge like poses or body landmarks are estimated and extracted to localize the discriminative regions <ref type="bibr" target="#b14">(Su et al. 2017;</ref><ref type="bibr" target="#b16">Wei et al. 2017</ref>). However, the performance of Re-ID in this case highly relies on the robustness of the pose or landmark estimation models. Unexpected errors like erroneous estimation of poses may greatly influence the identification result. The second type, attention-based approaches <ref type="bibr" target="#b14">(Liu et al. 2016;</ref><ref type="bibr" target="#b14">Liu et al. 2017;</ref><ref type="bibr" target="#b19">Zhao et al. 2017b;</ref><ref type="bibr" target="#b14">Li, Zhu, and Gong 2018;</ref><ref type="bibr" target="#b9">Fu et al. 2019)</ref>, focus on extracting the salient regions of interest (ROI) adaptively by localizing the high activations in the deep feature maps. The selected regions however lack semantic interpretation. The third type of methods crop deep feature maps into pre-defined patches or stripes by assuming the images are perfectly aligned <ref type="bibr" target="#b15">(Sun et al. 2017b;</ref>, and are thus prone to errors introduced by outliers.</p><p>To effectively learn partial discriminative features and  eliminate the negative effect caused by unexpected posevariant and unaligned cases, we propose a simple yet effective approach, called Horizontal Pyramid Matching (HPM). Our HPM aim to simultaneously exploit global and partial information of a person for the Re-ID task in a more robust and efficient manner. Specifically, we make three contributions as follows:</p><p>? We horizontally slice the deep feature maps into multiple spatial bins using various pyramid scales for the following pooling operation, which is named as Horizontal Pyramid Pooling (HPP), and learn to classify each spatial bins features output from different pyramid scales independently. Intuitively, using multiple scales of bins will incorporate a slack distance to mitigate the outliers issue caused by misalignment. Also, learning multi-scale information independently will enhance the discriminative information learned in all the scale-specific person parts. ? We combine the average and max pooling features in each partition. In particular, average pooling is able to perceive the global information of each spatial bin and takes the background context into account. In contrast, the max pooling targets on extracts the most discriminative information and ignore those interference information, mainly coming from similar clothing or background. Integrating them both thus balance the effectiveness of these two strategies in a global-local manner. ? We evaluate our proposed method on three mainstream person re-identification datasets, Market1501, DukeMTMC-ReID and CUHK03(with new protocol).The experimental results show that our model beats most of state-of-arts approaches by end-to-end trainable. We illustrate our HPM with one example shown in Figure 1. We first extract the feature representation of the given image with multiple convolutional layers and horizontally slice the feature maps at different pyramid scales. The feature representations generated by both global average pooling and max pooling of each partial scrip are then employed to conduct Re-ID independently. By learning the HPM in such a manner, the partial discriminative capability can be enhanced in a more effective way, which can overcome the disadvantages (e.g. sensitive to the missing key parts or misalignment) of current solutions . <ref type="figure" target="#fig_1">Figure 2</ref> shows the heatmaps of the last convolutional feature maps learned by w/ HPM and w/o HPM schemes. It is observed that more discriminative parts can be identified by our HPM, which leads to better person Re-ID results.</p><p>Extensive experiments and ablation study conducted on Market-1501, DukeMTMC-ReID and CUHK03 have demonstrated the effectiveness of each design. In particular, we achieve the mAP scores of 83.1%, 74.5% and 59.7% on the three benchmarks, which outperform the state-of-thearts more than 1.5%, 5.3% and 2.2%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>In this section, we review several closely related work including deep learning methods for Person Re-ID, part-based models, spatial pyramid pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep learning for Person Re-ID</head><p>Deep learning based method has dominated in Re-ID community <ref type="bibr" target="#b20">(Zheng, Yang, and Hauptmann 2016)</ref>.  first employed deep neural network to determine if a pair of input images belong to the same ID. In general, two types of models are used for person re-identification: verification and identification model. For the verification model, they adopt siamese neural network or triplet loss to pull the pair of images with same identity and push away that with different identity <ref type="bibr" target="#b0">(Ahmed, Jones, and Marks 2015;</ref><ref type="bibr" target="#b11">Hermans, Beyer, and Leibe 2017;</ref><ref type="bibr" target="#b8">Ding et al. 2015;</ref>. In (Hermans, Beyer, and Leibe 2017), Hermans et al.. proposed a variant of triplet loss to perform end-to-end deep metric learning, which can outperforms many other published methods by a large margin. However, generally, this kind of model may have a compromised efficiency on large gallery. This is because it does not make full use of Re-ID annotations.</p><p>For the identification model <ref type="bibr" target="#b17">(Xiao et al. 2016;</ref><ref type="bibr" target="#b15">Sun et al. 2017b)</ref>, it tries to learn a discriminative representation of given input image and it always yields superior accuracy compared with verification model. <ref type="bibr" target="#b17">Xiao et al. (Xiao et al. 2016</ref>) propose a novel dropout strategy to train a classification model with multiple datasets jointly. In , the verification and classification losses are combined together to learn a discriminative embedding and a similarity measurement at the same time. In <ref type="bibr" target="#b15">(Sun et al. 2017b</ref>), a Part-based Convolutional network is proposed to learn discriminative part-informed features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Part-based Model</head><p>Recently, many works generate deep representation from local parts for fine-grained discriminative features of person. This kind of part-based model can be divided into three categories. First one is based on some prior knowledge like pose estimation and landmark detection <ref type="bibr" target="#b14">Su et al. 2017;</ref><ref type="bibr" target="#b16">Wei et al. 2017)</ref>. These methods share a same drawback that is the gap lying between datasets for pose estimation and person retrieval. Second, several other works abandon the semantic cues for partition <ref type="bibr" target="#b14">Liu et al. 2017;</ref><ref type="bibr" target="#b19">Zhao et al. 2017b;</ref><ref type="bibr" target="#b14">Li, Zhu, and Gong 2018)</ref>. For example, Yao et al. ) employed the Part Loss Networks which enforces the deep network to learn representations for different parts and gain the discriminative power on unseen persons. Third, the partition is cropped into pre-defined patches <ref type="bibr" target="#b15">(Sun et al. 2017b;</ref>. Sunet al. <ref type="bibr" target="#b15">(Sun et al. 2017b)</ref> proposed Partbased Convolutional Baseline (PCB) to learn discriminative partition features. However, the PCB may suffer some outliers, which make the inconsistency in each partition, thus they proposed Refined Part Pooling (RPP) to enhance within-part consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial Pyramid Pooling</head><p>Since convolutional neural networks with the fully connectedly layer always require the fixed input size. In order to remove this constrain, He et al. <ref type="bibr" target="#b10">(He et al. 2014)</ref> proposed the Spatial Pyramid Pooling network, which is able to generate a fixed length output regardless of the input size and maintain spatial information by pooling in local spatial bins. Multilevel spatial pooling has also shown to be robust to object deformations. It can improve the performance of classification and object detection tasks. Similarly pyramid pooling module is also used in <ref type="bibr" target="#b18">(Zhao et al. 2017a)</ref>, the pyramid level pooling separates the feature map into different sub-regions and forms pooled representation for different locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Method</head><p>This section describes the structure of Horizontal Pyramid Matching(HPM) framework as shown in <ref type="figure" target="#fig_2">Fig 3.</ref> The input image is fed into the a backbone network to extract the feature maps. After that, we use horizontal spatial pyramid pooling module to obtain spatial information in each local and global spatial bin. For each horizontal spatial bin, we use both global average pooling operation and max pooling operation to obtain the features of global part and most discriminative part of person body. Then, convolutional layers are used to reduce the dimensions of the column feature maps from 2048 to 256 and each column feature is input into a non-share fully connectedly layer and followed by a softmax function to predict the ID of each input image. During testing, all these features are concatenated together to obtain the final Re-ID feature representations. More details will be given in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Horizontal Pyramid Matching</head><p>Backbone Network The HPM can take various network architecture like VGG (Simonyan and Zisserman 2014), Resnet <ref type="bibr" target="#b11">(He et al. 2016)</ref> and Google Inception (Szegedy et al. 2016) as the backbone. Our paper choose the Resnet50 as backbone network with some modifications following the previous state-of-the-art <ref type="bibr" target="#b15">(Sun et al. 2017b)</ref>. First, the average pooling layer and the fully connected layer are removed. Also, the stride of the conv4 1 is set to 1. As a result, the size extracted feature maps will be 1 16 of the input image size. Horizontal Pyramid Pooling (HPP) module HPP is inspired by Special Pyramid Pooling (SPP) <ref type="bibr" target="#b10">(He et al. 2014)</ref>, which is proposed to eliminate uncertain length of feature vectors caused by different input sizes of images. The differences between our HPP module and SPP mainly include two aspects: 1) motivation: HPP is designed to learn to enhance the discriminative information of partial person body at various scales, while SPP is to address the issue of inconsistent length of image feature vectors. 2) operation: Since the distribution of distinguish partitions of a person is from head to foot, HPP slices the feature maps into multiple scrips in a horizontal manner, which is different from SPP using a 2-D spatial manner. With HPP, we can obtain vectors of fixed length for person parts at different horizontal pyramid scales. These vectors are further fed into one convolutional layer and one fully-connected layer for learning classification. In this way, the discriminative ability of person parts can be captured from global to local, from coarse to fine.</p><p>Formally, denote the feature maps extracted by the backbone network as F . We adopt 4 pyramid scales within the HPP module and F is sliced into several spatial bins horizontally and equally according to different scales. Specifically, assume each spatial bin as F i,j . i, j stand for the index of scale and the index of bins in each scale. For instance, F 3,4 means the fourth bin in third pooling scale. Then, we pool each spatial bin F i,j by global average and max pooling to generate column feature vector, G i,j .</p><formula xml:id="formula_0">G i,j = avgpool(F i,j ) + maxpool(F i,j )</formula><p>After that, each G i,j is fed into a convolutional layer to reduce the dimensions from 2048 to 256, denote as H i,j . These H i,j with the same i can be considered as a description of the person. This kind of description covers more detailed partial features with the increasing pyramid scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Function</head><p>We leverage the classification-based model to tackle the person Re-ID task. Therefore, the target is to predict the ID of each person, thus person-specific feature representations can be then learned by the optimized classification model. We use a branch of fully connected layer as the classifier, each feature column vector H i,j is fed into a corresponding classifier F C i,j and following a softmax function to predict its ID. During training, the output of given image I is a set of predictions? i,j . Each? i,j can be formulated a? where the P is the total number of person ID, W i,j is the learnt weights of F C i,j , y is the ground truth ID of input image I. The loss function is sum of Cross Entropy loss of each output? i,j .</p><formula xml:id="formula_1">y i,j = argmax c?P exp((W c i,j ) T H i,j (I))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss</head><formula xml:id="formula_2">= N n=1 i,j CE(? n i,j , y n )</formula><p>where N is the size of mini-batch, CE is the Cross Entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variant of HPM</head><p>HPM may have some variants different from the basic framework describe above, e.g. different pyramid scales and pooling strategies.</p><p>Number of pyramid scales The HPM can have several different number of scales. Instead of the 4 scales, it can be any number up to the log 2 (h), where h is the height of feature map. The HPM structure with different pyramid scales is shown in <ref type="table">Table 1</ref>.The model will focus on more detailed and fine partitions of the given person with the increasing of pyramid scales. Since our loss function is a linear combination of each pyramid scales, if there are too many pyramid scales, the global information of the person may be underestimated. On the other hand, if too few pyramid scales, the features of local discriminative partition may be more difficult to extract. Thus, choosing a proper pyramid scales that can balance the global and local features is vital for the performance of HPM. Pooling strategies The HPM uses both average pooling and max pooling. The global average pooling is a traditional operation in many classification framework, because it enforces a corresponding relation between feature maps and categories. However, the global average pooling can lose some very discriminative information by the average operation. For example, if one partition of the person is very discriminative but surrounded by background, in this case, the global average pooling will obtain the average of the discriminative part and the background region, which may lead to a low response and miss it. To deal with this problem, we use both average pooling and max pooling, which can maintain the global relation with the identification and preserve the discriminative part.</p><p>We will provide extreme ablation experiments in the following section to validate the effectiveness of our settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Dataset and Evaluation Protocol</head><p>Market1501 <ref type="bibr" target="#b19">(Zheng et al. 2015)</ref> contains 32,668 images of 1,501 labeled persons of six camera views. There are 19,732 gallery images and 12,936 training images detected by <ref type="bibr">DPM (Felzenszwalb et al. 2010)</ref>, including 751 identities in the training set and 750 identities in the testing set. It also contains 500,000 images as some distractors, which may has a considerable influence on the retrieval accuracy.</p><p>DukeMTMC-ReID (Ristani et al. 2016; Zheng, Zheng, and Yang 2017c) is a subset of the DukeMTMC dataset. It contains 1,812 identities captured by 8 cameras. There are 2,228 query images, 16,522 training images and 17,661 gallery images, with 1,404 identities appear in more than two cameras.Also, similar with the Market1501, the rest 408 IDs are considered as distractor images. DukeMTMC-ReID is one of the most challenging re-ID datasets up to now with so many images from 8 multi-cameras.</p><p>CUHK03  consists of 14,097 cropped images from 1,467 identities. For each identity, images are captured from two cameras and there are about 5 images for each view. There are two ways to obtain the annotations: human labeled and detected by DPM. Our evaluation is based on the detected label image.</p><p>Evaluation Protocol In our experiment, we use Cumulative Matching Characteristic (CMC) curve and the mean average precision (mAP) to evaluate our approach. CMC represents the accuracy of the person retrieval, it is accurate when each query only has one ground truth. However, when multiple ground truths exist in the gallery, the goal is to return all right match to user. In this case, CMC may not have enough discriminative ability, but the mAP could reflect the recall. For Market-1501 and DukeMTMC-ReID. We use the evaluation packages provided by <ref type="bibr" target="#b19">(Zheng et al. 2015)</ref> and <ref type="bibr" target="#b20">(Zheng, Zheng, and Yang 2017c)</ref>, respectively. And for CUHK03, we adopt the new training/testing protocol proposed in <ref type="bibr" target="#b21">(Zhong et al. 2017)</ref>. Moreover, for simplicity, all results reported in this paper is under the single-query setting and does not use the re-ranking proposed in <ref type="bibr" target="#b21">(Zhong et al. 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>In order to obtain enough information from person image and proper size of feature map for horizontal pyramid pooling, we resize all the image to 384x128. For the backbone network, we use Resnet50 that initialized with the weights pretrained on ImageNet <ref type="bibr" target="#b7">(Deng et al. 2009</ref>). We remove the last fully connected layer and average pooling layer and set the stride of last resent conv4 1 from 2 to 1. The training images are augmented with horizontal flipping and normalization. The batch size is set to 64 and we train model for 60 epoch. The base learning rate is set to 0.1 and decay to 0.01 after 40 epochs. Notice that learning rate for all pretrained Resnet layer is set to 0.1 x base learning rate. The stochastic gradient descent (SGD) with 0.9 momentum is implemented in each mini-batch to update the parameters. During evaluation, we concatenate the feature vectors after the 1x1 convolution operation together to generate feature representation of query image. The feature from original image and horizontal flipped image are added up and normalized feature for retrieval evaluation. Our model is implemented on Pytorch platform and train with two NVIDIA TITAN X GPUs. All datasets share the same experiments setting as above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with the State-of-the-arts</head><p>Results on Market1501 Comparisons between HPM and state-of-art approaches on Market1501 are shown in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>The results show that our HPM achieves the mAP of 83.1% and Rank 1 accuracy 94.2%, which both surpass all existing works more than 1.5% and 0.4%, respectively. It should be noted that we do not conduct any post-processing operation ( e.g. the re-rank algorithm given by <ref type="bibr" target="#b21">(Zhong et al. 2017)</ref>), which can further bring a considerably improvement in terms of mAP. PCB <ref type="bibr" target="#b15">(Sun et al. 2017b</ref>) is closest competitor, which also leverages partial-based leaning for person Re-ID. However, there are mainly two disadvantages of PCB, i.e. 1) it splits the features maps into pre-defined patches (6 in PCB) with the assumption that most persons in the given images are well aligned, which not make scene and resist to some outliers; 2) its state-of-the-art results are benefited from a powerful post-processing approach called RPP, which enable the optimized model cannot be trained in an end-to-end manner. In contrast, our HPM splits the feature maps according to various pyramid scales, which is more robust compared with PCB in addressing the outliers that are not well aligned. In addition, our HPM can be endto-end learned and we believe that any post-processing operation can make a continued improvements upon the current results. From <ref type="table" target="#tab_3">Table 2</ref>, we can observe that our HPM makes a 5.4% improvements compared with PCB in mAP. Even without post-processing, our HPM is still better than    <ref type="table" target="#tab_3">Table 2</ref>. This dataset is challenging because it has 8 different camera and the person bounding box size varies drastically across different camera views, however, our HPM achieves even better performance on this dataset. Without any post-processing, it still achieves 74.8% on mAP and 86.6% on Rank 1 accuracy, which is better than all other state-of-the-art methods by a large margin, 5.3% and 3.3%. Note that our HPM is the first model that can achieve above 80% on mAP, which surpass all state-ofthe-art methods by more than 5.0%</p><p>Results on CUHK03 <ref type="table" target="#tab_3">Table 2</ref> shows results on CUHK03 when detected person bounding boxes are used for both training and testing. HPM achieves the best result of mAP, 59.7% under this setting. Although the Rank 1 accuracy of HPM is a little lower than PCB+RPP, there's a clear gap, more than 2%, between HPM and other methods, including the PCB+RPP, on mAP. And We believe that, as a end-toend and part-based model, the RPP can also boost the performance HPM further.  <ref type="figure" target="#fig_3">Figure 4 (a)</ref> shows the queries and the corresponding heatmaps 1 of the last convolutional feature maps. We observe that the discriminative abilities of multiple person parts are enhanced with our HPM. <ref type="figure" target="#fig_3">Figure 4</ref> (b) compares the Re-ID results of w/ HPM and w/o HPM schemes. It can be seen that our HPM is very effective in guaranteeing accurate Re-ID results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>To verify the effectiveness of each component and setting of HPM, we design several ablation study with different settings on Market-1501, DukeMTMC-ReID and CUHK03, including different number of pyramid scales, w/ and w/o using pyramid structure, different pooling strategies. Note that all unrelated settings are the same as HPM implementation detailed in Section 4.2</p><p>Effectiveness of Pyramid Structure Previous analysis shows that the HMP reaches the best performance with four pyramid scales, which has up to 8 partial bins on the feature map. In order to verify the effectiveness of pyramid structure, we remove other branches and just preserve the branch with 8 partial bins. From <ref type="table" target="#tab_4">Table 3</ref>, we can observe that Rank 1, Rank 5 and mAP drop from 94.2%, 97.5% and 82.7% to 92.0%, 96.3% and 76.4%, respectively. Such an operation is similar to PCB, which leverage 6 partial bins. The reason is that many persons are not well aligned in the images, and naively split the feature maps into a pre-defined number of bins cannot well resist to unaligned outliers. In addition, discriminative information may hardly be learned for some parts if we apply too dense division. In contrast, with our pyramid structure, we can formulate partial features from coarse to fine, which can finally form into a more robust feature representation for person images.  <ref type="table">Table 4</ref>: Performance comparison of the proposed method with different pyramid scales and different pooling strategies as described in Section3.4. PS is the abbreviation of Pyramid Scales.  <ref type="table">Table 4</ref> shows the Re-ID results of HPM with different pyramid scales, e.g. 1, 2, 4, 8. From these results, we can find that HPM reaches the best performance with four pyramid scales. Intuitively, the number of pyramid p determines the granularity of the partition feature. When p = 1, it is equivalent to global pooling. With the increasing the number of p, Rank 1 accuracy and mAP are significant improved from 88.1% and 71.2% to 93.2% and 79.5%, as illustrated in <ref type="figure" target="#fig_5">Fig 5.</ref> The reason why the HPM does not drops dramatically at some point as introduced in <ref type="bibr" target="#b15">(Sun et al. 2017b</ref>) is that the pyramid structure can combine both global and local features, which may increase the discriminative ability of very small partition. Since the last convolutional feature maps are with 24 horizontal units, we also try more dense pyramid scales, such as 12 and 24. However, more pyramid scales will bring additional computational cost and there is no obvious improvement can be observed. Therefore, we finally adopt 4 pyramid scales in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Pyramid Scales</head><p>Pooling Strategies Row4 and Row 5 in <ref type="table">Table 4</ref> shows the performance of HPM with different pooling strategies. It can be observed that max pooling performs better than average pooling in the most cases. The reason is that average pooling will take all locations of a particular parts into account and all the locations contribute equally to the final partial representation. Thus, the discriminative ability of the representation produced by average pooling can be easily influenced by the unrelated background patterns. In contrast, the global max pooling only preserve the largest response values for a local view. We consider these two pooling strategies are complementary in producing feature representations from global and local vies. Therefore, we integrate them into a unified model to take advantages from these two strategies. Experimental results in <ref type="table">Table 4</ref> demonstrate that mixing the two pooling strategies achieves better results compared with using either of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this work, we propose a novel Horizontal Pyramid Matching (HPM) approach to address the challenging person Re-ID task. The proposed HPM exploits partial information of each person to Re-ID, which successfully enhances the discriminative ability of partial feature and finally forms the into a more robust feature representation for the target person. In addition, we leverage both partial-based global average and max pooling to mine the discriminative information of each part in a global-local manner, which can further improve the robustness of partial features. All the components detailed in this work can be easily embedded into any other framework to make a further performance improvement. Extensive ablation studies and comparisons well demonstrate the effectiveness of our HPM approach. In the future, we are plan to simultaneously optimize Re-ID and other related tasks, such as human activity recognition <ref type="bibr" target="#b4">(Dai et al. 2017a;</ref><ref type="bibr" target="#b5">Dai et al. 2017b;</ref><ref type="bibr" target="#b6">Dai et al. 2019</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of the proposed Horizontal Pyramid Matching. We split a person into different horizontal parts of multiple scales. The feature representations produced by Global Average Pooling (GAP) and Global Max Pooling (GMP) of each part are then leveraged to learn to the person Re-ID independently.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Comparisons of results w/ HPM and w/o HPM in Person Re-ID.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Overview of the proposed Horizontal Pyramid Matching (HPM) approach. The input image firstly goes through a convolutional neural network to extract its feature maps. Then, the horizontal pyramid pooling is leveraged to producing feature representation of each part using both global average pooling and global max pooling. Finally, prediction of each part is fed into the classifier to conduct partial-level person Re-ID. During the testing stage, we concatenate features of parts at different pyramid scales to form the final feature representation of each image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative Results: (a) Queries and the corresponding discriminative heatmaps learned by the proposed HPM. (b) Comparisons of R5 of w/ HPM and w/o HPM schemes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Qualitative Result We visualize some examples in Figure 4. Concretely,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Impact of pyramid scales. Rank-1 accuracy and mAP are compared.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:1804.05275v4 [cs.CV] 10 Nov 2018</figDesc><table><row><cell>Query</cell><cell>Heatmap</cell><cell>R5</cell><cell>Heatmap</cell><cell>R5</cell></row><row><cell></cell><cell>w/ HPM</cell><cell></cell><cell></cell><cell>w/o HPM</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of the proposed method with the state-of-art on Market-1501, DukeMTMC-ReID and CUHK03 with new protocol. HPM is implemented with four pyramid scales and combine both average pooling and max pooling described inFig 3.</figDesc><table><row><cell>Model</cell><cell>R1</cell><cell>Market1501 R5</cell><cell>mAP</cell></row><row><cell>No Pyramid Structure</cell><cell>88.1</cell><cell>94.6</cell><cell>71.2</cell></row><row><cell>HPM</cell><cell cols="3">94.2(+6.1) 97.5(+2.9) 82.7(+11.5)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Evaluation of effectiveness of Pyramid Structure,</cell></row><row><cell>HPM uses four pyramid scales and combine both average</cell></row><row><cell>pooling and max pooling, Non pyramid structure split the</cell></row><row><cell>feature into same partitions as the last scale in HPM but</cell></row><row><cell>without pyramid structure</cell></row><row><cell>PCB+RPP ( i.e. 83.1% vs. 81.6%). Beyond PCB, the best</cell></row><row><cell>model aims to deal with different size of person Multi-</cell></row><row><cell>Scale (Chen et al. 2017) yields the mAP of 73.1% and Rank</cell></row><row><cell>1 accuracy of 88.9%. Our HPM model outperforms it by</cell></row><row><cell>5.3% and 10.0% on Rank 1 and mAP, respectively.</cell></row><row><cell>Results on DukeMTMC-ReID Person Re-ID results on</cell></row><row><cell>Duke MTMC-ReID are given in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>96.9 97.9 78.3 83.1 91.9 93.4 68.9 53.2 73.2 79.6 48.9 HPM + #PS 3 + Avg pool 256x(1+2+4) 92.3 97.2 97.9 79.3 84.5 92.4 94.1 70.8 58.2 76.7 83.1 52.8 HPM + #PS 4 + Avg pool 256x(1+2+4+8) 93.2 97.3 98.1 79.5 84.8 92.5 94.1 72.1 58.6 76.8 83.8 53.4 HPM + #PS 4 + Max pool 256x(1+2+4+8) 93.6 97.7 98.3 81.6 86.2 93.2 94.8 74.1 62.4 78.9 86.3 57.4 HPM + #PS 4 + Max+Avg pool 256x(1+2+4+8) 94.2 97.5 98.5 82.7 86.6 93.0 95.1 74.3 63.9 79.7 86.1 57.5</figDesc><table><row><cell>Model</cell><cell>Feature Dim</cell><cell>Market1501 R1 R5 R10 mAP R1 R5 R10 mAP R1 R5 R10 mAP DukeMTMC-ReID CUHK03</cell></row><row><cell>HPM + #PS 1 + Avg pool</cell><cell>256</cell><cell>88.1 94.6 96.4 71.2 79.3 89.7 91.9 61.0 39.2 61.1 71.6 37.3</cell></row><row><cell>HPM + #PS 2 + Avg pool</cell><cell>256x(1+2)</cell><cell>92.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">P p=1 exp((W p i,j ) T H i,j (I))</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We normalize each feature map of the last convolutional feature maps and sum them together to obtain the heatmap.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work is in part supported by IBM-ILLINOIS Center for Cognitive Computing Systems Research (C3SR) -a research collaboration as part of the IBM AI Horizons Network. Shi is supported in part by IARPA Deep Intermodal Video Analytics (IARPA DIVA). Fu and Zhou are supported by CloudWalk Technology.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An improved deep learning architecture for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jones</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marks ; Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep-person: Learning discriminative deep features for person re-identification</title>
		<idno type="arXiv">arXiv:1711.10658</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-level factorisation net for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hospedales</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09132</idno>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>A multi-task deep network for person re-identification</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Person re-identification by deep learning multiscale representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Temporal context network for activity localization in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient fine-grained classification and part localization using one compact network</title>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tan: Temporal aggregation network for dense multi-label action recognition</title>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep feature learning with relative distance comparison for person re-identification. Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<publisher>IEEE TPAMI</publisher>
		</imprint>
	</monogr>
	<note>Object detection with discriminatively trained part-based models</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sta: Spatial-temporal attention for large-scale videobased person re-identification</title>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scalable metric learning via weighted approximate rank component analysis</title>
		<idno type="arXiv">arXiv:1703.07737</idno>
	</analytic>
	<monogr>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning deep context-aware features over body and latent parts for person re-identification</title>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>IJCAI</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08122</idno>
		<idno>arXiv:1409.1556</idno>
	</analytic>
	<monogr>
		<title level="m">Very deep convolutional networks for largescale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>IEEE ICCV</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling</title>
		<idno type="arXiv">arXiv:1711.09349</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Rethinking the inception architecture for computer vision</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Glad: Global-local-alignment descriptor for pedestrian retrieval</title>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning deep feature representations with domain guided dropout for person re-identification</title>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>IEEE CVPR</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Alignedreid: Surpassing human-level performance in person re-identification</title>
		<idno type="arXiv">arXiv:1707.00798</idno>
		<idno>arXiv:1711.08184</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ICPR</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deeply-learned part-aligned representations for person re-identification</title>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Scalable person reidentification: A benchmark</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pedestrian alignment network for large-scale person re-identification</title>
		<idno type="arXiv">arXiv:1701.07732</idno>
		<idno>arXiv:1701.07717</idno>
	</analytic>
	<monogr>
		<title level="m">ACM TOMM. [Zheng, Zheng, and Yang</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>A discriminatively learned cnn embedding for person reidentification. Unlabeled samples generated by gan improve the person re-identification baseline in vitro</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Re-ranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
