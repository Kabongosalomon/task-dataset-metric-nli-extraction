<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the limit of English conversational speech recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zolt?n</forename><surname>T?ske</surname></persName>
							<email>zoltan.tuske@ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IBM Research AI</orgName>
								<orgName type="institution" key="instit2">Yorktown Heights</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Saon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IBM Research AI</orgName>
								<orgName type="institution" key="instit2">Yorktown Heights</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IBM Research AI</orgName>
								<orgName type="institution" key="instit2">Yorktown Heights</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On the limit of English conversational speech recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: encoder-decoder</term>
					<term>attention</term>
					<term>speech recognition</term>
					<term>AdamW</term>
					<term>Switchboard</term>
					<term>i-vector</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In our previous work we demonstrated that a single headed attention encoder-decoder model is able to reach state-of-theart results in conversational speech recognition. In this paper, we further improve the results for both Switchboard 300 and 2000. Through use of an improved optimizer, speaker vector embeddings, and alternative speech representations we reduce the recognition errors of our LSTM system on Switchboard-300 by 4% relative. Compensation of the decoder model with the probability ratio approach allows more efficient integration of an external language model, and we report 5.9% and 11.5% WER on the SWB and CHM parts of Hub5'00 with very simple LSTM models. Our study also considers the recently proposed conformer, and more advanced self-attention based language models. Overall, the conformer shows similar performance to the LSTM; nevertheless, their combination and decoding with an improved LM reaches a new record on Switchboard-300, 5.0% and 10.0% WER on SWB and CHM. Our findings are also confirmed on Switchboard-2000, and a new state of the art is reported, practically reaching the limit of the benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>End-to-end (E2E) automatic speech recognition (ASR) models directly map an acoustic feature sequence to a word sequence. Due to their universal capability to handle even non-monotonic alignment, attention models are widely used in many machine learning problems, e.g. translation <ref type="bibr" target="#b0">[1]</ref>. The extreme flexibility of these models also allows efficient transfer learning to a non-monotonic spoken language understanding problem from a monotonic speech recognition task <ref type="bibr" target="#b1">[2]</ref>. Furthermore, the attention model also holds the best record on many ASR tasks, e.g. <ref type="bibr" target="#b2">[3]</ref>. In <ref type="bibr" target="#b3">[4]</ref>, we have shown that attention models are able to reach state-of-the-art recognition performance even if only 300 hours of speech are available, using a long list of various regularization techniques. This work extends the best recipe proposed in that study to further advance recognition quality. The overview of the investigated methods and related works is presented in Section 2. The details about the updated experimental settings and results are presented in Section 3 and 4. In Section 5, the paper closes with conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methods</head><p>Extending our previous study in <ref type="bibr" target="#b3">[4]</ref>, by default we use various dropout methods, data augmentation and regularization approaches, curriculum learning and scheduled sampling techniques to mitigate the inherent data sparsity problem of direct sequence-to-sequence modeling <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>. On top of those, the following methods and modeling approaches are investigated in this paper. AdamW optimizer: The Adam optimizer combines adaptive stochastic gradient descent (SGD) optimization and classic momentum <ref type="bibr" target="#b17">[18]</ref>. Recently, a modification has been proposed, in which weight decay is decoupled from the adaptive gradient calculation <ref type="bibr" target="#b18">[19]</ref>. I-vector based speaker adaptation estimates an identity vector from incoming acoustic features. This vector transforms the parameters of a universal background model (UBM) in a shared sub-space to match the speaker dependent distribution. I-vectors have been successfully used for neural network based ASR, e.g. in <ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>. To combine with speed and tempo perturbation, the artificially created recordings of a speaker are treated as coming from a previously unseen speaker. Alternative log-Mel representations: in the standard feature extraction pipeline, amplitude spectra are usually extracted every 10ms, applying a 25-32ms time window. Studies on acoustic modeling of raw waveforms, however, suggest that higher temporal resolution might be beneficial <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. Therefore, we implement and investigate a reparametrized log-Mel extraction pipeline in which amplitude spectra are extracted every 2.5ms using a 10ms analysis window. Further, 7th root compression is applied instead of logarithmic, as in Gammatone or PLP features <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>. The alternative feature extraction pipeline is also extended with small-energy masking (SEM) perturbation <ref type="bibr" target="#b26">[27]</ref>. Relative to the peak energy in a given utterance, the method masks time-frequency bins with small energy in the Mel-spectral domain. We also propose to apply high energy clipping (HEC) distortion on the Mel-spectrum, on top of SEM. Denoting the amplitude spectrum as et,c, where t corresponds to the frame index and c the filterbank channel, the following function is applied: e (HEC) t,c = min(?c, et,c)</p><p>where ?c denotes a randomly chosen, channel dependent threshold above which the amplitude values get clipped, e.g. picking the 80th percentile per channel.</p><p>Convolutional self-attention modules: Recurrent hidden layers, e.g. long short-term memory (LSTM), are often chosen to model temporal dependencies in the speech signal <ref type="bibr" target="#b27">[28]</ref>. Selfattention based transformer models have been explored as encoder and also as decoder for ASR in <ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref>. Recently, the combination of convolutional neural network and transformer has been proposed and achieved state-of-the-art recognition results on read speech <ref type="bibr" target="#b2">[3]</ref>. Our study also investigates the effect of replacing the LSTM based encoder, as well as the replacement of our single-head LSTM decoder with a conformer. Self-attention based language models can easily capture long term dependencies and can outperform LSTM models <ref type="bibr" target="#b32">[33]</ref>. Further improvement was made to the model by introducing relative positional encoding and increased context length through segment-level recurrence <ref type="bibr" target="#b33">[34]</ref>. As shown by <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>, significant gain has been observed on the Switchboard test sets when the language model (LM) is trained at the conversation level. In this paper, we also investigate whether a cross sentence transformer or conformer LM outperforms the LSTM LM, similar to <ref type="bibr" target="#b36">[37]</ref>. External language model fusion through probability ratio: as proposed in <ref type="bibr" target="#b37">[38]</ref>, to decode with an end-to-end sequence posterior p (E2E) (w N 1 |x T 1 ) and external language model p (EXT) (w N 1 ), the language model learned by the E2E model p (E2E) (w N 1 ) has to be compensated.</p><formula xml:id="formula_1">p(w N 1 |x T 1 ) ? p (E2E) (w N 1 |x T 1 ) p (E2E) (w N 1 ) ? p (EXT) (w N 1 ) (2)</formula><p>where the fraction is proportional to p(x T 1 |w N 1 ), since p(x N 1 ) can be ignored during search. Thus, Eq. 2 is basically the famous hybrid model equation applied at the sequence level <ref type="bibr" target="#b38">[39]</ref>. Unlike the hybrid approach where the framewise priors can directly be estimated and integrated into the softmax layer, either using an alignment or by approximate marginalization using the training samples and the posterior distribution <ref type="bibr" target="#b39">[40]</ref>, it is difficult to estimate the sequence prior p (E2E) (w N 1 ). The usual solution is to estimate an additional, e.g. LSTM, model on the transcription of acoustic data and plug it into Eq. 2 after position-and sequence-level smoothing, according to the next paragraph. Model combination: an encoder-decoder model provides position-wise normalized scores p(wn|w n?1 1 , x T 1 ); thus, it is easy to apply classifier combination rules that use multiple models in making a decision <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>. Log-linear score combination and joint decoding with various models is an old idea in ASR <ref type="bibr" target="#b42">[43]</ref>, and shallow fusion simply corresponds to sequencelevel log-linear combination <ref type="bibr" target="#b43">[44]</ref>. Extending the concept to K LM and L E2E models, in this paper the following decision rule is used to find the optimal sequence:</p><formula xml:id="formula_2">wN 1 = argmax w N 1 ,N N n=1 K k=1 ?LM k log p k (wn|w n?1 1 ) + N n=1 L l=1 ?E2E l log p l (wn|w n?1 1 , x T 1 ) (3) + L l=1 ?CT l T t=1 I {?&gt;? l } (max n ? l,n,t ) + ?LENN ? ?</formula><p>where the first term corresponds to sequence level log-linear interpolation of multiple external language models, and includes the probability ratio model with negative weight. The second term corresponds to the combination of the end-to-end ASR models. The third term (applied only to single-head decoder) is the coverage term (CT), using decoder specific threshold ? l the indicator function I gives extra credit to hypotheses which cover the subsampled acoustic feature stream (T ? T ) with sharp decoder attention ?n,t <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref>. The last term controls the reward for emitting longer hypotheses <ref type="bibr" target="#b46">[47]</ref>. In addition, the position-wise priors and posteriors are also smoothed: pm(wn|?) = pm(wn|?) ?m / wn pm(wn|?) ?m .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental setup</head><p>Our research focuses on the standard Switchboard (SWB) English conversational telephony speech recognition benchmark. Data preparation follows the Kaldi s5c recipe <ref type="bibr" target="#b47">[48]</ref> and the work of <ref type="bibr" target="#b48">[49]</ref>. The setup is based on our previous best system; for further details refer to <ref type="bibr" target="#b3">[4]</ref>. Below, we focus mainly on the settings of the new components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Input features</head><p>Unless noted otherwise, 80-dimensional log-Mel features are extracted from a 25ms long window shifted by 10ms. When using the 2.5ms frame rate, the window size is reduced to 10ms, the number of Mel filters to 20, and the SpecAugment parameters are also adjusted, e.g. the temporal width of the mask is quadrupled. Results with SEM and HEC are only presented for high temporal resolution log-Mel features; standard features showed marginal improvement together with SpecAugment. SEM is turned on with 10% probability, the peak energy corresponds to the 95th percentile and the energy threshold is chosen randomly between -30 and -20dB. Instead of masking after normalization, we reset the Mel-filter outputs to the channel-wise utterancelevel mean values. The high energy clipping is applied 40% of the time, and the channel-wise threshold ?c is randomly selected from the interval of 80-100th percentile. The HEC operation is followed by rescaling to preserve the total energy of an utterance. Two frames of the high frame rate features are stacked and every second frame is skipped. The 100-dimensional i-vectors are extracted using a 2048-component 40-dimensional diagonal covariance Gaussian mixture UBM trained on PLP features transformed with LDA and semi-tied covariance transform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Sequence-to-sequence models</head><p>The 300 and 2k-hour systems are built on graphemic units created by 600 and 1000 BPE rules <ref type="bibr">[50]</ref>. Our default E2E model has a 6-layer bidirectional LSTM encoder and 2-layer unidirectional LSTM decoder. The conformer based decoder follows the structure of <ref type="bibr" target="#b49">[51]</ref>. The second multi-head attention units, which use absolute positional encoding and the encoded sequence as key and value, are inserted after the convolutional block. We note that our LSTM decoder has a single-head location-aware additive attention mechanism, while the conformer decoder has multiple layers of multiplicative multi-head attention <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b50">52]</ref>. In the SWB-300 experiments, the number of decoder and encoder conformer layers is limited to 3 and 12. The model and the inner dimensions of the feed-forward and convolution modules are set to 384, 1536, and 1280, and the number of heads and head dimension are 6 and 96. In the 2000-hour experiments 16-layer encoder and 6-layer decoder conformer modules are used, and the head and inner dimensions are increased to 128 and 2048. The dropout rates in the conformer modules are set to 15% and 20% for the 300 and 2000-hour setups. The input to the conformer encoder is processed by two 2D convolutional units with 128 and 256 output channels using 5x5 kernels, ReLU non-linearity and 10% dropout. The kernels are strided by two in each direction. Similarly, the LSTM encoder reduces the frame rate by a factor of 4 (or 8) to 1/40ms by pyramidal processing <ref type="bibr" target="#b51">[53]</ref>. The i-vectors are concatenated either to the inputs of every LSTM encoder layer or position-wise feed-forward layer of a conformer. In order to use a highly parallelized implementation, zoneout and scheduled sampling are not applied to the conformer decoder. In each model, the batch normalization layers are frozen and turned into global normalization layers in the middle of the training <ref type="bibr" target="#b3">[4]</ref>. The SWB-300 models are trained on 6 V100 GPUs with a batch size of 32 sequences per GPU. The SWB-2000 models are optimized on 24 GPUs with variable batch size of up to 128 sequences per GPU; processing 2000 hours of speech e.g. by conformer-conformer model took about 20 minutes. In every case, the first iterations are used to warm up the learning rate <ref type="bibr" target="#b52">[54]</ref>. The learning rate is then kept constant and annealed exponentially in the last 25% of the training by a total factor of 256. When switching from SGD to AdamW, the initial learning rate had to be reduced by a factor of 30. Overall 450-500k updates are performed until convergence to maximize sentence posterior probability (cross-entropy training).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Language models</head><p>The language models are trained on the 24M-word Switch-board+Fisher data. As in our previous study, the unidirectional LSTM language model has two layers and 2048 nodes per layer. The transformer-XL language model is based on the implementation of <ref type="bibr" target="#b33">[34,</ref><ref type="bibr">55]</ref>, having 10 self-attention layers, 8 heads with  set to 2048 and 512. The conformer LM has a similar structure, but the inner dimension in the feed-forward blocks is reduced to 1536 and 768, and the convolution module's inner dimension is set to 768, and batch normalization is deactivated. The macaron-like, second feed-forward units are also switched off. For both models, the dropout rates are set to 0.05, and the memory of the self-attention is unconstrained, limited only by the input sequence length. The cross-utterance models are initialized with the model trained on independent utterances. Their input is constructed by concatenation of successive utterances of a conversation channel up to 150 words. The denominator model of Eq. 2 follows the decoder structure of a corresponding E2E model, and is trained on the reference transcriptions of the acoustic data. All models are trained by SGD optimization with batch size of 256 sequences and Nesterov momentum <ref type="bibr" target="#b53">[56]</ref>. The decoding hyper parameters are optimized on Hub5'00, iteratively by one dimensional grid search. During search, the beam size is limited to 16. To validate our findings and avoid learning on the test, we also evaluated the best systems on RT03 and Hub5'01, measuring word error rate (WER).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison of SGD and AdamW optimizers, effect of i-vector on LSTM model</head><p>As can be seen in <ref type="table" target="#tab_0">Table 1</ref>, AdamW significantly improves the quality of the LSTM model, on average by 7% without LM and 4% after shallow fusion with cross-utterance LM. I-vectors improve the results further, and give small but consistent gain after decoding with LM. We note that the best result is produced with a 57M-parameter E2E model in <ref type="table" target="#tab_0">Table 1</ref>, and it already outperforms the 280M-parameter model developed in <ref type="bibr" target="#b3">[4]</ref>. In <ref type="table" target="#tab_1">Table 2</ref>, we test our alternative, high temporal resolution log-Mel speech representations. Comparing to the best results in <ref type="table" target="#tab_0">Table 1</ref>, it can be observed that the two features perform similarly. The systems turned out to be complementary, and their combination resulted in significant gain, see e.g. <ref type="table" target="#tab_4">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiments with conformers</head><p>In the next set of experiments, we analyzed whether conformer or LSTM elements are more beneficial in the encoder and decoder modules of an attention based E2E model. In <ref type="table" target="#tab_2">Table 3</ref>, the conformer-LSTM encoder-decoder shows 0.4% absolute gain over the LSTM-LSTM configuration. I-vectors have inconsistent effects on conformer-LSTM E2E. We also note that the conformer-encoder model ran significantly faster than the  LSTM based one. Using beam size of 4, the model achieved 0.12 real-time factor (RTF) without loss of accuracy on a single core of an Intel Xeon E5-2690v4. After 8-bit integer quantization of the feed-forward and LSTM weights, we measured 0.08 RTF on a single core of an Intel Xeon Platinum 8280M CPU and 0.1% WER degradation on Hub5'00. Switching to a conformer based decoder turned out to be less efficient than a single head LSTM decoder, and the performance degraded. For a more fair comparison, we also implemented scheduled sampling (SS) for the conformer decoder, and applied teacher forcing with 0.8 probability. As can be seen, SS indeed improves the conformer decoder results (by 2% relative), nevertheless the training time increases by more than 30%. Surprisingly, after decoding with external LM, the conformer-decoder results improve only 0.4% absolute, compared to the 0.8-0.9% gain we measure with conformer-LSTM model. For the sake of completeness, we also ran experiments with LSTM-encoder conformer-decoder models. Such model gives even worse results than the conformerconformer architecture, and we measure 0.4% absolute WER degradation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Effect of decoding with external language model</head><p>Shallow fusion showed large improvement even with the best performing conformer model in <ref type="table" target="#tab_2">Table 3</ref>. Additional experiments were designed to investigate the effect of probability ratio fusion and self-attention LMs. As can be seen in <ref type="table" target="#tab_3">Table 4</ref>, incorporating the external LM through probability ratio fusion results in 0.2-0.3% absolute WER improvement. The gain is consistent even if cross-utterance external LM is used. Further, transformer-XL LM gives additional 0.2% reduction. Conformer LM turned out to be not better than transformer-XL. Since the probability ratio approach already corresponds to LM combination, we also ran recognition experiments with combined cross-utterance LSTM and attention based LMs. As can be seen, slight improvement is measured over the best "single" LM result in <ref type="table" target="#tab_3">Table 4</ref>. Decoding with the best model runs at 0.6 RTF on a K80 GPU. We also ran probability ratio LM fusion experiments with conformer decoders (results not presented), in order to get a better understanding of the small effect of the external language model with such decoders we observed in Section 4.2. Although the probability ratio approach improved the conformer decoder result more (0.4% absolute WER improvement without i-vector), the conformer decoder still lagged significantly behind the LSTM decoder. The gap remained roughly the same irrespective of the type and depth of the denominator LM. We hypothesize that a deep multi-headed self-attention LM learns a significantly different mechanism than the decoder of a multi-headed ASR. Thus, a simple model plug-in is not effective in recovering a score proportional to the emission likelihood p(x T 1 |w N 1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Combination experiments</head><p>Besides decoding with the combination of self-attention, LSTM and probability ratio LMs, the application of multiple E2E models is also tested. The detailed results of the single and combined systems are shown in the upper section of <ref type="table" target="#tab_4">Table 5</ref>. The superiority of the conformer over LSTM encoder is also confirmed on the Hub5'01 and RT03 sets. The LSTM encoder trained on high temporal-resolution log-Mel and i-vector features is nevertheless fairly competitive, especially after decoding with combined LSTM+TrafoXL LM. The combination of E2E models without external LM resulted in significant gains, in some cases over 10% relative WER improvement over a very strong baseline. E.g. WER on Hub5'00 CHM improved from 13.0% to 11.9%. The i-vector has inconsistent effect on the combination. Surprisingly, the effect of decoding with external language models is not mitigated by the combination of E2E models, and additional 16-30% relative WER improvement is observed. We also note that our best 300-hour results could even match system combination results on 2000 hours published only a few years ago <ref type="bibr" target="#b54">[57]</ref>, which clearly indicates the great progress made in the recent years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Experiments on Switchboard-2000</head><p>Building similar systems on 2000 hours of speech data, we made the following observations. Without retraining the UBM and total variability matrix, i-vectors showed inconsistent gain on the evaluation sets. Scheduled sampling improved conformer decoder based E2E model by 3% relative on the larger dataset.</p><p>Not surprisingly, probability ratio fusion did not improve the results, because the E2E and LM were trained on the same dataset. Moreover, switching from SGD to AdamW did not show consistent improvement with large scale models; the objective comparison can be made by contrasting the best LSTM results in <ref type="bibr" target="#b3">[4]</ref> and the corresponding row in <ref type="table" target="#tab_4">Table 5</ref>.</p><p>As results indicate in <ref type="table" target="#tab_4">Table 5</ref>, a 100M-parameter conformerencoder outperforms the large scale LSTM models. On average, the best single-model performance is achieved by conformerconformer model, but also note the discrepancy on the Fisher subset of RT03. Overall, system and language model combination improves the state-of-the-art by 8-14% relative, compared to the previous best numbers reported in <ref type="bibr" target="#b3">[4]</ref>. The improvement is also confirmed on RT02 and RT04 sets, where the best combination achieves 6.3% and 5.2% WER. The performance of the best system combination on SWB part of Hub5'00 (4.3%) is clearly below the human error rate (5.1%) measured in <ref type="bibr" target="#b48">[49]</ref>. This can be attributed to fact that most of the speakers appear in the training data, decoder hyperparameters are optimized on Hub5'00, and the human error rate might also have been overestimated. We note that the 4-E2E combination with multiple LMs can run on a modern V100 GPU at 0.4 RTF. Considering that human performance, and thus transcription error rate, is at 6.8%, 6.0%, 4.5%, 4.7% WER on the CHM subset of Hub5'00 and on the RT0{2,3,4} sets <ref type="bibr" target="#b48">[49]</ref>, it is surprising to see how close we can actually get with 2000 or even only 300 hours of data using fairly simple and general attention models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We investigated a set of techniques on top of our best recipe for the Switchboard English speech recognition benchmark. We showed that complementary systems with similar performances can be developed using different input features or diverse encoder, decoder and language model structures. Using a more advanced optimizer and score combination techniques, our final Switchboard-300 system achieved 5.0% and 10.0% WER on the SWB and CHM subsets of Hub5'00. Joint decoding with such models trained on Switchboard-2000 achieves a new state of the art: the flexible attention models are able to reach the limit of many of the standard evaluation sets. Future work could focus on how to achieve such performance with a limited amount of transcriptions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Effect of AdamW optimizer and i-vector on LSTM based encoderdecoder with and without using external language model in shallow fusion. Measured on Switchboard-300.</figDesc><table><row><cell>optimizer ivec.</cell><cell>swb</cell><cell>w/o LM chm</cell><cell>tot.</cell><cell>swb</cell><cell>w/ LM chm</cell><cell>tot.</cell></row><row><cell>SGD</cell><cell>7.5</cell><cell>14.8</cell><cell>11.2</cell><cell>6.4</cell><cell>13.2</cell><cell>9.8</cell></row><row><cell>AdamW</cell><cell>6.9 6.8</cell><cell>13.8 13.5</cell><cell>10.4 10.2</cell><cell>6.1 6.0</cell><cell>12.6 12.4</cell><cell>9.4 9.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Experiments with alternative log-Mel speech representation on Switchboard-300 using LSTM model. WER measured after decoding with cross-utterance LM in shallow fusion.</figDesc><table><row><cell cols="3">logMel win. [ms] dim. step size</cell><cell>SEM HEC</cell><cell>optim. ivec.</cell><cell>swb</cell><cell>WER [%] hub5'00 chm</cell><cell>tot.</cell></row><row><cell>10</cell><cell>25</cell><cell>80</cell><cell></cell><cell>AdamW</cell><cell>6.0</cell><cell>12.4</cell><cell>9.3</cell></row><row><cell>2.5</cell><cell>10</cell><cell>20</cell><cell></cell><cell>SGD AdamW</cell><cell>6.5 6.2 6.2 6.2</cell><cell>13.1 13.0 12.3 12.0</cell><cell>9.8 9.6 9.3 9.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison of conformer and LSTM encoder/decoder blocks on Switchboard-300. External LSTM LM operates across utterances using shallow fusion.</figDesc><table><row><cell>model enc. dec.</cell><cell>optim.</cell><cell>sched. ivec. samp.</cell><cell>ext. LM swb</cell><cell>hub5'00 chm</cell><cell>tot.</cell></row><row><cell cols="2">LSTM LSTM AdamW</cell><cell></cell><cell>6.0</cell><cell cols="2">12.4 09.3</cell></row><row><cell></cell><cell>SGD</cell><cell></cell><cell>6.8</cell><cell cols="2">13.5 10.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>6.7</cell><cell cols="2">13.0 09.8</cell></row><row><cell>Conf. LSTM</cell><cell>AdamW</cell><cell></cell><cell>5.8 6.5</cell><cell cols="2">12.0 08.9 12.9 09.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell>5.9</cell><cell cols="2">11.8 08.9</cell></row><row><cell></cell><cell>SGD</cell><cell></cell><cell>6.9</cell><cell cols="2">14.2 10.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell>7.0</cell><cell cols="2">13.6 10.3</cell></row><row><cell>Conf. Conf.</cell><cell>AdamW</cell><cell></cell><cell>6.9 6.3</cell><cell cols="2">13.4 10.1 13.1 09.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell>6.8</cell><cell cols="2">13.3 10.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>6.1</cell><cell cols="2">12.9 09.5</cell></row><row><cell cols="2">LSTM Conf. AdamW</cell><cell></cell><cell>7.6</cell><cell cols="2">13.9 10.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Effect of external language model on conformer encoderdecoder model on Switchboard-300.</figDesc><table><row><cell></cell><cell>LM</cell><cell>PPL</cell><cell></cell><cell>WER [%]</cell><cell></cell></row><row><cell></cell><cell>#param. xutt. prob. ratio</cell><cell>CV</cell><cell>swb</cell><cell>hub5'00 chm</cell><cell>tot.</cell></row><row><cell></cell><cell>n/a</cell><cell></cell><cell>6.7</cell><cell>13.0</cell><cell>9.8</cell></row><row><cell>LSTM</cell><cell>57M</cell><cell>52.9 44.1</cell><cell>5.9 5.8 5.8 5.7</cell><cell>12.2 11.8 12.0 11.4</cell><cell>9.0 8.8 8.9 8.6</cell></row><row><cell cols="2">TrafoXL 35M</cell><cell cols="2">39.3 5.6</cell><cell>11.2</cell><cell>8.4</cell></row><row><cell>Conf.</cell><cell>42M</cell><cell cols="2">39.0 5.6</cell><cell>11.3</cell><cell>8.5</cell></row><row><cell cols="2">LSTM + TrafoXL</cell><cell></cell><cell>5.5</cell><cell>11.2</cell><cell>8.4</cell></row><row><cell cols="2">LSTM + TrafoXL + Conf.</cell><cell></cell><cell>5.4</cell><cell>11.1</cell><cell>8.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Overall results on Switchboard 300 and 2000 with single and combined end-to-end and cross-utterance language models.</figDesc><table><row><cell></cell><cell cols="2">id enc.</cell><cell cols="3">E2E dec. #parm. step [ms] fea.ext ivec.</cell><cell>ext. LM</cell><cell cols="2">hub5'00 swb chm</cell><cell cols="4">hub5'01 swb swb2p3 swb2p4 swb rt03</cell><cell>fsh</cell></row><row><cell></cell><cell>1 2 3</cell><cell>LSTM</cell><cell></cell><cell>57M 61M 56M</cell><cell>10</cell><cell>LSTM</cell><cell>6.0 5.8 6.0</cell><cell>12.0 12.1 12.0</cell><cell>6.6 6.6 6.7</cell><cell>8.8 8.6 8.6</cell><cell>12.8 12.8 12.5</cell><cell>13.6 14.0 13.2</cell><cell>7.5 7.7 7.7</cell></row><row><cell>SWB-300</cell><cell cols="2">4 5 Conf.</cell><cell>LSTM</cell><cell>60M 68M</cell><cell>2.5 10</cell><cell>LSTM+TrafoXL -LSTM LSTM+TrafoXL</cell><cell>5.9 5.7 6.7 5.7 5.5</cell><cell>11.5 11.3 13.0 11.4 11.2</cell><cell>6.5 6.1 7.1 6.2 6.1</cell><cell>8.5 8.3 9.2 7.8 7.7</cell><cell>11.6 11.3 13.7 11.4 11.4</cell><cell>13.0 12.6 15.7 12.8 12.6</cell><cell>7.2 7.0 9.1 7.2 7.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1+3+5 2+4+5</cell><cell></cell><cell>-LSTM+TrafoXL</cell><cell>6.1 5.1 5.0</cell><cell>11.9 10.1 10.0</cell><cell>6.4 5.5 5.3</cell><cell>8.8 6.9 7.0</cell><cell>12.6 10.5 10.4</cell><cell>13.7 10.7 10.8</cell><cell>8.0 6.1 6.0</cell></row><row><cell></cell><cell cols="2">6 LSTM 7</cell><cell></cell><cell>661M 663M</cell><cell>10 2.5</cell><cell>LSTM</cell><cell>4.6 4.9</cell><cell>07.8 07.7</cell><cell>5.2 5.5</cell><cell>6.1 6.2</cell><cell>10.0 09.8</cell><cell>08.0 08.4</cell><cell>6.6 6.7</cell></row><row><cell>SWB-2k</cell><cell cols="4">LSTM 9 Conf. Conf. 154M 8 Conf. 099M</cell><cell>10</cell><cell>-LSTM LSTM+TrafoXL -</cell><cell>4.8 4.7 4.6 4.7 4.5</cell><cell>08.0 07.6 07.6 07.6 07.3</cell><cell>5.2 5.0 4.9 5.0 4.9</cell><cell>6.4 6.1 6.2 6.1 5.8</cell><cell>10.3 09.7 09.5 10.0 09.4</cell><cell>08.2 07.7 07.8 08.0 07.6</cell><cell>6.7 5.9 5.9 7.0 6.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>6+7</cell><cell></cell><cell>LSTM+TrafoXL</cell><cell>4.5</cell><cell>07.2</cell><cell>4.9</cell><cell>5.7</cell><cell>09.2</cell><cell>07.6</cell><cell>6.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">6+7+8+9</cell><cell></cell><cell></cell><cell>4.3</cell><cell>06.8</cell><cell>4.6</cell><cell>5.5</cell><cell>09.0</cell><cell>07.2</cell><cell>5.9</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">End-to-end spoken language understanding without full transcripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K J</forename><surname>Kuo</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="906" to="910" />
		</imprint>
	</monogr>
	<note>Interspeech, 2020</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Conformer: Convolution-augmented transformer for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5036" to="5040" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Single headed attention based sequence-tosequence model for state-of-the-art results on Switchboard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>T?ske</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="551" to="555" />
		</imprint>
	</monogr>
	<note>Interspeech, 2020</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1207.0580" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using DropConnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Elastic spectral distortion for low resource speech recognition with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Obuchi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ASRU</publisher>
			<biblScope unit="page" from="309" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Audio augmentation for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3586" to="3589" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sequence noise injected training for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="6261" to="6265" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">SpecAugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2613" to="2617" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Enhanced MLP performance and fault tolerance resulting from synaptic weight noise during training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="792" to="802" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Zoneout: regularizing RNNs by randomly preserving hidden activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A simple weight decay can improve generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Hertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="950" to="957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-to-end speech recognition in English and Mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Speaker adaptation of neural network acoustic models using i-vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="55" to="59" />
		</imprint>
		<respStmt>
			<orgName>ASRU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Cumulative adaptation for BLSTM acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kitza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="754" to="758" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Advancing RNN transducer technology for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning multiscale features directly from waveforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1305" to="1309" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Acoustic modeling of speech waveform based on multi-resolution, neural network signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>T?ske</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schl?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICASSP</title>
		<imprint>
			<biblScope unit="page" from="4859" to="4863" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Perceptual linear predictive (PLP) analysis of speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1738" to="1752" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gammatone features and feature combination for large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schl?ter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="649" to="652" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Small energy masking for improved neural network training for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Indurthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="7684" to="7688" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Self-attentional acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sperber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3723" to="3727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A time-restricted self-attention layer for ASR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICASSP</title>
		<imprint>
			<biblScope unit="page" from="5874" to="5878" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Transformer transducer: A streamable speech recognition model with transformer encoders and RNN-T loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="7829" to="7833" />
		</imprint>
	</monogr>
	<note>Interspeech, 2020</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Speech-transformer: A no-recurrence sequence-to-sequence model for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICASSP</title>
		<imprint>
			<biblScope unit="page" from="5884" to="5888" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Character-level language modeling with deeper self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conf. on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3159" to="3166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Investigation on LSTM recurrent n-gram language models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>T?ske</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schl?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3358" to="3362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The Microsoft 2017 conversational speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICASSP</title>
		<imprint>
			<biblScope unit="page" from="5934" to="5938" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Training language models for long-span crosssentence evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="419" to="426" />
		</imprint>
		<respStmt>
			<orgName>ASRU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A density ratio approach to language model fusion in end-to-end automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Variani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ASRU</publisher>
			<biblScope unit="page" from="434" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Bourlard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
		<title level="m">Connectionist Speech Recognition: A Hybrid Approach</title>
		<meeting><address><addrLine>Norwell, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Semi-supervised maximum mutual information training of deep neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2630" to="2634" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On combining classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="226" to="239" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Combination and joint training of acoustic classifiers for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kirchhoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASR2000, ISCA Tutorial and Research Workshop</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="17" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Combining TDNN and HMM in a hybrid system for improved continuous-speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dugast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Devillers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Aubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="217" to="223" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">On using monolingual corpora in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>G?l?ehre</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1503.03535" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Towards better decoding and language model integration in sequence to sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1612.02695" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Modeling coverage for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="76" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Deep Speech: Scaling up end-toend speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.5567" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASRU</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">English conversational telephone speech recognition by humans and machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="132" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Chorowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: a neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="4960" to="4964" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Accurate, large minibatch SGD: Training imagenet in 1 hour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<idno>org/abs/1706.02677 [55</idno>
		<ptr target="https://github.com/NVIDIA/DeepLearningExamples" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A method of solving a convex programming problem with convergence rate O(1/k 2 )</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Soviet Mathematics Doklady</title>
		<imprint>
			<date type="published" when="1983" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="372" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">The CAPIO 2017 conversational speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Han</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1801.00059" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
