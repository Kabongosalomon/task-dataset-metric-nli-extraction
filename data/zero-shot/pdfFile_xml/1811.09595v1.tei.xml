<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spectral Multigraph Networks for Discovering and Fusing Relationships in Molecules</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Knyazev</surname></persName>
							<email>bknyazev@uoguelph.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Guelph</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Vector Institute for Artificial Intelligence</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
							<email>gwtaylor@uoguelph.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Guelph</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Vector Institute for Artificial Intelligence</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Canadian Institute for Advanced Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Lin</surname></persName>
							<email>xiao.lin@sri.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">R</forename><surname>Amer</surname></persName>
							<email>mohamed.amer@sri.com</email>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">SRI International</orgName>
								<address>
									<addrLine>201 Washington Rd Princeton</addrLine>
									<postCode>NJ08540</postCode>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Spectral Multigraph Networks for Discovering and Fusing Relationships in Molecules</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Spectral Graph Convolutional Networks (GCNs) are a generalization of convolutional networks to learning on graph-structured data. Applications of spectral GCNs have been successful, but limited to a few problems where the graph is fixed, such as shape correspondence and node classification. In this work, we address this limitation by revisiting a particular family of spectral graph networks, Chebyshev GCNs, showing its efficacy in solving graph classification tasks with a variable graph structure and size. Chebyshev GCNs restrict graphs to have at most one edge between any pair of nodes. To this end, we propose a novel multigraph network that learns from multi-relational graphs. We model learned edges with abstract meaning and experiment with different ways to fuse the representations extracted from annotated and learned edges, achieving competitive results on a variety of chemical classification benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convolutional Neural Networks (CNNs) have seen wide success in domains where data is restricted to a Euclidean space. These methods exploit properties such as stationarity of the data distributions, locality and a well-defined notation of translation, but cannot model data that is non-Euclidean in nature. Such structure is naturally present in many domains, such as chemistry, physics, social networks, transportation systems, and 3D geometry, and can be expressed by graphs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. By defining an operation on graphs analogous to convolution, Graph Convolutional Networks (GCNs) have extended CNNs to graph-based data. The earliest methods performed convolution in the spectral domain <ref type="bibr" target="#b2">[3]</ref>, but subsequent work has proposed generalizations of convolution in the spatial domain. There have been multiple successful applications of GCNs to node classification <ref type="bibr" target="#b3">[4]</ref> and link prediction <ref type="bibr" target="#b4">[5]</ref>, whereas we target graph classification similarly to <ref type="bibr" target="#b5">[6]</ref>.</p><p>Our focus is on multigraphs, a graph that is permitted to have multiple edges. Multigraphs are important in many domains, such as chemistry and physics. The challenge of generalizing convolution to graphs and multigraphs is to have anisotropic convolution kernels (such as edge detectors). Anisotropic models, such as MoNet <ref type="bibr" target="#b6">[7]</ref> and SplineCNN <ref type="bibr" target="#b7">[8]</ref>, rely on coordinate structure, work well for vision tasks, but are suboptimal for non-visual graph problems. Other general models exist <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, but making them efficient for a variety of tasks conflicts with the "no free lunch theorem".</p><p>Compared to non-spectral GCNs, spectral models have filters with more global support, which is important for capturing complex relationships. We rely on Chebyshev GCNs (ChebNet) <ref type="bibr" target="#b10">[11]</ref> that enjoy an explicit control of receptive field size. Even though it was originally derived from spectral methods <ref type="bibr" target="#b2">[3]</ref>, it does not suffer from their main shortcoming -sensitivity of learned filters to graph size and structure.</p><p>Contributions: We propose a scalable spectral GCN that learns from multigraphs by capturing multi-relational graph paths as well as multiplicative and additive interactions to reduce model complexity and learn richer representations. We also learn new abstract relationships between graph nodes, beyond the ones annotated in the datasets. To our knowledge, we are the first to demonstrate that spectral methods can efficiently solve problems with variable graph size and structure, where this kind of method is generally believed not to perform well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Multigraph Convolution</head><p>While we provide the background to understand our model, a review of spectral graph methods is beyond the scope of this paper. Section 6.1 of the Appendix reviews spectral graph convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Approximate spectral graph convolution</head><p>We consider an undirected, possibly disconnected, graph G = (V, E) with N nodes, V, and edges, E, having values in range [0, 1]. Nodes v i ? V usually represent specific semantic concepts such as atoms in a chemical compound or users in a social network. Nodes can also denote abstract blocks of information with common properties, such as superpixels in images. Edges e ij ? E define the relationships between nodes and the scope over which node effects may propagate.</p><p>In spectral graph convolution <ref type="bibr" target="#b2">[3]</ref>, the filter g ? R N is defined on an entire input space. Although it makes filters global, which helps to capture complex relationships, it is also desirable to have local support since the data often have local structure and since we want to learn filters independent on the input size N to make the model scalable.</p><p>To address this issue, we can model this filter as a function of eigenvalues ? (which is assumed to be constant) of the normalized symmetric graph Laplacian L: g = g(?). We can then approximate it as a sum of K terms using the Chebyshev expansion, where each term T k (?) = 2?T k?1 (?)?T k?2 (?) contains powers ? k . Finally, we apply the property of eigendecomposition:</p><formula xml:id="formula_0">L k = (U ?U T ) k = U ? k U T .<label>(1)</label></formula><p>By combining this property with the Chebyshev expansion of g(?), we exclude eigenvectors U ? R N ?N , that are often infeasible to compute, from spectral graph convolution, and instead express the convolution as a function of graph Laplacian L. In general, for the input X ? R N ?Xin with N nodes and X in -dimensional features in each node, the approximate convolution is defined as:</p><formula xml:id="formula_1">Y =X?,<label>(2)</label></formula><p>whereX ? R N ?XinK are features projected onto the Chebyshev basis T k (L) and concatenated for all orders k ? [0, K ? 1] and ? ? R XinK?Xout are trainable weights, whereL = L ? I.</p><p>This approximation scheme was proposed in <ref type="bibr" target="#b10">[11]</ref>, and Eq. 2 defines the convolutional layer in the Chebyshev GCN (ChebNet), which is the basis for our method. Convolution is an essential computational block in graph networks, since it permits the gradual aggregation of information from neighboring nodes. By stacking the operator in Eq. 2, we capture increasingly larger neighborhoods and learn complex relationships in graph-structured data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graphs with variable structure and size</head><p>The approximate spectral graph convolution (Eq. 2) enforces spatial locality of the filters by controlling the order of the Chebyshev polynomial K. Importantly, it reduces the computational complexity of spectral convolution from O(N 2 ) to O(K|E|), making it much faster in practice assuming the graph is sparsely connected and sparse matrix multiplication is implemented. In this work, we observe an important byproduct of this scheme: that learned filters become less sensitive to changes in graph structure and size due to excluding the eigenvectors U from spectral convolution, so that learned filters are not tied to U .</p><p>The only assumption that still makes a trainable filter? sensitive to graph structure is that we model it as a function of eigenvalues?(?). However, the distribution of eigenvalues of the normalized Laplacian is concentrated in a limited range, making it a weaker dependency on graphs than the spectral convolution via eigenvectors, so that learned filters generalize well to new graphs. Moreover, since we use powers ofL in performing convolution (Eq. 2), the distribution of eigenvalues? further contracts due to exponentiation of the middle term on the RHS of Eq. 1. We believe that this effect accounts for the robustness of learned filters to changes in graph size or structure ( <ref type="figure" target="#fig_0">Figure 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Graphs with multiple relation types</head><p>In the approximate spectral graph convolution (Eq. 2), the graph LaplacianL encodes a single relation type between nodes. Yet, a graph may describe many types of distinct relations. In this section, we address this limitation by extending Eq. 2 to a multigraph, i.e. a graph with multiple (R ? 1) edges (relations) between the same nodes encoded as a set of graph Laplacians {L (r) } R 1 , where R is an upper bound on the number of edges per dyad. Extensions to a multigraph can also be applied to early spectral models <ref type="bibr" target="#b2">[3]</ref> but, since ChebNet was shown to be superior in downstream tasks, we choose to focus on the latter model.</p><p>Two dimensional Chebyshev polynomial. The Chebyshev polynomial used in Eq. 2 (see Section 6.1 in Appendix for detail) can be extended for two variables (relations in our case) similarly to bilinear models, e.g. as in <ref type="bibr" target="#b11">[12]</ref>:</p><formula xml:id="formula_2">T ij (L (r1) ,L (r2) ) = T i (L (r1) )T j (L (r2) ), i, j = 0, ..., K ? 1,<label>(3)</label></formula><p>and, analogously, for more variables. For R = 2, the convolution is then defined as:</p><formula xml:id="formula_3">Y = [X 0,0 ,X 0,1 , ...,X i,j , ...,X K?1,K?1 ]?,<label>(4)</label></formula><formula xml:id="formula_4">whereX i,j = T i (L (r1) )T j (L (r2) )X.</formula><p>In this case, we allow the model to leverage graph paths consisting of multiple relation types ( <ref type="figure">Figure 2</ref>). This flexibility, however, comes at a great computational cost, which is prohibitive for a large number of relations R or large order K due to exponential growth of the number of parameters: ? ? R XinK R Xout . Moreover, as we demonstrate in our experiments, such multi-relational paths do not necessary lead to better performance.</p><p>Multiplicative and additive fusion. Motivated by multimodal fusion considered in the Visual Question Answering literature (e.g. <ref type="bibr" target="#b12">[13]</ref>), we propose the multiplicative operator:</p><formula xml:id="formula_5">Y = [f 0 (X (0) ) f 1 (X (1) ) ... f R?1 (X (R?1) )]?,<label>(5)</label></formula><p>where f r is a learnable differentiable transformation for relation type r andX (r) are features projected onto the Chebyshev basis T k (L (r) ). In this case, node features interact in a multiplicative way. The advantage of this method is that it can learn separate f r for each relation and has fewer trainable parameters preventing overfitting, which is especially important for large K and R. The element-wise multiplication in Eq. 5 can be replaced with summation to perform additive fusion.</p><formula xml:id="formula_6">v 4 v 2 v 5 v 3 v 1 v 4 v 2 v 5 v 3 v 1 Edge type r 1 Edge type r 2 2d Chebyshev Other methods v 4 v 2 v 5 v 3 v 1 v 4 v 2 v 5 v 3 v 1 Edge type r 1 Edge type r 2 2d Chebyshev Other methods v 4 v 2 v 5 v 3 v 1 v 4 v 2 v 5 v 3 v 1 Edge type r 1 Edge type r 2 2d</formula><p>Chebyshev Other methods (a) (b) <ref type="figure">Figure 2</ref>: Comparison of (a) the fusion method based on a two-dimensional (2d) Chebyshev polynomial (Eq. 3, 4) to (b) other proposed methods in case of a 2-hop filter (a filter averaging features of nodes located two edges away from the filter center -v 1 in this case). Note that (a) can leverage multi-relational paths and the filter centered at node v 1 can access features of the node v 3 , which is not possible for other methods (b). In this work, edge type r 1 can denote annotated relations, while r 2 can denote learned ones (Eq. 7). We also allow for three and more relation types.</p><p>Shared projections. Another potential strength of the approach in Eq. 5 is that we can further decrease model complexity by sharing parameters of f r between the relation types, so that the total number of trainable parameters does not depend on the number of relations R. Despite useful practical properties, as we demonstrate in the experiments, it is usually hard for a single shared f r to generalize between different relation types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concatenating edge features.</head><p>A more straightforward approach is to concatenate featuresX (r) for all R relation types and learn a single matrix of weights ? ? R XinKR?Xout :</p><formula xml:id="formula_7">Y = [X (0) ,X (1) , ...,X (R?1) ]?.<label>(6)</label></formula><p>This method, however, does not scale well for large R, since the dimensionality of ? grows linearly with R. Note that even though multi-relational paths are not explicit in Eq. 5 and 6, for a multilayer network, relation types will still communicate through node features. In <ref type="figure">Figure 2</ref>, node v 2 will contain features of node v 3 after the first convolutional layer, so that in the second layer the filter centered at node v 1 will have access to features of node v 3 by accessing features of node v 2 . Compared to the 2d polynomial convolution defined by Eq. 4, the concatenation-based, multiplicative and additive approaches require more layers to have a larger multi-relational receptive field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multigraph Convolutional Networks</head><p>A frequent assumption of current GCNs is that there is at most one edge between any pair of nodes in a graph. This restriction is usually implied by datasets with such structure, so that in many datasets, graphs are annotated with the single most important relation type, for example, whether two atoms in a molecule are bonded <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. Meanwhile, data is often complex and nodes tend to have multiple relationships of different semantic, physical, or abstract meanings. Therefore, we argue that there could be other relationships captured by relaxing this restriction and allowing for multiple kinds of edges, beyond those annotated in the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning edges</head><p>Prior work (e.g. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref>), proposed methods to learn from multiple edges, but similarly to the methods using a single edge type <ref type="bibr" target="#b16">[17]</ref>, they leveraged only predefined (annotated) edges in the data. We devise a more flexible model, which, in addition to learning from an arbitrary number of predefined relations between nodes (see Section 2.3), learns abstract edges jointly with a GCN. We propose to learn a new edge e (r)</p><p>ij between any pair of nodes v i and v j with features X i and X j using a trainable similarity function:</p><formula xml:id="formula_8">e (r) ij = exp (f edge (X i , X j )) k?[1,|V|] exp (f edge (X i , X k )) ,<label>(7)</label></formula><p>where the softmax is used to enforce sparse connections and f edge can be any differentiable function such as a multilayer perceptron in our work. This idea is similar to <ref type="bibr" target="#b17">[18]</ref>, built on the early spectral convolution model <ref type="bibr" target="#b2">[3]</ref>, which learned an adjacency matrix, but targeted classification tasks for non graph-structured data (e.g. document classification, with each document is represented as a feature vector). Moreover, we learn this matrix jointly with a more recent graph classification model <ref type="bibr" target="#b10">[11]</ref> and, additionally, efficiently fuse predefined and learned relations. Eq. 7 is also similar to that of <ref type="bibr" target="#b3">[4]</ref>, which used this functional form to predict an attention coefficient ? ij for some existing edge e ij . The attention model can only strengthen or weaken some existing relations, but cannot form new relations. We present a more general model that makes it possible to connect previously disconnected nodes and form new abstract relations. To enforce a symmetry of predicted edges we compute an average: (e (r)</p><p>ij + e (r) ji )/2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Layer pooling versus global pooling</head><p>Inspired by convolutional networks, previous works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8]</ref> built an analogy of pooling layers in graphs, for example, using the Graclus clustering algorithm <ref type="bibr" target="#b18">[19]</ref>. In CNNs, pooling is an effective way to reduce memory and computation, particularly for large inputs. It also provides additional robustness to local deformations and leads to faster growth of receptive fields. However, we can build a convolutional network without any pooling layers with similar performance in a downstream task <ref type="bibr" target="#b19">[20]</ref> -it just will be relatively slow, since pooling is extremely cheap on regular grids, such as images. In graph classification tasks, the input dimensionality, which corresponds to the number of nodes N = |V|, is often very small (? 10 2 ) and the benefits of pooling are less clear. Graph pooling, such as in <ref type="bibr" target="#b18">[19]</ref>, is also computationally intensive since we need to run the clustering algorithm for each training example independently, which limits the scale of problems we can address. Aiming to simplify the model while maintaining classification accuracy, we exclude pooling layers between conv. layers and perform global maximum pooling (GMP) over nodes following the last conv. layer. This fixes the size of the penultimate feature vector regardless of the number of nodes ( <ref type="figure">Figure 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset details</head><p>We evaluate our model on five chemical graph classification datasets frequently used in previous work: NCI1 and NCI109 <ref type="bibr" target="#b13">[14]</ref>, MUTAG <ref type="bibr" target="#b20">[21]</ref>, ENZYMES <ref type="bibr" target="#b21">[22]</ref>, and PROTEINS <ref type="bibr" target="#b22">[23]</ref>. For each dataset, there is a set of graphs with an arbitrary number of nodes N = |V| and undirected binary edges |E| of a single type (R = 1) and each graph G has a single, categorical label that is to be predicted. Dataset statistics are presented in <ref type="table">Table 2</ref> of the Appendix.</p><p>Every graph represents some chemical compound labeled according to its functional properties. In NCI1, NCI109 and MUTAG, edges correspond to atom bonds (types of bonds) and vertices -to atom properties; in ENZYMES, edges are formed based on spatial distance (edges connect nodes if those are neighbors along the amino acids sequence or if they are neighbors in space within the protein structure <ref type="bibr" target="#b22">[23]</ref>); in PROTEINS, edges are similarly formed based on spatial distance between amino acids in proteins. Since edges in these datasets are not directly related to the features of nodes they connect, we expect that learned edges will enrich graph structure and improve graph classification.</p><p>Node features X are discrete in these datasets and represesented as one-hot vectors of length X in . We do not use any additional node or edge attributes available for some of these datasets.</p><p>These datasets vary in the number of graphs (188 -4127), class labels (2 -6) and the number of nodes in a graph (2 -620) and, thereby, represent a comprehensive benchmark for our method. We follow the standard approach to evaluation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> and perform 10-fold cross-validation on these datasets. To minimize any random effects, we repeat experiments 10 times and report average classification accuracies together with standard deviations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Architectural details and experimental setup</head><p>In all experiments, we train a ChebNet with three graph convolutional layers followed by global max pooling (GMP) and 2 fully-connected layers <ref type="figure">(Figure 3</ref>). Batch normalization (BN) and the ReLU activation are added after each layer, whereas dropout is added only before the fully-connected layers. Projections f r (X (r) ) in Eq. 5 are modeled by a single layer neural network with C = 128 hidden units and the tanh activation. The edge prediction function f edge (see Eq. 7, Section 3.1) is  <ref type="figure">Figure 3</ref>: Graph classification pipeline. Each l th convolutional layer in our model takes the graph G l = (V l , E (r) ) and returns a graph with the same nodes and edges. Node features become increasingly global after each subsequent layer as the receptive field increases, while edges are propagated without changes. As a result, after several graph convolutional layers, each node in the graph contains information about its neighbors and the entire graph. By pooling over nodes we summarize the information collected by each node. Fully-connected layers follow global pooling to perform classification. Dashed orange edges denote connections learned as described in Section 3.1.  <ref type="table">Table 2</ref> of the Appendix.</p><formula xml:id="formula_9">G 1 =( V 1 , E ( r ) ) G l =(V l , E ( r ) ) G=( V , E )</formula><p>We train all models using the Adam optimizer <ref type="bibr" target="#b25">[26]</ref> with learning rate of 0.001, weight decay of 0.0001, and batch size of 32, the learning rate is decayed after 25, 35, and 45 epochs and the models are trained for 50 epochs as in <ref type="bibr" target="#b5">[6]</ref>. We run experiments for different fusion methods (Section 2.3) and Chebyshev orders K in range from 2 to 6 (Section 2.1) and report the best results in <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Previous works typically show strong performance on one or two datasets out of five that we use <ref type="table" target="#tab_1">(Table 1</ref>). In contrast, the Multigraph ChebNet, leveraging two relation types (annotated and learned, see Section 3.1), shows high accuracy across all datasets. On PROTEINS we outperform all previous methods, while on ENZYMES two recent works based on differentiable pooling <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b30">31]</ref> are better, however it is difficult to compare to their results without the standard deviation of accuracies. We also obtain competitive accuracy on NCI1 outperforming DGK <ref type="bibr" target="#b24">[25]</ref>, PSCN <ref type="bibr" target="#b28">[29]</ref>, and DGCNN <ref type="bibr" target="#b29">[30]</ref>. Importantly, the Multigraph ChebNet with two edge types, i.e. predefined dataset annotations and the learned edges (Section 3.1) consistently outperforms the baseline ChebNet with a single edge, which shows efficacy of our approach and demonstrates the complementary nature of predefined and learned edges. Lower results on NCI1 and NCI109 can be explained by the fact that the node  <ref type="figure">Figure 4</ref>: Comparison of edge fusion methods for 10 folds. We observe that some methods perform well for lower order K, such as 2d Chebyshev convolution winning in 18/50 cases for K = 2, while others perform better for higher K, such as Multiply-shared. Multiply generally performs well across different K. All fusion methods, except for Sum and Sum-shared, consistently outperform the Single-edge baseline. We also show the number of parameters in ? for the Chebyshev convolution layer depending on the number of input features X in , number of output features X out , number of relation types R, order K and some constant C (number of hidden units in a projection layer f r in Eq. 5). *Single-edge denotes using only annotated edges. All other methods additionally use the second edge, learned based on node features.</p><p>features in the graphs of these datasets are imbalanced with some features appearing only a few times in the dataset. This is undesirable for our method, which learns new edges based on features and the model can predict random values for unseen features. On MUTAG we surpass all but one method <ref type="bibr" target="#b28">[29]</ref>. But in this case the dataset is tiny, consisting of 188 graphs and the margin from the top method is not statistically significant.</p><p>Evaluation of edge fusion methods. We train a model using each of the edge fusion methods proposed in Section 2.3 and report the summary of results in <ref type="figure">Figure 4</ref>. We count the number of times each method outperforms the others treating all 10 folds independently. As expected, graph convolution based on the two-dimensional Chebyshev polynomial is better for lower orders of K, since it exploits multi-relational graph paths, effectively increasing the receptive field of filters. However, for larger K, the model complexity becomes too high due to quadratic growth of the number of parameters and performance degrades. Sharing weights for multiplicative or additive fusion generally drops performance with a few exceptions in the multiplicative case. This implies that predefined and learned edges are of a different nature. It would be interesting to validate these fusion methods on a larger number of relation types.</p><p>Speed comparison. We compare forward pass speed of the proposed method to the baseline Cheb-Net, MoNet <ref type="bibr" target="#b6">[7]</ref> and GCN <ref type="bibr" target="#b16">[17]</ref> ( <ref type="figure">Figure 5</ref>). Analgously to <ref type="bibr" target="#b16">[17]</ref>, we generate random graphs with N nodes and 2N edges. ChebNet with 2 edge types (Multigraph ChebNet) is on average two times slower than the baseline ChebNet with a single annotated edge. MoNet is in turn two times slower than Multigraph ChebNet. Multigraph ChebNet with 2d edge fusion is the slowest due to exponential growth of parameters, while GCN is the fastest, although the gap with the baseline ChebNet is small. Therefore, we believe that Multigraph ChebNet with certain edge fusion methods provides a relatively fast, scalable and accurate model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work and Discussion</head><p>Our method relies on a fast approximate spectral graph convolution known as ChebNet <ref type="bibr" target="#b10">[11]</ref>), which was designed for graph classification. A simplified and faster version of this model, Graph Convolutional Networks (GCN) <ref type="bibr" target="#b16">[17]</ref>, which is practically equivalent to the ChebNet with order K = 1, has shown impressive node classification performance on citation and knowledge graph datasets in the transductive learning setting. In all our experiments, we noticed that using more global filters (with larger K) is important (Tables 1). Other recent works <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b3">4]</ref> also focus on node classification and, therefore, are not empirically compared to in this work.  <ref type="figure">Figure 5</ref>: Speed comparison of the baseline ChebNet, ChebNet with two edge types (Multigraph ChebNet), MoNet <ref type="bibr" target="#b6">[7]</ref> and GCN <ref type="bibr" target="#b16">[17]</ref>. MoNet-NX refers to MoNet with X filters.</p><p>Recent work of <ref type="bibr" target="#b31">[32]</ref> proposed a differentiable alternative to clustering-based graph pooling, showing strong results in graph classification tasks, but at the high computational cost. To alleviate this, a more scalable approach based on dropping nodes <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b30">31]</ref> was introduced and can be integrated with our method to further improve results.</p><p>Closely related to our work, <ref type="bibr" target="#b6">[7]</ref> formulated the generalized graph convolution model (MoNet) based on a trainable transformation to pseudo-coordinates, which led to learning anisotropic kernels and excellent results in visual tasks. However, in non-visual tasks, when coordinates are not naturally defined, the performance is worse <ref type="table" target="#tab_1">(Table 1)</ref>. Notably, the computational cost (both memory and speed) of MoNet is higher than for ChebNet due to the patch operator in [7, Eq. (9)-(11)] ( <ref type="figure">Figure 5</ref>). The argument in favor of MoNet against ChebNet was the sensitivity of spectral convolution methods, including ChebNet, to changes in graph size and structure. We contradict this argument and show superior performance on chemical graph classification datasets. SplineCNN <ref type="bibr" target="#b7">[8]</ref> is similar to MoNet and is good at classifying both graphs and nodes, but it is also based on pseudo coordinates and, therefore, potentially has the same shortcoming of MoNet. So, its performance on general graph classification problems where coordinates are not well defined is expected to be inferior.</p><p>Another family of methods based on kernels <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27]</ref> shows strong performance on chemical datasets, but their application is limitted to small scale graph problems with discrete node features. Scalable extensions of kernel methods to graphs with continuous features were proposed <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b24">25]</ref>, but they showed weaker results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we address several limitations of current graph convolutional networks and show competitive graph classification results on a number of chemical datasets. First, we revisit the spectral graph convolution model based on the Chebyshev polynomial, commonly believed to inherit shortcomings of earlier spectral methods, and demonstrate its ability to learn from graphs of arbitrary size and structure. Second, we design and study edge fusion methods for multi-relational graphs, and show the importance of validating these methods for each task to achieve optimal performance. Third, we propose a way to learn new edges in a graph jointly with a graph classification model. Our results show that the learned edges are complimentary to edges already annotated, providing a significant gain in accuracy. Appendix 6.1 Overview of spectral graph convolution and its approximation</p><p>Following the notation of <ref type="bibr" target="#b10">[11]</ref>, spectral convolution on a graph G having N nodes is defined analogously to convolution in the Fourier domain (the convolution theorem) for some one-dimensional features over nodes x ? R N and filter g ? R N as <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b0">1]</ref>:</p><formula xml:id="formula_10">y = g x = U (U T g U T x) = U diag(?)U T x,<label>(8)</label></formula><p>where, U are the eigenvectors of the normalized symmetric graph Laplacian, L = I ? D ?1/2 AD ?1/2 , where A is an adjacency matrix of the graph G, D are node degrees. L = U ?U T follows from the definition of eigenvectors, where ? is a diagonal matrix of eigenvalues. The operator denotes the Hadamard product (element-wise multiplication),? = U T g and diag(?) is a diagonal matrix with elements of? in the diagonal.</p><p>The spectral convolution in <ref type="bibr" target="#b7">(8)</ref> can be approximated using the Chebyshev expansion, where T k (?) = 2?T k?1 (?) ? T k?2 (?) with T 0 (?) = 1 and T 1 (?) = ? (i.e. T k (?) terms contain powers ? k ) and the property of eigendecomposition:</p><formula xml:id="formula_11">L k = (U ?U T ) k = U ? k U T .<label>(9)</label></formula><p>Assuming eigenvalues ? are fixed constants, filter? can be represented as a function of eigenvalue? g(?), such that (8) becomes: g x = U?(?)U T x.</p><p>Filter?(?) can be then approximated as a Chebyshev polynomial of degree K (a weighted sum of T k (?) terms). Substituting the approximated?(?) into Eq. 10 and exploiting Eq. 9, the approximate spectral convolution takes the form of (see in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17]</ref> for further analysis and <ref type="bibr" target="#b34">[35]</ref> for derivations):</p><formula xml:id="formula_13">y = g x ? U K?1 k=0 ? k T k (?) U T x = K?1 k=0 ? k T k (L)x = [x 0 ,x 1 , ...,x K?1 ]?,<label>(11)</label></formula><p>whereL = 2L/? max ? I is a rescaled graph Laplacian with ? max as the largest eigenvalue of L,x k = T k (L)x ? R N are projections of input features onto the Chebyshev basis and ? = [? 0 , ? 1 , ..., ? K?1 ] are learnable weights shared across nodes. In this work, we further simplify the computation and fix ? max = 2 (? max varies from graph to graph), so thatL = L ? I = ?D ?1/2 AD ?1/2 and assume no loops in a graph.L has the same eigenvectors U as L, but its eigenvalues are? = ? ? 1. <ref type="table">Table 2</ref>: Dataset statistics and graph network architectures. These statistics can also be found in <ref type="bibr" target="#b35">[36]</ref> along with the datasets themselves. N = |V| -number of nodes in a graph, X in -input dimensionality. GC -graph convolution layer, FC -fully connected layer, D -dropout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Dataset statistics and network architectures</head><p>Dataset # graphs |V|min |V|max |V|avg Xin Architecture </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Histograms of eigenvalues of the rescaled graph LaplacianL for the (a) ENZYMES, (b) MUTAG and (c) NCI1 datasets. Due to the property of eigendecomposition (L k = U? k U T ) the distribution of eigenvalues shrinks when we take powers ofL to compute the approximate spectral graph convolution (Eq. 2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>XinKRXout 2d Cheb (Eq. 4) XinK R Xout Multiply (Eq. 5) C(XinKR + Xout) Sum (Eq. 5) C(XinKR + Xout) M-shared (Eq. 5) C(XinK + Xout) S-shared (Eq. 5) C(XinK + Xout)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Chemical graph classification results (average accuracy and standard deviation in %). Multigraph ChebNet obtains better results by leveraging two types of edges: annotated and learned, whereas all other models use only annotated edges. *We implemented MoNet, GCN and ChebNet. To make a fair comparison to Multigraph ChebNet, we use the same network architectures, batchnormalization, global max pooling. For MoNet, coordinates are defined using node degrees as in<ref type="bibr" target="#b6">[7]</ref>. The top result across all methods for each dataset is bolded.</figDesc><table><row><cell>Model</cell><cell>NCI1</cell><cell cols="4">NCI109 MUTAG ENZYMES PROTEINS</cell></row><row><cell>WL [24]</cell><cell cols="2">84.6?0.4 84.5?0.2</cell><cell>83.8?1.5</cell><cell>59.1?1.1</cell><cell>?</cell></row><row><cell>WL-OA [27]</cell><cell cols="2">86.1?0.2 86.3?0.2</cell><cell>84.5?1.7</cell><cell>59.9?1.1</cell><cell>76.4?0.4</cell></row><row><cell>structure2vec [28]</cell><cell>83.7</cell><cell>82.2</cell><cell>88.3</cell><cell>61.1</cell><cell>?</cell></row><row><cell>DGK [25]</cell><cell cols="2">80.3?0.5 80.3?0.3</cell><cell>87.4?2.7</cell><cell>53.4?0.9</cell><cell>75.7?0.5</cell></row><row><cell>PSCN [29]</cell><cell>78.6?1.9</cell><cell>?</cell><cell>92.6?4.2</cell><cell>?</cell><cell>75.9?2.8</cell></row><row><cell>ECC [6]</cell><cell>83.8</cell><cell>82.1</cell><cell>88.3</cell><cell>53.5</cell><cell>?</cell></row><row><cell>DGCNN [30]</cell><cell>74.4?0.5</cell><cell>?</cell><cell>85.8?1.7</cell><cell>?</cell><cell>76.3?0.2</cell></row><row><cell>Graph U-Net [31]</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>64.2</cell><cell>75.5</cell></row><row><cell>DiffPool [32]</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>62.5</cell><cell>76.3</cell></row><row><cell>MoNet [7] -ours*</cell><cell cols="2">69.8?0.2 70.0?0.3</cell><cell>84.2?1.2</cell><cell>36.4?1.2</cell><cell>71.9?1.2</cell></row><row><cell>GCN [17] -ours*</cell><cell cols="2">75.8?0.7 73.4?0.4</cell><cell>76.5?1.4</cell><cell>40.7?1.8</cell><cell>74.3?0.5</cell></row><row><cell cols="3">ChebNet [11] -ours* 83.1?0.4 82.1?0.2</cell><cell>84.4?1.6</cell><cell>58.0?1.4</cell><cell>75.5?0.4</cell></row><row><cell>Multigraph ChebNet</cell><cell cols="2">83.4?0.4 82.0?0.3</cell><cell>89.1?1.4</cell><cell>61.7?1.3</cell><cell>76.5?0.4</cell></row></table><note>a two layer neural network with 128 hidden units (32 for PROTEINS), which acts on concatenated node features. Detailed network architectures are presented in</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was developed with funding from the Defense Advanced Research Projects Agency (DARPA). The views, opinions and/or findings expressed are those of the author and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government. The authors also acknowledge support from the Canadian Institute for Advanced Research and the Canada Foundation for Innovation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Representation learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
	</analytic>
	<monogr>
		<title level="m">Methods and applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamic edgeconditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Splinecnn: Fast geometric deep learning with continuous b-spline kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Weichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="869" to="877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Two-dimensional chebyshev polynomials for image fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Omar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Mitianoudis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tania</forename><surname>Stathaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woosang</forename><surname>Kyoung-Woon On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.04325</idno>
		<title level="m">Hadamard product for low-rank bilinear pooling</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Comparison of descriptor spaces for chemical compound retrieval and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikil</forename><surname>Wale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="347" to="375" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al?n</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graphstructured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Weighted graph cuts without eigenvectors a multilevel approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqiang</forename><surname>Inderjit S Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Striving for simplicity: The all convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kumar Debnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosa</forename><forename type="middle">L</forename><surname>Lopez De Compadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gargi</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">J</forename><surname>Shusterman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corwin</forename><surname>Hansch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medicinal chemistry</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="786" to="797" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Brenda, the enzyme database: updates and major new developments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ida</forename><surname>Schomburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antje</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ebeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Gremse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Heldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregor</forename><surname>Huhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Schomburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="431" to="433" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>suppl</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soon</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sch?nauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Svn Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>suppl</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">Jan</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On valid optimal assignment kernels and applications to graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nils</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Louis</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Giscard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1623" to="1631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Discriminative embeddings of latent variable models for structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2702" to="2711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Inteligence</title>
		<meeting>AAAI Conference on Artificial Inteligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Towards sparse hierarchical graph classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?t?lina</forename><surname>Cangea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Jovanovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.01287</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08804</idno>
		<title level="m">Hierarchical graph representation learning with differentiable pooling</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph u-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anonymous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Submitted to the Seventh International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Wavelets on graphs via spectral graph theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="150" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Benchmark data sets for graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
