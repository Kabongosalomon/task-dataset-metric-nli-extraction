<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Medical Image Segmentation Using Squeeze-and-Expansion Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of High Performance Computing</orgName>
								<address>
									<region>A*STAR</region>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuchao</forename><surname>Sui</surname></persName>
							<email>xiuchao.sui@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of High Performance Computing</orgName>
								<address>
									<region>A*STAR</region>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangde</forename><surname>Luo</surname></persName>
							<email>xiangde.luo@std.uestc.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxing</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of High Performance Computing</orgName>
								<address>
									<region>A*STAR</region>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of High Performance Computing</orgName>
								<address>
									<region>A*STAR</region>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rick</forename><surname>Goh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of High Performance Computing</orgName>
								<address>
									<region>A*STAR</region>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Medical Image Segmentation Using Squeeze-and-Expansion Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Medical image segmentation is important for computer-aided diagnosis. Good segmentation demands the model to see the big picture and fine details simultaneously, i.e., to learn image features that incorporate large context while keep high spatial resolutions. To approach this goal, the most widely used methods -U-Net and variants, extract and fuse multi-scale features. However, the fused features still have small effective receptive fields with a focus on local image cues, limiting their performance. In this work, we propose Segtran, an alternative segmentation framework based on transformers, which have unlimited effective receptive fields even at high feature resolutions. The core of Segtran is a novel Squeeze-and-Expansion transformer: a squeezed attention block regularizes the self attention of transformers, and an expansion block learns diversified representations. Additionally, we propose a new positional encoding scheme for transformers, imposing a continuity inductive bias for images. Experiments were performed on 2D and 3D medical image segmentation tasks: optic disc/cup segmentation in fundus images (REFUGE'20 challenge), polyp segmentation in colonoscopy images, and brain tumor segmentation in MRI scans (BraTS'19 challenge). Compared with representative existing methods, Segtran consistently achieved the highest segmentation accuracy, and exhibited good cross-domain generalization capabilities. The source code of Segtran is released at https://github.com/askerlee/segtran.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automated Medical image segmentation, i.e., automated delineation of anatomical structures and other regions of interest (ROIs), is an important step in computer-aided diagnosis; for example it is used to quantify tissue volumes, extract key quantitative measurements, and localize pathology <ref type="bibr" target="#b17">[Schlemper et al., 2019;</ref><ref type="bibr" target="#b16">Orlando et al., 2020]</ref>. Good segmentation demands the model to see the big picture and fine details at * Corresponding Author. the same time, i.e., learn image features that incorporate large context while keep high spatial resolutions to output finegrained segmentation masks. However, these two demands pose a dilemma for CNNs, as CNNs often incorporate larger context at the cost of reduced feature resolution. A good measure of how large a model "sees" is the effective receptive field (effective RF) , i.e., the input areas which have non-negligible impacts to the model output.</p><p>Since the advent of U-Net <ref type="bibr" target="#b17">[Ronneberger et al., 2015]</ref>, it has shown excellent performance across medical image segmentation tasks. A U-Net consists of an encoder and a decoder, in which the encoder progressively downsamples the features and generates coarse contextual features that focus on contextual patterns, and the decoder progressively upsamples the contextual features and fuses them with fine-grained local visual features. The integration of multiple scale features enlarges the RF of U-Net, accounting for its good performance. However, as the convolutional layers deepen, the impact from far-away pixels decay quickly. As a results, the effective RF of a U-Net is much smaller than its theoretical RF. As shown in <ref type="figure" target="#fig_1">Fig.2</ref>, the effective RFs of a standard U-Net and DeepLabV3+ are merely around 90 pixels. This implies that they make decisions mainly based on individual small patches, and have difficulty to model larger context. However, in many tasks, the heights/widths of the ROIs are greater than 200 pixels, far beyond their effective RFs. Without "seeing the bigger picture", U-Net and other models may be misled by local visual cues and make segmentation errors.</p><p>Many improvements of U-Net have been proposed. A few typical variants include: U-Net++ <ref type="bibr" target="#b22">[Zhou et al., 2018]</ref> and U-Net 3+ <ref type="bibr" target="#b5">[Huang et al., 2020]</ref>, in which more complicated skip connections are added to better utilize multi-scale contextual features; attention U-Net <ref type="bibr" target="#b17">[Schlemper et al., 2019]</ref>, which employs attention gates to focus on foreground regions; 3D U-Net <ref type="bibr">[?i?ek et al., 2016]</ref> and V-Net <ref type="bibr" target="#b13">[Milletari et al., 2016]</ref>, which extend U-Net to 3D images, such as MRI volumes; Eff-UNet <ref type="bibr" target="#b0">[Baheti et al., 2020]</ref>, which replaces the encoder of U-Net with a pretrained EfficientNet <ref type="bibr" target="#b18">[Tan and Le, 2019]</ref>.</p><p>Transformers <ref type="bibr">[Vaswani et al., 2017]</ref> are increasingly popular in computer vision tasks. A transformer calculates the pairwise interactions ("self-attention") between all input units, combines their features and generates contextualized features. The contextualization brought by a transformer is analogous to the upsampling path in a U-Net, except that  it has unlimited effective receptive field, good at capturing long-range correlations. Thus, it is natural to adopt transformers for image segmentation. In this work, we present Segtran, an alternative segmentation architecture based on transformers. A straightforward incorporation of transformers into segmentation only yields moderate performance. As transformers were originally designed for Natural Language Processing (NLP) tasks, there are several aspects that could be improved to better suit image applications. To this end, we propose a novel transformer design Squeeze-and-Expansion Transformer, in which a squeezed attention block helps regularize the huge attention matrix, and an expansion block learns diversified representations. In addition, we propose a learnable sinusoidal positional encoding that imposes a continuity inductive bias for the transformer. Experiments demonstrate that they lead to improved segmentation performance. We evaluated Segtran on two 2D medical image segmentation tasks: optic disc/cup segmentation in fundus images of the REFUGE'20 challenge, and polyp segmentation in colonoscopy images. Additionally, we also evaluated it on a 3D image segmentation task: brain tumor segmentation in MRI scans of the BraTS'19 challenge. Segtran has consistently shown better performance than U-Net and its variants (UNet++, UNet3+, PraNet, and nnU-Net), as well as DeepLabV3+ <ref type="bibr" target="#b1">[Chen et al., 2018]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our work is largely inspired by DETR <ref type="bibr">[Carion et al., 2020]</ref>. DETR uses transformer layers to generate contextualized features that represent objects, and learns a set of object queries to extract the positions and classes of objects in an image. Although DETR is also explored to do panoptic segmentation <ref type="bibr" target="#b8">[Kirillov et al., 2019]</ref>, it adopts a two-stage approach which is not applicable to medical image segmentation. A followup work of DETR, Cell-DETR <ref type="bibr" target="#b17">[Prangemeier et al., 2020]</ref> also employs transformer for biomedical image segmentation, but its architecture is just a simplified DETR, lacking novel components like our Squeeze-and-Expansion transformer. Most recently, SETR <ref type="bibr" target="#b22">[Zheng et al., 2021]</ref> and TransU-Net <ref type="bibr" target="#b2">[Chen et al., 2021]</ref> were released concurrently or after our paper submission. Both of them employ a Vision Transformer (ViT) <ref type="bibr" target="#b2">[Dosovitskiy et al., 2021]</ref> as the encoder to extract image features, which already contain global contextual information. A few convolutional layers are used as the decoder to generate the segmentation mask. In contrast, in Segtran, the transformer layers build global context on top of the local image features extracted from a CNN backbone, and a Feature Pyramid Network (FPN) generates the segmentation mask.</p><p>[ <ref type="bibr" target="#b14">Murase et al., 2020]</ref> extends CNNs with positional encoding channels, and evaluates them on segmentation tasks. Mixed results were observed. In contrast, we verified through an ablation study that positional encodings indeed help Seg-tran to do segmentation to a certain degree.</p><p>Receptive fields of U-Nets may be enlarged by adding more downsampling layers. However, this increases the number of parameters and adds the risk of overfitting. Another way of increasing receptive fields is using larger stride sizes of the convolutions in the downsampling path. Doing so, however, sacrifices spatial precision of feature maps, which is often disadvantageous for segmentation <ref type="bibr" target="#b11">[Liu and Guo, 2020]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Squeeze-and-Expansion Transformer</head><p>The core concept in a transformer is Self Attention, which can be understood as computing an affinity matrix between different units, and using it to aggregate features:</p><formula xml:id="formula_0">Att_weight(X, X) = f (K(X), Q(X)) ? R N ?N , (1) Attention(X) = Att_weight(X, X) ? V (X), (2) X out = FFN(Attention(X)),<label>(3)</label></formula><p>where K, Q, V are key, query, and value projections, respectively. f is softmax after dot product. Att_weight(X, X) is the pairwise attention matrix between input units, whose i, jth element defines how much the features of unit j contributes to the fused (contextualized) features of unit i. FFN is a feedforward network to further transform the fused features. The basic transformer above is extended to a multi-head attention (MHA) <ref type="bibr">[Vaswani et al., 2017;</ref><ref type="bibr" target="#b20">Voita et al., 2019]</ref>, aiming to capture different types of associations between input units. Each of the N h heads computes individual attention wights and output features (C/N h -dimensional), and their output features are concatenated along the channel dimension into C-dimensional features. Different heads operate in exclusively different feature subspaces.</p><p>We argue that transformers can be improved in four aspects make them better suited for images:</p><p>1. In Eq.(2), the intermediate features Attention(X) are obtained by linearly combining the projected input features, where the attention matrix specifies the combination coefficients. As the attention matrix is huge: N ?N , with typically N &gt; 1000, it is inherently vulnerable to noises and overfitting. Reducing the attention matrix to lower rank matrices may help.</p><p>2. In traditional transformers, the output features are monomorphic: it has only one set of feature transformations (the multi-head transformer also has one set of transformations after concatenation), which may not have enough capacity to fully model data variations. Just like a mixture of Gaussians almost always better depicts a data distribution than a single Gaussian, data variations can be better captured using a mixture of k transformers.</p><p>3. In traditional transformers, the key and query projections are independently learned, enabling them to capture asymmetric relationships between tokens in natural language. However, the relationships between image units are often symmetric, such as whether two pixels belong to the same segmentation class.</p><p>4. Pixels in an image have strong locality and semantic continuity. The two mainstream positional encoding</p><formula xml:id="formula_1">1 2 3 ? (a) 1 ? ? ? 1 ? (b) 1 2 3 ? 1 2 3 ? 1 ? 2 ? 3 ? ? ? 1 ? 2 ? 3 ? ? ? 1 2 3 ? Figure 3: (a) Full self-attention (N ? N ) vs. (b) Squeezed At- tention Block (SAB).</formula><p>In SAB, first input units x1, ? ? ? , xN attend with a codebook c1, ? ? ? , cM , yielding projected codebook features c 1 , ? ? ? , c M , which then attend back with the input x1, ? ? ? , xN . The two attention matrices are N ? M and M ? N , respectively. The Squeeze-and-Expansion Transformer aims to improve in all the four aspects. The Squeezed Attention Block computes attention between the input and M inducing points <ref type="bibr" target="#b9">[Lee et al., 2019]</ref>, and compresses the attention matrices to N ?M . The Expanded Attention Block is a mixture-of-experts model with N m modes ("experts"). In both blocks, the query projections and key projections are tied to make the attention symmetric, for better modeling of the symmetric relationships between image units. In addition, a Learnable Sinusoidal Positional Encoding helps the model capture spatial relationships. <ref type="bibr" target="#b9">[Lee et al., 2019]</ref> proposes Induced Set Attention Block (ISAB) by bringing inducing points into the transformer. It was originally designed to learn good features of a set of unordered objects. Here we employ this design to "squeeze" the bloated attention matrix, so as to reduce noises and overfitting in image tasks. We rename ISAB as Squeezed Attention Block (SAB) to highlight its new role in this context 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Squeezed Attention Block</head><p>In SAB, inducing points are a set of M learned embeddings c 1 , ? ? ? , c M in an external discrete codebook. Usually M N , the number of input units. The inducing points are first transformed into new embeddings C = c 1 , ? ? ? , c M after attending with the input. The combination of these embeddings form the output features X out = x 1 , ? ? ? , x N ( <ref type="figure">Fig.3)</ref>:</p><formula xml:id="formula_2">C = Single-Head(X, C),<label>(4)</label></formula><formula xml:id="formula_3">X out = EAB(C , X),<label>(5)</label></formula><p>where Single-Head(?, ?) is a single-head transformer, and EAB(?, ?) is an Expanded Attention Block presented in the next subsection. In each of the two steps, the attention matrix is of N ? M , much more compact than vanilla transformers. SAB is conceptually similar to the codebook used for discrete representation learning in <ref type="bibr">[Esser et al., 2020]</ref>, but the discretized features are further processed by a transformer. SAB can trace its lineage back to low-rank matrix factorization, i.e., approximating a data matrix X n?n ? P n?d ? Q d?n , which is a traditional regularization technique against data noises and overfitting. Confirmed by an ablation study, SAB helps fight against noises and overfitting as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Expanded Attention Block</head><p>The Expanded Attention Block (EAB) consists of N m modes, each an individual single-head transformer. They output N m sets of contextualized features, which are then aggregated into one set using dynamic mode attention:</p><formula xml:id="formula_4">X (k) out = Mode (k) (X),<label>(6)</label></formula><formula xml:id="formula_5">B (k) = Linear (k) (X (k) out ), (7) with k ? {1, ? ? ? , N m }, G = softmax B (1) , ? ? ? , B (Nm) ,<label>(8)</label></formula><formula xml:id="formula_6">X out = X (1) out , ? ? ? , X (Nm) out ? G ,<label>(9)</label></formula><p>where the mode attention G ? R Nu?Nm is obtained by doing a linear transformation of each mode features, and taking softmax over all the modes. Eq.(9) takes a weighted sum over the modes to get the final output features X out . This dynamic attention is inspired by the Split Attention of the ResNest model <ref type="bibr" target="#b22">[Zhang et al., 2020b]</ref>. EAB is a type of Mixture-of-Experts <ref type="bibr" target="#b17">[Shazeer et al., 2017]</ref>, an effective way to increase model capacity. Although there is resemblance between multi-head attention (MHA) and EAB, they are essentially different, as shown in <ref type="figure" target="#fig_2">Fig.4</ref>. In MHA, each head resides in an exclusive feature subspace and provides unique features. In contrast, different modes in EAB share the same feature space, and the representation power largely remains after removing any single mode. The modes join forces to offer more capacity to model diverse data, as shown in an ablation study. In addition, EAB is also different from the Mixture of Softmaxes (MoS) transformer <ref type="bibr" target="#b22">[Zhang et al., 2020a]</ref>. Although MoS transformer also uses k sets of queries and keys, it shares one set of value transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learnable Sinusoidal Positional Encoding</head><p>A crucial inductive bias for images is the pixel locality and semantic continuity, which is naturally encoded by convolutional kernels. As the input to transformers is flattened into 1-D sequences, positional encoding (PE) is the only source to inject information about spatial relationships. On the one hand, this makes transformers flexible to model arbitrary shapes of input. On the other hand, the continuity bias of images is non-trivial to fully incorporate. This is a limitation of the two mainstream PE schemes: Fixed Sinusoidal Encoding and Discretely Learned Encoding <ref type="bibr">[Carion et al., 2020;</ref><ref type="bibr" target="#b2">Dosovitskiy et al., 2021]</ref>. The former is spatially continuous but lacks adaptability, as the code is predefined. The latter learns a discrete code for each coordinate without enforcing spatial continuity.</p><p>We propose Learnable Sinusoidal Positional Encoding, aiming to bring in the continuity bias with adaptability. Given a pixel coordinate (x, y), our positional encoding vector pos(x, y) is a combination of sine and cosine functions of linear transformations of (x, y):</p><formula xml:id="formula_7">pos i (x, y) = sin(a i x + b i y + c i ) if i &lt; C/2 cos(a i x + b i y + c i ) if i ? C/2,<label>(10)</label></formula><p>where i indexes the elements in pos, {a i , b i , c i } are learnable weights of a linear layer, and C is the dimensionality of image features. To make the PE behave consistently across different image sizes, we normalize (x, y) into [0, 1] 2 . When the input image is 3D, Eq.(10) is trivially extended by using 3D coordinates (x, y, z).</p><p>The encoding in Eq.(10) changes smoothly with pixel coordinates, and thus nearby units receive similar positional encodings, pushing the attention weights between them towards larger values, which is the spirit of the continuity bias. The learnable weights and sinusoidal activation functions make the code both adaptable and nonlinear to model complex spatial relationships <ref type="bibr" target="#b19">[Tancik et al., 2020]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Segtran Architecture</head><p>As a context-dependent pixel-wise classification task, segmentation faces a conflict between larger context (lower resolution) and localization accuracy (higher resolution). Segtran partly circumvents this conflict by doing pairwise feature contextualization, without sacrificing spatial resolutions. There are five main components in Segtran <ref type="figure" target="#fig_0">(Fig.1)</ref>: 1) a CNN backbone to extract image features, 2) input/output feature pyramids to do upsampling, 3) learnable sinusoidal positional encoding, 4) Squeeze-and-Expansion transformer layers to contextualize features, and 5) a segmentation head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CNN Backbone</head><p>We employ a pretrained CNN backbone to extract features maps with rich semantics. Suppose the input image is X 0 ? R H0?W0?D0 , where for a 2D image, D 0 = 1 or 3 is the number of color channels. For a 3D image, D 0 3 is the number of slices in the depth dimension. For 2D and 3D images, the extracted features are CNN(X 0 ) ? R C?H?W , and CNN(X 0 ) ? R C?H?W ?D , respectively. On 2D images, typically ResNet-101 or EfficientNet-D4 is used as the backbone. For increased spatial resolution, we change the stride of the first convolution from 2 to 1. Then H, W = H 0 /16, W 0 /16. On 3D images, 3D backbones like I3D [Carreira and Zisserman, 2017] could be adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Transformer Layers</head><p>Before being fed into the transformer, the visual features and positional encodings of each unit are added up before being fed to the transformer: X spatial = X visual + pos(coordinates(X)). X spatial is flattened across spatial dimensions to a 1-D sequence X 0 ? R Nu?C , where N u is the total number of image units, i.e., points in the feature maps.</p><p>The transformer consists of a few stacked transformer layers. Each layer takes input features X, computes the pairwise interactions between input units, and outputs contextualized features X out of the same number of units. The transformer layers used are our novel design Squeeze-and-Expansion Transformer (Section 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Feature Pyramids and Segmentation Head</head><p>Although the spatial resolution of features is not reduced after passing through the transformer layers, for richer semantics, the input features to transformers are usually high-level features from the backbone. They are of a low spatial resolution, however. Hence, we increase their spatial resolution with an input Feature Pyramid Network (FPN) <ref type="bibr" target="#b11">[Liu et al., 2018]</ref> and an output FPN, which upsample the feature maps at the transformer input end and output end, respectively.</p><p>Without loss of generality, let us assume the EfficientNet is the backbone. The stages 3, 4, 6, and 9 of the network are commonly used to extract multi-scale feature maps. Let us denote the corresponding feature maps as f 1 , f 2 , f 3 , f 4 , respectively. Their shapes are f i ? R Ci?Hi?Wi , with H i = H0 2 i , W i = W0 2 i . As described above, f (X 0 ) = f 4 is 1/16 of the original image, which is too coarse for accurate segmentation. Hence, we upsample it with an input FPN, and obtain upsampled feature maps f 34 :</p><formula xml:id="formula_8">f 34 = upsample ?2 (f 4 ) + conv 34 (f 3 ),<label>(11)</label></formula><p>where conv 34 is a 1 ? 1 convolution that aligns the channels of f 3 to f 4 , and upsample ?2 (?) is bilinear interpolation. f 34 is 1/8 of the original image, and is used as the input features to the transformer layers. As the transformer layers keep the spatial resolutions unchanged from input to output feature maps, the output feature maps g 34 is also 1/8 of the input image. Still, this spatial resolution is too low for segmentation. Therefore, we adopt an output FPN to upsample the feature maps by a factor of 4 (i.e., 1/2 of the original images). The output FPN consists of two upsampling steps:</p><formula xml:id="formula_9">f 12 = upsample ?2 (f 2 ) + conv 12 (f 1 ), g 1234 = upsample ?4 (g 34 ) + conv 24 (f 12 ),<label>(12)</label></formula><p>where conv 12 and conv 24 are 1 ? 1 convolutional layers that align the channels of f 1 to f 2 , and f 2 to f 4 , respectively. This FPN scheme is the bottom-up FPN proposed in <ref type="bibr" target="#b11">[Liu et al., 2018]</ref>. Empirically, it performs better than the original top-down FPN <ref type="bibr" target="#b10">[Lin et al., 2017]</ref>, as richer semantics in top layers are better preserved.</p><p>The segmentation head is simply a 1 ? 1 convolutional layer, outputting confidence scores of each class in the mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Three tasks were evaluated in our experiments: REFUGE20: Optic Disc/Cup Segmentation in Fundus Images. This task does segmentation of the optic disc and cup in fundus images, which are 2D images of the rear of eyes <ref type="figure" target="#fig_3">(Fig. 5</ref>). It is a subtask of the REFUGE Challenge 2 <ref type="bibr" target="#b16">[Orlando et al., 2020]</ref>, MICCAI 2020. 1200 images were provided for training, and 400 for validation. We also used two extra datasets, Drishti-GS dataset <ref type="bibr">[Sivaswamy et al., 2015]</ref> and <ref type="bibr">RIM-ONE v3 [Fumero et al., 2011]</ref> when training all models. The Disc/Cup dice scores of validation images were obtained from the official evaluation server.</p><p>Polyp: Polyp Segmentation in Colonoscopy Images. Polyps are fleshy growths in the colon lining that may become cancerous. This task does polyp segmentation in colonoscopy images <ref type="figure" target="#fig_3">(Fig. 5)</ref>. Two image datasets <ref type="bibr" target="#b4">[Fan et al., 2020]</ref> were used: CVC612 (CVC in short; 612 images) and Kvasir (1000 images). Each was randomly split into 80% training and 20% validation, and the training images were merged into one set.</p><p>BraTS19: Tumor Segmentation in MRI Images. This task focuses on the segmentation of gliomas, a common brain tumor in MRI scans. It was a subtask of the BraTS'19 challenge 3 <ref type="bibr" target="#b12">[Menze et al., 2015;</ref><ref type="bibr" target="#b0">Bakas et al., 2017]</ref>, MICCAI 2019. It involves four classes: the whole tumor (WT), the tumor core (TC), the enhancing tumor (ET) and background. Among them, the tumor core consists of the necrotic regions and non-enhancing tumors (red), as well as the enhancing tumor (yellow). 335 scans were provided for training, and 125 for validation. The dice scores of ET, WT and TC on the validation scans were obtained from the official evaluation server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablation Studies</head><p>Two ablation studies were performed on REFUGE20 to compare: 1) the Squeeze-and-Expansion Transformer versus Multi-Head Transformer; and 2) the Learnable Sinusoidal Positional Encoding versus two schemes as well as not using PE.</p><p>All the settings were variants of the standard one, which used three layers of Squeeze-and-Expansion transformer with four modes (N m = 4), along with learnable sinusoidal positional encoding. Both ResNet-101 and EfficientNet-B4 were evaluated to reduce random effects from choices of the backbone. We only reported the cup dice scores, as the disc segmentation task was relatively easy, with dice scores only varying ?0.005 across most settings. Type of Transformer Layers. <ref type="table" target="#tab_0">Table 1</ref> shows that Squeezeand-Expansion transformer outperformed the traditional multi-head transformers. Moreover, Both the squeeze attention block and the expansion attention block contributed to improved performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer Type</head><p>ResNet  Positional Encoding.  Number of Transformer Layers. <ref type="table" target="#tab_4">Table 3</ref> shows that as the number of transformer layers increased from 1 to 3, the performance improved gradually. However, one more layer caused performance drop, indicating possible overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison with Baselines</head><p>Ten methods were evaluated on the 2D segmentation tasks:   <ref type="bibr" target="#b4">[Fan et al., 2020]</ref>: The official PyTorch implementation 7 . The pretrained Res2Net-50 <ref type="bibr" target="#b5">[Gao et al., 2020]</ref> was recommended to be used as the encoder. ? DeepLabV3+ <ref type="bibr" target="#b1">[Chen et al., 2018]</ref>: A popular PyTorch implementation 8 , with a pretrained ResNet-101 as the encoder. ? Attention based U-Nets <ref type="bibr" target="#b15">[Oktay et al., 2018]</ref>: Attention U-Net (AttU-Net) and AttR2U-Net (a combination of AttU-Net and Recurrent Residual U-Net) were evaluated <ref type="bibr" target="#b0">9</ref> . They learn to focus on important areas by computing element-wise attention weights (as opposed to the pairwise attention of transformers). ? nnU-Net <ref type="bibr" target="#b6">[Isensee et al., 2021]</ref>: nnU-Net generates a custom U-Net configuration for each dataset based on its statistics. It is primarily designed for 3D tasks, but can also handle 2D images after converting them to pseudo-3D. The original pipeline is time-consuming, and we extracted the generated U-Net configuration and instantiated it in our pipeline to do training and test. ? Deformable U-Net <ref type="bibr" target="#b8">[Jin et al., 2019]</ref>  weights.</p><p>? TransU-Net <ref type="bibr" target="#b2">[Chen et al., 2021]</ref>: TransU-Net uses a hybrid of ResNet and ViT as the encoder, and a U-Net style decoder. The official implementation 12 was evaluated, by fine-tuning their pretrained weights.</p><p>? Segtran: Trained with either a pretrained ResNet-101 or EfficientNet-B4 as the backbone.</p><p>Three methods were evaluated on the 3D segmentation task:</p><p>? Extension of nnU-Net : An extension of the nnU-Net 13 with two sampling strategies.</p><p>? Bag of tricks (2nd place solution of the BraTS'19 challenge) <ref type="bibr" target="#b22">[Zhao et al., 2019]</ref>: The winning entry used an ensemble of five models. For fairness, we quoted the best single-model results ("BL+warmup").</p><p>? Segtran-3D: I3D [Carreira and Zisserman, 2017] was used as the backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Training Protocols</head><p>All models were trained on a 24GB Titan RTX GPU with the AdamW optimizer. The learning rate for the three transformer-based models were 0.0002, and 0.001 for the other models. On REFUGE20, all models were trained with a batch size of 4 for 10,000 iterations (27 epochs); on Polyp, the total iterations were 14,000 (31 epochs). On BraTS19, Segtran was trained with a batch size of 4 for 8000 iterations. The training loss was the average of the pixel-wise crossentropy loss and the dice loss. Segtran used 3 transformer layers on 2D images, and 1 layer on 3D images to save RAM. The number of modes in each transformer layer was 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results</head><p>Tables 4 and 5 present the evaluation results on the 2D and 3D tasks, respectively. Overall, the three transformer  It is worth noting that, Segtran (eff-B4) was among the top 5 teams in the semifinal and final leaderboards of the REFUGE20 challenge. Among either REFUGE20 or BraTS19 challenge participants, although there were several methods that performed slightly better than Segtran, they usually employed ad-hoc tricks and designs <ref type="bibr" target="#b16">[Orlando et al., 2020;</ref><ref type="bibr" target="#b22">Zhao et al., 2019]</ref>. In contrast, Segtran achieved competitive performance with the same architecture and minimal hyperparameter tuning, free of domainspecific strategies. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Cross-Domain Generalization</head><p>To explore how well different methods generalize to new domains, we trained three representative methods, U-Net, DeepLabV3+ and Segtran on the 1200 training images of REFUGE20. All the methods used a pretrained ResNet-101 as the encoder/backbone. The trained models were evaluated on both the REFUGE20 training images and the RIM-One dataset <ref type="bibr" target="#b4">[Fumero et al., 2011]</ref>. As RIM-One images have drastically different characteristics from REFUGE20, all models suffered severe performance drop, as shown in <ref type="table" target="#tab_10">Table 6</ref>. Nevertheless, Segtran had the least performance degradation, showing the best cross-domain generalization. <ref type="figure" target="#fig_5">Fig.6</ref> shows a RIM-One image and the corresponding soft segmentation masks (before thresholding) produced by different methods. The mask produced by Segtran contains the fewest artifacts. <ref type="table" target="#tab_11">Table 7</ref> presents the number of parameters and FLOPs of a few representative methods. In general, transformer-based methods consume more computation and GPU RAM than conventional methods. Our profiling showed that the number of parameters/FLOPs of Segtran are dominated by the output FPN, which vary drastically across different backbones. As the REFUGE   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Computational Efficiency</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Impact of Pretraining</head><p>Models for medical image tasks usually benefit from initialization with weights pretrained on natural images (e.g. Ima-geNet <ref type="bibr">[Deng et al., 2009]</ref>), as medical image datasets are typically small. To quantitatively study the impact of pretraining, <ref type="table" target="#tab_12">Table 8</ref> compares the performance of using pretrained weights vs. training from scratch of a few methods. Pretraining brought~2.5% increase of average dice scores to the two transformer-based models, and 1% to U-Net (ResNet-101).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REFUGE20</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this work, we present Segtran, a transformer-based medical image segmentation framework. It leverages unlimited receptive fields of transformers to contextualize features. Moreover, the transformer is an improved Squeeze-and-Expansion transformer that better fits image tasks. Segtran sees both the global picture and fine details, lending itself good segmentation performance. On two 2D and one 3D medical image segmentation tasks, Segtran consistently outperformed existing methods, and generalizes well to new domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Segtran architecture. It extracts visual features with a CNN backbone, combines them with positional encodings of the pixel coordinates, and flattens them into a sequence of local feature vectors. The local features are contextualized by a few Squeeze-and-Expansion transformer layers. To increase spatial resolution, an input FPN and an output FPN upsamples the features before and after the transformers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Effective receptive fields of 3 models, indicated by nonnegligible gradients in blue blobs and light-colored dots. Gradients are back-propagated from the center of the image. Segtran has nonnegligible gradients dispersed across the whole image (light-colored dots). U-Net and DeepLabV3+ have concentrated gradients. Input image: 576 ? 576.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>(a) Multi-head attention (MHA) vs. (b) Expanded attention block (EAB). In MHA, each head outputs an exclusive feature subset. In contrast, EAB outputs Nm sets of complete features from Nm modes, and aggregates them with dynamic mode attention.schemes[Carion et al., 2020;<ref type="bibr" target="#b2">Dosovitskiy et al., 2021]</ref> do not fully impose such an inductive bias. This bias could be imposed by an improved positional encoding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Top: Optic disc/cup segmentation in fundus images into 3 classes: disc (grey), cup (white), and background (black). Bottom: Polyp segmentation in colonoscopy images into 2 classes: polyp (white) and background (black).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>-101 Eff-B4 Cell-DETR (N h = 4)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Soft segmentation masks produced by different methods on a RIM-One image. The mask by Segtran has the fewest artifacts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>REFUGE'20 Fundus Optic Cup dice scores change with the type of transformer layers. Single-Mode implies No Expansion. Cell-DETR uses a multi-head transformer and discretely learned PE. N</figDesc><table /><note>h : number of attention heads in a MHA. Nm: number of modes in a Squeeze-and-Expansion transformer.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">compares learnable sinu-</cell></row><row><cell cols="3">soidal positional encoding with the two mainstream PE</cell></row><row><cell cols="3">schemes and no PE. Surprisingly, without PE, performance</cell></row><row><cell cols="3">of Segtran only dropped 1~2%. A possible explanation is that</cell></row><row><cell cols="3">the transformer may manage to extract positional information</cell></row><row><cell cols="3">from the CNN backbone features [Islam et al., 2020].</cell></row><row><cell>Positional Encoding</cell><cell cols="2">ResNet-101 Eff-B4</cell></row><row><cell>None</cell><cell>0.857</cell><cell>0.853</cell></row><row><cell>Discretely learned</cell><cell>0.852</cell><cell>0.860</cell></row><row><cell>Fixed Sinusoidal</cell><cell>0.857</cell><cell>0.849</cell></row><row><cell>Learnable Sinusoidal</cell><cell>0.862</cell><cell>0.872</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>REFUGE'20 Fundus Optic Cup dice scores change with the type of positional encoding (PE) schemes.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>REFUGE20 Optic Cup dice scores change with the number of transformer layers. Best performers with each backbone are highlighted.</figDesc><table><row><cell>? U-Net [Ronneberger et al., 2015]: The implementa-</cell></row><row><cell>tion in a popular library Segmentation Models.PyTorch</cell></row><row><cell>(SMP) was used 4 . The pretrained ResNet-101 was cho-</cell></row><row><cell>sen as the encoder. In addition, U-Net implemented in</cell></row><row><cell>U-Net++ (below) was evaluated as training from scratch.</cell></row><row><cell>? U-Net++ [Zhou et al., 2018]: A popular PyTorch imple-</cell></row><row><cell>mentation 5 . It does not provide options to use pretrained</cell></row><row><cell>encoders, and thus was only trained from scratch.</cell></row><row><cell>? U-Net3+ [Huang et al., 2020]: The official PyTorch im-</cell></row><row><cell>plementation 6 . It does not provide options to use pre-</cell></row><row><cell>trained encoders.</cell></row><row><cell>? PraNet</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Dice scores on REFUGE20 and Polyp validation sets.</cell></row><row><cell>R101: ResNet-101; R50: ResNet-50; eff-B4: EfficientNet-B4.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Dice scores on BraTS19 validation set. Only single-model performance is reported.</figDesc><table><row><cell>based methods, i.e., SETR, TransU-Net and Segtran achieved</cell></row><row><cell>best performance across all tasks. With ResNet-101 as the</cell></row><row><cell>backbone, Segtran performed slightly better than SETR and</cell></row><row><cell>TransU-Net. With EfficientNet-B4, Segtran exhibited greater</cell></row><row><cell>advantages.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Generalization of three methods, measured by drop of Optic Cup dice scores from the REFUGE20 training images to a new test domain RIM-One. The smaller the drop is, the better. All used ResNet-101 as the encoder/backbone.bottom-up FPNs we adopt are somewhat similar to Effi-cientDet<ref type="bibr" target="#b18">[Tan et al., 2020]</ref>, the model size/FLOPs are optimal when using EfficientNets. With ResNets as the backbone, Segtran has a significantly higher model size/FLOPs, and hence this choice of backbone is not recommended for efficiency-sensitive scenarios.</figDesc><table><row><cell></cell><cell cols="2">Params (M) FLOPs (G)</cell></row><row><cell>nnU-Net</cell><cell>41.2</cell><cell>16.3</cell></row><row><cell>AttU-Net</cell><cell>34.9</cell><cell>51.0</cell></row><row><cell>SETR (ViT)</cell><cell>307.1</cell><cell>91.1</cell></row><row><cell>TransU-Net (R50+ViT)</cell><cell>93.2</cell><cell>32.2</cell></row><row><cell>Segtran (R101)</cell><cell>166.7</cell><cell>152.8</cell></row><row><cell>Segtran (eff-B4)</cell><cell>93.1</cell><cell>71.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Number of parameters / FLOPs on a 256x256 input image.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Impact of using pretrained encoder weights.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We clarify that our contribution is a novel transformer architecture that combines SAB with an Expanded Attention Block.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://refuge.grand-challenge.org/Home2020/ 3 https://www.med.upenn.edu/cbica/brats-2019/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/qubvel/segmentation_models.pytorch/ 5 https://github.com/4uiiurz1/pytorch-nested-unet</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We are grateful for the help and support of Wei Jing. This research is supported by A*STAR under its Career Development Award (Grant No. C210112016), and its Human-Robot Collaborative Al for Advanced Manufacturing and Engineering (AME) programme (Grant No. A18A2b0046).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Advancing The Cancer Genome Atlas glioma MRI collections with expert segmentation labels and radiomic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baheti</surname></persName>
		</author>
		<ptr target="https://github.com/LeeJunHyun/Image_Segmentation10https://github.com/RanSuLab/DUNet-retinal-vessel-detection11https://github.com/fudan-zvg/SETR/References" />
	</analytic>
	<monogr>
		<title level="m">Carreira and A. Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<meeting><address><addrLine>Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko</addrLine></address></meeting>
		<imprint>
			<publisher>Nicolas Carion</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Transunet: Transformers make strong encoders for medical image segmentation</title>
		<idno type="arXiv">arXiv:2102.04306</idno>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<editor>Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei</editor>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>CVPR. Dosovitskiy et al., 2021. et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">2020] Patrick Esser, Robin Rombach, and Bj?rn Ommer. Taming transformers for high-resolution image synthesis</title>
		<idno>arxiv:2012.09841</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rim-one: An open retinal image database for optic nerve evaluation</title>
	</analytic>
	<monogr>
		<title level="m">24th International Symposium on CBMS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>MICCAI</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unet 3+: A full-scale connected unet for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">43</biblScope>
		</imprint>
	</monogr>
	<note>Res2net: A new multi-scale backbone architecture</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">nnu-net: a selfconfiguring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Isensee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">2021</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">How much position information do convolutional neural networks encode? In ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Islam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dunet: A deformable network for retinal vessel segmentation. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Panoptic segmentation</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Set transformer: A framework for attention-based permutation-invariant neural networks</title>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving brain tumor segmentation with multi-direction fusion and fine class prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaonan</forename><surname>Sun&amp;apos;ao Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BrainLes workshop, MICCAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The multimodal brain tumor image segmentation benchmark (brats)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Menze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Vnet: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Milletari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">How Can CNNs Use Image Position for Segmentation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Murase</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.03463</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attention u-net: Learning where to look for the pancreas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Oktay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MIDL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Refuge challenge: A unified framework for evaluating automated methods for glaucoma assessment from fundus photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Orlando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention-based transformers for instance segmentation of cells in microstructures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Prangemeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Jayanthi Sivaswamy, Subbaiah Krishnadas, Arunava Chakravarty, Gopal Joshi, and Ujjwal. A comprehensive retinal image dataset for the assessment of glaucoma from the optic nerve head analysis. JSM Biomedical Imaging Data Papers</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">53</biblScope>
		</imprint>
	</monogr>
	<note>IEEE International Conference on Bioinformatics and Biomedicine</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mingxing Tan, Ruoming Pang, and Quoc V. Le. Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Tancik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<editor>NeurIPS, 2020. [Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, undefinedukasz Kaiser, and Illia Polosukhin</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned</title>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Liqin Zheng, Chun Meng, and Bharat Biswal. 3d u-net based brain tumor segmentation and survival days prediction</title>
	</analytic>
	<monogr>
		<title level="m">BrainLes Workshop, MICCAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<idno type="arXiv">arXiv:2004.08955</idno>
	</analytic>
	<monogr>
		<title level="m">Resnest: Split-attention networks</title>
		<editor>Jianfeng Feng, Tao Xiang, Philip H.S. Torr, and Li Zhang</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>DLMIA workshop (MICCAI)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
