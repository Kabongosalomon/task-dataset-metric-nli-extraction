<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DyTox: Transformers for Continual Learning with DYnamic TOken eXpansion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Douillard</surname></persName>
							<email>arthur.douillard@heuritech.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Ram?</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Couairon</surname></persName>
							<email>gcouairon@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
							<email>matthieu.cord@sorbonne-universite.fr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sorbonne</forename><surname>Universit?</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heuritech</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meta</forename><surname>Ai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valeo</forename><surname>Ai</surname></persName>
						</author>
						<title level="a" type="main">DyTox: Transformers for Continual Learning with DYnamic TOken eXpansion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep network architectures struggle to continually learn new tasks without forgetting the previous tasks. A recent trend indicates that dynamic architectures based on an expansion of the parameters can reduce catastrophic forgetting efficiently in continual learning. However, existing approaches often require a task identifier at test-time, need complex tuning to balance the growing number of parameters, and barely share any information across tasks. As a result, they struggle to scale to a large number of tasks without significant overhead. In this paper, we propose a transformer architecture based on a dedicated encoder/decoder framework. Critically, the encoder and decoder are shared among all tasks. Through a dynamic expansion of special tokens, we specialize each forward of our decoder network on a task distribution. Our strategy scales to a large number of tasks while having negligible memory and time overheads due to strict control of the expansion of the parameters. Moreover, this efficient strategy doesn't need any hyperparameter tuning to control the network's expansion. Our model reaches excellent results on CIFAR100 and state-of-the-art performances on the large-scale ImageNet100 and ImageNet1000 while having fewer parameters than concurrent dynamic frameworks. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Most of the deep learning literature focuses on learning a model on a fixed dataset. However, real-world data constantly evolve through time, leading to ever-changing distributions: i.e., new classes or domains appeared. When a model loses access to previous classes data (e.g., for privacy reasons) and is fine-tuned on new classes data, it catastrophically forgets the old distribution. Continual learning models aim at balancing a rigidity/plasticity trade-off where old data are not forgotten (rigidity to changes) while learning new incoming data (plasticity to adapt). Despite recent 1 https://github.com/arthurdouillard/dytox. <ref type="figure">Figure 1</ref>: DyTox's continual learning performance on ImageNet1000: for each task, 100 new classes are learned while previously learned classes are not fully accessible but shouldn't be forgotten. Our strategy DyTox (in red) is stateof-the-art by a large margin. Note that at the initial step before the continual process begins (denoted by a dashed rectangle ), our model has performance comparable to other baselines: the performance gain is achieved by reducing catastrophic forgetting. Moreover, we have systematically fewer parameters than previous approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>advances, it is still an open challenge.</head><p>A growing amount of efforts have emerged to tackle catastrophic forgetting <ref type="bibr" target="#b59">[59,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b75">75,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b76">76]</ref>. Recent works <ref type="bibr" target="#b77">[77,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b64">64]</ref> dynamically expand the network architectures <ref type="bibr" target="#b77">[77,</ref><ref type="bibr" target="#b45">46]</ref> or re-arrange their internal structures <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b26">27]</ref>. Unfortunately at test-time, they require to know the task to which the test sample belongsin order to know which parameters should be used. More recently, DER <ref type="bibr" target="#b76">[76]</ref> and Simple-DER <ref type="bibr" target="#b47">[48]</ref> discarded the need for this task identifier by learning a single classifier on the concatenation of all produced embeddings by different sub-sets of parameters. Yet, these strategies induce dramatic memory overhead when tackling a large number of tasks, and thus need complex pruning as post-processing.</p><p>To improve the ease of use of continual learning frameworks for real-world applications, we aim to design a dynamically expandable representation (almost) 'for free' by having the three following properties: #1 limited memory overhead as the number of tasks grows, #2 limited time overhead at test time and #3 no setting-specific hyperparameters for improved robustness when faced to an unknown (potentially large) number of tasks.</p><p>To this end, we leverage the computer vision transformer ViT <ref type="bibr" target="#b15">[16]</ref>. Transformers <ref type="bibr" target="#b71">[71]</ref> offer a very interesting framework to satisfy the previously mentioned constraints. Indeed, we build upon this architecture to design a encoder/decoder strategy: the encoder layers are shared among all members of our dynamic network; the unique decoder layer is also shared but its forward pass is specialized by a task-specific learned token to produce task-specific embeddings. Thus, the memory growth of the dynamic network is extremely limited: only a 384d vector per task, validating property #1. Moreover, this requires no hyperparameter tuning (property #3). Finally, the decoder is explicitly designed to be computationally lightweight (satisfying property #2). We nicknamed our framework, DyTox, for DYnamic TOken eXpansion. To the best of our knowledge, we are the first to apply the transformer architecture to continual computer vision.</p><p>Our strategy is robust to different settings, and can easily scale to a large number of tasks. In particular, we validate the efficiency of our approach on CIFAR100, ImageNet100, and ImageNet1000 (displayed on <ref type="figure">Fig. 1</ref>) for multiple settings. We reach state-of-the-art results, with only a small overhead thanks to our efficient dynamic strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Continual learning models tackle the catastrophic forgetting of the old classes <ref type="bibr" target="#b67">[67,</ref><ref type="bibr" target="#b24">25]</ref>. In computer vision, most of continual learning strategies applied on large-scale datasets use rehearsal learning: a limited amount of the training data of old classes is kept during training <ref type="bibr" target="#b60">[60]</ref>. This data is usually kept in raw form (e.g., pixels) <ref type="bibr" target="#b59">[59,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10]</ref> but can also be compressed <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b34">35]</ref>, or trimmed <ref type="bibr" target="#b17">[18]</ref> to reduce memory overhead; others store only a model to generate new samples of past classes <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b66">66,</ref><ref type="bibr" target="#b44">45]</ref>. In addition, most approaches aim at limiting the changes in the model when new classes are learned. These constraints can be directly applied on the weights <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b79">79,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b7">8]</ref>, intermediary features <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b83">83,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b16">17]</ref>, prediction probabilities <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, or on the gradients <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b62">62]</ref>. All these constraint-based methods use the same static network architectures which doesn't evolve through time, usually a ResNet <ref type="bibr" target="#b29">[30]</ref>, a LeNet <ref type="bibr" target="#b41">[42]</ref>, or a small MLP.</p><p>Continual dynamic networks In contrast, our paper and others focus on designing dynamic architectures that best handle a growing training distribution <ref type="bibr" target="#b77">[77,</ref><ref type="bibr" target="#b45">46]</ref>, in particular by dynamically creating (sub-)members each specialized in one specific task <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b73">73]</ref>. Unfortunately, previous approaches often require the sample's task identifier at test-time to select the right subset of parameters. We argue this is an unrealistic assumption in a real-life situation where new samples could come from any task. Recently, DER <ref type="bibr" target="#b76">[76]</ref> proposed a dynamic expansion of the representation by adding a new feature extractor per task. All extractors' embeddings would then be concatenated and fed to a unified classifier, discarding the need for a task identifier at test-time. To limit an explosion in the number of parameters, they aggressively prune each model after each task using the HAT <ref type="bibr" target="#b64">[64]</ref> procedure. Unfortunately, the pruning is hyperparameter sensitive. Therefore, hyperparameters are tuned differently on each experiment: for example, learning a dataset in 10 steps or in 50 steps use different hyperparameters. While being impracticable, it is also unrealistic because the number of classes is not known in advance in a true continual situation. Simple-DER <ref type="bibr" target="#b47">[48]</ref> also uses multiple extractors, but its pruning method doesn't need any hyperparameters; the negative counterpart is that Simple-DER controls less the parameter growth (2.5x higher than a base model). In contrast, we propose a framework dedicated to continual learning that seamlessly enables a task-dynamic strategy, efficient on all settings, without any setting-dependant modification and at almost no memory overhead. We share early class-agnostic <ref type="bibr" target="#b52">[53]</ref> layers similarly to TreeNets <ref type="bibr" target="#b43">[44]</ref> and base our strategy on the Transformer architecture.</p><p>Transformers were first introduced for machine translation <ref type="bibr" target="#b71">[71]</ref>, with the now famous self-attention. While the original transformer was made of encoder and decoder layers, later transformers starting from BERT <ref type="bibr" target="#b13">[14]</ref> used a succession of identical encoder blocks. Then, ViT <ref type="bibr" target="#b15">[16]</ref> proposed to apply transformers to computer vision by using patches of pixels as tokens. Multiple recent works, including DeiT <ref type="bibr" target="#b69">[69]</ref>, CaiT <ref type="bibr" target="#b70">[70]</ref>, ConVit <ref type="bibr" target="#b11">[12]</ref>, and Swin <ref type="bibr" target="#b49">[50]</ref>, improved ViT with architecture and training procedures modifications. PerceiverIO <ref type="bibr" target="#b35">[36]</ref> proposed a general architecture whose output is adapted to different modalities using specific learned tokens, and whose computation is reduced using a small number of latent tokens. Despite being successful across various benchmarks, transformers have not yet been considered for continual computer vision to the best of our knowledge. Yet, we don't use the transformer architecture for its own sake, but rather because of the intrinsic properties of transformers; in particular, the seminal encoder/decoder framework allows us to build an efficient architecture with strong capabilities against catastrophic forgetting. , -%:( <ref type="figure">Figure 2</ref>: DyTox transformer model. An image is first split into multiple patches, embedded with a linear projection. The resulting patch tokens are processed by 5 successive Self-Attention Blocks (SAB) (Sec. 3.1). For each task (t = 1 . . . T ), the processed patch tokens are then given to the Task-Attention Block (TAB) (Sec. 3.2): each forward through the TAB is modified by a different task-specialized token ? t for t ? {1 . . . T } (Sec. 3.3). The T final embeddings are finally given separately to independent classifiers Clf t each predicting their task's classes C t . All |C 1:T | logits are activated with a sigmoid. For example, at task t = 3, one forward is done through the SABs and three task-specific forwards through the unique TAB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DyTox transformer model</head><p>Our goal is to learn a unified model that will classify an increasingly growing number of classes, introduced in a fixed amount of steps T . At a given step t ? {1 . . . T }, the model is exposed to new data belonging to new classes. Specifically, it learns from samples {(x t i , y t i )} i , where x t i is the i-th image of this task t and y t i is the associated label within the label set C t . All task label sets are exclusive:</p><formula xml:id="formula_0">C 0 ? C 1 . . . C T = ?.</formula><p>The main challenge is that the data are fully available only temporarily: following most previous works, only a few samples from previous tasks {1 . . . t ? 1} are available for training at step t as rehearsing data. Yet, the model should remain able to classify test data coming from all seen classes C 1:t . A table of notations is provided in the supplementary materials.</p><p>The <ref type="figure">Fig. 2</ref> displays our DyTox framework, which is made of several components (SAB, TAB, and Task Tokens) that we describe in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Background</head><p>The vision transformer <ref type="bibr" target="#b15">[16]</ref> has three main components: the patch tokenizer, the encoder made of Self-Attention Blocks, and the classifier.</p><p>Patch tokenizer The fixed-size input RGB image is cropped into N patches of equal dimensions and then projected with a linear layer to a dimension D. Both oper-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self Attention</head><p>Layer Norm MLP</p><formula xml:id="formula_1">! 5 ! 56%</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Patch Tokens</head><p>Layer Norm <ref type="figure">Figure 3</ref>: The Self-Attention Block (SAB) combines a Self-Attention (SA), two Layer Norms, and one MLP with a single hidden layer. As in a ResNet, two shortcuts are used with element-wise addition. ations, the cropping and projection, are done with a single 2D convolution whose kernel size is equal to its stride size. The resulting tensor x 0 ? R N ?D is extended with a learned class token x cls ? R D resulting in a tensor of shape R (N +1)?D . Following <ref type="bibr" target="#b25">[26]</ref>, a learned positional embedding p ? R (N +1)?D is added (element-wise).</p><p>Self-Attention (SA) based encoder The tokens are fed to a stack of transformer blocks that we denote here as Self-Attention Blocks (SABs):</p><p>x l = x l + SA l (Norm l,1 (x l )) ,</p><formula xml:id="formula_2">x l+1 = x l + MLP l (Norm l,2 (x l )) ,<label>(1)</label></formula><p>with SA a Self-Attention layer <ref type="bibr" target="#b71">[71]</ref>, Norm a layer normalization <ref type="bibr" target="#b2">[3]</ref>, and MLP a Multi-Layer Perceptron with a single hidden layer. We repeat these operations for each SAB, from l = 1 to l = L. The resulting tensor (which keeps the same dimension after every block) is x L ? R (N +1)?D . We display a visual illustration of a SA Block in <ref type="figure">Fig. 3</ref>.</p><p>Classifier In the original vision transformer (ViT <ref type="bibr" target="#b15">[16]</ref>), a learned vector called the "class token" is appended to the patch tokens after the tokenizer. This special class token, when processed after all the SABs, is given to a linear classifier with a softmax activation to predict the final probabilities. However, more recent works, as CaiT <ref type="bibr" target="#b70">[70]</ref>, propose instead to introduce the class token only at the ultimate or penultimate SAB to improve classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Task-Attention Block (TAB)</head><p>Contrary to previous transformer architectures, we don't have a class token, but rather what we nicknamed "task tokens"; the learned token of the i th task is denoted ? i . This special token will only be added at the last block. To exploit this task token, we define a new attention layer, that we call the Task-Attention. It first concatenates the patch tokens x L produced by the ultimate SAB with a task token ? i :</p><formula xml:id="formula_3">z i = [? i , x L ] ? R (N+1)?D .<label>(2)</label></formula><p>This is then given to the Task-Attention (TA), inspired by the Class-Attention of Touvron et al. <ref type="bibr" target="#b70">[70]</ref>:</p><formula xml:id="formula_4">Q i = W q ? i , K i = W k z i , V i = W v z i , A i = Softmax Q i ? K T i / d/h , O i = W o A i V i + b o ? R 1?D ,<label>(3)</label></formula><p>with d being the embedding dimension, and h the number of attention heads <ref type="bibr" target="#b71">[71]</ref>. Contrary to the classical Self-Attention, the Task-Attention defines its query (Q i ) only from the task-token ? i without using the patch tokens x L . The Task-Attention Block (TAB) is then a variation of the SAB where the attention is a Task-Attention (TA):</p><formula xml:id="formula_5">c = c + TA (Norm 1 (z)) , c = c + MLP (Norm 2 (c )) .<label>(4)</label></formula><p>Overall, our new architecture can be summarized by the repetition of SA Blocks {SAB l } L l=1 (defined in Eq. 1) ended by a single TA Block TAB (defined in Eq. 4):</p><formula xml:id="formula_6">e i = TAB ? ([? i , SAB l=L ? ... SAB l=1 (x 0 )]) ? R D . (5)</formula><p>The final embedding e i is fed to a classifier clf made of a Norm c and a linear projection parametrized by {W c , b c }:</p><formula xml:id="formula_7">y i = Clf(e i ) = W c Norm c (e i ) + b c .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dynamic task token expansion</head><p>We defined in the previous section our base network, made of a succession of SABs and ended by a single TAB.</p><p>As detailed, the TAB has two inputs: the patch tokens x L extracted from the image and a learned task-token ? i . We'll now detail how our framework evolves in a continual situation at each new step.</p><p>During the first step, there is only one task token ? 1 . At each new step, we propose to expand our parameter space by creating a new task token while keeping the previous ones. Thus, after t steps, we have t task tokens (? i for i ? {1 . . . t}). Given an image x -belonging to any of the seen tasks {1 . . . t} -our model tokenizes it into x 0 , and processes it through the multiple SABs: this outputs the patch tokens x L . Finally, our framework does as many forward passes through the TAB as there are tasks: critically, each TAB forward passes is executed with a different task token ? i , resulting in different task-specific forwards, each producing the task-specific embeddings e i (see <ref type="figure">Fig. 2</ref>):</p><formula xml:id="formula_8">e 1 = TAB([? 1 , x L ]) , e 2 = TAB([? 2 , x L ]) , . . . e t = TAB([? t , x L ]) .<label>(7)</label></formula><p>Rather than concatenating all embeddings {e 1 , e 2 , . . . , e t } together and feeding them to one classifier, we leverage task-specific classifiers. Each classifier clf i is made of a Norm i and a linear projection parametrized by</p><formula xml:id="formula_9">{W i , b i }, with W i ? R C i ?D and b ? R C i</formula><p>. It takes as input its taskspecific embedding e i and returns:</p><formula xml:id="formula_10">y i = Clf i (e i ) = ?(W i Norm i e i + b i ) ,<label>(8)</label></formula><p>the predictions for the classes</p><formula xml:id="formula_11">y i ? C i , where ?(x) = 1 /(1+e ?x )</formula><p>is the sigmoid activation. In comparison with the softmax activation, the element-wise sigmoid activation reduces the overconfidence in recent classes. Consequently, the model is better calibrated, which is an important attribute of continual model <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b75">75,</ref><ref type="bibr" target="#b81">81]</ref>. The loss is the binary-cross entropy. The independent classifiers paradigm <ref type="bibr" target="#b65">[65]</ref> coupled with the sigmoid activation and binary crossentropy loss exclude explicitly a late fusion <ref type="bibr" target="#b57">[57]</ref> of the task embeddings resulting in more specialized classifiers.</p><p>The overall structure of the DyTox strategy is illustrated in <ref type="figure">Fig. 2</ref>. We also show in Algo. 1 the pseudo-code of a forward pass at test-time after having learned the task t. Critically, the test image can belong to any of the previously seen tasks {1 . . . t}. Our dynamic task token expansion is more efficient than a naive parameter expansion that would create a new copy of the whole network for each new task.</p><p>(1) Our expansion is limited to a new task token per new task, which is only d = 384 new parameters. This is small compared to the total model size (? 11 million parameters).</p><p>The memory overhead is thus almost null.</p><p>(2) The computationally intensive blocks (i.e., the SABs) are executed Algorithm 1 DyTox's forward pass at step t Input: x 0 (initial patch tokens), y ( ground-truth labels) Output:? 1:t (predictions for all classes of C 1:t )</p><formula xml:id="formula_12">1: x L ? SAB l=L ?... SAB l=1 (x 0 ) Sec. 3.1 2: for i ? 1; i ? t; i++ do 3: e i ? TAB([? i , x L ]) Sec. 3.2 4:? i ? Clf i (e i ) Sec. 3.3 5: end for 6:? 1:t ? [? 1 , . . . ,? t ]</formula><p>only once despite learning multiple tasks. In contrast, the TAB has as many forwards as there are tasks. Though, this induces minimal overhead because the Task-Attention has a linear complexity w.r.t the number of patches while the Self-Attention is quadratic. Therefore, the time overhead is sub-linear. We quantitatively show this in Sec. 4.</p><p>Context The current transformer paradigm starting from BERT <ref type="bibr" target="#b13">[14]</ref> and continuing with ViT <ref type="bibr" target="#b15">[16]</ref> is based on a en-coder+classifier structure. Differently, our dynamic framework strays is a resurgence of the encoder/decoder structure of the original transformer <ref type="bibr" target="#b71">[71]</ref>: the encoder is shared (both in memory and execution) for all outputs. The decoder parameters are also shared, but its execution is taskspecific with each task token, with each forward akin to a task-specific expert chosen from a mixture of experts <ref type="bibr" target="#b51">[52]</ref>. Moreover, multi-tasks text-based transformers have natural language tokens as an indicator of a task <ref type="bibr" target="#b55">[55]</ref> (e.g. "summarize the following text"), in our context of vision we used our defined task tokens as indicators.</p><p>Losses Our model is trained with three losses: (1) the classification loss L clf , a binary-cross entropy, (2) a knowledge distillation <ref type="bibr" target="#b30">[31]</ref> L kd applied on the probabilities, and (3) the divergence loss L div . The distillation loss helps to reduce forgetting. It is arguably quite naive, and more complex distillation losses <ref type="bibr" target="#b63">[63,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b18">19]</ref> could further improve results. The divergence loss, inspired from the "auxiliary classifier" of DER <ref type="bibr" target="#b76">[76]</ref>, uses the current last task's embedding e t to predict (|C t | + 1) probabilities: the current last task's classes C t and an extra class representing all previous classes that can be encountered via rehearsal. This classifier is discarded at test-time and encourages a better diversity among task tokens. The total loss is:</p><formula xml:id="formula_13">L = (1 ? ?)L clf + ?L kd + ?L div ,<label>(9)</label></formula><p>with ? a hyperparameter set to 0.1 for all experiments. ? correspond to the fraction of the number of old classes over the number of new classes |C 1:t?1 | |C 1:t | as done by <ref type="bibr" target="#b81">[81]</ref>. Therefore, ? is automatically set; this removes the need to finely tune this hyperparameter. . In addition to the top-1 accuracy, we also compare the top-5 accuracy on ImageNet. We report the "Avg" accuracy which is the average of the accuracies after each step as defined by <ref type="bibr" target="#b59">[59]</ref>. We also report the final accuracy after the last step ("Last"). Finally, in our tables, "#P" denotes the parameters count in million after the final step.</p><p>Implementation details As highlighted in <ref type="table" target="#tab_1">Table 1</ref>, our network has the same structure across all tasks. Specifically, we use 5 Self-Attention Blocks (SABs), 1 Task-Attention Block (TAB). All 6 have an embedding dimension of 384 and 12 attention heads. We designed this shallow transformer to have a comparable parameters count to other baselines, but also made it wider than usual "tiny" models <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b69">69,</ref><ref type="bibr" target="#b70">70]</ref>. We tuned all hyperparameters for CI-FAR100 with 10 steps on a validation set made of 10% of the training set, and then kept them fixed for all other settings, ImageNet included. The only difference between the two datasets is that ImageNet images are larger; thus the patch size is larger, and overall the base transformer has slightly more parameters on ImageNet than on CIFAR (11.00M vs 10.72M) because of a bigger positional embedding. We use the attention with spatial prior (introduced by ConViT <ref type="bibr" target="#b11">[12]</ref>) for all SABs which allows training transformers on a small dataset (like CIFAR) without pretraining on large datasets or complex regularizations. Following previous works <ref type="bibr" target="#b59">[59,</ref><ref type="bibr" target="#b76">76]</ref>, we use for all models (baselines included) 2,000 images of rehearsal memory for CIFAR100   and ImageNet100, and 20,000 images for ImageNet1000. The implementations of the continual scenarios are provided by Continuum <ref type="bibr" target="#b19">[20]</ref>. Our network implementation is based on the DeiT <ref type="bibr" target="#b69">[69]</ref> code base which itself uses extensively the timm library <ref type="bibr" target="#b74">[74]</ref>. The code is released publicly 2 .</p><p>The full implementation details are in the appendix.</p><p>2 https://github.com/arthurdouillard/dytox.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Quantitative results</head><p>ImageNet We report performances in <ref type="table" target="#tab_3">Table 2</ref> on the complex ImageNet dataset. The ? marks the DER with settingspecific pruning, and DER w/o P is for the DER without pruning. In ImageNet100, DyTox reaches 69.10% and outperforms DER ? by +3.04 percentage points (p.p ) in "Last" top-1 accuracy. Though, DyTox and DER w/o P somehow perform similarly in "Avg" accuracy on this setup, as highlighted in the performance evolution displayed in <ref type="figure" target="#fig_0">Fig. 4</ref>. Most importantly, on the larger-scale ImageNet1000, Dy-Tox systematically performs best on all metrics despite having lower parameters count. Specifically, DyTox reaches 71.29% in "Avg" top-1 accuracy, and 63.34% in "Last" top-1 accuracy. This outperforms the previous state-of-the-art DER w/o P (68.84% in "Avg", 60.16% in "Last") which has 10 ResNet18 in parallel and 116.89M parameters. Compared to the pruned DER ? , DyTox has a +4.56 p.p in top-1 and a +1.51 p.p in top-5 for the "Avg" accuracy. All models evolutions on ImageNet1000 are illustrated in <ref type="figure">Fig. 1</ref>: DyTox constantly surpasses previous state-of-the-art models -despite having a comparable performance at the first step and fewer parameters.</p><p>DyTox is able to scale correctly while handling seamlessly the parameter growth by sharing most of the weights across tasks. In contrast, DER had to propose a complex pruning method; unfortunately, this pruning required different hyperparameter values for different settings. Despite this, the pruning in DER ? is less efficient when classes diversity increase: DER ? doubles in size between  <ref type="table">Table 3</ref>: Results on CIFAR100 averaged over three different class orders. Baselines results are come from <ref type="bibr" target="#b76">[76]</ref>. The ? symbol means that <ref type="bibr" target="#b76">[76]</ref> needed setting-sensitive hyperparameters. Moreover, its reported parameters count was an average over all steps: the final parameters count (necessarily higher) was not available. ImageNet100 and ImageNet1000 ([76] reports 7.67M vs. 14.52M) while handling the same amount of tasks <ref type="bibr" target="#b9">(10)</ref>. Note that these parameter counts reported for DER ? in <ref type="bibr" target="#b76">[76]</ref> are in fact averages over all steps: the final parameters count (necessarily higher) was not available and thus is not reported in our tables. Simple-DER also applies pruning but without hyperparameter tuning; while simpler, the pruning is also less efficient and induces larger model (28.00M parameters).</p><p>CIFAR100 <ref type="table">Table 3</ref> shows results for all approaches on CIFAR100. The more steps there are, the larger the forgetting is and thus the lower the performances are. Those settings are also displayed in <ref type="figure" target="#fig_1">Fig. 5</ref> after each task. In every setting, DyTox is close to DER w/o P for much fewer parameters (up to 52x less). Critically, DyTox is significantly above other baselines: e.g. DyTox is up to +25% in "Last" accuracy in the 50 steps setup.  Tox is combined with this MixUp procedure, nicknamed as DyTox+, this significantly improves the state-of-the-art in "Avg" accuracy in all three settings of <ref type="table">Table 3</ref>. We also provide in the appendix further improvement for this new continual training procedure providing even larger gain on both CIFAR100 and ImageNet100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Model introspection on CIFAR100</head><p>Memory overhead We only add a vector of size d = 384 per task; thus, the overhead in memory (not considering the growing classifier which is common for all continual models) is only of +0.004% per step. Even in the challenging setting of CIFAR100 with 50 tasks, our memory overhead is almost null (+0.2%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computational overhead</head><p>The vast majority of the computation is done in the SABs, thus shared among all tasks. The dynamical component of our model is located at the ultimate TAB. Moreover, the Task-Attention, contrary to the Self-Attention, has a time complexity linear in terms of tokens and not quadratic reducing the time overhead to an acceptable sub-linear amount. Overall, for each new task, one forward pass takes 2.24% more time than for the base transformer.</p><p>Training procedure introspection Our DyTox+ strategy with MixUp really reduces catastrophic forgetting and does not just improve raw performances. This is shown in <ref type="table">Ta</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model ablations</head><p>We ablate the importance of the different components of DyTox in <ref type="table">Table 5</ref>. We add on the base transformer a naive knowledge distillation <ref type="bibr" target="#b30">[31]</ref> and a finetuning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b76">76]</ref> applied after each task on a balanced set of new data and rehearsal data. Finally, our DyTox strategy exploits directly the very nature of transformers (separated task information from the pixels information) to tackle catastrophic forgetting with three components: (1) a task token expansion, (2) a divergence classifier, and (3) independent classifiers. All three greatly improve over the baseline K n o w l e d g e D i s t i l l a t i o n F i n e t u n i n g T o k e n E x p a n s i o n D i v e r g e n c e C l a s s i fi e r I n d e n d e p e n t C l a s s i fi e r s  <ref type="table">Table 5</ref>: Ablations of the different key components of our DyTox architecture. We report the average accuracy and the last accuracy on CIFAR100 for the setting with 50 steps.</p><p>transformer (42.21% ? 52.34% in "Last") while having almost no memory overhead (+0.2%). The divergence classifier improves the diversity between task tokens: we observed that the minimal Euclidean distance between them increases by 8%. Moreover, we also remarked that having independent classifiers reduces the Chaudhry et al.'s forgetting <ref type="bibr" target="#b7">[8]</ref> by more than 24%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose DyTox, a new dynamic strategy for continual learning based on transformer architecture. In our model, self-attention layers are shared across all tasks, and we add task-specific tokens to achieve task-specialized embeddings through a new task-attention layer. This architecture allows us to dynamically process new tasks with very little memory overhead and does not require complex hyperparameter tuning. Our experiments show that our framework scales to large datasets like ImageNet1k with state-of-the-art performances. Moreover, when a large number of tasks is considered (i.e. CIFAR100 50 steps) our number of parameters increases reasonably contrary to previous dynamic strategies. Limitations: True continual learning aims at learning an almost unlimited number of tasks with low forgetting. No current approaches are yet able to do so. Thus, forgetting is not yet solved for continual learning but our model is a step forward in that direction. Broader impact: Machine learning models often are biased, with some classes suffering from lower performances. Studying forgetting in continual learning provides insights about the difference in performances between classes. Our task-specialized model could help reduce these biases. Acknowledgments: This work was partly supported by ANR grant VISA DEEP (ANR-20-CHIA-0022), and the HPC resources of IDRIS AD011011706.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Symbol Meaning</head><formula xml:id="formula_14">(x t i , y t i ) Input sample &amp; its label from the t th task C t</formula><p>Label set of the t th task C <ref type="bibr">1:t</ref> All labels from all seen tasks ? t Task token of the t th task Clf t Independent classifier of the t th task SAB l l th Self-Attention Block TAB Task-Attention Block <ref type="table">Table 6</ref>: Notations used in the paper.</p><p>A. Appendix <ref type="table">Table 6</ref> summarizes the notations used along this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Experimental details</head><p>Datasets We use three datasets: CIFAR100 <ref type="bibr" target="#b39">[40]</ref>, Ima-geNet100, and ImageNet1000 <ref type="bibr" target="#b12">[13]</ref>. CIFAR100 is made of 50,000 train RGB images and 10,000 test RGB images of size 32 ? 32 for 100 classes. ImageNet1000 contains 1.2 million RGB train images and 50,000 validation RGB images of size 224 ? 224 for 1000 classes. ImageNet100 is a subset of 100 classes from ImageNet1000. We follow POD-Net <ref type="bibr" target="#b18">[19]</ref> and DER <ref type="bibr" target="#b76">[76]</ref> and use the same 100 classes they've used. Fine details about the datasets, like the class orders, can be found in the provided code in the options files (see readme).</p><p>Implementation For all datasets, we train the model for 500 epochs per task with Adam <ref type="bibr" target="#b37">[38]</ref> with a learning rate of 5e ?4 , including 5 epochs of warmup. Following UCIR <ref type="bibr" target="#b31">[32]</ref>, PODNet <ref type="bibr" target="#b18">[19]</ref>, and DER <ref type="bibr" target="#b76">[76]</ref>, at the end of each task (except the first) we finetuned our model for 20 epochs with a learning rate of 5e ?5 on a balanced dataset. In DyTox, we applied the standard data augmentation of DeiT <ref type="bibr" target="#b69">[69]</ref> but we removed the pixel erasing <ref type="bibr" target="#b82">[82]</ref>, MixUp <ref type="bibr" target="#b80">[80]</ref>, and Cut-Mix <ref type="bibr" target="#b78">[78]</ref> augmentations for fair comparison. In contrast, in DyTox+ we used a MixUp <ref type="bibr" target="#b80">[80]</ref> with beta distribution ?(0.8, 0.8). During all incremental tasks (t &gt; 1), the old classifiers Clf i , i &lt; t and the old task tokens ? i , i &lt; t parameters are frozen. During the finetuning phase where classes are rebalanced <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b76">76]</ref>, these parameters are optimized, but the SABs are frozen.</p><p>Hyperparameter tuning In contrast with previous works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b76">76]</ref>, we wanted stable hyperparameters, tuned for a single setting and then applied on all experiments. This avoids optimizing for the number of tasks, which defeats the purpose of continual learning <ref type="bibr" target="#b21">[22]</ref>. We tuned hyperparameters for DyTox using a validation subset made of 10% of the training dataset, and this only on the CIFAR100 experiment  <ref type="table">Table 7</ref>: Hyperparameters that were tuned from the codebase of <ref type="bibr" target="#b69">[69]</ref>. We ran a gridsearch on CIFAR100 10 steps on a validation set made of 10% of the training set, and kept fixed the chosen hyperparameters for all experiments (any number of steps and any datasets).</p><p>with 10 steps. We provide in <ref type="table">Table 7</ref> the chosen hyperparameters. Results in the main paper shows that those hyperparameters reach state-of-the-art on all other settings and notably on ImageNet.</p><p>Baselines E2E <ref type="bibr" target="#b4">[5]</ref> and Simple-DER <ref type="bibr" target="#b47">[48]</ref> results come from their respective papers. All other baseline results are taken from the DER paper <ref type="bibr" target="#b76">[76]</ref>. We now further describe their contributions. iCaRL <ref type="bibr" target="#b59">[59]</ref> uses a knowledge distillation loss <ref type="bibr" target="#b30">[31]</ref> and at test-time predicts using a k-NN from its features space. E2E <ref type="bibr" target="#b4">[5]</ref> learns a model with knowledge distillation and applies a finetuning after each step. UCIR <ref type="bibr" target="#b31">[32]</ref> uses cosine classifier and euclidean distance between the final flattened features as a distillation loss. BiC <ref type="bibr" target="#b75">[75]</ref> uses a knowledge distillation loss and also re-calibrates <ref type="bibr" target="#b27">[28]</ref> the logits of the new classes using a simple linear model trained on validation data. WA <ref type="bibr" target="#b81">[81]</ref> uses a knowledge distillation loss and re-weights at each epoch the classifier weights associated to new classes so that they have the same average norm as the classifier weights of the old classes. POD-Net <ref type="bibr" target="#b18">[19]</ref> uses a cosine classifier and a specific distillation loss (POD) applied at multiple intermediary features of the ResNet backbone. RPSNet <ref type="bibr" target="#b56">[56]</ref> uses knowledge distillation and manipulates subnetworks in its architecture, following the lottery ticket hypothesis <ref type="bibr" target="#b23">[24]</ref>. DER <ref type="bibr" target="#b76">[76]</ref> creates a new ResNet per task. All ResNets' embeddings are concatenated and fed to a unique classifier. ResNets are pruned using HAT <ref type="bibr" target="#b64">[64]</ref> masking procedure. Note that DER pruning has multiple hyperparameters that are set differently according to the settings. Furthermore, the reported parameters count, after pruning, in <ref type="bibr" target="#b76">[76]</ref> is an average of the count over all steps: the final parameters count (necessarily higher) wasn't available. Finally, Simple-DER <ref type="bibr" target="#b47">[48]</ref> is similar to DER, with a simpler pruning method which doesn't require any hyperparameter tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Parameter sharing of the TAB</head><p>Previous dynamic methods as DER <ref type="bibr" target="#b76">[76]</ref> and Simple-DER <ref type="bibr" target="#b47">[48]</ref> shared no parameters between tasks until the fi-  <ref type="table">Table 8</ref>: Investigation of the parameter sharing of TAB. We report the "Avg" accuracy and the "Last" accuracy for the 50 steps setting on CIFAR100. The second row corresponds to DyTox. nal classifier. We proposed instead with DyTox to share the encoder (SABs) and the decoder (TAB) parameters across tasks, leading to a minimal memory overhead while also sharing common information between tasks. In <ref type="table">Table 8</ref>, we compare the impact of sharing the TAB per task -and only maintain different tokens per task. In the first row, a different TAB is created per task, while in the second row the same TAB is used -which is the DyTox strategy. A different TAB per task leads to better results (56% v.s. 52% in "Last" accuracy) because the network can be more diverse with each TAB specialized to its associated task. This increased diversity has a drawback: the memory overhead is too important (97M v.s. 10M parameters). We find in practice that DyTox strikes a good balance between memory overhead and continual performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Novel continual training procedure</head><p>DyTox++ We nicknamed DyTox+ our model when combined with a novel continual procedure based on MixUp <ref type="bibr" target="#b80">[80]</ref>. We now refine DyTox+ into DyTox++ by adding a new component during the training: the Sharpness-Aware Minimizer (SAM) <ref type="bibr" target="#b40">[41]</ref>. Indeed, aiming for wider minima is particularly important in continual learning <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b72">72]</ref>. This is because sharp task-specific minima lead to overspecialization to a particular task and consequently to a forgetting of all other tasks. Weights constraints as EWC <ref type="bibr" target="#b38">[39]</ref> or second-order optimization <ref type="bibr" target="#b42">[43]</ref> have similar motivations. SAM estimates the worst closest parameters during a first forward/backward pass, and then optimizes the loss w.r.t. to them during a second forward/pass. In consequence, Dy-Tox++ optimizes the loss not w.r.t. the current parameters but w.r.t. a region of possible parameters leading to wide local minima that span across multiple tasks. In practice, we used the Adaptive SAM (ASAM) <ref type="bibr" target="#b40">[41]</ref>, an extension of SAM that is more robust to hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DyTox+ and DyTox++ experiments</head><p>The computational overhead of ASAM is lower than more complex secondorder methods, but it still doubles the number of forward and backward passes. For this reason, we didn't include it in our main experiment but propose in <ref type="table">Table 9</ref> and <ref type="table" target="#tab_1">Table 10</ref> experiments on CIFAR100 and ImageNet100. The gain provided by MixUp then ASAM on our model (DyTox++) leads to a consistent improvement of +4.7% in "Avg" compared to the previous State-of-the-Art DER <ref type="bibr" target="#b76">[76]</ref> on CI-FAR100 50 steps <ref type="table">(Table 9</ref> and +4.6% on ImageNet100 10 steps <ref type="table" target="#tab_1">(Table 10</ref>). Future works could consider the promising Look-SAM <ref type="bibr" target="#b48">[49]</ref> to reduce the time overhead.</p><p>Training procedure introspection In <ref type="table" target="#tab_1">Table 11</ref>, we compare DyTox+ and DyTox++ on CIFAR100 in a joint setting (no continual) and in a continual setting with 50 steps. In the joint setting, our model slightly benefits from both MixUp and ASAM: the gain is limited (+1.79 p.p.). On the other hand, those two methods greatly improve the extreme continual setting of 50 steps (+6.42 p.p.). This shows that the gain is not due to absolute improvements of the model performance. Moreover, using the Chaudrhy et al.'s forgetting <ref type="bibr" target="#b7">[8]</ref> measure, we compare how much a model has forgotten relatively to its previous tasks. This metric is therefore agnostic to absolute performance improvements. Dy-Tox had a forgetting of 33.15%, DyTox+ of 31.50%, and DyTox++ of 30.47%: a total reduction of 2.68 p.p . This validates our novel training procedures that are particularly efficient for continual learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Patch size effect on forgetting</head><p>Our model is the first application of transformers for continual computer vision. A key component of the transformer architecture is the patch tokenizer. The number of patch tokens in an image is determined by the patch size: a larger patch size means less tokens, and vice-versa. We wondered about the effect of the patch size on forgetting and tested three different kind of patch sizes in <ref type="table" target="#tab_1">Table 12</ref>. Echoing results in vision transformers <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b69">69]</ref>, a smaller patch size (4 vs. 8 and 16) performs best in a joint training. However, the forgetting defined by Chaudhry et al. <ref type="bibr" target="#b7">[8]</ref> is relatively similar, with 33.15% for a patch of size of 4, and 33.20% for a patch size of 16. Therefore, we argue that the transformer architecture is hardly sensitive to the patch resolution w.r.t its forgetting in continual learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. ResNet backbone</head><p>DyTox is made of two main components: the SABs and the unique TAB. The TAB structure, taking in input both patch tokens and a task token, is crucial to our strategy. Yet, the SAB could be of any kind of features extractor, based on convolutions or attentions. Following the hybrid network proposed in ablations by Dosovitskiy et al. <ref type="bibr" target="#b15">[16]</ref>, we tried to replace the collection of SABs by a ResNet18. The final features of the ResNet, before global pooling, of shape (W ?H ?D) can be seen as W ?H tokens of D dimension. We made a few modifications to this ResNet to boost its performance, namely removed the fourth and ultimate layer, and added a pointwise convolution with 504 output channels (so it can be divided by the <ref type="bibr" target="#b11">12</ref>   <ref type="table">Table 9</ref>: Results on CIFAR100 averaged over three different class orders. WA and DER w/o P results are reported from <ref type="bibr" target="#b76">[76]</ref>. DyTox+ uses MixUp in addition of the DyTox strategy, DyTox++ further adds a sharpness-aware minimization <ref type="bibr" target="#b40">[41]</ref>.     TAB), a batch normalization <ref type="bibr" target="#b33">[34]</ref>, and a ReLU activation. These simple modifications are sufficient for our proof of concept, and thus we also didn't tune deeply this model. We display in <ref type="table" target="#tab_1">Table 13</ref> the comparison of the two backbones on CIFAR100 50 steps: (1) with ResNet, and (2) with SABs (DyTox). Performances are slightly lower than DyTox with SABs, however, they are still significantly higher than previous state-of-the-art like WA <ref type="bibr" target="#b81">[81]</ref>, especially in "Last" accuracy. Moreover, the parameters count is comparable to DyTox. This experiment shows that our DyTox framework, while designed with a transformer backbone in mind, is also efficient on non-token-based architectures such as a ResNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>#P</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6. Alternative task decoders</head><p>We investigate here other approaches for conditioning features to the different tasks. Residual Adapters <ref type="bibr" target="#b58">[58]</ref> adds a different residual branch made of a pointwise convolution for each domain the model is learned (e.g. CIFAR then ImageNet then SVHN). This model needs the task/dataset/domain identifier at test-time to determine which residual to use. For VQA task <ref type="bibr" target="#b1">[2]</ref>, FiLM <ref type="bibr" target="#b54">[54]</ref> proposes to modify the visual features using the the textual query.</p><p>We adapt these two feature conditioning strategies for our transformer backbone architecture. We perform a global token pooling after the ultimate SAB, and apply for each learned task, a residual adapter or a FiLM. Residual adapter in our case is a MLP, and FiLM parameters are directly learned. As for DyTox, we forward each task-specific em-CIFAR100 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Erratum</head><p>In Continual Learning, usually features are extracted once from images without data augmentations, and a procedure (random, closest <ref type="bibr" target="#b4">[5]</ref>, or even iCaRL iterative selection "herding" <ref type="bibr" target="#b59">[59]</ref>) is applied to select rehearsal images. In this section, we call this family of selections Global Memory.</p><p>In DyTox, when running our algorithm on multiple GPUs for data parallelism, we actually extracted features with data augmentations, and furthermore each GPU extracted features using their own slightly different data augmentations. As a result, each GPU could select different samples, and thus the effective memory size (across all processes) could be up to N times bigger than asked, with N the number of GPUs. We call that Distributed Memory.</p><p>Therefore, DyTox's results in this paper, while interesting, are not exactly comparable to the compared baselines. This section acts as an erratum. That said, we believe that this Distributed Memory have merits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Interest of the Distributed Memory</head><p>? In the first place, it has similarity with the Federated Learning literature. In our case, each process/GPU/machine has access to its own memory. Which would make sense, as more machines would also mean a bigger disk storage.</p><p>? Because each GPU extracts with different data augmentations, the overlap of chosen among GPUs is very low: on CIFAR100 with 2 GPUs, the overlap is usually between 1% and 10%. This means that very representative/useful examples will be selected twice (and thus seen more often). But also that each GPU, because of the iterative nature of the rehearsal gathering of iCaRL, will select data belonging to different modes, leading to increased diversity.</p><p>It results in a direct gain of performance on CIFAR as highlighted in <ref type="table" target="#tab_1">Table 15</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Updated results</head><p>Here are the updated results. Global Memory uses 20 samples per class in total (as other baselines), Distributed Memory also uses 20 samples per class, but split across GPUs. e.g. on CIFAR100 with 2 GPUs, each GPU only samples 10 samples per class at most. So the overall effective memory size constraint is respected.</p><p>Overall, DyTox keeps strong performance, more often than not above WA <ref type="bibr" target="#b81">[81]</ref> the third contender. Moreover, Dy-Tox+, our improved version also presented in the original paper version still reaches excellent performance, due to the forgetting reduction we explained before.</p><p>Gray color symbolizes results presented in the original paper version.</p><p>CIFAR experiments were run on 2 GPUs, all are with distributed memory and are shown in <ref type="table" target="#tab_1">Table 16</ref>.</p><p>ImageNet experiments, shown in <ref type="table" target="#tab_1">Table 17</ref> in global memory were run with 8 GPUs, and with distributed memory with 4 GPUs (thus only 5 images/class/GPU which probably explains here the slightly lower results compared to global memory).    <ref type="table" target="#tab_1">Table 16</ref>: Results on CIFAR100. Updated version with an erratum for Global vs. Distributed memory of the <ref type="table">table Table 3</ref>. Gray color symbolizes results presented in the original paper version where up to 2 times the amount of rehearsal samples was used.   <ref type="table" target="#tab_3">table Table 2</ref>. Gray color symbolizes results presented in the original paper version where up to 4 times the amount of rehearsal samples was used.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Performance evolution on ImageNet100. The top-5 accuracy (%) is reported after learning each task. Our model DyTox (in red) surpasses significantly most baselines, and is of equal performance as the complex DER that uses pruning with setting-specific hyperparameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Performance evolution on CIFAR100. The top-1 accuracy (%) is reported after learning each task. Left is evaluated with 10 steps, middle with 20 steps, and right with 50 steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>10</head><label>10</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>DyTox's architectures for CIFAR and ImageNet. The only difference between the two architectures is the patch size, as the image sizes vary between datasets.</figDesc><table><row><cell>Hyperparameter</cell><cell cols="2">CIFAR ImageNet</cell></row><row><cell># SAB</cell><cell>5</cell><cell></cell></row><row><cell># CAB</cell><cell>1</cell><cell></cell></row><row><cell># Attentions Heads</cell><cell>12</cell><cell></cell></row><row><cell>Embed Dim</cell><cell>384</cell><cell></cell></row><row><cell>Input Size</cell><cell>32</cell><cell>224</cell></row><row><cell>Patch Size</cell><cell>4</cell><cell>16</cell></row><row><cell>4. Experiments</cell><cell></cell><cell></cell></row><row><cell cols="2">4.1. Benchmarks &amp; implementation</cell><cell></cell></row><row><cell cols="3">Benchmarks &amp; Metrics We evaluate our model on CI-</cell></row><row><cell cols="3">FAR100 [40], ImageNet100 and ImageNet1000 [13] (de-</cell></row><row><cell cols="3">scriptions in the supplementary materials) under different</cell></row><row><cell cols="3">settings. The standard continual scenario in ImageNet has</cell></row><row><cell cols="3">10 steps: thus we add 10 new classes per step in Ima-</cell></row><row><cell cols="3">geNet100, and 100 new classes per step in ImageNet1000.</cell></row></table><note>In CIFAR100, we compare performances on 10 steps (10 new classes per step), 20 steps (5 new classes per step), and 50 steps (2 new classes per step)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>77.18 66.70 93.23 87.52 116.89 68.84 60.16 88.17 82.86 DER ? [76] -76.12 66.06 92.79 88.38 -66.73 58.62 87.08 81.89 DyTox 11.01 77.15 69.10 92.04 87.98 11.36 71.29 63.34 88.59 84.49</figDesc><table><row><cell></cell><cell></cell><cell cols="3">ImageNet100 10 steps</cell><cell></cell><cell></cell><cell cols="3">ImageNet1000 10 steps</cell><cell></cell></row><row><cell></cell><cell>#P</cell><cell cols="2">top-1</cell><cell cols="2">top-5</cell><cell>#P</cell><cell cols="2">top-1</cell><cell cols="2">top-5</cell></row><row><cell>Methods</cell><cell></cell><cell>Avg</cell><cell>Last</cell><cell>Avg</cell><cell>Last</cell><cell></cell><cell>Avg</cell><cell>Last</cell><cell>Avg</cell><cell>Last</cell></row><row><cell>ResNet18 joint</cell><cell>11.22</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">95.10 11.68</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>89.27</cell></row><row><cell>Transf. joint</cell><cell>11.00</cell><cell>-</cell><cell>79.12</cell><cell>-</cell><cell>93.48</cell><cell>11.35</cell><cell>-</cell><cell>73.58</cell><cell>-</cell><cell>90.60</cell></row><row><cell>E2E [5]</cell><cell>11.22</cell><cell>-</cell><cell>-</cell><cell cols="2">89.92 80.29</cell><cell>11.68</cell><cell>-</cell><cell>-</cell><cell cols="2">72.09 52.29</cell></row><row><cell>Simple-DER [48]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="5">28.00 66.63 59.24 85.62 80.76</cell></row><row><cell>iCaRL [59]</cell><cell>11.22</cell><cell>-</cell><cell>-</cell><cell cols="7">83.60 63.80 11.68 38.40 22.70 63.70 44.00</cell></row><row><cell>BiC [32]</cell><cell>11.22</cell><cell>-</cell><cell>-</cell><cell cols="3">90.60 84.40 11.68</cell><cell>-</cell><cell>-</cell><cell cols="2">84.00 73.20</cell></row><row><cell>WA [81]</cell><cell>11.22</cell><cell>-</cell><cell>-</cell><cell cols="7">91.00 84.10 11.68 65.67 55.60 86.60 81.10</cell></row><row><cell>RPSNet [56]</cell><cell></cell><cell>-</cell><cell>-</cell><cell cols="2">87.90 74.00</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DER w/o P [76]</cell><cell>112.27</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results on ImageNet-100 and ImageNet-1000 datasets, learned with 10 steps of respectively 10 and 100 new classes. E2E<ref type="bibr" target="#b4">[5]</ref> and Simple-DER<ref type="bibr" target="#b47">[48]</ref> results come from their respective papers, and used a different class ordering. Other results come from<ref type="bibr" target="#b76">[76]</ref>. The ? symbol means that<ref type="bibr" target="#b76">[76]</ref> needed setting-sensitive hyperparameters. Moreover, its reported parameters count was an average over all steps([76]  reported 14.52M on ImageNet1000): the final parameters count (necessarily higher) was not available.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>65.27 ? 1.02 50.74 11.22 61.20 ? 0.83 43.75 11.22 56.08 ? 0.83 36.62 UCIR [32] 11.22 58.66 ? 0.71 43.39 11.22 58.17 ? 0.30 40.63 11.22 56.86 ? 0.83 73.66 ? 0.02 60.67 ? 0.34 10.74 72.27 ? 0.18 56.32 ? 0.61 10.77 70.20 ? 0.16 52.34 ? 0.26 DyTox+ 10.73 75.54 ? 0.10 62.06 ? 0.25 10.74 75.04 ? 0.11 60.03 ? 0.45 10.77 74.35 ? 0.05 57.09 ? 0.13</figDesc><table><row><cell></cell><cell></cell><cell>10 steps</cell><cell></cell><cell></cell><cell>20 steps</cell><cell></cell><cell></cell><cell>50 steps</cell><cell></cell></row><row><cell>Methods</cell><cell>#P</cell><cell>Avg</cell><cell>Last</cell><cell>#P</cell><cell>Avg</cell><cell>Last</cell><cell>#P</cell><cell>Avg</cell><cell>Last</cell></row><row><cell cols="2">ResNet18 Joint 11.22</cell><cell>-</cell><cell>80.41</cell><cell>11.22</cell><cell>-</cell><cell>81.49</cell><cell>11.22</cell><cell>-</cell><cell>81.74</cell></row><row><cell>Transf. Joint</cell><cell>10.72</cell><cell>-</cell><cell>76.12</cell><cell>10.72</cell><cell>-</cell><cell>76.12</cell><cell>10.72</cell><cell>-</cell><cell>76.12</cell></row><row><cell>iCaRL [59]</cell><cell cols="9">11.22 37.09</cell></row><row><cell>BiC [75]</cell><cell cols="2">11.22 68.80 ? 1.20</cell><cell>53.54</cell><cell cols="2">11.22 66.48 ? 0.32</cell><cell>47.02</cell><cell cols="2">11.22 62.09 ? 0.85</cell><cell>41.04</cell></row><row><cell>WA [81]</cell><cell cols="2">11.22 69.46 ? 0.29</cell><cell>53.78</cell><cell cols="2">11.22 67.33 ? 0.15</cell><cell>47.31</cell><cell cols="2">11.22 64.32 ? 0.28</cell><cell>42.14</cell></row><row><cell>PODNet [19]</cell><cell cols="2">11.22 58.03 ? 1.27</cell><cell>41.05</cell><cell cols="2">11.22 53.97 ? 0.85</cell><cell>35.02</cell><cell cols="2">11.22 51.19 ? 1.02</cell><cell>32.99</cell></row><row><cell>RPSNet [56]</cell><cell>56.5</cell><cell>68.60</cell><cell>57.05</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">DER w/o P [76] 112.27 75.36 ? 0.36</cell><cell>65.22</cell><cell cols="2">224.55 74.09 ? 0.33</cell><cell>62.48</cell><cell cols="2">561.39 72.41 ? 0.36</cell><cell>59.08</cell></row><row><cell>DER  ? [76]</cell><cell>-</cell><cell>74.64 ? 0.28</cell><cell>64.35</cell><cell>-</cell><cell>73.98 ? 0.36</cell><cell>62.55</cell><cell>-</cell><cell>72.05 ? 0.55</cell><cell>59.76</cell></row><row><cell>DyTox</cell><cell>10.73</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>"Last" accuracy and forgetting<ref type="bibr" target="#b7">[8]</ref> on CIFAR100 for the joint (1 step, no continual) and 50 steps settings.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>TAB parameter sharing? #P Avg Last 97.59 72.20 56.00 10.77 70.20 52.34</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>69.46 ? 0.29 53.78 11.22 67.33 ? 0.15 47.31 11.22 64.32 ? 0.28 42.14 DER w/o P [76] 112.27 75.36 ? 0.36 65.22 224.55 74.09 ? 0.33 62.48 561.39 72.41 ? 0.36 59.08 DyTox 10.73 73.66 ? 0.02 60.67 ? 0.34 10.74 72.27 ? 0.18 56.32 ? 0.61 10.77 70.20 ? 0.16 52.34 ? 0.26 DyTox+ 10.73 75.54 ? 0.10 62.06 ? 0.25 10.74 75.04 ? 0.11 60.03 ? 0.45 10.77 74.35 ? 0.05 57.09 ? 0.13 DyTox++ 10.73 77.10 ? 0.08 64.53 ? 0.08 10.74 76.57 ? 0.18 62.44 ? 0.22 10.77 75.45 ? 0.19 58.76 ? 0.28 DyTox+ 10.73 76.74 ? 1.08 67.04 ? 0.10 10.74 76.25 ? 0.30 62.85 ? 0.16 10.77 74.16 ? 1.89 59.10 ? 0.99 DyTox++ 10.73 77.01 ? 1.21 67.53 ? 0.37 10.74 76.81 ? 0.43 64.27 ? 0.81 10.77 75.53 ? 2.79 59.51 ? 1.61</figDesc><table><row><cell></cell><cell></cell><cell>10 steps</cell><cell></cell><cell></cell><cell>20 steps</cell><cell></cell><cell></cell><cell>50 steps</cell><cell></cell></row><row><cell>Methods</cell><cell>#P</cell><cell>Avg</cell><cell>Last</cell><cell>#P</cell><cell>Avg</cell><cell>Last</cell><cell>#P</cell><cell>Avg</cell><cell>Last</cell></row><row><cell cols="2">ResNet18 Joint 11.22</cell><cell>-</cell><cell>80.41</cell><cell>11.22</cell><cell>-</cell><cell>81.49</cell><cell>11.22</cell><cell>-</cell><cell>81.74</cell></row><row><cell>Transf. Joint</cell><cell>10.72</cell><cell>-</cell><cell>76.12</cell><cell>10.72</cell><cell>-</cell><cell>76.12</cell><cell>10.72</cell><cell>-</cell><cell>76.12</cell></row><row><cell>WA [81]</cell><cell>11.22</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">attention heads of the</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Results on ImageNet-100 with 10 steps of 10 new classes each. WA and DER w/o P results are reported from<ref type="bibr" target="#b76">[76]</ref>. DyTox+ uses MixUp in addition of the Dy-</figDesc><table><row><cell cols="4">Tox strategy, DyTox++ further adds a sharpness-aware min-</cell></row><row><cell>imizer.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Joint (1 step)</cell><cell cols="2">50 steps</cell></row><row><cell>Training</cell><cell>Last (?)</cell><cell cols="2">Last (?) Forgetting (?)</cell></row><row><cell>DyTox</cell><cell>76.12</cell><cell>52.34</cell><cell>33.15</cell></row><row><cell>DyTox+</cell><cell>78.86+1.39</cell><cell>59.10+4.75</cell><cell>24.81-1.65</cell></row><row><cell>DyTox++</cell><cell>78.70+0.40</cell><cell>59.51+1.67</cell><cell>26.70-1.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11 :</head><label>11</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">"Last" accuracy and forgetting [8] on CI-</cell></row><row><cell cols="4">FAR100 for the joint (1 step, no continual) and 50 steps</cell></row><row><cell>settings.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Joint (1 steps)</cell><cell></cell><cell>50 steps</cell></row><row><cell>Patch size</cell><cell>Last (?)</cell><cell cols="2">Last (?) Forgetting (?)</cell></row><row><cell>4</cell><cell>76.12</cell><cell>52.34</cell><cell>33.15</cell></row><row><cell>8</cell><cell>67.65</cell><cell>43.93</cell><cell>35.44</cell></row><row><cell>16</cell><cell>50.15</cell><cell>31.49</cell><cell>33.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12 :</head><label>12</label><figDesc>Patch size effect on continual for the joint (1 step, no continual) and 50 steps settings on CIFAR100. We choose a patch size of 4 for our main experiments: yet, it has only few impact on forgetting.</figDesc><table><row><cell>Encoder</cell><cell>#P</cell><cell>Avg</cell><cell>Last</cell></row><row><cell>ResNet</cell><cell cols="3">10.68 68.53 50.05</cell></row><row><cell>SABs</cell><cell cols="3">10.77 70.20 52.34</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 13 :</head><label>13</label><figDesc>Hybrid network on CIFAR100 50 steps. While the features extractor is made of SABs in DyTox, here we instead use a modified ResNet18. Our framework still works well with a convolution-based approach.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 14 :</head><label>14</label><figDesc>58] 70.00 52.38 91.25 85.00 FiLM [54] 69.42 54.05 89.49 81.40 TAB (ours) 70.20 52.34 92.04 87.98 Alternative task conditioner on CIFAR100 50 steps and ImageNet100 10 steps. While the simpler Residual Adapters and FiLM perform similarly to our TAB on CIFAR100, they forget significantly more on the complex ImageNet100.bedding to the respective task-specific classifier. We showcase the continual performance inTable 14on CIFAR100 50 steps and ImageNet100 10 steps. On the former dataset, smaller and easier, the residual adapters and FiLM have similar performance as our TAB approach. On the other hand, as soon as the task complexity increases with the more detailed ImageNet100 dataset, FiLM and Residual adapter based conditioning strategies forget significantly more than our complete DyTox framework: TAB outperform the Residual Adapters by +2.98 p.p in "Last" top-5 accuracy and FiLM by +6.58 p.p .</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">ImageNet100</cell></row><row><cell></cell><cell cols="2">Top-1</cell><cell cols="2">Top-5</cell></row><row><cell>Task decoder</cell><cell>Avg</cell><cell>Last</cell><cell>Avg</cell><cell>Last</cell></row><row><cell>Residual Adapters [</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc>67.33 51.68 67.30 48.45 64.39 43.47 Distributed 1,000 2,000 71.50 57.76 68.86 51.47 64.82 45.61</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>steps</cell><cell cols="2">20 steps</cell><cell>50 steps</cell></row><row><cell cols="3">Memory Type # Memory / GPU Total effective # memory</cell><cell>Avg</cell><cell>Last</cell><cell>Avg</cell><cell>Last</cell><cell>Avg</cell><cell>Last</cell></row><row><cell>Global</cell><cell>2,000</cell><cell>2,000</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 15 :</head><label>15</label><figDesc>Results on CIFAR100 for Distributed vs Global Memory when run with 2 GPUs. Global memory uses the same 2k samples across each GPU, while Distributed memory uses 1k samples per memory with potential overlap across sets. 73.66 ? 0.02 60.67 ? 0.34 10.74 72.27 ? 0.18 56.32 ? 0.61 10.77 70.20 ? 0.16 52.34 ? 0.26 DyTox+ 10.73 75.54 ? 0.10 62.06 ? 0.25 10.74 75.04 ? 0.11 60.03 ? 0.45 10.77 74.35 ? 0.05 57.09 ? 0.13</figDesc><table><row><cell></cell><cell></cell><cell>10 steps</cell><cell></cell><cell></cell><cell>20 steps</cell><cell></cell><cell></cell><cell>50 steps</cell><cell></cell></row><row><cell>Methods</cell><cell>#P</cell><cell>Avg</cell><cell>Last</cell><cell>#P</cell><cell>Avg</cell><cell>Last</cell><cell>#P</cell><cell>Avg</cell><cell>Last</cell></row><row><cell>ResNet18 Joint</cell><cell>11.22</cell><cell>-</cell><cell>80.41</cell><cell>11.22</cell><cell>-</cell><cell>81.49</cell><cell>11.22</cell><cell>-</cell><cell>81.74</cell></row><row><cell>Transf. Joint</cell><cell>10.72</cell><cell>-</cell><cell>76.12</cell><cell>10.72</cell><cell>-</cell><cell>76.12</cell><cell>10.72</cell><cell>-</cell><cell>76.12</cell></row><row><cell>iCaRL [59]</cell><cell cols="2">11.22 65.27 ? 1.02</cell><cell>50.74</cell><cell cols="2">11.22 61.20 ? 0.83</cell><cell>43.75</cell><cell cols="2">11.22 56.08 ? 0.83</cell><cell>36.62</cell></row><row><cell>UCIR [32]</cell><cell cols="2">11.22 58.66 ? 0.71</cell><cell>43.39</cell><cell cols="2">11.22 58.17 ? 0.30</cell><cell>40.63</cell><cell cols="2">11.22 56.86 ? 0.83</cell><cell>37.09</cell></row><row><cell>BiC [75]</cell><cell cols="2">11.22 68.80 ? 1.20</cell><cell>53.54</cell><cell cols="2">11.22 66.48 ? 0.32</cell><cell>47.02</cell><cell cols="2">11.22 62.09 ? 0.85</cell><cell>41.04</cell></row><row><cell>WA [81]</cell><cell cols="2">11.22 69.46 ? 0.29</cell><cell>53.78</cell><cell cols="2">11.22 67.33 ? 0.15</cell><cell>47.31</cell><cell cols="2">11.22 64.32 ? 0.28</cell><cell>42.14</cell></row><row><cell>PODNet [19]</cell><cell cols="2">11.22 58.03 ? 1.27</cell><cell>41.05</cell><cell cols="2">11.22 53.97 ? 0.85</cell><cell>35.02</cell><cell cols="2">11.22 51.19 ? 1.02</cell><cell>32.99</cell></row><row><cell>RPSNet [56]</cell><cell>56.5</cell><cell>68.60</cell><cell>57.05</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DER w/o P [76]</cell><cell cols="2">112.27 75.36 ? 0.36</cell><cell>65.22</cell><cell cols="2">224.55 74.09 ? 0.33</cell><cell>62.48</cell><cell cols="2">561.39 72.41 ? 0.36</cell><cell>59.08</cell></row><row><cell>DER  ? [76]</cell><cell>-</cell><cell>74.64 ? 0.28</cell><cell>64.35</cell><cell>-</cell><cell>73.98 ? 0.36</cell><cell>62.55</cell><cell>-</cell><cell>72.05 ? 0.55</cell><cell>59.76</cell></row><row><cell cols="2">DyTox 10.73 DyTox distMem 10.73</cell><cell>71.50</cell><cell>57.76</cell><cell>10.74</cell><cell>68.86</cell><cell>51.47</cell><cell>10.77</cell><cell>64.82</cell><cell>45.61</cell></row><row><cell cols="2">DyTox+ distMem 10.73</cell><cell>74.10</cell><cell>62.34</cell><cell>10.74</cell><cell>71.62</cell><cell>57.43</cell><cell>10.77</cell><cell>68.90</cell><cell>51.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head></head><label></label><figDesc>63.80 11.68 38.40 22.70 63.70 44.00 77.18 66.70 93.23 87.52 116.89 68.84 60.16 88.17 82.86 DER ? [76] -76.12 66.06 92.79 88.38 -66.73 58.62 87.08 81.89 DyTox 11.01 77.15 69.10 92.04 87.98 11.36 71.29 63.34 88.59 84.49 DyTox+ 11.01 79.22 69.06 93.72 88.82 71.85 57.94 90.72 83.52 11.36 68.14 59.75 87.03 82.93 DyTox+ globalMem 11.01 77.62 65.94 93.15 88.78 11.36 73.21 64.56 91.09 87.07 77.15 67.70 93.17 89.42 11.36 70.88 60.00 90.53 85.25</figDesc><table><row><cell></cell><cell></cell><cell cols="3">ImageNet100 10 steps</cell><cell></cell><cell></cell><cell cols="3">ImageNet1000 10 steps</cell><cell></cell></row><row><cell></cell><cell>#P</cell><cell cols="2">top-1</cell><cell cols="2">top-5</cell><cell>#P</cell><cell cols="2">top-1</cell><cell cols="2">top-5</cell></row><row><cell>Methods</cell><cell></cell><cell>Avg</cell><cell>Last</cell><cell>Avg</cell><cell>Last</cell><cell></cell><cell>Avg</cell><cell>Last</cell><cell>Avg</cell><cell>Last</cell></row><row><cell>ResNet18 joint</cell><cell>11.22</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">95.10 11.68</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>89.27</cell></row><row><cell>Transf. joint</cell><cell>11.00</cell><cell>-</cell><cell>79.12</cell><cell>-</cell><cell>93.48</cell><cell>11.35</cell><cell>-</cell><cell>73.58</cell><cell>-</cell><cell>90.60</cell></row><row><cell>E2E [5]</cell><cell>11.22</cell><cell>-</cell><cell>-</cell><cell cols="2">89.92 80.29</cell><cell>11.68</cell><cell>-</cell><cell>-</cell><cell cols="2">72.09 52.29</cell></row><row><cell>Simple-DER [48]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="5">28.00 66.63 59.24 85.62 80.76</cell></row><row><cell cols="7">iCaRL [59] 83.60 BiC [32] 11.22 --11.22 --90.60 84.40 11.68</cell><cell>-</cell><cell>-</cell><cell cols="2">84.00 73.20</cell></row><row><cell>WA [81]</cell><cell>11.22</cell><cell>-</cell><cell>-</cell><cell cols="7">91.00 84.10 11.68 65.67 55.60 86.60 81.10</cell></row><row><cell>RPSNet [56]</cell><cell></cell><cell>-</cell><cell>-</cell><cell cols="2">87.90 74.00</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DER w/o P [76]</cell><cell cols="6">112.27 11.01</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="6">DyTox globalMem 11.01 DyTox distMem 11.01 73.96 62.20 91.29 85.60</cell><cell>11.36</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DyTox+ distMem</cell><cell>11.01</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 17 :</head><label>17</label><figDesc>Results on ImageNet-100 and ImageNet-1000 datasets. Updated version with an erratum for Global vs. Distributed memory of the</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Memory aware synapses: Learning what (not) to forget</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahaf</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesca</forename><surname>Babiloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE European Conference on Computer Vision (ECCV)</title>
		<meeting>the IEEE European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NeurIPS 2016 Deep Learning Symposium</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Il2m: Class incremental learning with dual memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eden</forename><surname>Belouadah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-end incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicol?s</forename><surname>Manuel J Mar?n-Jim?nez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Guil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE European Conference on Computer Vision (ECCV)</title>
		<meeting>the IEEE European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modeling the background for incremental learning in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Cermelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vicinal risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Riemannian walk for incremental learning: Understanding forgetting and intransigence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arslan</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puneet</forename><surname>Dokania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thalaiyasingam</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE European Conference on Computer Vision (ECCV)</title>
		<meeting>the IEEE European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient lifelong learning with a-gem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arslan</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On tiny episodic memories in continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arslan</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thalaiyasingam</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Puneet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dokania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML) Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Routing networks with co-training for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Effrosyni</forename><surname>Mark Patrick Collier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Kokiopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">icmlws</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Convit: Improving vision transformers with soft convolutional inductive biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>St?phane D&amp;apos;ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulio</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sagun</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In arXiv preprint library, 2021. (pages 2, 5)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning without memorizing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat Vikram</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Chuan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR), 2021</title>
		<meeting>the International Conference on Learning Representations (ICLR), 2021</meeting>
		<imprint/>
	</monogr>
	<note>pages 2, 3, 4, 5, 10</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Plop: Learning without forgetting for continual semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Douillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Dapogny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Tackling catastrophic forgetting and background shift in continual semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Douillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Dapogny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In arXiv preprint library, 2021. (page 2</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Podnet: Pooled outputs distillation for small-tasks incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Douillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Ollion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Valle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE European Conference on Computer Vision (ECCV), 2020</title>
		<meeting>the IEEE European Conference on Computer Vision (ECCV), 2020</meeting>
		<imprint>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
	<note>pages 1, 2, 5, 7, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Continuum: Simple management of complex continual learning scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Douillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timoth?e</forename><surname>Lesort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshop, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Orthogonal gradient descent for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navid</forename><surname>Azizan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Mott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Internaltional Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards robust evaluations of continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML) Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">PathNet: Evolution Channels Gradient Descent in Super Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chrisantha</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Banarse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yori</forename><surname>Zwols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>arXiv preprint library, 2017. (pages 1, 2)</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Catastrophic forgetting in connectionist networks. Trends in cognitive sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>French</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Continual learning via neural pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siavash</forename><surname>Golkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS) Workshop, 2019</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Remind your neural network to prevent catastrophic forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><forename type="middle">L</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kushal</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robik</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE European Conference on Computer Vision (ECCV)</title>
		<meeting>the IEEE European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS) Workshop, 2015. (pages 5</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning a unified classifier incrementally via rebalancing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saihui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019</meeting>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note>pages 1, 2, 5, 6, 7, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Compacting, picking and growing for unforgetting continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Hao</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-En</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Hung</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Song</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS), 2019</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Memory-efficient incremental learning through feature adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmet</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE European Conference on Computer Vision (ECCV)</title>
		<meeting>the IEEE European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Perceiver io: A general architecture for structured inputs &amp; outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skanda</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Carreira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fearnet: Braininspired model for incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>pages 1, 2, 10</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungmin</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeongseop</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunseo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">Kwon</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Object recognition with gradient-based learning. In Shape, contour and grouping in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Continual learning with extended kroneckerfactored approximate curvature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janghyeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hyeong Gwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggyu</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page" from="2020" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Why m heads are better than one: Training a diverse ensemble of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senthil</forename><surname>Purushwalkam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>In arXiv preprint library</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Generative models from the perspective of continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timoth?e</forename><surname>Lesort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Caselles-Dupr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Garcia-Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Stoian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Filliat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learn to grow: A continual structure learning framework for overcoming catastrophic forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR), 2019</title>
		<meeting>the International Conference on Learning Representations (ICLR), 2019</meeting>
		<imprint>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE European Conference on Computer Vision (ECCV)</title>
		<meeting>the IEEE European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Preserving earlier knowledge in continual learning with the help of all previous feature extractors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoyun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhong</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
	<note>In arXiv preprint library, 2021. (pages 1, 2, 6, 9</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Sharpness-aware minimization in large-batch training: Training vision transformer in minutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Open-Review</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Gradient episodic memory for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Mixture of experts: a literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Masoudnia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Ebrahimpour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="275" to="293" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Feature visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schubert</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Distill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">An adaptive random path selection approach for incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jathushan</forename><surname>Rajasegaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep multimodal learning: A survey on recent advances and trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhanesh</forename><surname>Ramachandram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning multiple visual domains with residual adapters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017</meeting>
		<imprint/>
	</monogr>
	<note>pages 1, 2, 5, 6, 7, 9, 12, 13, 14</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting, rehearsal and pseudorehearsal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Robins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Connection Science</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
		<title level="m">Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint library</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Gradient projection memory for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gobinda</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isha</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaushik</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting with hard attention to the task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Serr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D?dac</forename><surname>Sur?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Miron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>pages 1, 2, 9</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Ensembles and encoders for task-free continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Kaplanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jovana</forename><surname>Mitrovi?</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In arXiv preprint library, 2021. (page 4</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Continual learning with deep generative replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanul</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehong</forename><surname>Jung Kwon Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Lifelong learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer Learning to Learn</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">On mixup training: Improved calibration and predictive uncertainty for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gopinath</forename><surname>Sunil Thulasidasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Chennupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmoy</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michalak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers and distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint/>
	</monogr>
	<note>2021. (pages 2, 5, 6, 9</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Going deeper with image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2021</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV), 2021</meeting>
		<imprint/>
	</monogr>
	<note>pages 2, 4, 5</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS), 2017</title>
		<imprint/>
	</monogr>
	<note>pages 2, 3, 4, 5</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Rehearsal revealed: The limits and merits of revisiting samples in continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Verwimp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><forename type="middle">De</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshop</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshop</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Batchensemble: An alternative approach to efficient ensemble and lifelong learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeming</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Large scale incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuancheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019</meeting>
		<imprint>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
	<note>pages 1, 4, 7, 9</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Der: Dynamically expandable representation for class incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangwei</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021</meeting>
		<imprint/>
	</monogr>
	<note>pages 1, 2, 5, 6, 7, 8, 9, 10, 11, 13, 14</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Lifelong learning with dynamically expandable networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehong</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeongtae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Continual learning through synaptic intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedemann</forename><surname>Zenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Maintaining discrimination and fairness in class incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojun</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shutao</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020</meeting>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note>pages 4, 5, 6, 7, 9</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">M2kd: Multi-model and multilevel knowledge distillation for incremental learning. arXiv preprint library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
