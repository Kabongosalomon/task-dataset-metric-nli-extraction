<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PseCo: Pseudo Labeling and Consistency Training for Semi-Supervised Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">1?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
							<email>liangding@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
							<email>shanshan.zhang@njust.edu.cnwangyujie</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PseCo: Pseudo Labeling and Consistency Training for Semi-Supervised Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Semi-supervised Learning, Object Detection</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we delve into two key techniques in Semi-Supervised Object Detection (SSOD), namely pseudo labeling and consistency training. We observe that these two techniques currently neglect some important properties of object detection, hindering efficient learning on unlabeled data. Specifically, for pseudo labeling, existing works only focus on the classification score yet fail to guarantee the localization precision of pseudo boxes; For consistency training, the widely adopted random-resize training only considers the label-level consistency but misses the feature-level one, which also plays an important role in ensuring the scale invariance. To address the problems incurred by noisy pseudo boxes, we design Noisy Pseudo box Learning (NPL) that includes Prediction-guided Label Assignment (PLA) and Positive-proposal Consistency Voting (PCV). PLA relies on model predictions to assign labels and makes it robust to even coarse pseudo boxes; while PCV leverages the regression consistency of positive proposals to reflect the localization quality of pseudo boxes. Furthermore, in consistency training, we propose Multi-view Scale-invariant Learning (MSL) that includes mechanisms of both label-and feature-level consistency, where feature consistency is achieved by aligning shifted feature pyramids between two images with identical content but varied scales. On COCO benchmark, our method, termed PSEudo labeling and COnsistency training (PseCo), outperforms the SOTA (Soft Teacher) by 2.0, 1.8, 2.0 points under 1%, 5%, and 10% labelling ratios, respectively. It also significantly improves the learning efficiency for SSOD, e.g., PseCo halves the training time of the SOTA approach but achieves even better performance. Code is available at https://github.com/ligang-cs/PseCo.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the rapid development of deep learning, many computer vision tasks achieve significant improvements, such as image classification <ref type="bibr" target="#b1">[2]</ref>, object detection <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b8">9]</ref>,  etc. Behind these advances, plenty of annotated data plays an important role <ref type="bibr" target="#b22">[23]</ref>. However, labeling accurate annotations for large-scale data is usually timeconsuming and expensive, especially for object detection, which requires annotating precise bounding boxes for each instance, besides category labels. Therefore, employing easily accessible unlabeled data to facilitate the model training with limited annotated data is a promising direction, named Semi-Supervised Learning, where labeled data and unlabeled data are combined together as training examples. Semi-Supervised for Image Classification (SSIC) has been widely investigated in previous literature, and the learning paradigm on unlabeled data can be roughly divided into two categories: pseudo labeling <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18]</ref> and consistency training <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b21">22]</ref>, each of which receives much attention. Recently, some works (e.g., FixMatch <ref type="bibr" target="#b18">[19]</ref>, FlexMatch <ref type="bibr" target="#b27">[28]</ref>) attempt to combine these two techniques into one framework and achieve state-of-the-art performance. In Semi-Supervised Object Detection (SSOD), some works borrow the key techniques (e.g. pseudo labeling, consistency training) from SSIC, and directly apply them to SSOD. Although these works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b25">26]</ref> obtain gains from unlabeled data, they neglect some important properties of object detection, resulting in sub-optimal results. On the one hand, compared with image classification, pseudo labels of object detection are more complicated, containing both category and location information. On the other hand, object detection is required to capture stronger scale-invariant ability than image classification, as it needs to carefully deal with the targets with rich scales. In this work, we present a SSOD framework, termed PSEudo labeling and COnsistency training (PseCo), to integrate object detection properties into SSOD, making pseudo labeling and consistency training work better for object detection tasks.</p><p>In pseudo labeling, the model produces one-hot pseudo labels on unlabeled data by itself, and only pseudo labels whose scores are above the predefined score threshold are retained. As for object detection, the pseudo label consists of both category labels and bounding boxes. Although category labels can be guaranteed to be accurate via setting a high score threshold, the localization quality of pseudo box fails to be measured and guaranteed. It has been validated in previous works that the classification score is not strongly correlated with the precision of box localization <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">26]</ref>. In <ref type="figure" target="#fig_1">Fig. 1(a)</ref>, we compute the precision of pseudo boxes under various Intersection-over-Union (IoU) thresholds, via comparing produced pseudo boxes with ground-truths. Under loose criterion (IoU=0.3), precision can reach 81%, but it will drop to 31% when we lift the IoU threshold to 0.9. This dramatic precision gap indicates coarse pseudo boxes whose IoUs belong to [0.3,0.9] account for 50%. If these noisy pseudo boxes are used as targets to train the detector, it must hinder the optimization, resulting in slow convergence and inefficient learning on unlabeled data. Furthermore, we analyze the negative effects brought by noisy pseudo boxes on classification and regression tasks as follows, respectively.</p><p>For the classification task, noisy pseudo boxes will mislead the label assignment, where labels are assigned based on IoUs between proposals and gt boxes (pseudo boxes in our case). As shown in <ref type="figure" target="#fig_1">Fig. 1(c)</ref>, a background proposal is taken as foreground due to a large IoU value with a poorly localized pseudo box. As a result, the IoU-based label assignment will fail on unlabeled data and confuse decision boundaries between foreground and background. To address this issue, we design a prediction-guided label assignment strategy for unlabeled data, which assigns labels based on predictions of the teacher, instead of IoUs with pseudo boxes as before, making it robust for poorly localized pseudo boxes.</p><p>For the regression task, it is necessary to measure the localization quality of pseudo boxes. We propose a simple yet effective method to achieve this, named Positive-proposal Consistency Voting. We empirically find that regression consistency from positive proposals is capable of reflecting the localization quality of corresponding pseudo boxes. In <ref type="figure" target="#fig_1">Fig. 1(b)</ref>, we visualize the relations between predicted consistency and their true IoUs, where their positive correlations can be found. Therefore, it is reasonable to employ the estimated localization quality (i.e., regression consistency from positive proposals) to re-weight the regression losses, making precise pseudo boxes contribute more to regression supervisions.</p><p>Apart from pseudo labeling, we also analyze the consistency training for SSOD. Consistency training enforces the model to generate similar predictions when fed with perturbed versions of the same image, where perturbations can be implemented by injecting various data augmentations. Through consistency training, models can be invariant to different input transformations. Current SSOD methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b13">14]</ref> only apply off-the-shelf, general data augmentations, most of which are borrowed from image classification. However, different from classification, object detection is an instance-based task, where object scales usually vary in a large range, and detectors are expected to handle all scale ranges. Therefore, learning strong scale-invariant ability via consistency training is important. In scale consistency, it should be allowed for the model to predict the same boxes for input images with identical contents but varied scales. To ensure label consistency, random-resizing is a common augmentation, which resizes input images and gt boxes according to a randomly generated resize ratio. Be-sides label consistency, feature consistency also plays an important role in scaleinvariant learning, but it is neglected in previous works. Thanks to the pyramid structure of popular backbone networks, feature alignment can be easily implemented by shifting feature pyramid levels according to the scale changes. Motivated by this, we introduce a brand new data augmentation technique, named Multi-view Scale-invariant Learning (MSL), to learn label-level and feature-level consistency simultaneously in a simple framework.</p><p>In summary, we delve into two key techniques of semi-supervised learning (e.g., pseudo labeling and consistency training) for SSOD, and integrate object detection properties into them. On COCO benchmarks, our PseCo outperforms the state-of-the-art methods by a large margin, for example, under 10% labelling ratio, it can improve a 26.9% mAP baseline to 36.1% mAP, surpassing previous methods by at least 2.0%. When labeled data is abundant, i.e., we use full COCO training set as labeled data and extra 123K unlabeled2017 as unlabeled data, our PseCo improves the 41.0% mAP baseline by +5.1%, reaching 46.1% mAP, establishing a new state of the art. Moreover, PseCo also significantly boosts the convergence speed, e.g. PseCo halves the training time of the SOTA (Soft Teacher <ref type="bibr" target="#b25">[26]</ref>), but achieves even better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Semi-supervised learning in image classification. Semi-supervised learning can be categorized into two groups: pseudo labeling (also called self-training) and consistency training, and previous methods design learning paradigms based on one of them. Pseudo labeling <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b24">25]</ref> iteratively adds unlabeled data into the training procedure with pseudo labels annotated by an initially trained network. Here, only model predictions with high confidence will be transformed into the one-hot format and become pseudo labels. Noisy Student Training <ref type="bibr" target="#b24">[25]</ref> injects noise into unlabeled data training, which equips the model with stronger generalization through training on the combination of labeled and unlabeled data. On the other hand, consistency training <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b0">1]</ref> relies on the assumption that the model should be invariant to small changes on input images or model hidden states. It enforces the model to make similar predictions on the perturbed versions of the same image, and perturbations can be implemented by injecting noise into images and hidden states. UDA <ref type="bibr" target="#b23">[24]</ref> validates the advanced data augmentations play a crucial role in consistency training, and observes the strong augmentations found in supervised-learning can also lead to obvious improvements in semi-supervised learning.</p><p>Recently, some works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28]</ref> attempt to combine pseudo labeling and consistency training, achieving state-of-the-art performance. FixMatch <ref type="bibr" target="#b18">[19]</ref> firstly applies the weak and strong augmentations to the same input image, respectively, to generate two versions, then uses the weakly-augmented version to generate hard pseudo labels. The model is trained on strongly-augmented versions to align predictions with pseudo labels. Based on FixMatch, FlexMatch <ref type="bibr" target="#b27">[28]</ref> proposes to adjust score thresholds for different classes during the generation of pseudo labels, based on curriculum learning. It has been widely validated that pseudo labeling and consistency training are two powerful techniques in semi-supervised image classification, hence in this work, we attempt to integrate object detection properties into them and make them work better for semi-supervised object detection. Semi-supervised learning in object detection. STAC <ref type="bibr" target="#b19">[20]</ref> is the first attempt to apply pseudo labeling and consistency training based on the strong data augmentations to semi-supervised object detection, however, it adopts two stages of training as Noisy Student Training <ref type="bibr" target="#b24">[25]</ref>, which prevents the pseudo labels from updating along with model training and limits the performance. After STAC, <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b13">14]</ref> borrow the idea of Exponential Moving Average (EMA) from Mean Teacher <ref type="bibr" target="#b21">[22]</ref>, and update the teacher model after each training iteration to generate instant pseudo labels, realizing the end-to-end framework. To pursue high quality of pseudo labels and overcome confirmation bias, Instant-Teaching <ref type="bibr" target="#b29">[30]</ref> and ISMT <ref type="bibr" target="#b26">[27]</ref> introduce model ensemble to aggregate predictions from multiple teacher models which are initialized differently; similarly, Humble Teacher <ref type="bibr" target="#b20">[21]</ref> ensembles the teacher model predictions by taking both the image and its horizontally flipped version as input. Although these ensemble methods can promote the quality of pseudo labels, they also introduce considerable computation overhead. Unbiased Teacher <ref type="bibr" target="#b13">[14]</ref> replaces traditional Cross-entropy loss with Focal loss <ref type="bibr" target="#b11">[12]</ref> to alleviate the class-imbalanced pseudo-labeling issue, which shows strong performance when labeled data is scarce. Soft Teacher <ref type="bibr" target="#b25">[26]</ref> uses teacher classification scores as classification loss weights, to suppress negative effects from underlying objects missed by pseudo labels. Different from previous methods, our work elaborately analyzes whether the pseudo labeling and consistency training can be directly applied to SSOD, but gets a negative answer. To integrate object detection properties into these two techniques, we introduce Noisy Pseudo box Learning and Multi-view Scale-invariant Learning, obtaining much better performance and faster convergence speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We show the framework of our PseCo in <ref type="figure">Fig. 2</ref>. On the unlabeled data, PseCo consists of Noisy Pseudo box Learning (NPL) and Multi-view Scale-invariant Learning (MSL). In the following parts, we will introduce the basic framework, the proposed NPL and MSL, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The basic framework</head><p>At first, we directly apply standard pseudo labeling and consistency training to SSOD, building our basic framework. Following previous works <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b29">30]</ref>, we also adopt Teacher-student training scheme, where the teacher model is built from the student model at every training iteration via Exponential Moving Average (EMA). We randomly sample labeled data and unlabeled data based on a sample </p><formula xml:id="formula_0">L l = L l cls + L l reg .<label>(1)</label></formula><p>On the unlabeled data, we firstly apply weak data augmentations (e.g. horizontal flip, random resizing) to input images, and then feed them to the teacher model for pseudo label generation. Considering the detection boxes tend to be dense even after NMS, we set a score threshold ? and only retain boxes with scores above ? as pseudo labels. After that, strong augmentations (e.g. cutout, rotation, brightness jitter) 3 will be performed on the input image to generate the training example for student model. Since high classification scores do not lead to precise localization, we abandon bounding box regression on unlabeled data, as done in <ref type="bibr" target="#b13">[14]</ref>. Actually, applying the box regression loss on unlabeled data will cause unstable training in our experiments. Foreground-background imbalance <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12]</ref> is an intrinsic issue in object detection, and it gets worse under the semi-supervised setting. A high score threshold ? is usually adopted to guarantee the precision of pseudo labels, but it also results in scarcity of pseudo labels, aggravating the imbalance of foreground/background. Moreover, there also exists foreground-foreground imbalance, exactly, training examples from some specific categories can be limited when labeled data is scarce, which makes the model prone to predict the dominant classes, causing biased prediction. To alleviate these imbalance issues, we follow the practice of Unbiased Teacher <ref type="bibr" target="#b13">[14]</ref>, and replace the standard crossentropy loss with focal loss <ref type="bibr" target="#b11">[12]</ref>:</p><formula xml:id="formula_1">L u cls = ?? t (1 ? p t ) ? log(p t ), p t = p, if y = 1, 1 ? p, otherwise,<label>(2)</label></formula><p>where parameters ? t and ? adopt default settings in original focal loss paper <ref type="bibr" target="#b11">[12]</ref>. The overall loss function is formulated as:</p><formula xml:id="formula_2">L = L l + ?L u ,<label>(3)</label></formula><p>where ? is used to control the contribution of unlabeled data. In theory, our proposed method is independent of the detection framework and can be applied on both one-stage and two-stage detectors. However, considering all previous methods are based on Faster R-CNN <ref type="bibr" target="#b16">[17]</ref> detection framework, for a fair comparison with them, we also adopt Faster R-CNN as the default detection framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Noisy Pseudo Box Learning</head><p>In SSOD, pseudo labels contain both category and location. Since the score of pseudo labels can only indicate the confidence of pseudo box categories, the localization quality of pseudo boxes is not guaranteed. Imprecise pseudo boxes will mislead the label assignment and regression task, making learning on unlabeled data inefficient. Motivated by this, we introduce Prediction-guided Label Assignment and Positive-proposal Consistency Voting to reduce negative effects on the label assignment and regression task, respectively. Prediction-guided Label Assignment. The standard label assignment strategy in Faster R-CNN <ref type="bibr" target="#b16">[17]</ref> only takes the IoUs between proposals and gt boxes (pseudo boxes in our case) into consideration and assigns foreground to those proposals, whose IoUs are above a pre-defined threshold t (0.5 as default). This strategy relies on the assumption that gt boxes are precise, however, this assumption does not hold for unlabeled data obviously. As a result, some low-quality proposals will be mistakenly assigned as positive, confusing the classification boundaries between foreground and background. One specific example is shown in <ref type="figure" target="#fig_1">Fig. 1(c)</ref>, where a proposal with the true IoU as 0.39 is mistakenly assigned as positive.</p><p>To address this problem, we propose Prediction-guided Label Assignment (PLA), which takes teacher predictions as auxiliary information and reduces dependency on IoUs. In Teacher-student training scheme, not only can the detection results (after NMS) of teacher perform as pseudo labels, but also teacher's dense predictions (before NMS) are able to provide guidance for student model training. We share the proposals generated by the teacher RPN with the student, so that teacher predictions on these proposals can be easily transferred to student. To measure the proposal quality (q) comprehensively, the classification confidence and localization precision of teacher predictions are jointly employed, concretely, q = s ? ? u 1?? , where s and u denote a foreground score and an IoU value between the regressed box and the ground truth, respectively. ? controls the contribution of s and u in the overall quality. On unlabeled data, we first construct a candidate bag for each ground truth g by the traditional IoU-based strategy, where the IoU threshold t is set to a relatively low value, e.g., 0.4 as default, to contain more proposals. Within each candidate bag, the proposals are firstly sorted by their quality q, then top-N proposals are adopted as positive samples and the rest are negatives. The number N is decided by the dynamic k estimation strategy proposed in OTA <ref type="bibr" target="#b2">[3]</ref>, specifically, the IoU values over the candidate bag is summed up to represent the number of positive samples. The proposed PLA gets rid of strong dependencies on IoUs and alleviates negative effects from poorly localized pseudo boxes, leading to clearer classification boundaries. Furthermore, our label assign strategy integrates more teacher knowledge into student model training, realizing better knowledge distillation. Positive-proposal Consistency Voting. Considering the classification score fails to indicate localization quality, we introduce a simple yet effective method to measure the localization quality, named Positive-proposal Consistency Voting (PCV). Assigning multiple proposals to each gt box (or pseudo box) is a common practice in CNN-based detectors <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b28">29]</ref>, and we observe that the consistency of regression results from these proposals is capable of reflecting the localization quality of the corresponding pseudo box. Regression consistency ? j for pseudo box (indexed by j) is formulated as:</p><formula xml:id="formula_3">? j = N i=1 u j i N ,<label>(4)</label></formula><p>where u denotes an IoU value between the predicted box and the pseudo box, as defined above; N denotes the number of positive proposals, assigned to the pseudo box j. After obtaining ? j , we employ it as the instance-wise regression loss weight:</p><formula xml:id="formula_4">L u reg = 1 M N M j=1 ? j N i=1 |reg j i ?r eg j i |,<label>(5)</label></formula><p>where reg andr eg refer to the regression output and ground-truth, respectively. In <ref type="figure" target="#fig_1">Fig. 1(b)</ref>, we depict the scatter diagram of the relation between prediction consistency ? of pseudo boxes and their true IoUs. It is obvious that ? is positively correlated with true IoUs. Note that, some dots falling in the orange ellipse are mainly caused by annotation errors. We visualize some examples in <ref type="figure" target="#fig_6">Fig. 5</ref>, where the pseudo boxes accurately detect some objects, which are missed by the ground truths.  play a crucial role <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> in achieving competitive performance. Through injecting the perturbations into the input images, data augmentations equip the model with robustness to various transformations. From the perspective of scale invariance, we regard the common data augmentation strategy (e.g. random-resizing) as label-level consistency since it resizes the label according to the scale changes of input images. Unfortunately, existing works only involve the widely adopted label-level consistency but fail to consider the feature-level one. Since detection network usually has designs of rich feature pyramids, feature-level consistency is easy to implement across paired inputs <ref type="bibr" target="#b15">[16]</ref> and should be considered seriously. In this paper, we propose Multi-view Scale-invariant Learning (MSL) that combines both label-and feature-level consistency into a simple framework, where feature-level consistency is realized by aligning shifted pyramid features between two images with identical content but different scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-view Scale-invariant Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature alignment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-view Scale-Invariant Learning</head><p>To be specific, two views, namely V 1 and V 2 , are used for student training in MSL. We denote the input image for the teacher model as V 0 . Views V 1 and V 2 are constructed to learn label-and feature-level consistency, respectively. Among them, V 1 is implemented by vanilla random resizing, which rescales the input V 0 and pseudo boxes according to a resize ratio ? randomly sampled from the range [? min , ? max ] ([0.8, 1.3] as default). For feature consistency learning, we firstly downsample V 1 by even number times (2x as default) to produce V 2 , then combine V 1 and V 2 into image pairs. Upsampling is also certainly permitted, but we only perform downsampling here for GPU memory restriction. Because the spatial sizes of adjacent FPN layers always differ by 2x, the P3-P7 layers 4 of V 1 can align well with P2-P6 layers of V 2 in the spatial dimension. Through feature alignment, the same pseudo boxes can supervise the student model training on both V 1 and V 2 . Integrating label consistency and feature consistency into consistency learning leads to stronger scale-invariant learning and significantly accelerates model convergence, as we will show later in the experiments. Comparisons between label consistency and feature consistency are shown in <ref type="figure" target="#fig_3">Fig. 3</ref>.</p><p>Learning scale-invariant representation from unlabeled data is also explored by SoCo <ref type="bibr" target="#b22">[23]</ref>. However, we claim there are two intrinsic differences between MSL and SoCo: (1) MSL models scale invariance from both label consistency and image feature consistency, while SoCo only considers object feature consistency. Through aligning dense image features of shifted pyramids between paired images, our MSL can provide more comprehensive and dense supervisory signals than the SoCo, which only performs consistency on sparse objects. (2) SoCo implements feature consistency via contrastive learning, which is designed for the pretraining; in contrast, our MSL uses bounding box supervision to implement consistency learning and can be integrated into the detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Evaluation Protocol</head><p>In this section, we conduct extensive experiments to verify the effectiveness of PseCo on MS COCO benchmark <ref type="bibr" target="#b12">[13]</ref>. There are two training sets, namely the train2017 set, containing 118k labeled images, and the unlabeled2017 set, containing 123k unlabeled images. The val2017 with 5k images is used as validation set, and we report all experiment results on val2017. The performance is measured by COCO average prevision (denoted as mAP). Following the common practice of SSOD <ref type="bibr" target="#b19">[20]</ref>, there are two experimental settings: Partially Labeled Data and Fully Labeled Data, which are described as follows: Partially Labeled Data. We randomly sample 1, 2, 5, and 10% data from train2017 as labeled data, and use the rest as unlabeled. Under each labelling ratio, we report the mean and standard deviation over 5 different data folds. Fully Labeled Data. Under this setting, we take train2017 as the training labeled set and unlabeled2017 as the training unlabeled set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>For a fair comparison, we adopt Faster R-CNN <ref type="bibr" target="#b16">[17]</ref> with FPN <ref type="bibr" target="#b10">[11]</ref> as the detection framework, and ResNet-50 <ref type="bibr" target="#b4">[5]</ref> as the backbone. The confidence threshold ? is set to 0.5, empirically. We set ? as 4.0 to control contributions of unlabeled data in the overall losses. The performance is evaluated on the Teacher model. Training details for Partially Labeled Data and Fully Labeled Data are described below: Partially Labeled Data. All models are trained for 180k iterations on 8 GPUs. The initial learning rate is set as 0.01 and divided by 10 at 120k and 160k iterations. The training batch in each GPU includes 5 images, where the sample ratio between unlabeled data and labeled data is set to 4:1. Fully Labeled Data. All models are trained for 720k iterations on 8 GPUs. Mini-batch in each GPU is 8 with the sample ratio between unlabeled and labeled data as 1:1. The learning rate is initialized to 0.01 and divided by 10 at 480k and 680k iterations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with State-of-the-Art Methods</head><p>We compare the proposed PseCo with other state-of-the-art methods on COCO val2017 set. Comparisons under the Partially Labeled Data setting are first conducted, with results reported in Tab. 1. When labeled data is scarce (i.e., under 1% and 2% labelling ratios), our method surpasses the state-of-the-art method, Unbiased Teacher <ref type="bibr" target="#b13">[14]</ref>, by 1.7% and 3.5%, reaching 22.4 and 27.8 mAP, respectively. When more labeled data is accessible, the SOTA method is transferred to Soft Teacher <ref type="bibr" target="#b25">[26]</ref>. Our method still outperforms it by 1.8% and 2.0% under 5% and 10% labelling ratios, respectively. Therefore, the proposed method outperforms the SOTAs by a large margin, at least 1.7%, under all labelling ratios. Compared with the supervised baseline, PseCo obtains even better performance with only 2% labeled data than the baseline with 10% labeled data, demonstrating the effectiveness of proposed semi-supervised learning techniques. Moreover, we also compare the convergence speed with the previous best method (Soft Teacher <ref type="bibr" target="#b25">[26]</ref>) in <ref type="figure">Fig. 4</ref>, where convergence curves are depicted under 10% and 5% labelling ratios. It is obvious that our method has a faster convergence speed, specifically, our method uses only 2/5 and 1/4 iterations of Soft Teacher to achieve the same performance under 10% and 5% labelling ratios respectively. Although we employ an extra view (V 2 ) to learn featurelevel consistency, it only increases the training time of each iteration by 25% (from 0.72 sec/iter to 0.91 sec/iter), due to the low input resolution of V 2 . In summary, we halve the training time of SOTA approach but achieve even better performance, which validates the superior learning efficiency of our method on unlabeled data.</p><p>The experimental results under the Fully Labeled Data setting are reported in Tab. 1, where both results of comparison methods and their supervised baseline are listed. Following the practice in Soft Teacher <ref type="bibr" target="#b25">[26]</ref>, we also apply weak augmentations to the labeled data and obtain a strong supervised baseline, 41.0 mAP. Although with a such strong baseline, PseCo still achieves larger improvements (+5.1%) than others and reaches 46.1 mAP, building a new state of the art. Some qualitative results are shown in <ref type="figure" target="#fig_6">Fig. 5</ref>.  <ref type="bibr" target="#b25">[26]</ref>. Here, we reproduce Soft Teacher using their source codes. (c) depicts the comparison between V 1 and V 1 &amp;V 2 . In legend, the numbers in brackets refer to mAP. Performance is evaluated on the teacher. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>We conduct detailed ablation studies to verify key designs. All ablation studies are conducted on a single data fold from the 10% labelling ratio.   localization quality <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14]</ref>. Naive confidence thresholding will introduce some coarse bounding boxes for regression tasks. To alleviate this issue, Unbiased Teacher <ref type="bibr" target="#b13">[14]</ref> abandons regression losses on unlabeled data (denoted as "abandon reg"); Humble Teacher <ref type="bibr" target="#b20">[21]</ref> aligns the regression predictions between the teacher and student on selected top-N proposals (dubbed "reg consistency"); Soft Teacher <ref type="bibr" target="#b25">[26]</ref> introduces the box jittering to calculate prediction variance on jittered pseudo boxes, which is used to filter out poorly localized pseudo boxes. In Tab. 4a, we compare our Positive-proposal Consistency Voting (PCV) with these methods. PCV obtains the best performance, concretely, on AP 75 , PCV surpasses two competitors, reg consistency and box jittering, by 0.9% and 0.5%, respectively. Although both PCV and box jittering <ref type="bibr" target="#b25">[26]</ref> rely on prediction variance, there exist great differences. Firstly, PCV produces localization quality by intrinsic proposals, thus it avoids extra network forward on jittered boxes, enjoying higher training efficiency. Moreover, unlike the box jittering, which meticulously tunes the variance threshold, PCV is free of hyper-parameters. Study on different hyper-parameters of PLA. We first investigate the performance using different ? in PLA, which balances the influence of classification score (s) and localization precision (u) in the proposal quality. Through a coarse search shown in Tab. 4b, we find that combining s and u yields better performance than using them individually. We then carry out experiments to study the robustness of the IoU threshold t, which is used to build the candidate bag. From the Tab 4c, using lower t to construct a bigger candidate bag is preferred. Analysis of Multi-view Scale-invariant Learning. We propose the MSL to model scale invariance from the aspects of both label-and feature-level consistency. The studies on them are reported in Tab. 3. At first, we construct a single-scale training baseline without scale variance, where the input images for the teacher and student are kept on the same scale. It obtains 32.7 mAP. Next,  we apply the different scale jitter on the teacher and student to implement labellevel consistency, which surpasses the single-scale training by 1.2 mAP. Based on the label consistency, we further introduce the view V 2 to perform feature consistency learning. It obtains +1.0% improvements, reaching 34.9 mAP. Apart from performance gains, the feature consistency can also significantly boost the convergence speed as depicted in <ref type="figure">Fig. 4(c)</ref>. To validate the improvements introduced by the V 2 come from comprehensive scale-invariant learning, instead of vanilla multi-view training, we also add an extra view V ? 2 besides the V 1 , where V ? 2 is downsampled from V 1 by 2x and performs label consistency as V 1 . From the Tab. 3b, vanilla multi-view training with only label consistency hardly brings improvements against the single V 1 (33.9 vs 33.9%). Effect of Focal Loss. In Tab. 5, we compare the Cross Entropy (CE) Loss and Focal Loss. Thanks to the Focal Loss, an improvement of +0.6 mAP is achieved against the CE Loss. On the other hand, even with the CE Loss, our PseCo still surpasses the Soft Teacher by a large margin, i.e., 1.7 mAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we elaborately analyze two key techniques of semi-supervised object detection (e.g. pseudo labeling and consistency training), and observe these two techniques currently neglect some important properties of object detection. Motivated by this, we propose a new SSOD framework, PseCo, to integrate object detection properties into SSOD. PseCo consists of Noisy Pseudo box Learning (NPL) and Multi-view Scale-invariant Learning (MSL). In NPL, predictionguided label assignment and positive-proposal consistency voting are proposed to perform the robust label assignment and regression task using noisy pseudo boxes, respectively. Based on the common label-level consistency, MSL additionally designs a novel feature-level scale-invariant learning, which is neglected in prior works. To validate the effectiveness of our method, extensive experiments are conducted on COCO benchmark. Experimental results validate PseCo surpasses the SOTAs by a large margin both in accuracy and efficiency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>?</head><label></label><figDesc>Corresponding author. arXiv:2203.16317v2 [cs.CV] 20 Jul 2022 (a) Precision of pseudo boxes Relations between real quality and prediction consistency (c) Wrong label results brought by the Noisy Pseudo Box</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>(a) The precision of pseudo boxes under various IoU thresholds. (b) The scatter diagram of the relation between the prediction consistency and their true localization quality. Some dots falling in the orange ellipse are caused by annotation errors. We show some examples inFig. 5. (c) One specific example to demonstrate that noisy pseudo boxes will mislead label assignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Different from image classification, in object detection, object scales vary in a large range and detectors hardly show comparable performance on all scales. Therefore, learning scale-invariant representations from unlabeled data is considerably important for SSOD. In consistency training, strong data augmentations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Comparisons between label-level consistency learning and feature-level consistency learning. For label consistency, labels are aligned according to the resize ratio ?; for feature consistency, features are aligned by shifting the feature pyramid level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>effects of view 2 Fig. 4 :</head><label>24</label><figDesc>Comparison of model convergence speed. In (a) and (b), we compare PseCo against Soft Teacher</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( b )</head><label>b</label><figDesc>Detection results of supervised baseline (c) Detection results of our method (a) Pseudo boxes produced by the teacher model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>(a) Some pseudo boxes (in yellow) detect objects, missed by ground-truths (in red). Numbers above the pseudo box refer to the predicted consistency ?. (b)(c) are the results of the supervised baseline and our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Fig. 2: The framework of our PseCo. Each training batch consists of both labeled and unlabeled images. On the unlabeled images, the student model trains on view V 1 and V 2 at the same time, taking the same pseudo boxes as supervisions. View V 0 refers to input images for the teacher model.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Labeled Data</cell><cell></cell><cell></cell><cell>Unlabeled Data</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Pseudo label:</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>View 0</cell><cell></cell><cell>(x1,y1,x2,y2,label,?)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GT Boxes</cell><cell></cell><cell cols="2">inference</cell><cell>Teacher Model</cell><cell>Score Filter</cell><cell>Consistency Voting</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Student Model</cell><cell></cell><cell>Random</cell><cell>EMA Update</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Resize</cell><cell>Student Model</cell><cell>Resize Box</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>???</cell><cell>R-CNN Head</cell><cell>Pred</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>P3 P4</cell><cell>P7</cell></row><row><cell>View 0</cell><cell cols="3">View 1</cell><cell>:</cell><cell>Label Consistency</cell><cell></cell><cell>View 1</cell></row><row><cell>View 1</cell><cell cols="3">View 2</cell><cell>:</cell><cell>Feature Consistency</cell><cell>2x</cell><cell></cell><cell>???</cell><cell>R-CNN Head</cell><cell>Pred</cell></row><row><cell></cell><cell>?</cell><cell>:</cell><cell cols="3">Regression Consistency</cell><cell>Downsample</cell><cell>View 2</cell><cell>P2 P3 Backbone w/ FPN P6</cell></row><row><cell></cell><cell></cell><cell cols="4">: Supervision</cell><cell cols="3">Multi-view Scale-invariant Learning</cell><cell>box Learning Noisy Pseudo</cell></row></table><note>ratio to form the training batch. On the labeled data, the student model is trained in a regular manner, supervised by the ground-truth boxes:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparisons with the state-of-the-art methods on val2017 set under the Partially Labeled Data and Fully Labeled Data settings. Humble Teacher [21] 16.96?0.35 21.74?0.24 27.70?0.15 31.61?0.28 37.6</figDesc><table><row><cell>Method</cell><cell>1%</cell><cell cols="2">Partially Labeled Data 2% 5%</cell><cell>10%</cell><cell>Fully Labeled Data</cell></row><row><cell cols="5">Supervised baseline 12.20?0.29 16.53?0.12 21.17?0.17 26.90?0.08</cell><cell>41.0</cell></row><row><cell>STAC [20]</cell><cell cols="5">13.97?0.35 18.25?0.25 24.38?0.12 28.64?0.21 39.5</cell><cell>?0.3 ? ?? ? 39.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+4.8 ? ?? ? 42.4</cell></row><row><cell>ISMT [27]</cell><cell cols="5">18.88?0.74 22.43?0.56 26.37?0.24 30.53?0.52 37.8</cell><cell>+1.8 ? ?? ? 39.6</cell></row><row><cell cols="6">Instant-Teaching [30] 18.05?0.15 22.45?0.15 26.75?0.05 30.40?0.05 37.6</cell><cell>+2.6 ? ?? ? 40.2</cell></row><row><cell cols="6">Unbiased Teacher [14] 20.75?0.12 24.30?0.07 28.27?0.11 31.50?0.10 40.2</cell><cell>+1.1 ? ?? ? 41.3</cell></row><row><cell>Soft Teacher [26]</cell><cell>20.46?0.39</cell><cell>-</cell><cell cols="3">30.74?0.08 34.04?0.14 40.9</cell><cell>+3.6 ? ?? ? 44.5</cell></row><row><cell>PseCo (ours)</cell><cell cols="5">22.43?0.36 27.77?0.18 32.50?0.08 36.06?0.24 41.0</cell><cell>+5.1 ? ?? ? 46.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation studies on each component of our method. MSL represents Multi-view Scale-invariant Learning; NPL represents Noisy Pseudo box Learning. In MSL, V 1 and V 2 are constructed for label-and feature-level consistency, respectively. In NPL, PCV and PLA stand for Positive-proposal Consistency Voting and Prediction-guided Label Assignment, respectively.</figDesc><table><row><cell>V1</cell><cell>MSL</cell><cell>V2</cell><cell>PCV</cell><cell>NPL</cell><cell>PLA</cell><cell>mAP</cell><cell>AP50</cell><cell>AP75</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>26.8</cell><cell>44.9</cell><cell>28.4</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>33.9(+7.1)</cell><cell>55.2</cell><cell>36.0</cell></row><row><cell>?</cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>34.9(+8.1)</cell><cell>56.3</cell><cell>37.1</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell>34.8(+8.0)</cell><cell>55.1</cell><cell>37.4</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell>?</cell><cell>35.7(+8.9)</cell><cell>56.4</cell><cell>38.4</cell></row><row><cell>?</cell><cell></cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell>36.0(+9.2)</cell><cell>56.9</cell><cell>38.7</cell></row><row><cell>?</cell><cell></cell><cell>?</cell><cell>?</cell><cell></cell><cell>?</cell><cell>36.3(+9.5)</cell><cell>57.2</cell><cell>39.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Effect of individual component. In Tab. 2, we show effectiveness of each component step by step. When only using 10% labeled data as training examples, it obtains 26.8 mAP.Next, we construct the semi-supervised baseline by applying V 1 on unlabeled data for label-level consistency learning. The baseline does not consider any adverse effects incurred by coarse pseudo boxes and obtains 33.9 mAP. Furthermore, by leveraging additional view V 2 , the feature-level scaleinvariant learning is enabled, and an improvement of +1.0 mAP is found. On the other hand, to alleviate the issue of coarse pseudo boxes, we introduce PCV to suppress the inaccurate regression signals, improving the baseline from 33.9 to 34.8 mAP. After that, we replace the traditional IoU-based label assignment strategy with the PLA and enjoy another +0.9 mAP gain. Finally, when combing MSL and NPL together, it achieves the best performance, 36.3 mAP. Comparison with other regression methods. Scores of pseudo boxes can only indicate the confidence of predicted object category, thus they fail to reflect</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Analysis of Multi-view Scale-invariant learning, which contains both the label-and feature-level consistency.</figDesc><table><row><cell cols="2">(a) Study on label consistency.</cell><cell cols="2">(b) Study on feature consistency.</cell></row><row><cell>method</cell><cell>mAP APS APM APL</cell><cell>method</cell><cell>mAP APS APM APL</cell></row><row><cell cols="2">single-scale training 32.7 19.0 36.0 42.5</cell><cell cols="2">vanilla multi-view training 33.9 20.9 37.2 43.0</cell></row><row><cell cols="2">label consistency 33.9 19.1 37.2 44.4</cell><cell>feature consistency</cell><cell>34.9 22.1 38.2 43.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies related to Positive-proposal Consistency Voting (PCV) and Prediction-guided Label Assignment (PLA).</figDesc><table><row><cell cols="2">(a) Comparison between our PCV</cell><cell>(b) Study on hyper-</cell><cell>(c) Study on IoU</cell></row><row><cell cols="2">and other regression methods.</cell><cell>parameter ?.</cell><cell>threshold t.</cell></row><row><cell>method abandon reg [14]</cell><cell>mAP AP50 AP75 33.9 55.2 36.0</cell><cell>? mAP AP50 AP75</cell><cell>t mAP AP50 AP75</cell></row><row><cell cols="2">reg consistency [21] 34.2 55.1 36.5</cell><cell>0 35.2 56.1 37.8</cell><cell>0.3 35.7 56.2 38.6</cell></row><row><cell>box jittering [26]</cell><cell>34.5 54.9 36.9</cell><cell>0.5 35.7 56.4 38.4</cell><cell>0.4 35.7 56.4 38.4</cell></row><row><cell>PCV (ours)</cell><cell>34.8 55.1 37.4</cell><cell>1.0 35.4 55.7 38.4</cell><cell>0.5 35.5 56.1 38.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on Focal Loss. method mAP AP 50 AP 75 PseCo w/ CE Loss 35.7 55.6 38.9 PseCo w/ Focal Loss 36.3 57.2 39.2</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We adopt the same data augmentations as Soft Teacher<ref type="bibr" target="#b25">[26]</ref>, please refer to<ref type="bibr" target="#b25">[26]</ref> for more augmentation details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Px refers to the FPN layer whose feature maps are downsampled by 2 x times.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ota: Optimal transport assignment for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yoshie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="303" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Acquisition of localization confidence for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="784" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on challenges in representation learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">896</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gradient harmonized single-stage detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8577" to="8584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Knowledge distillation for object detection via rank mimicking and prediction-guided feature imitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1306" to="1313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generalized focal loss: Towards efficient representation learning for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unbiased teacher for semi-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09480</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-scale aligned distillation for low-resolution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14443" to="14453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Probability of error of some adaptive pattern-recognition machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Scudder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="363" to="371" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fixmatch: Simplifying semi-supervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="596" to="608" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A simple semi-supervised learning framework for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04757</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Humble teachers teach better students for semi-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3132" to="3141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Aligning pretraining for detection via object-level contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6256" to="6268" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Endto-end semi-supervised object detection with soft teacher</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3060" to="3069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Interactive self-training with mean teachers for semi-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5941" to="5950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okumura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shinozaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Varifocalnet: An iou-aware dense object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dayoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sunderhauf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8514" to="8523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Instant-teaching: An end-to-end semi-supervised object detection framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4081" to="4090" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
