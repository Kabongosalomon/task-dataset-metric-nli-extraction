<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An animated picture says at least a thousand words: Selecting Gif-based Replies in Multimodal Dialog</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyao</forename><surname>Wang</surname></persName>
							<email>xingyaow@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jurgens</surname></persName>
							<email>jurgens@umich.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">An animated picture says at least a thousand words: Selecting Gif-based Replies in Multimodal Dialog</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Online conversations include more than just text. Increasingly, image-based responses such as memes and animated gifs serve as culturally recognized and often humorous responses in conversation. However, while NLP has broadened to multimodal models, conversational dialog systems have largely focused only on generating text replies. Here, we introduce a new dataset of 1.56M text-gif conversation turns and introduce a new multimodal conversational model PEPE THE KING PRAWN for selecting gif-based replies. We demonstrate that our model produces relevant and high-quality gif responses and, in a large randomized control trial of multiple models replying to real users, we show that our model replies with gifs that are significantly better received by the community.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Conversations are central to many online social platforms. While most conversations are textbased, computer mediated dialog also affords alternative forms of communication, such as emoji or stickers like bitmoji, that allow users to express themselves <ref type="bibr" target="#b4">(Tang and Hew, 2019;</ref><ref type="bibr">Konrad et al., 2020)</ref>. Increasingly, these visual forms of communication have become common in social media <ref type="bibr">(Bourlai and Herring, 2014;</ref><ref type="bibr">Highfield and Leaver, 2016)</ref>, with a notable use of the reaction gif <ref type="bibr">(Bakhshi et al., 2016;</ref><ref type="bibr">Miltner and Highfield, 2017)</ref>. These gifs are short video sequences that depict a particular scene and sometimes contain text that acts as a meta-commentary <ref type="bibr">(Eppink, 2014)</ref>. As a result, conversations become multimodal where individuals reply to one another using combinations of text and gifs <ref type="figure">(Figure 1</ref>). While conversational AI systems have been developed in a purely textbased setting, such systems do not capture the full multimodal behavior seen online. Here, we study multimodal conversation by introducing new dialog models for selecting gif replies in conversation.</p><p>PizzaMagic: Ahhhhh!!! The EMNLP deadline is in 24 hours!! CasualModel: <ref type="figure">Figure 1</ref>: Gif responses in conversation like the one shown above are embodied dialog that use visual imagery to convey reactions and emotions. This paper develops a system to select the appropriate gif response to messages. (PDF best viewed with Adobe Acrobat) Conversation analysis is central to NLP and multiple approaches have analyzed this dialog structure <ref type="bibr">(Jurafsky et al., 1998;</ref><ref type="bibr">Pareti and Lando, 2018;</ref><ref type="bibr">Cohn et al., 2019)</ref> and developed conversational agents to engage with people (e.g., <ref type="bibr">Fang et al., 2018;</ref><ref type="bibr" target="#b10">Xu et al., 2020;</ref><ref type="bibr">Hong et al., 2020)</ref>. Recent work has focused on generating open domain social chatbots that engage in sustained conversations in a natural way <ref type="bibr">(Ram et al., 2018)</ref>. Because many of these systems are designed to support voicebased dialog, they overlook non-textual forms of interaction used in social media conversations. In parallel, multimodal NLP systems have been developed for image data, often focusing on image-totext tasks such as image captioning <ref type="bibr">(Melas-Kyriazi et al., 2018;</ref><ref type="bibr">Sharma et al., 2018)</ref> and visual question answering <ref type="bibr" target="#b1">(Antol et al., 2015;</ref><ref type="bibr">Huang et al., 2019;</ref><ref type="bibr">Khademi, 2020)</ref>. More recent work has focused on the reverse text-to-image dimension, such as generating an image from a description <ref type="bibr" target="#b10">(Niu et al., 2020;</ref><ref type="bibr">Ramesh et al., 2021)</ref>. Our work unites these two strands of research by integrating imagebased communication into conversational agents.</p><p>Our paper offers three main contributions. First, arXiv:2109.12212v2 [cs.CL] 29 Sep 2021 we propose the new task of selecting gif responses in multimodal conversation analysis and introduce a new dataset of 1,562,701 real-world conversation turns with gif replies. Second, we introduce a new model PEPE THE KING PRAWN that fuses image and text-based features to select a relevant gif response. In in-house experiments, we show that our model substantially outperforms strong baseline models at selecting the exact gif used in real data and, in a manual test of the quality of the best responses, achieves an nDCG of 0.8145 on the annotated test set. Third, in a real-world test, we deploy our model as a part of a large-scale randomized controlled trial and show that the gif replies produced by our model are more highly voted by the community. Data, code, and models are available at https://github.com/xingyaoww/gif-reply.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">GIF Communications</head><p>Gifs have been widely adopted in communication as a natural form of embodied speech where the visual imagery conveys emotions or a reaction as a response <ref type="bibr">(Bakhshi et al., 2016;</ref><ref type="bibr" target="#b6">Tolins and Samermit, 2016)</ref>. These gifs commonly come from widelyknown cultural products, such as movies or television shows, which provides common knowledge for how they could be interpreted <ref type="bibr">(Eppink, 2014;</ref><ref type="bibr">Miltner and Highfield, 2017)</ref>. However, a single gif may have multiple interpretations, depending on the context, cultural knowledge of its content, and the viewer <ref type="bibr">(Jiang et al., 2017)</ref>. As a result, a single gif can serve multiple functions in communication <ref type="bibr" target="#b6">(Tolins and Samermit, 2016)</ref>.</p><p>Gifs have grown in their use through increasing affordances by platforms like Tumblr, Reddit, Imgur, and Twitter that allow gifs to be natively displayed like text in conversation threads <ref type="bibr">(Jiang et al., 2018)</ref>. Further, gif-based keyboards have been introduced that allow users to search for gifs that have been tagged with keywords or other metadata <ref type="bibr">(Griggio et al., 2019</ref>). Yet, these technologies require that gif data be prepared with sufficient tags to be searchable or to have sufficient data to use collaborative filtering techniques for recommendations <ref type="bibr">(Jiang et al., 2018, p.9)</ref>. As a result, there is a clear gap in identifying appropriate response gifs directly from the text, which this work fills.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head><p>Despite the widespread use of gifs, no standard dataset exists for text and gif replies. Further, al-though platforms like Twitter support gif replies, these gifs are not canonicalized to identify which responses correspond to the same gif. Therefore, we construct a new dataset for this task by collecting responses, matching their images, and augmenting this data with metadata about the gif, where possible. A visual description of the whole procedure can be found in Appendix <ref type="figure">Figure 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Gif Response Data</head><p>Gifs have many uses <ref type="bibr">(Miltner and Highfield, 2017)</ref> and so we use a two-step approach to collect data that focus specifically on those likely to be used in conversation. First, gif responses are collected from Twitter by identifying all replies to Englishlanguage tweets containing animated_gif as embedded media. Tweets were collected from a ?10% sample of Twitter from March 13th, 2019 to Jan 24th, 2020, totaling 42,096,566 tweets with a gif that we were able to retrieve. Twitter does not canonicalize its gifs so two separate gif files may actually have the same imagery. Further, these files may not be identical due to small differences such as color variations or aspect ratios. To identify uses of the reference gifs, we use Average Hash from the imagehash library to create low-dimensional representations of each gif where hash distance corresponds to perceptual distance. Since gifs are animated and may contain varying scenes, we compute the hash for the first, middle, and final frames, concatenating these into a single hash. Two gifs are considered the same if (i) they have identical hashes or (ii) their hamming distance is &lt; 10 and gifs with that hash have been used more than 500 times in Twitter. This latter condition was selected after manual evaluation of thresholds to trade-off between increasing the size of the training data and reducing potential noise caused by matching error. A visual example of this process can be found in Appendix <ref type="figure">Figure 8</ref>.</p><p>Not all gif responses in the Twitter data are conversational or appropriate for wider re-use. Therefore, we filter these responses to only those gifs whose imagery matches gifs hosted by the Giphy website, which is the backend for many gif-based keyboards. Giphy contains a wide collection of gifs that are curated to remove content inappropriate for general use (e.g., violent or sexual imagery). Gifs on the platform are categorized (e.g., "reaction" or "celebrities") and we identify 28 categories containing 972 keywords likely to contain gifs used in conversation. A total of 2,095,993 gifs linked to those keywords were ultimately retrieved and stored as image hashes. Additional details of categories and keywords are in Appendix B.</p><p>After the matching image hashes to filter replies, we identify 115,586 unique gifs, referred to as reference gifs, and 1,562,701 tweet replies using one of these gifs, which forms our official dataset. Figure 2 shows these gifs' frequency in the data; much like words, a handful of gifs receive widespread use, while a long tail of gifs are rarely used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Gif Metadata</head><p>We augment our gif data with information about their content. Some gifs have text that transcribes what a person is saying in the gif's scene or is a meta-commentary on the content. This text is extracted using paddleOCR <ref type="bibr">(Du et al., 2020)</ref>. Since some gifs are long enough to contain multiple utterances, we run OCR on four frames sampled from each quartile of the gif's length. Roughly 50% (58,020) of gifs contain at least one extracted word from the selected frames, with an mean of 5.5 extracted words per gif across the dataset.</p><p>Second, some gif repositories like Giphy allow users to tag gifs with information on their content or theme, e.g., "face palm" or "movie." We collect tags for the 115K reference gifs used in Twitter, obtaining 39,651 unique tags. These user-generated tags were moderately noisy due to orthographic variations like spelling, capitalization, and spacing. Therefore, we merge tags by (i) lower-casing the text and (ii) performing a manual merge for similar word forms (e.g., "excited" and "exciting"). To minimize noise, we retain only tags that have been used with at least five gifs and where those gifs have been used at least 1000 times in total; this process removes many low-frequency tags that are either overly-specific or idiosyncratic in their use.</p><p>Finally, we performed a manual inspection of all remaining tags to remove tags that are too general (e.g., "emotion") and retain only noun, adjective, and verb tags (words or multi-word expressions) that describe specific emotions or actions. A total of 241 unique tags were retained (Appendix C). 6.0% of gifs have at least one tag associated with them (mean 1.9 tags). However, these tagged gifs account for 38.7% of the replies in our dataset, suggesting tags are only available for more-popular gifs. Our dataset represents roughly an order of magnitude more data and more tags than the closest related dataset of Chen et al. (2017) that contained 23K gifs with 17 manually-curated emotions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Gif Reply Models</head><p>We introduce a series of models for producing a gif response in conversation. Each model will select a gif from the 115K gifs in our dataset as a response to a text-based message. This task is related to but distinct from work on image-text matching <ref type="bibr">(Lee et al., 2018)</ref>, which aims to find an image describing a piece of text, or text-to-image (e.g., <ref type="bibr" target="#b9">Wen et al., 2015;</ref><ref type="bibr" target="#b11">Xu et al., 2018)</ref>, which generates an image from a text description. Here, we aim to select gifs that reflect natural continuations or reactions to a message in a dialog, akin to how gifs are used in social media. For all models, additional details on the training procedures and hyperparameters are provided in Appendix A. The three models that follow use varying degrees of information about the gifs and text to select a response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Tag-based Predictions</head><p>The first model uses tags as a shared representation for characterizing gifs and text. Analogous to how object tags are used as anchor points for image-text matching <ref type="bibr" target="#b14">(Li et al., 2020)</ref> and pivot languages are used in machine translation <ref type="bibr">(Cheng et al., 2017)</ref>, we use tags to bridge information between the text in a tweet and the visual content of a gif. Here, each gif becomes associated with a set of tags describing its conversational functions and for each text, we predict the set of tags for gifs responses to it-in essence, predicting what types of responses are most appropriate. We describe both of these processes next and how gifs are ultimately selected.</p><p>Estimating Gif Tags Only 6.0% of the gifs in our data have associated tags. Therefore we train a neural model to predict tags using known tags as training data. To capture any changes in emotion or imagery across the gif, we make separate predictions for four frames sampled across the gif (the same used in ?3.2). Each frame is passed through an EfficientNet-based <ref type="bibr" target="#b3">(Tan and Le, 2019)</ref> GIF encoder, shown in <ref type="figure" target="#fig_1">Figure 3</ref>, to extract a lowdimensional feature vector from each frame. These frame embeddings are fused using the attention mechanism from a transformer encoder layer. The output of the transformer feeds into a fully connected layer, which is trained as a multi-label classifier using binary cross-entropy to predict which tags should be present. Predicting Response Tags for Text For each message, we predict the k-hot distribution of tags for a gif response by training a BERTweet model <ref type="bibr">(Nguyen et al., 2020)</ref>, which has been pre-trained on a large corpus of Twitter data (shown as "Tweet Encoder" in <ref type="figure" target="#fig_1">Figure 3</ref>). The model with an additional fully connected layer is trained as a multilabel classifier using binary cross-entropy, using the tags for the gifs used in reply (if known). Tag-based Gif Selection At inference time, given a message, we use the text-to-tag model to predict a k-hot distribution over tags. Then, we select the gif whose estimated tag distribution is closest in Euclidean distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">CLIP variant</head><p>The second model uses an end-to-end training approach based on the architecture of OpenAI CLIP <ref type="bibr">(Radford et al., 2021)</ref>. The architecture features two encoders, one for text and one for images. During training, the encoders are updated using contrastive loss that maximizes the cosine similarity of paired image-text representations and minimizes the cosine similarity of random pairs of images and texts. We replicate the CLIP architecture and training procedure, using BERTweet to encode text and EfficientNet <ref type="bibr" target="#b3">(Tan and Le, 2019)</ref> to encode a composite image of four frames from the gif (compared with BERT and ResNet in their implementation). While originally designed to select an image for a text description, our model is trained to select a gif reply for a text message-a more challenging task than the image retrieval task used in the original CLIP setup, as the message may not contain words describing elements of the gif. At inference time, given a tweet, we use the trained tweet encoder to extract its representation and compute its cosine similarity with each encoded representation for our gifs. The gif with the highest cosine similarity is returned as the best response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">PEPE THE KING PRAWN</head><p>Our final model, KING PRAWN 1 (referred to as "PEPE".) selects gif responses by using a richer set of multimodal features to create a gif representation. Rather than encode the gif solely from its image content, we use a multimodal encoder that captures (i) any text it might have, (ii) the types of objects present in the gif, and (iii) object regions as visual features. We encode these gif aspects using an OSCAR transformer <ref type="bibr" target="#b14">(Li et al., 2020)</ref> to create a unified representation, shown in <ref type="figure" target="#fig_1">Figure 3</ref> (bottom). Object names and regions of interest feature vectors are extracted using a pre-trained bottom-up attention model <ref type="bibr" target="#b0">(Anderson et al., 2018)</ref>.</p><p>As input to the OSCAR encoder, the captions to each of the gif's four frames are concatenated together with an "[INTER_FRAME_SEP]" separator token. We filter object areas detected by the bottomup attention model <ref type="bibr" target="#b0">(Anderson et al., 2018)</ref> and we keep all objects with probability &gt;0.5. We then concatenate object names together with the same inter-frame separator between names of different frames. Together, the caption text, object names, and image-region features are fed into the OSCAR transformer encoder to generate a GIF feature vector; the transformer is initialized with the default OSCAR weights. We use BERTweet to encode text. The entire PEPE model is trained end-to-end using contrastive loss, similar to the CLIP model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>We initially evaluate the methods in two ways. First, we use traditional classification-based evaluation, testing whether the models can reproduce the observed gif replies. However, some messages could have multiple valid gif responses. Therefore, as a second test, we evaluate the model in a retrieval setting, measuring whether its most-probable responses are good quality for a message. Experimental Setup Models are trained and tested on a dataset containing 1,562,701 Tweet-  GIF pairs associated with 115,586 unique gifs, where 605,063 tweet-gif pairs are associated with at least one tag. Using the finalized 241 unique tags as classes for multi-label classification, we split the dataset by stratify on tags using the iterative train-test split method provided by scikit-multilearn library <ref type="bibr">(Sechidis et al., 2011;</ref><ref type="bibr" target="#b2">Szyma?ski and Kajdanowicz, 2017)</ref> to create a 80:10:10 train, dev, and test split which is finalized to train the models described in ?4. Following BERTweet (Nguyen et al., 2020), we preprocess tweets in our dataset using NLTK TweetTokenizer for tokenization, emoji package to translate emotion icons, and converted mentions and links to special "@USER" and "HTTPURL" tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Annotated Data</head><p>To test whether each model's predictions are valid responses, we annotate the ten most-probable gif predictions for a subset of the tweets in our test data. Many tweets in our test set require substantial context to understand due to having few tokens, linking to URLs that provide extra knowledge, mentioning other users in directed communication. These factors suggest social context or general knowledge aids in the recipient's understanding of the gif's intentions. While the model can still benefit from training on such examples, judging the appropriateness of response is difficult without access to the social context. Therefore, to reduce interpretation ambiguity, we annotate only tweets without URLs or user mentions and having at least 10 tokens. This process selects tweets with sufficient content to judge appropriateness independent of the larger social context.</p><p>Two annotators (the authors) were shown a list of potential gif responses for a tweet and asked to judge whether this is an appropriate gif response (a binary rating). Gifs were selected from the ten most-probable replies for each system and collectively shown in random order to prevent knowing which system generated each reply. A total of 2,500 gif-tweet pairings were annotated. Annotators attained a Krippendorf's ? of 0.462; while moderate agreement, this value is expected given known differences in how people interpret and value gif responses based on their familiarity with its content, message interpretation, and life-experience <ref type="bibr">(Jiang et al., 2018)</ref>. We follow the evaluation setup from other retrieval-based dialog systems (e.g. <ref type="bibr" target="#b13">Yu et al., 2021;</ref><ref type="bibr">Kumar and Callan, 2020)</ref> and use normalized Discounted Cumulative Gain (nDCG), which measures whether more appropriate gif responses are ranked higher. A gif's appropriateness score is the sum of annotators' ratings. Results The PEPE model was able to identify relevant and good-quality gif responses, as shown by its performances on the test data <ref type="table" target="#tab_1">(Table 1)</ref> and annotated data <ref type="table" target="#tab_2">(Table 2)</ref>. Performance on the test set is expected to be low, given the challenge of identifying the exact gif used for a tweet when multiple possible gifs are likely to be equally valid. However, the PEPE model is still able to identify the exact gif (out of 115K) in its top 10 predictions for 3% of the data, substantially outperforming all  other models. Performance on the annotated data <ref type="table" target="#tab_2">(Table 2)</ref> provides a more realistic assessment of whether models can generate high-quality replies, as it measures whether the models' replies themselves were good. The PEPE model attains substantially higher performance (p&lt;0.01) than other models. While the CLIP variant model performs well, the contentagnostic Distribution sampling baseline performs nearly as well. This baseline's high performance speaks to the multiple interpretations of gifs and the ease at which readers can make connections between a gif and message. Indeed, even the randomgif model has a non-zero nDCG, highlighting the ability for an arbitrary gif to still be considered appropriate. We speculate that popular gifs may be popular because of this ease of multiple interpretations. <ref type="table" target="#tab_4">Table 4</ref> shows the top predictions for models and baselines for two example messages, illustrating the variety of relevant gifs; the PEPE and random baseline replies for the second message exemplify the type of gifs that can be widely applied to many messages, often to humorous effects. Ablation study PEPE fuses multiple types of input, which may uniquely contribute to model's ability to select gif replies. To understand how these inputs each contribute, we performed an ablation study on the annotated test set by removing one input from Oscar GIF Encoder shown in <ref type="figure" target="#fig_1">Figure 3 (</ref>  <ref type="table">Table 3</ref>: Results for ablated versions of PEPE where specific input is removed (cf. <ref type="table" target="#tab_2">Table 2)</ref> show that all input forms contribute to the ability to select replies. and evaluating the model's resulting gifs on the same test instances.</p><p>The ablated model performances, shown in Table 3, reveal that each input is useful for selecting gifs. 2 Object features capture visual information about what specifically is present in the gif (beyond the discrete names of what is present, e.g., "person" or "building") and show that multimodality is important for high performance-predicting replies just from a gif's caption and categorized content are insufficient. Similarly, the caption of a gif (if present) is important, as the text can help make explicit the intended interpretation of a gif.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Field Experiment</head><p>To test the generalizability of our models and quality of their responses, we conduct a large-scale randomized controlled trial (RCT) that has the models respond to real users and measure their perception of reply quality. 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setup</head><p>Gifs were posted to the Imgur platform, which is a highly active social media community that supports both image and text-based interactions. On Imgur, users may create posts, which contain one or more images with optional commentary, or comment on posts or replies. Similar to pre-2018 Twitter, comments are limited to 140 characters. Imgur conversations are threaded and frequently contain both image and text comments. Like Reddit, users may upvote and downvote content, providing a score of how well it was received by the community; we use</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parent Tweets</head><p>Tag-based CLIP variant PEPE Dist. Samp. Random That wonderful feeling you get when you arrive to a business dinner that you're supposedly paying for...and realize you've forgotten your credit card I'm convinced some of y'all don't get laid this score in our experiments to evaluate quality.</p><p>Our experiment focuses on generating Gif-based replies to top-level text comments (comments made directly to the post). This setup mirrors the conversational data our models were trained on. Imgur supports several ways of filtering its stream of posts. To ensure that our replies have sufficient visibility, we select posts that have already receive 10 comments and appear in the "most viral" sorting. From these posts, we reply to the top-rated text comment. The RCT runs from 8 AM to 8 PM (local time), making at most 10 replies per hour.</p><p>Not all topics or comments are suitable for automated responses and great care was taken to prevent potential harm to the community. Through multiple rounds of testing which replies would be responded to, we curated a list of keywords that could lead to potential controversial replies, such as terms about religion or race (full list in Appendix D). Any comment containing a token or lemma matching a word on this list is excluded and not replied to. As a further safeguard, experimenters monitored all replies to remove any that were deemed inappropriate. See the Ethics Section ( ?9) for a longer discussion of safeguards.</p><p>The field experiment consists of five arms, corresponding to the three trained models and the two baseline models. During each trial, one model is selected and generates a response; the trained model replies with the most probable gif. <ref type="bibr">4</ref> Not all models are equally likely to perform well and so to make the most use of our trial budget, we use Thompson sampling <ref type="bibr">(Russo et al., 2018)</ref> to randomly select which arm of the trial to use. Thompson sampling builds a probability model for the estimated reward of each arm (here, the score a reply receives) and samples from the model such that higher-rewarding arms are sampled more frequently. As a result, this method can provide tighter estimates for the reward of the most useful arms. Scores in Imgur have a skewed distribution, with few comments receiving very high scores and most receiving near the default score (1). Therefore, we use Poisson Thompson sampling. Some comments may be downvoted to receive scores below zero, so for simplicity, we truncate these scores to 0.</p><p>We initialize the reward estimates for our experiment by selecting one of the five models in a round-robin manner to reply to an Imgur comment for 3 days. These initial scores act as priors for Thompson sampling to update Poisson distributions for each model. In the trial, we choose a model by sampling from the up distributions using all previous days' scores as the prior. The experiment ran from April 15th, 2021 to August 30th, 2021, and models generated a total of 8,369 replies.</p><p>To evaluate the results of the RCT, we construct a Negative Binomial regression on the dependent variable of the score received for a model's reply, truncating negative scores to zero. The Negative binomial was chosen instead of Poisson due to over-dispersion in the score variable. The models are treated as a categorical variable, using the random model as a reference. Since the score will depend, in part, on the attention received by the parent post and comment (higher-rated comments are displayed first), we include linear effects for the post and parent comment. Finally, we include five text-related variables to control for the con- tent of the parent comment: the topic distribution (Appendix <ref type="table" target="#tab_10">Table 9</ref>) from a 10-topic model (dropping one topic due to collinearity), the sentiment and subjectivity of the message estimated using TextBlob library, the length of the comment, and whether the comment contained a question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results</head><p>The field experiment demonstrates that the PEPE model is able to generate significantly higherscoring responses. <ref type="figure" target="#fig_2">Figure 4</ref> shows the Negative Binomial regression coefficients for the three models and empirical distribution baseline, with the random gif model as a reference; full regression results are shown in <ref type="table" target="#tab_8">Appendix Table 6</ref>. The PEPE model substantially outperforms all other models (p&lt;0.01) in this real-world setting. Surprisingly, despite performing second-best in our annotated evaluations, the CLIP model performs worst, with its replies receiving fewer upvotes than the two baselines that randomly select gifs. We investigate potential explanations for these performances next. The Random and Distributional-sampling baseline models perform surprisingly well relative to models that take the text and gif content into account, with only the PEPE model outperforming them. The performance of the random baselines matches prior work showing people are still able to draw some connection between their interpretation and the reply <ref type="bibr">(Madden, 2018, p.29)</ref>. Further, we observed that, when the model's reply truly seemed random, some users replied say they upvoted solely because they enjoyed the gif.</p><p>As a follow-up experiment, we tested whether models could be getting higher (or lower) scores by repeatedly picking the same gifs that are skewed towards a positive or negative reaction. <ref type="figure" target="#fig_3">Figure 5</ref> shows the score distribution for the top ten most fre-  <ref type="table" target="#tab_9">Table 7.</ref> quently used gifs (visual examples in Appendix Table 7) for each of the three trained models and reveals surprisingly divergent behavior for how the community reacts. Each model had a different set of most-used gifs, indicating the models did not converge to a universal set of common replies. Indeed, a gif's frequency-of-use and mean reply score were uncorrelated in all three models (r ?-0.01, p&gt;0.73 for all models). The most-used gifs for each model had average scores that were positive, but the distributions for each gif show that some uses were occasionally downvoted. This high variance in scores indicates that a gif's intrinsic qualities are not solely responsible for the received score and, instead, appropriate use in context is plays a significant part in community reception.</p><p>We examined whether models relied on the same set of gifs. <ref type="figure" target="#fig_4">Figure 6</ref> shows the distribution of gif uses by each model, indicating that the tag-based model relied frequently on a small set of gifs. However, the PEPE and CLIP variant models were substantially more varied, indicating they draw from the long-tail of possible gifs. Do any of our models spark more subsequent conversation? We fit a separate Negative Binomial regression on the total number of comments made to our reply, using the same IVs as the score regression and include the reply's score itself as another IV. This model (Appendix <ref type="table">Table 8</ref>) shows that both the distributional-sampling baseline and PEPE models produced replies that led to fewer subsequent comments (p&lt;0.01)-despite the PEPE model having the most-upvoted replies. However, the score of the gif reply was positively associated (p&lt;0.01) indicating that more appropriate replies do receive more subsequent conversation. We speculate that the random models may have led to more conversation due to users replying to express confusion about why the particular gif was used. This result points to a need to understand what text and visual factors in gifs influence the volume of subsequent dialog and an opportunity to optimize gif models for both quality and number of conversation turns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>This work draws upon two strands of research from dialog systems and multimodal NLP. Conversational dialog systems have traditionally been built upon large-scale dialog corpora from social media platforms (Bessho et al., 2012) such as Twitter. Our approaches are fundamentally information retrieval based systems that mirror the approach by text-based conversational systems that retrieve ex-isting messages from a large social media corpus as potential replies and rank these to select a response. Our work mirrors models that use neural networks for ranking <ref type="bibr" target="#b12">(Yan et al., 2016;</ref><ref type="bibr">Inaba and Takahashi, 2016;</ref><ref type="bibr">Penha and Hauff, 2021, e.g.,)</ref>; however, we note that many recent knowledge-grounded and open domain models use encoder-decoder methods to improve versatility and applicability (e.g., <ref type="bibr">Ghazvininejad et al., 2018;</ref><ref type="bibr">Gao et al., 2019;</ref><ref type="bibr" target="#b14">Zhou et al., 2020)</ref>. Generative approaches are likely inappropriate for gif-based conversation as gifs are more akin to mimetic artifacts that build on cultural knowledge (Eppink, 2014), making synthesizing a new gif from scratch likely less effective.</p><p>All three models used here rely on joint embedding spaces for gif and text. Multiple works in NLP have been proposed to align these representations <ref type="bibr">(Kiros et al., 2014;</ref><ref type="bibr" target="#b7">Wang et al., 2016)</ref>, often for particular applications such as visual question answering <ref type="bibr" target="#b1">(Antol et al., 2015)</ref>. Recent work has focused on embeddings these media with a single encoder that takes both text and images as input (e.g., <ref type="bibr" target="#b8">Wang et al., 2019;</ref><ref type="bibr">Chen et al., 2020)</ref>, in contrast to our model that uses separate image and text encoders ( <ref type="figure" target="#fig_1">Figure 3)</ref>; these multimodal encoders are prohibitively computationally expensive to use in our setting during inference time, as the model would need to be run on each gif (and message) to rank replies, compared with our model that only needs to encode text. However, performance and efficiency improvements in aligning image and text representations would likely benefit our task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>People like using gifs in online conversations-gifs are a fun and playful way to communicate. However, modern NLP conversational agents operate only by text. Here, we introduce a new dataset of 1.56M conversation turns using gifs, including captions and metadata, and develop a new conversational model PEPE THE KING PRAWN that selects appropriate gif responses for messages through comparing encoded gif and text representations. In two evaluations, we show that PEPE is able to generate highly-relevant gif responses and in a largescale RCT, we show that the gif replies from the PEPE model received significantly higher scores from the general public. Our work demonstrates the opportunity for using NLP methods to successfully engage in multimodal conversations.</p><p>The interactive nature of the RCT necessitated a close consideration of ethical issues <ref type="bibr" target="#b5">(Thieltges et al., 2016)</ref>. Prior to beginning the RCT, the study team obtained IRB approval to interact with users. While necessary in the legal sense, IRB approval is not sufficient to justify the ethical grounds of the study. The primary risks of the study are if the automated models respond with an inappropriate gif or respond to a message that is not suitable for automated response (e.g., discussing the death of a loved one or making an offensive statement). These risks were mitigated in multiple ways throughout the dataset construction and field experiment.</p><p>First, the selection criteria for which comments we reply to was designed to only reply to content that was already deemed appropriate by the community. By selecting only posts that had received sufficient upvotes to be called "viral" and were already receiving comments, we mitigate the risk of engaging in topics or conversations that are inappropriate according to the norms of the Imgur community, as these posts would be removed by moderators or would have received sufficient downvotes to stay in obscurity.</p><p>Second, by focusing on the top-voted comment to these posts, we again reply to content that has already been deemed high-quality by the comment. This comment-level criteria substantially lowers the risk of our models commenting on inappropriate comments (e.g., a comment insulting another user), as these comments are readily downvoted by the community prior to our intervention.</p><p>Third, we employed extensive filtering to avoid replying to any comment containing a potentially sensitive topic, e.g., a discussion of race or trauma (keywords are listed in Appendix D). The initial set of keywords was developed through examining potentially sensitive topics and then iteratively added to by simulating which messages our RCT would reply to and examining whether it would be appropriate. During the field RCT, experimenters continuously monitored the comments to ensure no harm was being done. Ultimately, only three comments were removed during the initial two days, which was due to a bug in the lemmatization and these comments should have been filtered out by our earlier criteria; these comments were removed quickly and we did not observe any notable response from the community.</p><p>Fourth, one risk is replying with an inappropri-ate gif, which is mitigated by the use of Giphy to seed our initial gifs. As this platform is curated and does not host objectively offensive gifs (e.g., overly-violent content), our initial gif set is relatively free of objectionable gifs. Because our model learns directly from gifs' frequency of use, unless objectively offensive gifs are widely used, they are unlikely to be deployed from our RCT; we speculate that few objectively offensive gifs are widely used and, in practice, we have not identified any during the study period or when examining hundreds of random gifs in our data (or used in the RCT). Finally, one risk is that by learning gif responses from observed data, our models may reinforce cultural stereotypes that are encoded in the gifs themselves (Erinn, 2019), e.g., the association of African American individuals with strong emotions. While our gif data is relatively clean of overtly offensive gifs, we acknowledge that our model likely does inadvertently perpetuate some of these latent biases in the data. However, the success of our model suggests a future mitigation strategy for platforms suggesting gifs: as biases become known, our approach can be used to suggest less-biased gifs as potential responses to mitigate future harm.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Details on Model Training</head><p>Following, we provide additional details on how each of the three models was trained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Tag-based Model</head><p>EfficientNet-based Tag Classifier Gifs are reshaped to 224 by 224 pixel while keeping the aspect ratio by padding and normalized to a mean of 0.5 and standard deviation of 0.5 for each channel before feeding into the EfficientNet-based model. We selected unique GIFs from the finalized dataset that has at least one associated tag and using the iterative train test split on k-hot tag representation to select 5% of those GIFs for validation. The Effi-cientNet tag classifier was trained for 100 epochs on a batch size of 32, using AdamW optimizer with learning rate 1e-5 and weight decay 1e-3. The best validation performance was achieved at the 40th epoch with macro-f1 of 0.30 in predicting 241 multi-label classes. Early experiment shows that transformer encoder layer (macro-f1 of 0.30) out performs linear layer (macro-f1 of 0.19) in fusing multi-frame gif features on the development set, therefore transformer encoder layer is used to fuse features of different frames in our implementation. Tweet-to-tag classifier Using the finalized dataset mentioned in ?3, we use tweet as input, and the k-hot tag representation of that tweet instance as ground truth label to train the multi-label classifier along with the tweet encoder for 241 classes. Additionally, we filter out tweets from the finalized dataset that do not have corresponding twitter tags before training. The model with the best validation performance is selected to perform subsequent evaluation and field experiments. The tweet encoder was trained for 100 epochs with a batch size of 32. The learning rate was set to 1e-5 with 1e-3 weight decay using AdamW optimizer. The best validation macro-f1 was 0.07 achieved at the 70th epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 CLIP variant</head><p>The evaluation performance for model selection is measured by nDCG. For every tweet-gif pair in the validation set, we measure the top 30 predicted GIFs from the model using the tweet as input. The relevance of an occurring ground truth gif in the top 30 predictions given a tweet is set 1 for the nDCG calculation. CLIP variant is trained on the same finalized dataset using contrastive loss. It was trained for 16 epochs with a batch size of 16 using AdamW optimizer of learning rate 1e-5 and weight decay 1e-3. Best validation performance is achieved at epoch 6 with an nDCG value of 0.015.</p><p>We replace the Transformer encoder layer with a linear Layer on Efficient GIF Encoder from <ref type="figure">Figure</ref> 3, and use this as our GIF Encoder for the CLIP variant. Image inputs to the GIF encoder are normalized following the official CLIP implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 PEPE</head><p>The PEPE model follows most configurations from the CLIP variant model, but replace the EffcientNet GIF encoder with an Oscar GIF encoder based on Oscar pre-trained multi-modal transformer <ref type="bibr" target="#b14">(Li et al., 2020)</ref>.</p><p>Extra metadata are extracted from GIFs in the finalized dataset for further training. Captions within the GIF are extracted using PaddleOCR <ref type="bibr">(Du et al., 2020)</ref>, and only extracted text with probability greater than 0.9 are kept as caption metadata.</p><p>Object tags and their corresponding features are extracted with bottom-up attention <ref type="bibr" target="#b0">(Anderson et al., 2018)</ref> using py-bottom-up-attention package. Object instances are filtered to only keep instances that have a score higher than 0.5, then object tags and their corresponding features are extracted from these instances. Final object features of dimension 2054 are obtained by concatenating feature output with dimension 2048 from Faster-RCNN with scaled box position coordinates of the object following <ref type="bibr" target="#b14">(Li et al., 2020)</ref>.</p><p>The PEPE model is trained on the finalized dataset with extracted caption and object metadata. It was trained for 16 epochs with a batch size of 8 using AdamW optimizer of learning rate 1e-6 and weight decay 1e-3. Preprocessing for GIFs is Gif reply score post score ?0.0002 * * * (0.00003) comment score 0.001 * * * (0.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dependent variable:</head><p>Cumulative number of replies received gif reply score 0.096 * * * (0.010) post score ?0.0004 * * * (0.0001) comment score 0.0002 (0.0002) CLIP variant model Note: * p&lt;0.1; * * p&lt;0.05; * * * p&lt;0.01 <ref type="table">Table 8</ref>: Negative Binomial regression on cumulative number of replies received. The random-gif baseline is set as the reference category for model comparison.</p><p>Topic Dirichlet parameter Keywords 0 0.1172 people fuck trump shit make thing country n't vote fucking 1 0.20164 good time love kid make cat dog day year guy 2 0.09554 pay work money people make job year buy time company 3 0.11245 post make read people good time thing imgur video work 4 0.06541 car live year drive day place time road city back 5 0.05672 eat make food good water drink taste cheese pizza coffee 6 0.06662 people covid die vaccine life make work problem mask n't 7 0.0888 movie play game good watch show love great time song 8 0.02752 wear mask red shirt woman hair white man hat black 9 0.14292 back make put hand time guy car head thing big </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The frequency distribution of gifs in our data roughly follows a log-normal distribution, with a few gifs used often, while a long tail of gifs are used relatively infrequently.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The different encoder modules used to construct the models in ?4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Negative Binomial regression coefficients for each model on predicting a gif reply's score, using the random-gif model as the reference category; bars show standard error and *** denotes significance at 0.01.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Score distributions for most-frequently used gifs show few are universally skewed positive. Boxes show quartile ranges; gifs are in Appendix</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Gif use frequency by each model, shown as frequency-vs-rank log-scaled with first-order line fit (jitter added for separation).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Family &amp; Pets related) ?0.264 (0.412) topic 2 (Employment related) ?1.182 * * (0.549) topic 3 (Social media related) 1.381 * * * (0.421) topic 4 (Transportation related) ?0.021 (0.514) topic 5 (Food related) ?0.896 (0.567) topic 6 (COVID related)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Oscar GIF Encoder Tweet Encoder</head><label></label><figDesc></figDesc><table><row><cell>EfficientNet GIF Encoder</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EfficientNet EfficientNet EfficientNet EfficientNet-b0 (shared weight)</cell><cell>Transformer Encoder Layer</cell><cell>Tweet We 're getting married today !</cell><cell>BERTweet-base Transformer</cell></row><row><cell>4 selected frames 4 x 224 x 224 (after reshape)</cell><cell>feature vector for downstream task 1 x 512</cell><cell></cell><cell>feature vector for downstream task 1 x 512</cell></row><row><cell>OCR</cell><cell>extracted caption Aww , thank you</cell><cell></cell><cell></cell></row><row><cell>Bottom-up Attention</cell><cell cols="2">extracted object names face woman [INTER_FRAME_SEP] face woman</cell><cell>Oscar Multimodal Transformer</cell></row><row><cell></cell><cell>extracted object feature vectors</cell><cell></cell><cell>feature vector for downstream task</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1 x 512</cell></row><row><cell>4 selected frames</cell><cell></cell><cell></cell><cell></cell></row><row><cell>4 x 224 x 224 (after reshape)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Models' precision-at-k on selecting the exact gif used as a response for a tweet in the test set; this performance is an underestimate of each model, as many model-predicted gifs may be appropriate.</figDesc><table><row><cell>Model</cell><cell>Top-1</cell><cell>Top-5</cell><cell>Top-10</cell></row><row><cell>Tag-based</cell><cell cols="3">0.000000 0.000092 0.000119</cell></row><row><cell>Random</cell><cell cols="3">0.000020 0.000059 0.000158</cell></row><row><cell>CLIP variant</cell><cell cols="3">0.000488 0.001669 0.002783</cell></row><row><cell cols="4">Distribution sampling 0.000996 0.005098 0.009780</cell></row><row><cell>PEPE</cell><cell cols="3">0.005375 0.018723 0.030918</cell></row><row><cell>Model</cell><cell></cell><cell>nDCG</cell><cell></cell></row><row><cell>Random</cell><cell></cell><cell>0.3273</cell><cell></cell></row><row><cell>Tag-based</cell><cell></cell><cell>0.4526</cell><cell></cell></row><row><cell cols="3">Distribution sampling 0.4969</cell><cell></cell></row><row><cell>CLIP variant</cell><cell></cell><cell>0.5934</cell><cell></cell></row><row><cell>PEPE</cell><cell></cell><cell>0.8145</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Models' nDCG scores at proposing appropri- ate gif replies, measured from annotations on the top 10 most probable gif replies of each model.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>i.e., a gif's caption, object names, or objects' visual features)</figDesc><table><row><cell>Model</cell><cell>nDCG</cell></row><row><cell>PEPE</cell><cell>0.8145</cell></row><row><cell>PEPE without object names</cell><cell>0.7665</cell></row><row><cell>PEPE without caption</cell><cell>0.7559</cell></row><row><cell cols="2">PEPE without object features 0.7533</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Model-selected replies to messages (paraphrased for privacy). Click an image to view the gif on Giphy.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Saeideh Bakhshi, David A. Shamma, Lyndon Kennedy, Yale Song, Paloma de Juan, and Joseph Jofish Kaye. 2016. Fast, cheap, and good: Why animated gifs engage us. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, San Jose, CA, USA, May 7-12, 2016, pages 575-586. ACM.</figDesc><table><row><cell>Jialun "Aaron" Jiang, Casey Fiesler, and Jed R Brubaker. 2018. "The Perfect One" Understanding Category Subcategory</cell><cell>Hao Fang, Hao Cheng, Maarten Sap, Elizabeth Clark, Ari Holtzman, Yejin Choi, Noah A. Smith, and Mari Conference on Empirical Methods in Natural Lan-guage Processing: System Demonstrations, pages 9-</cell></row><row><cell>Communication Practices and Challenges with An-Cartoons &amp; Comics aqua teen hunger force</cell><cell>Ostendorf. 2018. Sounding board: A user-centric 14, Online. Association for Computational Linguis-</cell></row><row><cell>Michael S Bernstein, Margaret Levi, David Magnus, Betsy Rajala, Debra Satz, and Charla Waeiss. 2021. Esr: Ethics and society review of artificial intelli-gence research. ArXiv preprint, abs/2106.11521. Fumihiro Bessho, Tatsuya Harada, and Yasuo Ku-niyoshi. 2012. Dialog system using real-time crowd-sourcing and Twitter large-scale corpus. In Proceed-ings of the 13th Annual Meeting of the Special Inter-est Group on Discourse and Dialogue, pages 227-imated GIFs. Proceedings of the ACM on human-Celebrities richard pryor computer interaction, 2(CSCW):1-20. Daniel Jurafsky, Elizabeth Shriberg, Barbara Fox, and Reactions angry Emotions happy Traci Curl. 1998. Lexical, prosodic, and syntactic Anime bleach cues for dialog acts. In Discourse Relations and Dis-Art &amp; Design psychedelic course Markers. Mahmoud Khademi. 2020. Multimodal neural graph guistics. 7188, Online. Association for Computational Lin-ciation for Computational Linguistics, pages 7177-Proceedings of the 58th Annual Meeting of the Asso-memory networks for visual question answering. In Nature sunrise Transportation bicycle</cell><cell>and content-driven social chatbot. In Proceedings of the 2018 Conference of the North American Chap-ter of the Association for Computational Linguis-tics: Demonstrations, pages 96-100, New Orleans, Louisiana. Association for Computational Linguis-tics. Jianfeng Gao, Michel Galley, and Lihong Li. 2019. Neural Approaches to Conversational AI: Ques-tion Answering, Task-oriented Dialogues and Social Chatbots. Now Foundations and Trends. Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen-tau Yih, and tics. Tianrui Niu, Fangxiang Feng, Lingxuan Li, and Xiaojie Wang. 2020. Image synthesis from locally related texts. In Proceedings of the 2020 International Con-ference on Multimedia Retrieval, pages 145-153. Resources Association (ELRA). (LREC 2018), Miyazaki, Japan. European Language Conference on Language Resources and Evaluation acts. In Proceedings of the Eleventh International Silvia Pareti and Tatiana Lando. 2018. Dialog intent structure: A hierarchical schema of linked dialog</cell></row><row><cell>231, Seoul, South Korea. Association for Computa-tional Linguistics. Elli Bourlai and Susan C Herring. 2014. Multimodal communication on tumblr: "i have so many feels!". Ryan Kiros, Ruslan Salakhutdinov, and Richard S Zemel. 2014. Unifying visual-semantic embeddings with multimodal neural language models. ArXiv preprint, abs/1411.2539.</cell><cell>Michel Galley. 2018. A knowledge-grounded neural Gustavo Penha and Claudia Hauff. 2021. On the cal-conversation model. In Proceedings of the Thirty-ibration and uncertainty of neural learning to rank Second AAAI Conference on Artificial Intelligence, models for conversational search. In Proceedings (AAAI-18), the 30th innovative Applications of Arti-of the 16th Conference of the European Chapter ficial Intelligence (IAAI-18), and the 8th AAAI Sym-of the Association for Computational Linguistics:</cell></row><row><cell>In Proceedings of the 2014 ACM conference on Web Artie Konrad, Susan C Herring, and David Choi. 2020.</cell><cell>posium on Educational Advances in Artificial Intel-Main Volume, pages 160-170, Online. Association</cell></row><row><cell>science, pages 171-175. Sticker and emoji use in facebook messenger: impli-</cell><cell>ligence (EAAI-18), New Orleans, Louisiana, USA, for Computational Linguistics.</cell></row><row><cell>cations for graphicon change. Journal of Computer-</cell><cell>February 2-7, 2018, pages 5110-5117. AAAI Press.</cell></row><row><cell>Weixuan Chen, Ognjen Oggi Rudovic, and Rosalind W Mediated Communication, 25(3):217-235.</cell><cell></cell></row><row><cell>Picard. 2017. Gifgif+: Collecting emotional ani-mated gifs with clustered multi-task learning. In 2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII), pages 510-517. IEEE. Vaibhav Kumar and Jamie Callan. 2020. Making in-formation seeking easier: An improved pipeline for conversational search. In Findings of the Associa-tion for Computational Linguistics: EMNLP 2020, pages 3971-3980, Online. Association for Compu-</cell><cell>Carla F Griggio, Joanna Mcgrenere, and Wendy E Mackay. 2019. Customizations and expression breakdowns in ecosystems of communication apps. Proceedings of the ACM on Human-Computer Inter-action, 3(CSCW):1-26.</cell></row><row><cell>Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El tational Linguistics.</cell><cell>Tim Highfield and Tama Leaver. 2016. Instagrammat-</cell></row><row><cell>Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. 2020. UNITER: Universal Image-TExt representation learning. In ECCV. Kuang-Huei Lee, X. Chen, G. Hua, H. Hu, and Xi-aodong He. 2018. Stacked cross attention for image-text matching. ArXiv preprint, abs/1803.08024.</cell><cell>ics and digital methods: Studying visual social me-dia, from selfies and gifs to memes and emoji. Com-munication research and practice, 2(1):47-62.</cell></row><row><cell>Yong Cheng, Qian Yang, Yang Liu, Maosong Sun, and Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xi-</cell><cell>Chung Hoon Hong, Yuan Liang, Sagnik Sinha Roy,</cell></row><row><cell>Wei Xu. 2017. Joint training for pivot-based neural aowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu,</cell><cell>Arushi Jain, Vihang Agarwal, Ryan Draves, Zhizhuo</cell></row><row><cell>machine translation. In Proceedings of the Twenty-Li Dong, Furu Wei, et al. 2020. Oscar: Object-</cell><cell>Zhou, William Chen, Yujian Liu, Martha Miracky,</cell></row><row><cell>Sixth International Joint Conference on Artificial In-semantics aligned pre-training for vision-language</cell><cell>et al. 2020. Audrey: A personalized open-domain</cell></row><row><cell>telligence, IJCAI 2017, Melbourne, Australia, Au-tasks. In European Conference on Computer Vision,</cell><cell>conversational bot. In Alexa Prize Proceedings.</cell></row><row><cell>gust 19-25, 2017, pages 3974-3980. ijcai.org. pages 121-137. Springer.</cell><cell></cell></row><row><cell>Michelle Cohn, Chun-Yen Chen, and Zhou Yu. 2019. A large-scale user study of an Alexa Prize chatbot: Effect of TTS dynamism on perceived quality of so-John Savery Madden. 2018. The Phenomenological Exploration of Animated GIF Use in Computer-Mediated Communication. Ph.D. thesis, University</cell><cell>Pingping Huang, Jianhui Huang, Yuqing Guo, Min Qiao, and Yong Zhu. 2019. Multi-grained attention with object-level grounding for visual question an-swering. In Proceedings of the 57th Annual Meet-</cell></row><row><cell>cial dialog. In Proceedings of the 20th Annual SIG-of Oklahoma.</cell><cell>ing of the Association for Computational Linguis-</cell></row><row><cell>dial Meeting on Discourse and Dialogue, pages 293-306, Stockholm, Sweden. Association for Computa-Luke Melas-Kyriazi, Alexander Rush, and George Han. 2018. Training for diversity in image paragraph cap-tional Linguistics. Yuning Du, Chenxia Li, Ruoyu Guo, Xiaoting Yin, Weiwei Liu, Jun Zhou, Yifan Bai, Zilin Yu, Yehua Yang, Qingqing Dang, et al. 2020. PP-OCR: A Prac-tioning. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 757-761, Brussels, Belgium. Association for Computational Linguistics.</cell><cell>tics, pages 3595-3600, Florence, Italy. Association for Computational Linguistics. Michimasa Inaba and Kenichi Takahashi. 2016. Neural utterance ranking model for conversational dialogue systems. In Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dia-</cell></row><row><cell>tical Ultra lightweight OCR system. ArXiv preprint, abs/2009.09941. Kate M Miltner and Tim Highfield. 2017. Never gonna GIF you up: Analyzing the cultural signifi-</cell><cell>logue, pages 393-403, Los Angeles. Association for Computational Linguistics.</cell></row><row><cell>cance of the animated GIF. Social Media+ Society, Jason Eppink. 2014. A brief history of the gif (so far). Journal of visual culture, 13(3):298-306. 3(3):2056305117725223.</cell><cell>Fiesler. 2017. Understanding diverse interpretations Jialun "Aaron" Jiang, Jed R Brubaker, and Casey</cell></row><row><cell>Dat Quoc Nguyen, Thanh Vu, and Anh Tuan Nguyen.</cell><cell>of animated GIFs. In Proceedings of the 2017 CHI</cell></row><row><cell>Wong Erinn. 2019. Digital blackface: How 21st cen-2020. BERTweet: A pre-trained language model</cell><cell>Conference Extended Abstracts on Human Factors</cell></row><row><cell>tury internet language reinforces racism. for English tweets. In Proceedings of the 2020</cell><cell>in Computing Systems, pages 1726-1732.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Examples of GIF categories on GIPHY</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Negative Binomial regression on score of the gif reply. The random-gif baseline is set as the reference category for models.</figDesc><table><row><cell>Reactions</cell><cell>judging you</cell></row><row><cell cols="2">Transportation truck</cell></row><row><cell cols="2">Transportation spaceship</cell></row><row><cell cols="2">Transportation van</cell></row><row><cell cols="2">Transportation submarine</cell></row><row><cell cols="2">Transportation motorcycle</cell></row><row><cell cols="2">Transportation bmw</cell></row><row><cell cols="2">Transportation helicopter</cell></row><row><cell cols="2">Transportation chevrolet</cell></row><row><cell cols="2">Transportation volkswagen</cell></row><row><cell cols="2">Transportation boat</cell></row><row><cell cols="2">Transportation bus</cell></row><row><cell cols="2">Transportation porsche</cell></row><row><cell cols="2">Transportation tank</cell></row><row><cell cols="2">Transportation audi</cell></row><row><cell cols="2">Transportation toyota</cell></row><row><cell cols="2">Transportation airplane</cell></row><row><cell cols="2">Transportation hovercraft</cell></row><row><cell cols="2">Transportation nissan</cell></row><row><cell cols="2">Transportation bicycle</cell></row><row><cell cols="2">Transportation train</cell></row><row><cell cols="2">Transportation rocket</cell></row><row><cell cols="2">Transportation yacht</cell></row><row><cell cols="2">Transportation ferrari</cell></row><row><cell cols="2">Transportation honda</cell></row><row><cell cols="2">Transportation sailboat</cell></row><row><cell cols="2">Transportation car</cell></row><row><cell cols="2">Transportation tesla</cell></row><row><cell>Holidays</cell><cell>mardi gras</cell></row><row><cell>Holidays</cell><cell>oktoberfest</cell></row><row><cell>Holidays</cell><cell>kwanzaa</cell></row><row><cell>Holidays</cell><cell>fathers day</cell></row><row><cell>Holidays</cell><cell>fourth of july</cell></row><row><cell>Holidays</cell><cell>mothers day</cell></row><row><cell>Holidays</cell><cell>yom kippur</cell></row><row><cell>Holidays</cell><cell>st patricks day</cell></row><row><cell>Holidays</cell><cell>memorial day</cell></row><row><cell>Holidays</cell><cell>cinco de mayo</cell></row><row><cell>Holidays</cell><cell>labor day</cell></row><row><cell>Holidays</cell><cell>rosh hashanah</cell></row><row><cell>Holidays</cell><cell>new years</cell></row><row><cell>Holidays</cell><cell>passover</cell></row><row><cell>Science</cell><cell>global warming</cell></row><row><cell>Science</cell><cell>astronomy</cell></row><row><cell>Science</cell><cell>physics</cell></row><row><cell>Science</cell><cell>laser</cell></row><row><cell>Science</cell><cell>stars</cell></row><row><cell>Science</cell><cell>robot</cell></row><row><cell>Science</cell><cell>atoms</cell></row><row><cell>Science</cell><cell>meteor</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table /><note>Examples of top 10 most frequently used gifs across all models in the RCT. Click an image to view the gif on Giphy. Images are ordered from most-used (top) to tenth-most (bottom).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Topic modeling keywords for Imgur Comments</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">KING PRAWN refers to "selecKting INteresting Gifs for Personal RespAWNses." In this crazy muppet-name-landgrab world we live in, our only regret is that we couldn't get "Pepino Rodrigo Serrano Gonzales" to fit as a bacronym, which we leave to future work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The performance decrease for removing object names is statistically significant (p&lt;0.01, bootstrapped). The decreases for removing captions and objects' visual features are significant from the name-removal model (p&lt;0.01) but the two models are statistically equivalent (p&gt;0.19).3 This experiment was ruled as Not Regulated by the University of Michigan IRB (HUM00197631). However, IRB approval is not sufficient to prevent harm(Bernstein et al.,  2021)  and significant precautions were taken to minimize potential risk (See ?9) .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Due to a bug, early experimental trials for the CLIP and PEPE models used the tenth most-probable gif; however, using the ratings in the annotated data, a t-test of the difference in quality for most-and tenth-most probable gifs showed no statistically-significant difference in quality for both models (p&gt;0.1). Therefore, we include this data in our results.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the reviewers, area chairs, and senior area chairs for their thoughtful comments and feedback. We also thank the Blablablab for helpful feedback and letting us deploy PEPE to the group's Slack and putting up with the ridiculous gif replies and Imgur for being a wonderful community. This material is based upon work supported by the National Science Foundation under Grant No. 2007251.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Animals skunk</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Match Animated GIF with Hash</head><p>Finalized Dataset 1,562,701 pairs (605,063 pairs associated with selected tags) ......   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>115,586 unique GIF IDs</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00636</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">VQA: visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.279</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A network perspective on stratification of multi-label data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Szyma?ski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Kajdanowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First International Workshop on Learning with Imbalanced Domains: Theory and Applications</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="22" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Emoticon, emoji, and sticker use in computer-mediated communication: A review of theories and research findings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khe Foon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Communication</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The devil&apos;s triangle: Ethical considerations on developing bot detection methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andree</forename><surname>Thieltges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Hegelich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI Spring Symposium Series</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gifs as embodied enactments in text-mediated conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackson</forename><surname>Tolins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrawat</forename><surname>Samermit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research on Language and Social Interaction</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="75" to="91" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning deep structure-preserving image-text embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.541</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="5005" to="5013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">CAMP: cross-modal adaptive message passing for text-image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00586</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10-27" />
			<biblScope unit="page" from="5763" to="5772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Omg ur funny! computer-aided humor with an application to chat</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaomiao</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy</forename><surname>Baym</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Tamuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Teevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Susan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kalai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCC</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="86" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Conversational graph grounded policy learning for open-domain conversation generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Yu</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.166</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1835" to="1845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attngan: Fine-grained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00143</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="1316" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to respond with deep neural networks for retrievalbased human-computer conversation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1145/2911451.2911542</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting>the 39th International ACM SIGIR conference on Research and Development in Information Retrieval<address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016-07-17" />
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Few-shot conversational dense retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/2105.04166</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The design and implementation of XiaoIce, an empathetic social chatbot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
		<idno type="DOI">10.1162/coli_a_00368</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="93" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">compliment, compliments, concerned, confused, congratulations, cool, crazy, creeping, cringe, crushing, cry, curtsy, cute, damn, dance, dancing, deadpan stare, debate, depressed, dickhead, disagree, disappointed, disapprove, disbelief, disgust, dislike, diss, divertente, dont care, doubt, doubtful, drink, drinking, drunk, dubious, dying, eating, eating popcorn, embarassed, engrossed, ennui, excited, face palm, faint, fingers crossed, flirt, flushed, freaking out, frustrated, fuck, fun, funny, gagging, get well, glare, good luck, gossip, grateful, gratitude, great, great job, grin, hahahah, happy, happy dance, head shake, hide, high five, hilarious, honestly, hope, horror, hugs, hugs love, hysterical, ill, impressed, incredulous, insult, interested, interesting, judge, judging you, just, keep going, kiss, laugh, leaving, lets go, lies, like, looking, looking around, love you, lovely, luv u, luv you, mad, mind blown, mock, motivational, moved, muah, much appreciated, nah, nasty, need, nervous, nice one, no, nod, not amused, not funny, not interested, oh shit, overwhelmed, panic, partying, perfect, pissed, please, pleased, pointing, praise, pray, pregnant, proud, pumped, questioning, raises hand, realization, relief, respect, reunited, roast, roll eyes, sad, sadness, salute, sarcastic, savage, scared, scary, screaming, secret, seriously, sexy, shame, shock, shook, shrug, shut up, shy, sigh, sips tea, sitting, sleepy, sloth, smart, smile, smug, sobbing, sorpren, sorry, spit, stoked, stressed, stunned, success, sudden realization, surprise, suspicious, sweating, swoon, swooning, take notes, tantrum, tears, thank, think, thirsty, thumbs down, thumbs up, tired, too funny, touched, unamused, unbelievable, uncomfortable, unhappy, unimpressed, unsure, upset, vomit, waiting, wave, weary, weird, whatever, will, wince, wink, wrestling, yawn, yell, yes, yum D List of filtering keywords on Imgur experiment depression, depressing, mental, health, death, dead, alcohol, alcoholism, weed, drugs, addiction, covid, beer, stoned, black, white, arabic, hispanic, latino, latina, latinx, police, cop, racism, racists, race, sexism, sexist, sexy, armed, overthrow, government, republican, democrats, maga, liberal, liberals, conservative, conservatives, offender, victim, disability, disabled, jerking, PD, gun, shots, fired, cops, officer, officers, killing, murder, murdered, kill, kills, killed, murders, shoot, taser, bystander, trigger, handgun, pansexual, sexuality, homosexual, gay, lesbian, corona, virus, coronavirus, vaccine, vaccinated, viruses, vaccination, die, fascist, fascists, antifa, sharia, islam, islamic, christian, jewish, muslim, blasphemy, blasphemic, death, conviction, church, priest, pastor, religious, religion, sharia, shia, sunni, judge, bible, qaran, torah, hindu, hindus, christians, jew, jews, muslims, islamist, execute, murder, captive, captives, malpractice, insurance</title>
		<imprint>
			<pubPlace>kabul</pubPlace>
		</imprint>
	</monogr>
	<note>C List of selected tags from GIPHY adorable, agreed, amazing, amused, angry, annoyed, anxiety, anxious, applause, approval, approve, aw, awesome, awkward, bad, beautiful, best wishes, blank stare, blink, blush, bored, bow, bravo, but why, buy, bye, captivated, celebrate, cheeky, cheering, cheers, clap, come on, comic. insured, threat, threatening, war, troops, violence, fighting, conflict, medicine, prescription, drug, dying, hospice, life, doctor, hospital, nurse, pedophiles, pedophile, bitch, republicans, democrat, coup, tax, recession, pedo, criminal, criminals, politician, politicians, health, healthcare, america, american, voter, voting, votes, vote, voters, citizen, immigrants, immigrant, citizens, candian, canada, eu, european, trump, red, blue, cancer, slavery, slaves, slave, disease, sickness, sorry, nazi, nazis, death, pro-death, pro-life, profile, abortion, aborted, aborting, victims, jail, whore, slut, rape, raped, raping, behead, beheadings, beheaded, torture, tortured, torturing, taliban, afghanistan, soldier, soldiers</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
