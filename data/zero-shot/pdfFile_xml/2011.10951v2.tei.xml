<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Class Unique Features in Fine-Grained Visual Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-03-16">16 Mar 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runkai</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Elecholic, Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Jinan University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijia</forename><surname>Yu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinqi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Jinan University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ding</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">The Chinese Uni-versity of Hong Kong Shenzhen</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><forename type="middle">Victor</forename><surname>Cheng</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
							<email>&lt;liuli@cuhk.edu.cn&gt;.s</email>
						</author>
						<title level="a" type="main">Learning Class Unique Features in Fine-Grained Visual Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-03-16">16 Mar 2021</date>
						</imprint>
					</monogr>
					<note>6 Shenzhen research institute of big data, the Chinese University of Hong Kong Shenzhen, Shenzhen, China. Correspondence to: Li Liu</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A major challenge in Fine-Grained Visual Classification (FGVC) is distinguishing various categories with high inter-class similarity by learning the feature that differentiate the details. Conventional cross entropy trained Convolutional Neural Network (CNN) fails this challenge as it may suffer from producing inter-class invariant features in FGVC. In this work, we innovatively propose to regularize the training of CNN by enforcing the uniqueness of the features to each category from an information theoretic perspective. To achieve this goal, we formulate a minimax loss based on a game theoretic framework, where a Nash equilibria is proved to be consistent with this regularization objective. Besides, to prevent from a feasible solution of minimax loss that may produce redundant features, we present a Feature Redundancy Loss (FRL) based on normalized inner product between each selected feature map pair to complement the proposed minimax loss. Superior experimental results on several influential benchmarks along with visualization show that our method gives full play to the performance of the baseline model without additional computation and achieves comparable results with stateof-the-art models.</p><p>Convolutional Neural Networks (CNN) achieves a great success in the computer vision domain. The large diversity in standard visual recognition tasks make it possible for CNN to well learn discriminative features by minimizing the cross entropy (CE) loss. When it comes to Fine-Grained Visual Classification (FGVC), different categories with highly similar appearance leads to inter-class invariants and intra-class variants, which limit the performance of standard CE trained CNN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. Three images of similar bird species from CUB-200-2011 dataset are selected to present the major challenge in FGVC. The first row presents the original images and the second row presents the activation maps. The activation maps are obtained from forward propagating the image of Red Winged Blackbird to the CE trained CNN and extracting the penultimate layer feature maps. Then the channel that has the largest mean activation value is selected. An observation is that the same channel also has high activation value when we input the other two categories of images (Brewer Blackbird and Rusty Blackbird).</p><p>Over the past few years, FGVC has attracted lots of attention in research community. Early works <ref type="bibr" target="#b41">(Zhang et al., 2014;</ref><ref type="bibr" target="#b2">Branson et al.;</ref><ref type="bibr" target="#b38">Wei et al., 2016)</ref> utilize multi-stage architecture that consists of a localization network and a classification network. The localization network is responsible for detecting discriminative regions, which requires bounding box or part annotations for training. Then the classification network works on the cropped regions given by the localization network. However, these approaches depend on annotations and cannot be trained end-to-end, thus lead to extra training cost. To solve the above mentioned challenges, recent approaches manage to develop end-to-end networks that only require weak supervision. Commonly these approaches outperform baseline models by mimicking human actions like attention mechanism and part localization <ref type="bibr" target="#b9">Fu et al., 2017;</ref><ref type="bibr" target="#b30">Sun et al., 2018;</ref><ref type="bibr" target="#b35">Wang et al., 2018;</ref><ref type="bibr">Chen et al., 2019;</ref><ref type="bibr" target="#b5">Ding et al., 2019;</ref><ref type="bibr" target="#b37">Wang et al., 2020)</ref>. However, these works focus on discriminative parts on spatial domain. Even if the regions of interest are correctly cropped or detected, CNN will inevitably encode unnecessary information that may mix up arXiv:2011.10951v2 <ref type="bibr">[cs.</ref>LG] 16 Mar 2021 with other categories. As shown in <ref type="figure">Fig. 1</ref>, the problem is, although the model can well localize the discriminative parts, the channel that responsible for detecting the feature will also be activated when encounter a similar texture. That means localization is not sufficient for learning a good feature if the filters can not precisely encode the unique feature. It is a challenge to extract the features that contain unique information about the categories.</p><p>In this work, we propose an explicit regularization objective for encoding unique features with a theoretical guarantee. Our motivation is based on an assumption that a unique feature should contain only the information of a specific category and not any other categories. In other words, for a given image, we expect the extracted features to be highly correlated with the target class without extra information about the non-target classes. We call this kind of features Class Unique Features (CUFs). To achieve this goal, we formulate CUF using the Mutual Information (MI), from which we deduce an explicit regularization objective, i.e., Maximum Non-Target distribution Entropy (MaxNTE). To efficiently optimize the objective, we propose a game theoretic framework to simplify the problem formulation. Under this game theoretic framework, the existence of Nash equilibria and the consistency between the outcome and our objective are proved rigorously.</p><p>In summary, our contribution includes:</p><p>1. We innovatively formulate our assumption to an ideal CUF learner from an information theoretic perspective and deduce an explicit regularization objective.</p><p>2. We construct a game-theoretic framework between the model and the adversary. On this basis, we arrive at a simple yet efficient minimax (MM) loss to achieve the regularization goal. To reduce the feature redundancy brought by the minimax loss, we further propose a Feature Redundancy Loss (FRL), encouraging the model to focus on multiple discriminative parts, as a complement to the minimax loss.</p><p>3. Experimental results on influential benchmarks of both FGVC and standard visual classification show that our method outperforms the baseline models by a large margin and achieves state-of-the-art (SOTA) results on FGVC-Aircraft and Standard Cars dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Fine-Grained Visual Classification</head><p>Recently, FGVC is a research hotspot in the field of computer vision. We mainly discuss related works according to the following three research branches.</p><p>Attention mechanism and part localization were ex-plored to settle this problem, as the model is able to learn to pay attention to the region or features that contain inter-class variations. Benefited from the interaction of part learning and feature learning, Multi-Attention CNN (MA-CNN) was proposed in  to extract part-based finegrained features.  utilized recurrent architecture to repeatedly crop and scale the regions of interest by attention mechanism. <ref type="bibr" target="#b30">(Sun et al., 2018)</ref> proposed One-Squeeze Multi-Excitation that generate multiple attention map based on Multi-Attention Multi Class Constraint to efficiently obtain the highly discriminative part.  used 1 ? 1 convolution kernel as a discriminative patch detector and designed an asymmetric, multi-channel structure to enhance the learning of discriminative mid-level patches. <ref type="bibr">(Chen et al., 2019)</ref> shuffled the local regions to enforce the network to focus on the most discriminative patches. <ref type="bibr" target="#b5">(Ding et al., 2019)</ref> used sparse attention for feature sampling to capture detailed visual evidence without losing the context information.</p><p>High-order statistics were explored for aggregating features to improve the first-order statistics such as max pooling and average pooling because they were difficult to capture the diversity of features among different categories. <ref type="bibr" target="#b23">(Lin et al., 2015)</ref> produced an image descriptor via pooling the outer product from two CNN feature extractor subbranches, which was able to model local pairwise feature interactions in a translational invariant manner. <ref type="bibr" target="#b10">(Gao et al., 2016)</ref> approximated bilinear pooling operation by applying low-dimensional approximation of the polynomial kernel to speed up the computation. <ref type="bibr" target="#b34">(Wang et al., 2019a)</ref> inserted Matrix Power Normalized COVariance (MPN-COV) block into the final layer of convolutions to obtain a global representation by second order statics.</p><p>Regularization based methods usually do not need extra computation and thus are much light weight compared with the above mentioned methods. They developed efficient training manner that can boost the performance of simple baseline models. <ref type="bibr" target="#b7">(Dubey et al., 2018b)</ref> formulated the relation between model selection and feature diversity, and utilizing the idea of maximum entropy to minimize the lower bound of Frobenius norm of the weights and thus improved the performance of models in fine-grained visual tasks. <ref type="bibr" target="#b6">(Dubey et al., 2018a)</ref> minimized the L2 distance between the prediction probability distribution of the random sample pairs of the training set to confuse the network and prevent from overfitting. Our method is also based on regularization of output distribution, and thus can be a simple and lightweight tool to be used among similar tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Label smoothing</head><p>Label smoothing <ref type="bibr" target="#b31">(Szegedy et al., 2016)</ref> was first proposed to prevent deep learning model from overconfident in clas-sification problem. The characteristic of softmax function makes it impossible for a model to convergent to the hard 0 and 1 targets ). Thus the model may keep seeking for extreme prediction and become overfitting. Label smoothing introduces uniform noise distribution u to the ground truth labels by replacing 0 and 1 targets with n?1 and 1 ? , where is a hyperparameter determines the amount of smoothing. Our method produces uniformly distributed probabilities on non-target class, which is similar to the ground truth of label smoothing. However, our method is different from label smoothing in terms of motivation, training manner and resulting outputs. More precisely, 1) our motivation is to achieve an assumption on MI between extracted features and output distribution, while LS is proposed to inject noise to the labels to prevent from extreme logits and overfitting. 2) We use a minimax loss to achieve our objective while LS directly takes the designed target to supervise the model. 3) In terms of the regularization results, our method leads to uniform distribution on non-target classes, but LS can not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Mutual Information</head><p>Mutual Information (MI) is a measure of information in information theory, which indicates the mutual dependencies between two random variables. More specifically, it quantifies the amount of information about another random variable when one of the variables is observed. MI has been used in the field of deep learning. <ref type="bibr" target="#b32">(Tishby &amp; Zaslavsky, 2015)</ref> firstly showed that Deep Neural Networks (DNN) can be quantified by the mutual information between the layers and the input and output variables. They provide a novel perspective that the goal of DNN is to optimize Information Bottleneck (IB) trade off between compression and prediction.  wielded the rich knowledge about mutual information into the construction of encoder, called Deep InforMax (DIM), which maximized the mutual information between the inputs and the high-level representation. Belghazi et al. <ref type="bibr" target="#b0">(Belghazi et al., 2018)</ref> proposed Mutual Information Neural Estimator (MINE) and applied it to Generative Adversarial Networks (GANs) <ref type="bibr" target="#b13">(Goodfellow et al., 2014)</ref> to improve the reconstruction quality and alleviate mode-drop in GANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.">Game Theory</head><p>Game Theory provides mathematical models of strategic interaction among intelligent decision makers <ref type="bibr" target="#b26">(Myerson, 1991)</ref>. It is a mathematical theory and method to study the phenomenon of competition, and was studied in <ref type="bibr" target="#b27">(Osborne et al., 2004)</ref> the interaction between the formulaic incentive structures. With the increasing popularity of Artificial Intelligent, game theory has been applied to different fields including Multi-Agent Reinforcement Learning <ref type="bibr" target="#b1">(Bowling &amp; Veloso, 2000)</ref> and GANs <ref type="bibr" target="#b13">(Goodfellow et al., 2014;</ref><ref type="bibr">Good-fellow, 2016)</ref>. In this work, we apply the game theory to the object recognition task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>Our goal is to extract features that do not contain information from non-target classes, i.e., CUF. This can be formulated by minimizing the MI between extracted feature and the non-target output distribution. By the formulation, we further deduce a regularization objective, where a key finding is derived (i.e., all output probabilities over non-target classes should be uniformly distributed). To efficiently optimize this objective function, we propose a minimax loss to simplify the reformulated regularization objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Mutual Information based Problem Formulation</head><p>We denote the input space by X , the label space by C = {1, 2, . . . , n}, where n &gt; 2 is the number of classes. The training data are all i.i.d sample pairs (x, y), where y is in the form of a n-dimensional one-hot vector.</p><p>Let ? be a parametric function mapping from the input space to the feature space. Overall parameter set of the model is ?. Consider one of the fix target categories t ? C, and the corresponding input X t (i.e., images that belong to the class t).? C\t is the predicted output of non-target classes with distribution</p><formula xml:id="formula_0">q C\t = sof tmax(z C\t ), where z C\t = [z 1 , z 2 , . . . , z t?1 , z t+1 , . . . , z n ].</formula><p>Note that all the random variables y, z, q, t are dependent on input x, here we omit the dependence for brevity.</p><p>The problem formulation can be written as minimizing the MI between predicted output of non-target classes? C\t and extracted features ?(X t ), i.e. I ? (? C\t ; ?(X t )). However, this MI cannot be computed in practice as the distribution of ?(X t ) is intractable. To resolve the intractability issue, we apply the data processing inequality (Cover &amp; Thomas, 2012) and use the obtained upper bound, the MI between input X t of class t and predicted output of non-target classe? Y C\t , as our objective:</p><formula xml:id="formula_1">I ? (? C\t ; X t ),<label>(1)</label></formula><p>where I ? (? C\t ; X t ) represents the mutual information under model parameter ?.</p><p>According to the property of MI, i.e., I(A; B) = H(A) ? H(A|B), where H is the entropy, and A, B are two random variables, Eq. 1 can be decomposed into the difference between entropy and conditional entropy:</p><formula xml:id="formula_2">I ? (? C\t ; X t ) = H ? (? C\t ) ? H ? (? C\t |X t ),</formula><p>where H ? (?) denotes the entropy under model parameter ?. Computing H ? (? C\t ) involves the marginalization over X t which is computationally intractable in practice. Thus, instead of directly optimizing Eq. 1, we consider optimizing its upper bound. From the fact that entropy reaches its upper bound when all the probabilities are equal, we have:</p><formula xml:id="formula_3">H ? (? C\t ) ? ?(n ? 1) 1 n ? 1 log 1 n ? 1 = log (n ? 1),</formula><p>The following lemma gives a theoretic justification for using the upper bound to replace Eq. 1.</p><p>Lemma 1. When the conditional probability distribution over non-target classes is uniform, the MI in Eq. 1 is 0, and hence I ? (? C\t ; ?(X t )) = 0.</p><p>Lemma 1 shows that making the conditional distribution of the non-target classes uniform is desired. Thus we formulate the problem using the upper bound as maximizing H ? (? C\t |X t ), which promotes the distribution to be uniform. When the mutual information is I ? (? C\t ; ?(X t )) = 0, this suggests that the extracted features contains no information about the non-target classes.</p><p>In practice, we maximize the empirical conditional entropy for each class t:</p><formula xml:id="formula_4">1 N t Nt i=1 H ? (? C\t |x t i ),</formula><p>where N t is the number of training samples in class t.</p><p>We use the empirical conditional entropy as a regularization term with a weight parameter ? into the CE objective function to form the overall objective function (i.e., MaxNTE):</p><formula xml:id="formula_5">L M axN T E = E x?X [D CE (y||q(x; ?)) ? ?H ? (? C\t |X t )].<label>(2)</label></formula><p>Here, H ? (? C\t |X t ) has a reachable upper bound. However, directly taking this as the objective function may not be the best choice for our goal, since the gradients become extremely small when closing to the upper bound. From Lemma 1, we know a sufficient condition for I ? (? C\t ; ?(X t )) = 0 is to enforce the conditional distribution of non-target classes to be uniform. In the following, we propose an efficient minimax loss based on game theory to achieve this target, which is lightweight and also insensitive to the choice of hyper-parameter. The comparison of MaxNTE and the new proposed loss will be provided in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Game Theoretic Framework</head><p>In this section, we introduce the game theoretic framework in detail. The resulting loss function will be shown in Eq. 3, which is used as the major part of our main method in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">PRELIMINARIES</head><p>Here we introduce some basic game-theoretic definitions <ref type="bibr" target="#b26">(Myerson, 1991</ref>) that we will use later.</p><p>Definition 1. A strategic game is a tuple G = I, (A i ) i?I , (u i ) i?I , where I is a nonempty set of players, A i is the set of actions available to each player i ? I, A = i?I A i is the profiles of actions and u i : A ? R defines the payoff function for each player i ? I. A two player strictly competitive game or zero-sum game is the strategic game G with I = {1, 2} and for all a ? A: u 1 (a) = ?u 2 (a).</p><p>Definition 2. A mixed strategy set S i is the set of all probability distributions over A i . s i ? S i defines a mixed strategy for each player i ? I, and s i (a k i ) is the probability that player i plays a k i ? A i . In a two player game, the expected payoff of player i playing a mixed strategy against pure strategy a * ?i can be calculated as:</p><formula xml:id="formula_6">U i (s) = a k i ?Ai s i (a k i )u i (a k i , a * ?i ).</formula><p>Likewise, the expected payoff of playing a pure strategy a * i against mixed strategy can be calculated as:</p><formula xml:id="formula_7">U i (s) = a k ?i ?A?i s ?i (a k ?i )u i (a * i , a k ?i ),</formula><p>where ?i denotes the player other than i.</p><p>Definition 3. Let G be the strategic game, i ? I be a player, and s ?i ? S ?i be a strategy profile of players other than i. Then a strategy s * i ? S i is a best response of player i to s ?i if:</p><formula xml:id="formula_8">?s i ? S i , U i (s * i , s ?i ) ? U i (s i , s ?i )</formula><p>. Definition 4. A Nash equilibrium of the strategic game G is a action profile s * ? S such that for every player i, s * i is the best response to s * ?i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">p ? q ZERO-SUM STRATEGIC GAME</head><p>We define a zero-sum strategic game played between the model and a designed adversary with loss of the model defined as D CE (p||q). We let p be the ground truth label vector, normally one-hot encoding, while in our work we specifically design it for the objective. Here, p is the strategy of the adversary and q is the strategy of the model. We assume that the model is a classifier with confidence q t on the target class. The model aims to assign the rest of the probability 1 ? q t to non-target classes to minimize the loss. The adversary is the controller of the ground truth with fixed p t for the target class, and it aims to maximize the loss via adjusting the distribution on non-target class of p. This is a dynamic game that the two players play in order, in which the model goes first, and the adversary can adjust the strategy according to the previous action of the model. <ref type="figure" target="#fig_0">Fig. 2</ref> gives an overview of the proposed game theoretic framework.</p><p>Definition 5. For p t , q t ? (0, 1), the defined strategic game is a tuple G = (P, Q), (A P , A Q ), (u P , u Q ) with:</p><formula xml:id="formula_9">A P = {(p i ) 1?i?n , p i ? (0, 1), i =t p i = 1 ? p t }. A Q = {(q i ) 1?i?n , q i ? (0, 1), i =t q i = 1 ? q t }. u P = D CE (p||q) = ?u Q .</formula><p>By Definition 1, G is a two-player zero-sum game. In the following, we will prove the existence of Nash equilibrium.</p><p>Theorem 1. For p t , q t ? (0, 1), we have:</p><formula xml:id="formula_10">?q ? A Q , p * = arg max p D CE (p||q),</formula><p>where p * = (p * i ) 1?i?n :</p><formula xml:id="formula_11">p * i = ? ? ? ? ? p t , i = t; 1 ? p t , i = k; 0, otherwise ,</formula><p>for any k = arg min z (q C\t ) z .</p><p>Note that in the rest of the paper, if there exists more than one minimum value in q C\t , k = arg min z (q C\t ) z refers to randomly taking one of them.</p><p>Theorem 2. For p t , q t ? (0, 1), we have:</p><formula xml:id="formula_12">q * = arg min q?A Q D CE (p * ||q), where q * i = q t , i = t; 1?qt n?1 , otherwise.</formula><p>Theorem 1 gives the worst case payoff for the model q.</p><p>However, since p * depends on the index of the minimum value in q, they are not the best responses to each other. For example, when q = q * , the adversary chooses one of the indexes of the minimum values in q to determine p * . Once p * is fixed, q is no longer the best responses to p * , since there exist a better q to get a higher payoff (e.g., change the position of the minimum value). Thus, we need randomize p to avoid this situation. Specifically, when q = q * , the model uniformly distributes the probabilities over non-target classes. The adversary can randomly choose one of them since they are all the smallest value and the adversary's strategy becomes a mixed strategy. In this case, the two strategies form a Nash equilibrium. To show this mathematically, we give the following theorem.</p><p>Theorem 3. Define an action subset for the adversary:</p><formula xml:id="formula_13">a * P ? A P = ? ? ? ? ? (p i )|p i = ? ? ? ? ? p t , i = t; 1 ? p t , i = k; 0, otherwise ? ? ? ? ? ,</formula><p>where k = {1, 2, . . . , t ? 1, t + 1, . . . , n}.</p><p>For the model:</p><formula xml:id="formula_14">a * Q = (q i )|q i = q t , i = t; 1?qt n?1 , otherwise .</formula><p>Then we have the following strategies:</p><formula xml:id="formula_15">s * P (p) = 1 n?1 , p ? a * P ; 0,</formula><p>otherwise.  <ref type="figure">Figure 3</ref>. The feature redundancy problem in MM trained model. As shown in the first row, the three channels with the largest activation values detect the same region of the bird. That means the model's prediction is over dependent on the single feature. Once the region is cropped, as shown in the image below, the model cannot correctly predict the object category. The introduced FRL is shown to be able to eliminate this problem.</p><formula xml:id="formula_16">s * Q (q) = 1, q ? a * Q ; 0, otherwise,</formula><p>such that s * = (s * P , s * Q ) forms a Nash equilibrium.</p><p>The detailed proofs of the three theorems are in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3.">MINIMAX LOSS</head><p>The Nash equilibrium in a two player zero-sum game is equivalent to a minimax solution <ref type="bibr" target="#b8">(Ferreira et al., 2012)</ref>. Thus, by training with the worst-case payoff D CE (p * ||q), we expect that the model output ultimately converges to the best response q * . Finally our proposed minimax loss (MM) is defined as:</p><formula xml:id="formula_17">L M M = E x?X [D CE (p * ||q)] = E x?X [?p t log q(x; ?) t ? (1 ? p t ) log q(x; ?) k ],<label>(3)</label></formula><p>where k = arg min a (q C\t ) a . Here, we leave p t as a hyperparameter to weight the regularizer of the objective corresponding to the class t. When p t is set to 1 for all classes, the loss function is equivalent to the standard CE loss. It is worth noting that, in regularization methods such as label smoothing and confidence penalty, the number of log operations increases with the number of categories. MM has only one more log operation than the cross entropy loss, while gains more performance in many tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Feature Redundancy Loss</head><p>MM promotes the feature uniqueness of each category. There may exist more than one solutions that having this property. In some cases, the obtained features can be redundant, different feature maps of a specific category are almost the same, as shown in the first row of <ref type="figure">Fig. 3</ref>. We want the extracted features to be more diverse, because single feature can be unreliable especially when the training set is small. Combining multiple features for decision can avoid wrong prediction under unexpected cases such as occlusion and make the model more robust. The second row of <ref type="figure">Fig. 3</ref> shows the case that one of the important regions is blocked, in which MM trained model that rely on single feature fails to make a correct prediction. Therefore we add an additional regularization term to choose more diverse features while maintaining the class uniqueness.</p><p>To enforce the difference of feature maps, we use normalized inner product to measure the similarity among feature maps of top activation as a loss function, named Feature Redundancy Loss. Specifically, in each forward sample, we select the feature maps that has top K activation values before global average pooling. Let the shape of the selected feature maps ? = ?(x) from a sample x be (K, H, W ), we calculate the normalized inner product between each pairs.</p><formula xml:id="formula_18">L F RL = K?1 i=1 K j=i+1 ? i ? j ? i ? j<label>(4)</label></formula><p>The loss can be calculated parallelly using tensor operation, thus a simple yet efficient trick. Our final loss is the weighted sum of MM and FRL:</p><formula xml:id="formula_19">L = L M M + ?L F RL<label>(5)</label></formula><p>3. Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Experimental Setup</head><p>For evaluating our method, we use the following three benchmarks: CUB-200-2011 <ref type="bibr" target="#b33">(Wah et al., 2011)</ref>, FGVC-Aircraft <ref type="bibr" target="#b25">(Maji et al., 2013)</ref>, Stanford Cars <ref type="bibr" target="#b20">(Krause et al., 2013)</ref>. Further more, we assesses the effect of our method on standard visual classification benchmarks: CIFAR-10 <ref type="bibr" target="#b21">(Krizhevsky et al., 2009</ref>), CIFAR-100 <ref type="bibr" target="#b21">(Krizhevsky et al., 2009)</ref>, <ref type="bibr">STL-10 (Coates et al., 2011)</ref>. Different methods are compared using ResNet18 <ref type="bibr" target="#b16">(He et al., 2016)</ref>, VGGNet11 <ref type="bibr" target="#b29">(Simonyan &amp; Zisserman, 2014)</ref>, DenseNet161 <ref type="bibr" target="#b18">(Huang et al., 2017)</ref> as the backbone models. The statistics of six datasets and the implementation details are introduced in Appendix.</p><p>We first compare our proposed MM with MaxNTE. Then we quantitatively compare our proposed method with different methods on FGVC tasks as well as the standard visual classification tasks. Finally we conduct visualization to further show the effect of our method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Comparison between MaxNTE and MM</head><p>We compare MM with the MaxNTE that directly minimize Eq. 2 on CIFAR-10 dataset. Note that when p t set to 1 for all classes (i.e. 1 ? p t set to 0) and ? set to 0, both of the losses are equal to the cross entropy loss. In MaxNTE, as shown in <ref type="figure">Fig. 4, the left figure,</ref> the test accuracy is decreasing when ? is increasing, the maximum entropy reaches its bottleneck at about 2.12. The right figure shows that as 1 ? p t (hyperparameter) increases, the entropy gradually approaches its upper bound (log(n ? 1) = log 9 ? 2.19) with stable test accuracy. The results show that MM can achieve our goal in a more efficient way, thus we apply it to the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Quantitative results</head><p>Fine-Grained Visual Classification From <ref type="table" target="#tab_2">Table 1</ref>, our proposed method improves the performance of three baseline models (i.e.,  across all three datasets (i.e., <ref type="bibr">CUB-200-2011, FGVC-Aircraft and Stanford Cars)</ref>. For example, training VGGNet-11 with MM obtained significant improvements of 2.50% on average across three datasets compared with CE. LS performs better than CE since it also encourages the model to produce an output close to our objective. Besides, DenseNet-161 with MM achieves best results compared with other baselines.</p><p>The overall experimental results compared with recent works including SOTA are shown in <ref type="table" target="#tab_3">Table 2</ref>. Our methods outperforms regularization-based methods (e.g. Max-Ent and PC) across all three datasets. While SOTA models achieve excellent results, they rely on extra structure or computational cost. Our proposed methods can fully bring out the potential of the baseline models and achieve SOTA in FGVC-Aircraft and Stanford Cars by only regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Standard Visual Classification</head><p>We compare our method with two output regularization based methods: Confidence Penalty (CP) <ref type="bibr" target="#b28">(Pereyra et al., 2017)</ref> and Label Smoothing (LS) <ref type="bibr" target="#b31">(Szegedy et al., 2016)</ref>. As shown in <ref type="table" target="#tab_4">Table 3</ref>. our method outperforms several output regularization based methods across almost all the datasets and architectures. In CIFAR-10 and CIFAR-100, the improvements are not significant, and the test accuracy of VGGNet-11 with confidence penalty is slightly higher than that of our method in CIFAR-10. In STL-10, our method outperforms three baselines by a large margin.</p><p>Ablation Study We perform ablation experiments to show how different parts of our method work. As shown in <ref type="table" target="#tab_5">Table  4</ref>, both MM and FRL greatly improves the performance of the baseline model. MM brings greater improvement overall, because CE can not precisely encode the unique feature even with FRL. Moreover, as FRL works by reducing the similarity of different feature maps, the effect of FRL on baseline model shows that feature redundancy also exists in regular training using CE. It is worth noting that hyperparameters ?, K and p t have little affect on the proposed MM and FRL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Qualitative Result</head><p>To show in detail how our approach works, we visualize the penultimate layer feature maps with top activation values. We up-sample the feature maps to match the original image by bi-linear interpolation. As shown in <ref type="figure">Fig. 5</ref>, CE trained model (in the first row, column 2, 3) correctly localize the important parts of the bird, but it will cover irrelevant areas, which may lead to false triggering of other features (see the second and the third row). The filters trained by MM that responsible for detecting the unique features of Red Winged Blackbird do not response to other species as CE trained filters do. <ref type="figure">Fig. 6</ref> shows the second row demonstrate the different effects of our proposed MM and FRL. With our regularization, the model become more concentration so as to avoid introducing information about irrelevant categories. However,  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this work, we provide an information theoretic point of view, to address the major challenge in FGVC, i.e., learning the features unique to categories. We formulate the aim to minimizing the MI between the learned features and non-target classes, based on which we deduce an explicit regularization objective. To efficiently achieve our objective, we construct a game-theory based framework to derive a stable minimax loss, which is proved to converge to a Nash equilibrium. Furthermore, FRL is proposed to avoid over depending on single feature as a complement of MM. As a  <ref type="figure">Figure 5</ref>. The comparison of the learned filters of models trained with CE and our MM. We select the three images of similar bird species to demonstrate the effect of our method. The two channels with the largest activation values according to the forward pass of Red Winged Blackbird (the images in the first row) are presented. Although CE trained filters are able to capture the important features, they are confused with other categories. MM trained filters can extract precisely the unique features which will not be activated when encounter similar objects. result, the model is able to extract the most distinctive parts of the object and reduce the influence of background noise. By only regularization, our proposed methods bring the potential of the baseline models into full play and achieves competitive results with SOTA models without extra computational cost.  <ref type="figure">Figure 6</ref>. The comparison of the filters with largest activation values of different models. The left side present the original image. On the right side, the first row is the model trained with CE as the baseline. The second row is the model trained with MM and the third row is the model trained with MM and FRL. Compared with baseline, MM minimizes the activation of irrelevant regions, so as to produce a clear and focused activation map. On the basis of the effect of MM, FRL encourages the model to focus on different discriminative regions.</p><p>In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5157-5166, 2019.</p><p>Coates, A., Ng, A., and Lee, H. An analysis of singlelayer networks in unsupervised feature learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 215-223, 2011.</p><p>Cover, T. M. and Thomas, J. A. Elements of Information Theory. John Wiley &amp; Sons, 2012.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>(a) During the training phase (before convergence), the model keep promoting the smallest probability in the non-target model output by assign 1 ? pt to the index of the minimum value in q C\t , i.e.., q3 in (a). (b) After iterations of training, the model output distribution will finally be uniformly distributed over non-target classes. When reaching a convergence, a Nash equilibrium exists between the optimal solution of the model and the adversary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Comparison with three baseline models.</figDesc><table><row><cell>Backbone</cell><cell>Method</cell><cell>CUB</cell><cell>Aircraft</cell><cell>Cars</cell></row><row><cell></cell><cell>CE</cell><cell>81.32 ? 0.31</cell><cell>89.89 ? 0.14</cell><cell>88.50 ? 0.21</cell></row><row><cell>ResNet-18</cell><cell>LS</cell><cell>81.83 ? 0.22</cell><cell>89.77 ? 0.27</cell><cell>91.06 ? 0.18</cell></row><row><cell></cell><cell>MM</cell><cell>83.14 ? 0.18</cell><cell>90.37 ? 0.14</cell><cell>91.74 ? 0.11</cell></row><row><cell></cell><cell>CE</cell><cell>77.76 ? 0.28</cell><cell>85.38 s? 0.66</cell><cell>87.32 ? 0.47</cell></row><row><cell>VGGNet-11</cell><cell>LS</cell><cell>77.94 ? 0.23</cell><cell>87.57 ? 0.19</cell><cell>89.46 ? 0.32</cell></row><row><cell></cell><cell>MM</cell><cell>80.41 ? 0.15</cell><cell>87.72 ? 0.22</cell><cell>89.83 ? 0.28</cell></row><row><cell>DenseNet-161</cell><cell>CE LS MM</cell><cell>86.69 ? 0.32 87.63 ? 0.15 87.98 ? 0.14</cell><cell>90.94 ? 0.15 92.65 ? 0.21 93.34 ? 0.19</cell><cell>94.21 ? 0.12 94.27 ? 0.16 94.72 ? 0.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Comparison with SOTA methods. * means the best performance among regularization-based methods.the regularization objective will lead to redundancy features, i.e. all the feature maps point on the same area, which means that the model become overdependent on single feature, and may be harmful for generalization. FRL well eliminates this problem by forcing the model to distract its attention. The final results are shown in the last row ofFig. 6, the model attention become both focused and diversified. With a clearer activation maps, we can better see how the model works.</figDesc><table><row><cell>Method</cell><cell>CUB</cell><cell>Aircraft</cell><cell>Cars</cell></row><row><cell>B-CNN ((Lin et al., 2015))</cell><cell>84.1</cell><cell>84.1</cell><cell>91.3</cell></row><row><cell>CBP ((Gao et al., 2016))</cell><cell>84.3</cell><cell>84.1</cell><cell>91.2</cell></row><row><cell>KP ((Cui et al., 2017))</cell><cell>86.2</cell><cell>86.9</cell><cell>92.4</cell></row><row><cell>iSQRT-COV ((Li et al., 2018))</cell><cell>88.7</cell><cell>91.4</cell><cell>93.3</cell></row><row><cell>MA-CNN ((Zheng et al., 2017))</cell><cell>86.5</cell><cell>91.8</cell><cell>92.8</cell></row><row><cell>RA-CNN ((Fu et al., 2017))</cell><cell>85.3</cell><cell>92.5</cell><cell>93.0</cell></row><row><cell>MAMC ((Sun et al., 2018))</cell><cell>86.5</cell><cell>-</cell><cell>93.0</cell></row><row><cell>DFL-CNN ((Wang et al., 2018))</cell><cell>87.4</cell><cell>-</cell><cell>93.8</cell></row><row><cell>NTS-Net ((Yang et al., 2018))</cell><cell>87.5</cell><cell>91.4</cell><cell>93.9</cell></row><row><cell>MaxEnt ((Dubey et al., 2018b))</cell><cell>86.5</cell><cell>89.2</cell><cell>92.9</cell></row><row><cell>PC ((Dubey et al., 2018a))</cell><cell>86.9</cell><cell>89.8</cell><cell>93.0</cell></row><row><cell>DCL ((Chen et al., 2019))</cell><cell>87.8</cell><cell>93.0</cell><cell>94.5</cell></row><row><cell>S3N ((Ding et al., 2019)</cell><cell>88.5</cell><cell>92.8</cell><cell>94.7</cell></row><row><cell>DF-GMM ((Wang et al., 2019b)</cell><cell>88.8</cell><cell>93.8</cell><cell>94.8</cell></row><row><cell>MGE-CNN ((Zhang et al., 2019)</cell><cell>89.4</cell><cell>-</cell><cell>93.9</cell></row><row><cell>GCL ((Wang et al., 2020)</cell><cell>88.3</cell><cell>93.2</cell><cell>94.0</cell></row><row><cell>API-Net ((Zhuang et al., 2020))</cell><cell>90.0</cell><cell>93.9</cell><cell>95.3</cell></row><row><cell>ELoPE ((Hanselmann &amp; Ney, 2020)</cell><cell>88.5</cell><cell>93.5</cell><cell>95.0</cell></row><row><cell>DFL ((Liu et al., 2020)</cell><cell>89.1</cell><cell>93.4</cell><cell>94.3</cell></row><row><cell>CIN ((Gao et al., 2020))</cell><cell>88.1</cell><cell>92.8</cell><cell>94.5</cell></row><row><cell>ACNet ((Ji et al., 2020)</cell><cell>88.1</cell><cell>92.4</cell><cell>94.6</cell></row><row><cell>DenseNet161+MM(Ours)</cell><cell>88.0</cell><cell>93.3</cell><cell>94.7</cell></row><row><cell>DenseNet161+MM+FRL(Ours)</cell><cell>88.5*</cell><cell>94.0</cell><cell>95.2*</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Comparison with three baseline models on standard visual classification tasks.</figDesc><table><row><cell>Backbone</cell><cell>Method</cell><cell>CIFAR-10</cell><cell>CIFAR-100</cell><cell>STL-10</cell></row><row><cell></cell><cell>CE</cell><cell>92.26?0.08</cell><cell>70.37?0.33</cell><cell>79.80?0.31</cell></row><row><cell>VGGNet-11</cell><cell>CP LS</cell><cell>92.62?0.05 92.28?0.06</cell><cell>70.30?0.19 71.34?0.07</cell><cell>80.17?0.14 80.41?0.08</cell></row><row><cell></cell><cell>MM</cell><cell>92.43?0.06</cell><cell>71.62?0.18</cell><cell>82.26?0.09</cell></row><row><cell></cell><cell>CE</cell><cell>94.94?0.12</cell><cell>75.79?0.03</cell><cell>83.44?0.23</cell></row><row><cell>ResNet-18</cell><cell>CP LS</cell><cell>95.11?0.01 95.08?0.11</cell><cell>76.01?0.31 76.24?0.21</cell><cell>83.75?0.02 84.03?0.01</cell></row><row><cell></cell><cell>MM</cell><cell>95.33?0.12</cell><cell>76.64?0.07</cell><cell>85.42?0.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Ablation study using DenseNet-161 as the baseline model.</figDesc><table><row><cell>Minimax</cell><cell>FRL</cell><cell>CUB</cell><cell>Aircraft</cell><cell>Cars</cell></row><row><cell></cell><cell></cell><cell>86.69 ? 0.32</cell><cell>91.26 ? 0.15</cell><cell>94.21 ? 0.12</cell></row><row><cell></cell><cell></cell><cell>87.98 ? 0.14</cell><cell>93.34 ? 0.19</cell><cell>94.74 ? 0.11</cell></row><row><cell></cell><cell></cell><cell>87.14 ? 0.16</cell><cell>92.71 ? 0.48</cell><cell>94.71 ? 0.04</cell></row><row><cell></cell><cell></cell><cell>88.48 ? 0.24</cell><cell>93.96 ? 0.11</cell><cell>95.18 ? 0.14</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mutual information neural estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rajeshwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="531" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An analysis of stochastic game theory for multiagent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Veloso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Bird species categorization using pose normalized deep convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tech</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Destruction and construction learning for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Kernel pooling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2921" to="2930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Selective sparse sampling for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6599" to="6608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pairwise confusion for fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="70" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Maximumentropy fine grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="637" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Minimax theorem and nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A M</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C P</forename><surname>Matos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Filipe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Coelho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for finegrained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4438" to="4446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Compact bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="317" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Channel interaction networks for fine-grained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10818" to="10825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.00160</idno>
		<title level="m">tutorial: Generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Learning</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Elope: Fine-grained visual classification with efficient localization, pooling and embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hanselmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1247" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<title level="m">Learning deep representations by mutual information estimation and maximization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attention convolutional binary neural tree for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10468" to="10477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International IEEE Workshop on 3D Representation and Recognition</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards faster training of global covariance pooling networks by iterative matrix square root normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="947" to="955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Filtration and distillation: Enhancing region attention for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11555" to="11562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Game theory: Analysis of conflict harvard univ</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Myerson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<publisher>Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">An introduction to game theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Osborne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Oxford university press</publisher>
			<biblScope unit="volume">3</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Regularizing neural networks by penalizing confident output distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06548</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-attention multi-class constraint for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="805" to="821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep learning and the information bottleneck principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zaslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Information Theory Workshop (ITW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.06836</idno>
		<title level="m">Deep cnns meet global covariance pooling: Better representation and generalization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning a discriminative filter bank within a cnn for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4148" to="4157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Weakly supervised fine-grained image classification via correlation-guided discriminative learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1851" to="1860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Graphpropagation based correlation learning for weakly supervised fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12289" to="12296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Mask-cnn: Localizing parts and selecting descriptors for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06878</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning to navigate for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="420" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning a mixture of granularity-specific experts for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8331" to="8340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Partbased r-cnns for fine-grained category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="834" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning multiattention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5209" to="5217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning attentive pairwise interaction for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">2002</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
