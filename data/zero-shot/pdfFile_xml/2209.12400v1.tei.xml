<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generalized Parametric Contrastive Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Jiequan</forename><surname>Cui</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Zhisheng</forename><surname>Zhong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Zhuotao</forename><surname>Tian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Shu</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Bei</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
						</author>
						<title level="a" type="main">Generalized Parametric Contrastive Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Representation Learning</term>
					<term>Contrastive Learning</term>
					<term>OOD Robustness</term>
					<term>Long-tailed Recognition</term>
					<term>Semantic Segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose the Generalized Parametric Contrastive Learning (GPaCo/PaCo) which works well on both imbalanced and balanced data. Based on theoretical analysis, we observe supervised contrastive loss tends to bias on high-frequency classes and thus increases the difficulty of imbalanced learning. We introduce a set of parametric class-wise learnable centers to rebalance from an optimization perspective. Further, we analyze our GPaCo/PaCo loss under a balanced setting. Our analysis demonstrates that GPaCo/PaCo can adaptively enhance the intensity of pushing samples of the same class close as more samples are pulled together with their corresponding centers and benefit hard example learning. Experiments on long-tailed benchmarks manifest the new state-of-the-art for long-tailed recognition. On full ImageNet, models from CNNs to vision transformers trained with GPaCo loss show better generalization performance and stronger robustness compared with MAE models. Moreover, GPaCo can be applied to semantic segmentation task and obvious improvements are observed on 4 most popular benchmarks. Our code is available at https://github.com/dvlab-research/Parametric-Contrastive-Learning. .hk; ? J. Jia and S. Liu are with the SmartMore.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>C ONVOLUTIONAL neural networks (CNNs) have achieved great success in various tasks, including image classification <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, object detection <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> and semantic segmentation <ref type="bibr" target="#b4">[5]</ref>. Especially, with the rise of neural network search <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, performance of CNNs have further taken a big step. However, The huge progress highly depends on large-scale and high-quality datasets, such as ImageNet <ref type="bibr" target="#b10">[11]</ref>, MS COCO <ref type="bibr" target="#b11">[12]</ref> and Places <ref type="bibr" target="#b12">[13]</ref>. But when deal with real-world applications, generally we face the long-tailed distribution problem -a few classes contain many instances, while most classes contain only a few instances. Learning in such an imbalanced setting is challenging as the low-frequency classes can be easily overwhelmed by high-frequency ones. Without considering this situation, CNNs will suffer from significant performance degradation.</p><p>Contrastive learning <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> is a major research topic due to its success in self-supervised representation learning. <ref type="bibr">Khosla</ref>  <ref type="bibr" target="#b19">[20]</ref> extend non-parametric contrastive loss into non-parametric supervised contrastive loss by leveraging label information, which trains representation in the first stage and learns the linear classifier with the fixed backbone in the second stage. Though supervised contrastive learning works well in a balanced setting, for imbalanced datasets, our theoretical analysis shows that highfrequency classes will have a higher lower bound of loss and contribute much higher importance than low-frequency classes when equipping it in training. This phenomenon leads to model bias on high-frequency classes and increases the difficulty of imbalanced learning. As shown in <ref type="figure" target="#fig_3">Fig. 2</ref>, when the model is trained with supervised contrastive loss on ImageNet-LT, the gradient norm varying from the most frequent class to the least one is rather steep. In particular, the gradient norm dramatically decreases for the top 200 most frequent classes.</p><p>Previous work <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> explored to rebalance in traditional supervised cross-entropy learning. In this paper, we tackle the above mentioned imbalance issue in supervised contrastive learning and make use of contrastive learning for long-tailed recognition.</p><p>To rebalance in supervised contrastive learning, we introduce a set of parametric class-wise learnable centers into supervised contrastive learning. We name our algorithm Parametric Contrastive Learning (PaCo) shown in <ref type="figure" target="#fig_5">Fig. 3</ref> (a). With such a simple and yet effective operation, we theoretically prove that the optimal values for the probability that two samples are a true positive pair (belonging to the same class), varying from the most frequent class to the least frequent class, are more balanced. Thus their lower bound of loss values are better organized. This phenomenon means the model takes more care of low-frequency classes, making the PaCo loss benefit imbalanced learning. <ref type="figure" target="#fig_3">Fig. 2</ref> shows that, with our PaCo loss in training, gradient norm varying from the most frequent class to the least one are moderated better than supervised contrastive learning, which matches our analysis.</p><p>Further, we analyze the PaCo loss under a balanced setting. Our analysis demonstrates that with more samples clustered around their corresponding centers in training, the PaCo loss increases the intensity of pushing samples of the same class close, which benefits hard examples learning.</p><p>MoCo <ref type="bibr" target="#b15">[16]</ref> enables small mini-batch training for selfsupervised contrastive learning with a momentum encoder and a queue. <ref type="bibr" target="#b33">[34]</ref> claims that it is the "stop-gradient" but not the momentum encoder that is necessary to avoid collapsing solutions for self-supervised learning. We examine the ViT-L +0.3 -1.7 -2.4 +0.0 +2.8</p><p>(c) Comparison with MAE <ref type="bibr" target="#b13">[14]</ref> ViT models on full ImageNet and out-of-distribution robustness.   Rebalance in contrastive learning. We collect the average L2 norm of the gradient of weights in the last classifier layer on ImageNet-LT. Category indices are sorted by their image counts. The gradient norm varying from the most frequent class to the least one is steep for supervised contrastive learning <ref type="bibr" target="#b19">[20]</ref>. In particular, the gradient norm dramatically decreases for the top 200 most frequent classes. Trained with PaCo, the gradient norm is better balanced. effects of each component in PaCo including augmentation strategy, loss function (relation to multi-task), momentum encoder and queue size. Simplifying it to Generalized Parametric Contrastive Learning (GPaCo) by removing the momentum encoder boosts model performance and robustness. Moreover, GPaco demonstrates its great generality with various tasks in <ref type="figure" target="#fig_1">Fig 1.</ref> Finally, we conduct experiments on imbalanced data including long-tailed version of CIFAR <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, ImageNet <ref type="bibr" target="#b36">[37]</ref>, Places <ref type="bibr" target="#b36">[37]</ref> and iNaturalist 2018 <ref type="bibr" target="#b37">[38]</ref>. Experimental results show that we create a new record for long-tailed recognition. On balanced data, We testify the effectiveness of GPaCo on full ImageNet <ref type="bibr" target="#b10">[11]</ref> and CIFAR <ref type="bibr" target="#b38">[39]</ref>. Compared with MAE models, we achieve better generalization ability and stronger robustness. With GPaCo loss for semantic segmentation, obvious improvements are obtained on popular benchmarks including ADE20K <ref type="bibr" target="#b39">[40]</ref>, COCO-Stuff <ref type="bibr" target="#b40">[41]</ref>, PASCAL Context <ref type="bibr" target="#b41">[42]</ref> and Cityscapes <ref type="bibr" target="#b42">[43]</ref>. Our key contributions are as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We identify the shortcoming of supervised contrastive learning under an imbalanced setting -it tends to bias high-frequency classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We extend supervised contrastive loss to the PaCo loss, which is more friendly to imbalanced learning, by introducing a set of parametric class-wise learnable centers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We examine the necessary of components in PaCo and simplify PaCo to GPaCo, observing that the momentum encoder can hurt model performance and should be removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>GPaCo can benefit the training on imbalanced data and balanced data varying from CNNs to vision transformers. Experiments on tasks e.g., longtailed classification, full ImageNet/CIFAR classifcation, out-of-distribution robustness, and semantic segmentation demonstrate the generality of GPaCo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Re-sampling/re-weighting. The most classical way to deal with long-tailed datasets is to over-sample low-frequency class images <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref> or under-sample highfrequency class images <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b46">[47]</ref>. However, Oversampling can suffer from heavy over-fitting to low-frequency classes especially on small datasets. For under-sampling, discarding a large portion of high-frequency class data inevitably causes degradation of the generalization ability of CNNs. Re-weighting <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref> the loss functions is an alternative way to rebalance by either enlarging weights on more challenging and sparse classes or randomly ignoring gradients from high-frequency classes <ref type="bibr" target="#b53">[54]</ref>. However, with large-scale data, re-weighting makes CNNs difficult to optimize during training <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>.</p><p>One/two-stage Methods. Since deferred re-weighting and re-sampling were proposed by Cao <ref type="bibr" target="#b35">[36]</ref>, Kang <ref type="bibr" target="#b27">[28]</ref> and Zhou <ref type="bibr" target="#b54">[55]</ref> observed re-weighting or re-sampling strategies could benefit classifier learning while hurting representation learning. Kang <ref type="bibr" target="#b27">[28]</ref> proposed to decompose representation and classifier learning. It first trains the CNNs with uniform sampling, and then fine-tune the classifier with classbalanced sampling while keeping parameters of representation learning fixed. Zhou <ref type="bibr" target="#b54">[55]</ref> proposed one cumulative learning strategy, with which they bridge representation learning and classifier re-balancing.</p><p>The two-stage design is not for end-to-end frameworks. Tang <ref type="bibr" target="#b30">[31]</ref> analyzed the reason from the perspective of causal graph and concluded that the bad momentum causal effects played a vital role. Cui <ref type="bibr" target="#b29">[30]</ref> proposed residual learning mechanism to address this issue. <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> is a framework that learns similar/dissimilar representations from data that are organized into similar/dissimilar pairs. An effective contrastive loss function, called InfoNCE <ref type="bibr" target="#b55">[56]</ref>, is</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-parametric Contrastive Loss. Contrastive learning</head><formula xml:id="formula_0">L q,k + ,{k ? } = ? log exp(q?k + /? ) exp(q?k + /? ) + k ? exp(q?k ? /? ) ,<label>(1)</label></formula><p>where q is a query representation, k + is for the positive (similar) key sample, and {k ? } denotes negative (dissimilar) key samples. ? is a temperature hyper-parameter. In the instance discrimination pretext task <ref type="bibr" target="#b56">[57]</ref> for self-supervised learning, a query and a key form a positive pair if they are dataaugmented versions of the same image. It forms a negative pair otherwise. Traditional cross-entropy with linear fc layer weight w and true label y among n classes is expressed as</p><formula xml:id="formula_1">L cross?entropy = ? log exp(q?w y ) n i=1 exp(q?w i ) .<label>(2)</label></formula><p>Compared to it, InfoNCE does not get involved with parametric learnable parameters. To distinguish our proposed parametric contrastive learning from previous ones, we treat the InfoNCE as a non-parametric contrastive loss following <ref type="bibr" target="#b57">[58]</ref>.</p><p>Chen <ref type="bibr" target="#b14">[15]</ref> used self-supervised contrastive learning Sim-CLR to first match the performance of a supervised ResNet-50 with only a linear classifier trained on self-supervised representation on full ImageNet. He <ref type="bibr" target="#b15">[16]</ref> proposed MoCo and Chen <ref type="bibr" target="#b16">[17]</ref> extended MoCo to MoCo v2, with which small batch size training can also achieve competitive results on full ImageNet <ref type="bibr" target="#b10">[11]</ref>. In addition, many other methods <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> are also proposed to further boost performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PARAMETRIC CONTRASTIVE LEARNING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Supervised Contrastive Learning</head><p>Khosla <ref type="bibr" target="#b19">[20]</ref> extended the self-supervised contrastive loss with label information into supervised contrastive loss. Here we present it in the framework of MoCo <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> as</p><formula xml:id="formula_2">L i = ? z+?P (i) log exp(z + ? T (x i )) z k ?A(i) exp(z k ? T (x i )) .<label>(3)</label></formula><p>MoCo framework <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> consists of two networks with the same structure, i.e., query network and key network. The ) and label y, B v1 and B v2 are fed into the query network and key network respectively and we denote their outputs as Z v1 and Z v2 . Especially, Z v2 is used to update the momentum queue.</p><p>In Eq. (3), x i is the representation for image X i in B v1 obtained by the encoder of query network. The transform T (?) also belongs to the query network. We write</p><formula xml:id="formula_3">A(i) = {z k ? queue ? Z v1 ? Z v2 }\{z k ? Z v1 : k = i}, P (i) = {z k ? A(i) : y k = y i }.</formula><p>In implementation, the loss is usually scaled by 1 |P (i)| and a temperature ? is applied like in Eq. (1). Different from self-supervised contrastive loss, which treats query and key as a positive pair if they are the data-augmented version of the same image, supervised contrastive loss treats them as one positive pair if they belong to the same class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Theoretical Motivation</head><p>Analysis of Supervised Contrastive Learning. <ref type="bibr">Khosla [20]</ref> introduced supervised contrastive learning to encourage more compact representation. We observe that it is not directly applicable to long-tailed recognition. As shown in <ref type="table" target="#tab_0">Table 1</ref>, the performance significantly decreases compared with traditional supervised cross-entropy. From an optimization point of view, supervised contrastive loss concentrates more on high-frequency classes than low-frequency ones, which is unfriendly for imbalanced learning. Remark 1. (Optimal value for supervised contrastive learning). When supervised contrastive loss converges, the optimal value for the probability that two samples are a true positive pair with label y is</p><formula xml:id="formula_4">1 K y , where, q(y)</formula><p>is the frequency of class y over the whole dataset, queue is the momentum queue in MoCo <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> and K y ? length(queue) ? q(y). Proof See supplementary material.</p><p>Interpretation. As indicated by Remark 1, high-frequency classes have a higher lower bound of loss value and contribute much more importance than low-frequency classes in training. Thus the training process can be dominated by high-frequency classes. To handle this issue, we introduce a  set of parametric class-wise learnable centers for rebalancing in contrastive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Rebalance in Contrastive Learning</head><p>As described in <ref type="figure" target="#fig_5">Fig. 3</ref> (a), we introduce a set of parametric class-wise learnable centers C = {c 1 , c 2 , ..., c n } into the original supervised contrastive learning, and called this new form Parametric Contrastive Learning (PaCo). Correspondingly, the loss function is changed to</p><formula xml:id="formula_5">L i = z+?P (i)?{cy} ?w(z + ) log exp(z + ? T (x i )) z k ?A(i)?C exp(z k ? T (x i )) ,<label>(4)</label></formula><formula xml:id="formula_6">where w(z + ) = ?, z + ? P (i) 1.0, z + ? {c y } and z ? T (x i ) = z ? G(x i ), z ? A(i) z ? F(x i ), z ? C.</formula><p>Following Chen <ref type="bibr" target="#b16">[17]</ref>, the transform G(?) is a two-layer MLP while F(?) is the identity mapping, i.e., F(x) = x. ? is one hyper-parameter in (0,1). P (i) and A(i) are the same with supervised contrastive learning in Eq. (3). In implementation, the loss is scaled by 1 z+?P (i)?{cy} w(z + ) and a temperature ? is applied like in Eq. (3). Remark 2. (Optimal value for parametric contrastive learning) When parametric contrastive loss converges, the optimal value for the probability that two samples are a true positive pair with label y is ? 1 + ? ? K y and the optimal value for the probability that a sample is closest to its corresponding center c y among C is</p><formula xml:id="formula_7">1 1 + ? ? K y ,</formula><p>where q(y) is the frequency of class y over the whole dataset, queue is the momentum queue in MoCo <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> and K y ? length(queue) ? q(y). Proof See the Supplementary Material.</p><p>Interpretation. Suppose the most frequent class y h has K y h ? q(y h ) ? length(queue) and the least frequent class y t has K yt ? q(y t )?length(queue). As indicated by Remarks 2 and 1, the optimal value for the probability that two samples are a true positive pair varying from the most frequent class to the least one is rebalanced from</p><formula xml:id="formula_8">1 K y h ?? 1 K yt to 1 1 ? + K y h ?? 1 1 ? + K yt .</formula><p>The smaller ?, the more uniform the optimal value from the most frequent class to the least one is, friendly to low-frequency classes learning. However, when ? decreases, the intensity of contrast among samples will be weaker, the intensity of contrast between samples and centers will be stronger. The whole loss becomes closer to supervised cross-entropy. To make good use of contrastive learning and rebalance at the same time, we observe that ?=0.05 is a reasonable choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">PaCo under balanced setting</head><p>For balanced datasets, all classes have the same frequency, i.e., q * =q(y i )=q(y j ) and K * =K yi =K yj for any class y i and class y j . In this case, PaCo reduces to an improved version of multi-task with weighted sum of supervised crossentropy loss and supervised contrastive loss. The connection between PaCo and multi-task is</p><formula xml:id="formula_9">ExpSum = c k ?C exp(c k ?F(x i )) + z k ?A(i) exp(z k ? G(x i )).</formula><p>We also write the PaCo loss as</p><formula xml:id="formula_10">L i = z+?P (i)?{cy} ?w(z + ) log exp(z + ? T (x i )) z k ?A(i)?C exp(z k ? T (x i )) = ? log exp(c y ? F(x i )) ExpSum ? ? z+?P (i) log exp(z + ? G(x i )) ExpSum = L sup +?L supcon ?(log P sup +?K * log P supcon ) = L sup +?L supcon ?(log P sup +?K * log(1 ? P sup )) , where ? ? ? ? ? ? ? ? ? P sup = c k ?C exp(c k ?F(x i )) ExpSum ; P supcon = z k ?A(i) exp(z k ? G(x i )) ExpSum .<label>(5)</label></formula><p>Multi-task learning combines supervised cross-entropy loss and supervised contrastive loss with a fixed weighted scalar. When these two losses conflict, the training can suffer from slower or sub-optimization. Our PaCo contrarily adjust the intensity of supervised cross-entropy loss and supervised contrastive loss in an adaptive way and potentially avoids conflict as analyzed in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Analysis of PaCo under balanced setting</head><p>As indicated by Eq. (5), compared with multi-task, PaCo has an additional loss item:</p><formula xml:id="formula_11">L extra = ? log(P sup ) ? ?K * log(1 ? P sup ).<label>(6)</label></formula><p>Here, we take full ImageNet as an example, i.e., q * = 0.001, length(queue) = 8192, ? = 0.05, ?K * = 0.41. Then the function curve for L extra is shown in <ref type="figure" target="#fig_5">Fig. 3</ref> (b). With P sup increases from 0 to 1.0, the function value decreases until P sup = 0.71 and then goes up, which implies L extra obtains the smallest loss value when P sup = 0.71. Note that, when the whole PaCo loss in Eq. (4) achieves the optimal solution, P sup = 0.71 still establishes as demonstrated by Remark 2. With P sup increases in the training, we analyze how does it affect the intensity of supervised contrastive loss and supervised cross-entropy loss in the following.</p><p>bf Adaptive weighting between L sup and L supcon . Note that the optimal value for the probability that two samples are a true positive pair with label y is 0.035 as indicated by Remark 2. We suppose p l , p h ? (0, 0.71) and p l &lt; p h . To achieve the optimal value, when P sup =p, the supervised contrastive loss value L supcon must decrease as in Eq. <ref type="bibr" target="#b6">(7)</ref>.</p><formula xml:id="formula_12">L supcon = ? z+?P (i) log exp(z + ? G(x i )) z k ?A(i) exp(z k ? G(x i )) = ? z+?P (i) log exp(z+?G(xi)) ExpSum z k ?A(i) exp(z k ?G(xi)) ExpSum = ? K * log 0.035 1 ? p .<label>(7)</label></formula><p>Here P sup increases from p l to p h , L supcon must decrease to a much smaller loss value to achieve the optimal solution, which implies the need to make two different class samples much more discriminative, i.e., increasing inter-class margins, and thus the intensity of supervised contrastive loss will enlarge.</p><p>An intuition is that as P sup increases, more samples are pulled together with their corresponding centers. Along with stronger intensity of supervised contrastive loss at that time, it is more likely to push hard examples close to those samples that are already around right centers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Center Learning Rebalance</head><p>PaCo balances the contrastive learning (for moderating contrast among samples). However the center learning also needs to be balanced, which has been explored in <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b58">[59]</ref>. We incorporate Balanced Softmax <ref type="bibr" target="#b26">[27]</ref> into the center learning.</p><p>Then the PaCo loss is changed from Eq. (4) to what follows:</p><formula xml:id="formula_13">L i = z+?P (i)?{cy} ?w(z + ) log ?(z + , T (x i )) z k ?A(i)?C ?(z k , T (x i )) , (8) where ?(z k , T (x i )) = exp(z k ? G(x i )), z k ? A(i); exp(z k ? F(x i )) ? q(y k ), z k ? C.</formula><p>We emphasize that Balanced Softmax is only a practical remedy for center learning rebalance. The theoretical analysis remains as a future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Generalized Parametric Contrastive Learning</head><p>MoCo <ref type="bibr" target="#b15">[16]</ref> and MoCo v2 <ref type="bibr" target="#b16">[17]</ref> use a momentum encoder and a queue to allow small mini-batch training and achieve even better performance than SimCLR <ref type="bibr" target="#b14">[15]</ref> which requires large mini-batch training, in self-supervised learning.</p><p>To simplify the framework of PaCo, we validate how each component, i.e., augmentation strategy, formulation of GPaCo/PaCo, the momentum encoder and queue size, make effects in Sec. 4.5. Interestingly, we observe that the derived Generalized Parametric Contrastive Learning (GPaCo) by removing the momentum encoder boosts performance on imbalanced and balanced data again.</p><p>In addition to long-tailed recognition, GPaCo models enjoy better generalization ability across CNNs (ResNets) to vision transformers (ViT models) on full ImageNet and CIFAR, which is demonstrated in Sec. 4.2 and Sec. 4.3.</p><p>Besides image classification, GPaCo shows its great generality on other tasks, e.g., out-of-distribution robustness, and semantic segmentation.</p><p>To demonstrate the advantages of GPaCo on out-ofdistribution data, we load MAE <ref type="bibr" target="#b13">[14]</ref> pre-trained weights and then fine-tune on full ImageNet with same training strategy as in <ref type="bibr" target="#b13">[14]</ref>. Compared with MAE baselines, GPaCo models achieve much stronger robustness across different benchmarks, which is discussed in Sec. 4.3.</p><p>For semantic segmentation task, GPaCo treats each pixel as an example. However, huge number of pixels make it unpractical to directly apply GPaCo to pixel classification. To reduce computational cost and GPU memory, we randomly sample 8192 pixels from down-sampled pixel features. These selected pixel features go through a 3-layer mlp transform and then are fed into GPaCo, which is adopted as an auxiliary loss for training optimization. GPaCo models on semantic segmentation show obvious improvements compared with baselines across 4 most popular datasets in semantic segmentation community. More details are introduced in Sec. 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Long-tailed Recognition</head><p>We follow the common evaluation protocol <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b36">[37]</ref> in long-tailed recognition -that is, training models on the long-tailed source label distribution and evaluating their performance on the uniform target label distribution. We conduct experiments on long-tailed version of CIFAR-100  <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, Places <ref type="bibr" target="#b36">[37]</ref>, ImageNet <ref type="bibr" target="#b36">[37]</ref> and iNaturalist 2018 <ref type="bibr" target="#b37">[38]</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-100-LT datasets.</head><p>We use the long-tailed version of CIFAR datasets with the same setting as those used in <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b60">[61]</ref>. They control the degrees of data imbalance with an imbalance factor ?. ?= Nmax Nmin where N max and N min are the numbers of training samples for the most and least frequent classes respectively. Following <ref type="bibr" target="#b54">[55]</ref>, we conduct experiments with imbalance factors 100, 50, and 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet-LT and Places-LT. ImageNet-LT and Places-LT</head><p>were proposed in <ref type="bibr" target="#b36">[37]</ref>. ImageNet-LT is a long-tailed version of ImageNet dataset <ref type="bibr" target="#b10">[11]</ref> by sampling a subset following the Pareto distribution with power value ?=6. It contains 115.8K images from 1,000 categories, with class cardinality ranging from 5 to 1,280. Places-LT is a long-tailed version of the large-scale scene classification dataset Places <ref type="bibr" target="#b12">[13]</ref>. It consists of 184.5K images from 365 categories with class cardinality ranging from 5 to 4,980.</p><p>iNaturalist 2018. The iNaturalist 2018 <ref type="bibr" target="#b37">[38]</ref> is one species classification dataset, which is on a large scale and suffers from extremely imbalanced label distribution. It is composed of 437.5K images from 8,142 categories. In addition to the extreme imbalance, the iNaturalist 2018 dataset also confronts the fine-grained problem <ref type="bibr" target="#b61">[62]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details. For image classification on</head><p>ImageNet-LT, we used ResNet-50, ResNeXt-50-32x4d, and ResNeXt-101-32x4d as our backbones for experiments. For iNaturalist 2018, we conduct experiments with ResNet-50 and ResNet-152. All models were trained using SGD optimizer with momentum ? = 0.9.</p><p>Contrastive learning benefits from longer training compared with traditional supervised learning with crossentropy as Chen <ref type="bibr" target="#b14">[15]</ref> concluded, which is also validated by previous work of <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. MoCo <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, BYOL <ref type="bibr" target="#b17">[18]</ref> and SWAV <ref type="bibr" target="#b18">[19]</ref> train 800 epochs for model convergence. Supervised contrastive learning <ref type="bibr" target="#b19">[20]</ref> trains 350 epochs for feature learning and another 350 epochs for classifier learning.</p><p>Following MoCo <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, when we train models with GPaCo/PaCo, the learning rate decays by a cosine scheduler from 0.04/0.02 to 0 with batch size 128 on 4 GPUs in 400 epochs. The temperature is set to 0.2. ? is 0.05. For a fair comparison, we re-implement baselines with the same training time and RandAugment <ref type="bibr" target="#b59">[60]</ref> for recent state-of-thearts of Decouple <ref type="bibr" target="#b27">[28]</ref>, Balanced Softmax <ref type="bibr" target="#b26">[27]</ref> and RIDE <ref type="bibr" target="#b28">[29]</ref>. Especially, for RIDE, based on model ensemble, we compare with it under comparable inference latency in <ref type="figure" target="#fig_1">Fig. 1 (a)</ref>. For Places-LT, following previous setting <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b36">[37]</ref>, we choose ResNet-152 as the backbone network, pre-train it on the full ImageNet-2012 dataset (provided by torchvision), and finely tune it for 30 epochs on Places-LT. The learning rate decays by a cosine scheduler from 0.02 to 0 with batch size 128. The temperature is set to 0.2. ? is 0.02/0.05 for GPaCo and PaCo. For CIFAR-100-LT, we strictly follow the setting of <ref type="bibr" target="#b26">[27]</ref> for fair comparison. A smaller temperature of 0.07 and ? = 0.01 are adopted. <ref type="table" target="#tab_1">Table 2</ref> shows extensive experimental results for comparison with recent SOTA methods. We observe that Balanced Softmax <ref type="bibr" target="#b26">[27]</ref> still achieves comparable results with Decouple <ref type="bibr" target="#b27">[28]</ref> across various backbones under such strong training setting on ImageNet-LT, consistent with what is claimed in the original paper. For RIDE that is based on model ensemble, we analyze the real inference speed by calculating inference time with a batch of 64 images on Nvidia GeForce 2080Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison on ImageNet-LT.</head><p>We observe RIDEResNet with 3 experts even has higher inference latency than a standard ResNeXt-50-32x4d (15.3ms vs 13.1ms); RIDEResNeXt with 3 experts yields higher inference latency than a standard ResNeXt-101-32x4d (26ms vs 25ms). This result is in accordance with the conclusion that network fragmentation reduces the degree of parallelism and thus decreases efficiency in <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b62">[63]</ref>. For fair comparison, we do not apply knowledge distillation tricks for all these methods. As shown in <ref type="figure" target="#fig_1">Fig. 1 (a)</ref> and <ref type="table" target="#tab_1">Table 2</ref>, under comparable inference latency, GPaCo/PaCo significantly surpasses these baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison on Places-LT. The experimental results on</head><p>Places-LT are summarized in <ref type="table" target="#tab_2">Table 3</ref>. Due to the architecture change of RIDE, it is not applicable to load the publicly pre-trained model on full ImageNet, while GPaCo/PaCo is more flexible because the network architecture is the same as those of <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Under fair training setting by finely tuning 30 epochs without RandAugment, PaCo surpasses SOTA Balanced Softmax by 2.6%. GPaCo even achieves 41.7% after removing the momentum encoder. An interesting observation is that RandAugment has little effect on the Places-LT dataset. A similar phenomenon can be  observed on the iNaturalist 2018 dataset. More evaluation numbers are in the supplementary file. They can be intuitively understood since RandAugment is designed for ImageNet classification, which inspires us to explore general augmentations across different domains.</p><p>Comparison on iNaturalist 2018. Comparison on CIFAR-100-LT. The experimental results on CIFAR-100-LT are listed in <ref type="table">Table 8</ref>. For the CIFAR-100-LT dataset, we mainly compare with the SOTA method Balanced Softmax <ref type="bibr" target="#b26">[27]</ref> with the same training setting where Cutout <ref type="bibr" target="#b64">[65]</ref> and AutoAugment <ref type="bibr" target="#b65">[66]</ref> are used in training. As shown in <ref type="table">Table 8</ref>, PaCo consistently outperforms Balanced Softmax across different imbalance factors with such a strong setting. Specifically, PaCo surpasses Balanced Softmax by 1.2%, 1.8% and 1.2% under imbalance factor 100, 50 and 10 respectively, which testify the effectiveness of our PaCo method. GPaCo models show better performance than PaCo models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Full ImageNet and CIFAR Recognition</head><p>As analyzed in Section 3.4, for balanced datasets, PaCo reduces to an improved version of multi-task learning, which adaptively adjusts the intensity of supervised crossentropy loss and supervised contrastive loss. To verify the effectiveness of PaCo under this balanced setting, we conduct experiments on full ImageNet and full CIFAR. They are indicative to compare GPaCo/PaCo with supervised contrastive learning (Supcon) <ref type="bibr" target="#b19">[20]</ref>. Note that, under full ImageNet and CIFAR, we remove the rebalance in center learning, i.e., Balanced Softmax, for fair comparisons.</p><p>Full ImageNet. In the implementation, we transfer hyperparameters of GPaCo/PaCo on ImageNet-LT to full Ima-geNet without modification. SGD optimizer with momentum ? = 0.9 is used. ?=0.05, temperature is 0.2 and queue size is 8,192. For multi-task training, the supervised contrastive loss is an additional regularization and the loss weight is extensively explored from 0.1 to 1.0 in Sec. 4.5. The same data augmentation strategy is applied as in GPaCo/PaCo, which is discussed in Section 4.5.</p><p>The experimental results are summarized in <ref type="table" target="#tab_4">Table 5</ref>. With SimAugment, our ResNet-50 PaCo model achieves 78.7% top-1 accuracy, which outperforms supervised contrastive learning model by 0.8%. Equipped with strong augmentation, i.e., RandAugment <ref type="bibr" target="#b59">[60]</ref>, the performance further improves to 79.3%. ResNet-101/200 trained with PaCo consistently surpass supervised contrastive learning. Replacing PaCo with GPaCo, The ResNet-50 model boosts to 79.7% top-1 accuracy.</p><p>Full CIFAR-100. For CIFAR implementation, we follow supervised contrastive learning and train ResNet-50 with only the SimAugment. Compared with full ImageNet, we adopt a smaller temperature of 0.07, ? = 0.01 and batch size 256 with learning rate 0.1. As shown in <ref type="table">Table 9</ref>, on CIFAR-100, PaCo outperforms supervised contrastive learning by 2.6%, which validates the advantages of PaCo. Again, the GPaCo model achieves 80.3% top-1 accuracy, significantly surpassing supervised contrastive learning by 3.8%. Note that, following <ref type="bibr" target="#b8">[9]</ref>, we use a weight-decay of 5e-4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Out-of-distribution Robustness</head><p>He et al. <ref type="bibr" target="#b13">[14]</ref> justifies that masked autoencoders are scalable vision learners. With such a simple pre-training strategy that masking random patches of the input image and reconstructing the missing pixels, the good representation is learned and well transffered to downstream tasks, i.e., full ImageNet classification, object detection, semantic segmentaiton, and robustness on out-of-distribution data. In this section, we verify that GPaCo can enhance model generalization ability and robustness on full ImageNet and its variants when compared with MAE <ref type="bibr" target="#b13">[14]</ref> models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness Evaluation.</head><p>We extensively evaluate the model performance on out-of-distribution robustness using following benchmarks: 1) ImageNet-C <ref type="bibr" target="#b66">[67]</ref>, with various common image corruptions; 2) ImageNet-R <ref type="bibr" target="#b67">[68]</ref>, which contains natural renditions of ImageNet object classes with different textures and local image statistics; 3) ImageNet-Sketch <ref type="bibr" target="#b68">[69]</ref>, which includes sketch images of the same ImageNet classes collected online.</p><p>Comparison with MAE Models <ref type="bibr" target="#b13">[14]</ref>. We summarize experimental results in <ref type="table" target="#tab_3">Table 4</ref>. Top-1 accuracy is reported on full ImageNet, ImageNet-R, and ImageNet-S. Mean Corruption Error (mCE) and Relative Mean Corruption Error (rel. mCE) <ref type="bibr" target="#b66">[67]</ref> are used for ImageNet-C. mCE is to measure absolute robustness to corruptions while rel. mCE is a better metric when we compare models with different top-1 accuracy. Compared with MAE models, we achieve better performance on full ImageNet, surpassing them by 0.4 and 0.3 respectively for ViT-B <ref type="bibr" target="#b63">[64]</ref> and ViT-L <ref type="bibr" target="#b63">[64]</ref>. On ImageNet-C, ImageNet-R, and ImageNet-S, GPaCo models usually outperforms MAE models by 1.7 ? 3.3, demonstrating much stronger robustness on out-of-distribution data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Semantic Segmentation</head><p>In this section, we transfer GPaCo to the downstream task, i.e., semantic segmentation. In implementation, we use GPaCo as an auxiliary loss. Specifically, for each image, we randomly sample 8192 pixel features on the donwn-sampled feature map and feed them into GPaCo loss for training optimization.  <ref type="bibr" target="#b41">[42]</ref>, the subset of 59 frequent classes is used following previous work. Cityscapes <ref type="bibr" target="#b42">[43]</ref> consists of 19 classes, covering street scenes from 50 different cities.</p><p>Training and Evaluation. We implement our GPaCo in mmseg codebase <ref type="bibr" target="#b69">[70]</ref>. On ADE20K <ref type="bibr" target="#b39">[40]</ref>, we follow <ref type="bibr" target="#b70">[71]</ref> to experiment with Swin transformers and UperNet <ref type="bibr" target="#b71">[72]</ref>. On COCO-Stuff <ref type="bibr" target="#b40">[41]</ref>, Pascal Context <ref type="bibr" target="#b41">[42]</ref> and Cityscapes <ref type="bibr" target="#b42">[43]</ref>, we use ResNet-50/101 and deeplabv3+. Default training hyper-parameters in baselines are adopted in the GPaCo model training phase, i.e., the standard random scale jittering between 0.5 and 2.0, random horizontal flipping, random cropping, as well as random color jittering. We report both single scale (s.s.) and multi-scale (m.s.) evaluation results on validation data. For multi-scale inference, scales of 0.5, 0.75, 1.0, 1.25, 1.5, 1.75 are used.</p><p>Results. Experimental results are summarized in <ref type="table" target="#tab_7">Table 7</ref>. On ADE20K, obvious improvements are observed from Swin-T to Swin-L models when equipped with GPaCo, outperforming baselines by 1.0, 1.5, 0.8 mIoU respectively. On COCO-Stuff and PASCAL Context, GPaCo models significantly surpass baselines by around 1.1 ? 1.7 mIoU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>Data augmentation strategy for PaCo &amp; GPaCo. Data augmentation is the key for success of contrastive learning as indicated by Chen <ref type="bibr" target="#b14">[15]</ref>. For PaCo &amp; GPaCo, we also conduct ablation studies for different augmentation strategies. Several observations are intriguingly different from those of <ref type="bibr" target="#b72">[73]</ref>. We experiment with the following ways of data augmentation.   <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> that applies random flips and color jitters followed by Gaussian blur.</p><p>? RandAug [60]: A two stage augmentation policy that uses random parameters in place of parameters tuned by AutoAugment. The random parameters do not need to be tuned and hence reduces the search space.</p><p>? RandAugStack: RandAug followed by Gaussian blur.</p><p>For the common random resized crop used along with the above three strategies, work of <ref type="bibr" target="#b72">[73]</ref> explains that the optimal hyper-parameter for random resized crop is (0.2,1) in self-supervised contrastive learning. This setting is also adopted by other work of <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. However, in this paper, we observe severe performance degradation on ImageNet-LT with ResNet-50 (55.0% vs 52.2%) for PaCo when we change the hyper-parameter from (0.08,1) to (0.2, 1). This is because PaCo involves center learning while other self-supervised frameworks only apply non-parametric contrastive loss as described in Section 2. Note that the same phenomenon is also observed on traditional supervised learning with cross-entropy loss.</p><p>Another observation is that GPaCo can make better use of strong augmentations compared with PaCo. The work of <ref type="bibr" target="#b73">[74]</ref> demonstrates that directly applying strong data augmentation in MoCo <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> does not work well. Here we observe a similar conclusion with RandAug <ref type="bibr" target="#b59">[60]</ref> for PaCo. However, after removing the momentum encoder, GPaCo can achieve better performance with stronger augmentation as shown in <ref type="table" target="#tab_0">Table 10</ref>.</p><p>Multi-task Learning. Re-weighting is a classical method for dealing with imbalanced data. Here we directly apply the re-weighting method of Cui <ref type="bibr" target="#b34">[35]</ref> in contrastive learning to compare with PaCo. Moreover, Balanced softmax (BalSfx) <ref type="bibr" target="#b26">[27]</ref>, as one state-of-the-art method for traditional cross-entropy in long-tailed recognition, is also applied to contrastive learning rebalance. The experimental results are summarized in <ref type="table" target="#tab_0">Table 11</ref>. It is obvious PaCo significantly surpasses the two baselines. PaCo/GPaCo balances the contrastive learning (for moderating contrast among samples). However the center learning also needs to be balanced, which has been explored in <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b58">[59]</ref>. To compare with state-of-the-art methods in long-tailed recognition, we incorporate Balanced Softmax (BalSfx) <ref type="bibr" target="#b26">[27]</ref> into the center learning. As shown in <ref type="table" target="#tab_0">Table 12</ref>, after rebalance in center learning, GPaCo boosts performance to 58.9%, surpassing baselines.</p><p>We also verify the advantage of GPaCo over multi-task learning on balanced data -full ImageNet and full CIFAR-100. As shown in <ref type="table" target="#tab_0">Table 13</ref>, extensive values for the weight between cross-entropy and supervised contrastive learning have been explored. GPaCo obviously achieve much higher performance.</p><p>GPaCo with Queue Length. The queue is designed to enlarge the number of samples for GPaCo/PaCo loss. We examine the effects of different queue size on model performance. Empirical study on ImageNet-LT with ResNeXt-50 is conducted. As shown in <ref type="figure" target="#fig_6">Fig 4,</ref> the larger queue size usually leads to better performance. However, when the queue size increase from 4096 to 8192, the performance gains become much smaller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Necessary of Two Views in GPaCo.</head><p>In training of GPaCo, a positive pair consists of two images belonging to the same class. We explore whether it is necessary to generate two views as in the self-supervised learning <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. As shown in <ref type="table" target="#tab_0">Table 14</ref>, performance and robustness significantly drops with single view training, which implies the vast importance of two views training in GPaCo.</p><p>Model Ensemble. Models trained on imbalanced data can easily suffer from over-fitting issue on low-frequency classes, thus leading to high variance of model predictions. RIDE proposes an improved version of model ensemble strategy and has verified the advantages of model ensemble for long-tailed recognition. However, GPaCo/PaCo improves single model performance with better feature representation. We go deeper to explore the model ensemble strategy on GPaCo models. As shown in <ref type="table" target="#tab_0">Table 15</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we have proposed the Generalized Parametric Contrastive Learning (GPaCo/PaCo), which can deal with both imbalanced and balanced data well. It is based on the theoretical analysis of supervised contrastive learning and rebalance from the convergent optimal values. On balanced data, our analysis of PaCo demonstrates that it can adaptively enhance the intensity of pushing two samples of the same class close as more samples are pulled together with their corresponding centers, which can potentially benefit hard examples learning in training.</p><p>We conduct experiments on various benchmarks of CIFAR-LT, ImageNet-LT, Places-LT, and iNaturalist 2018. The experimental results show that we create a new stateof-the-art for long-tailed recognition. With balanced data, experimental results on full ImageNet and CIFAR show that GPaCo models have better generalization ability and stronger robustness. Transfering GPaCo to semantic segmentation, obvious improvements are obtained. For an image X i and its label y i , the expectation number of positive pairs with respect to X i will be:</p><formula xml:id="formula_14">K yi = q(y i ) * (length(queue) + batchsize * 2 ? 1) ? length(queue) ? q(y i ),<label>(9)</label></formula><p>q(y i ) is the class frequency over the whole dataset. Here the "?" establishes because batchsize ? length(queue) in training process. Note that we use such approximation just for simplification. Our analysis holds for the precise K yi . In what follows, we prove the optimal values for supervised contrastive loss.</p><p>Suppose training samples are i.i.d. To minimize the supervised contrastive loss for sample X i , according to Eq. (3), we rewrite:</p><formula xml:id="formula_15">? ? ? ? ? ? ? ? ? ? ? P (i) = {z + 1 , z + 2 , ..., z + Ky i }; p + i = exp(z + i ? T (x i )) z k ?A(i) exp(z k ? T (x i )) ; p + sum = p + 1 + p + 2 + ... + p + Ky i .</formula><p>Then the supervised contrastive loss will be:</p><formula xml:id="formula_16">L i = ? z+?P (i) log exp(z + ? T (x i )) z k ?A(i) exp(z k ? T (x i )) = ?(log p + 1 + log p + 2 + ... + log p + Ky i ).</formula><p>For obtaining its optimal solution, we define the Lagrange multiplier form of L i as:</p><formula xml:id="formula_17">l = ?(log p + 1 + log p + 2 + ... + log p + Ky i ) + ?(p + 1 + p + 2 + ... + p + Ky i ? p + sum ),<label>(10)</label></formula><p>where ? is the Lagrange multiplier. The first order conditions of Eq. (10) w.r.t. ? and p + i can be written as follows:</p><formula xml:id="formula_18">? ? ? ? ? ? ? ?l ?p + i = ? 1 p + i + ? = 0; ?l ?? = p + 1 + p + 2 + ... + p + Ky i ? p + sum = 0.<label>(11)</label></formula><p>From Eq. (11), the optimal solution for p * i will be p + sum K yi . Note that p + sum ? [0, 1], with a specific p + sum , the minimal loss value of L i is:</p><formula xml:id="formula_19">L i = ?K yi log p + sum K yi .<label>(12)</label></formula><p>Thus, when p + sum = 1.0, L i achieves minimum with the optimal value p + i = 1 K yi which is exactly the probability that two samples of the same class are a true positive pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">PROOF TO REMARK 2</head><p>For the image X i and its label y i , Eq. (9) still establishes for our parametric contrastive loss. To minimize the parametric contrastive loss for sample X i , according to Eq. (4), we similarly rewrite:</p><formula xml:id="formula_20">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? P (i) = {z + 1 , z + 2 , ..., z + Ky i } p + i = exp(z + i ? T (x i )) z k ?A(i)?C exp(z k ? T (x i )) p + c = exp(c y ? T (x i )) z k ?A(i)?C exp(z k ? T (x i )) p + sum = p + 1 + p + 2 + ... + p + Ky i + p + c .</formula><p>Then the parametric contrastive loss will be:</p><formula xml:id="formula_21">L i = z+?P (i)?{cy} ?w(z + ) log exp(z + ? T (x i )) z k ?A(i)?C exp(z k ? T (x i ))<label>(13)</label></formula><p>= ? log p + c + ? ? (log p + 1 + log p + 2 + ... + log p + ky i</p><p>) .</p><p>For obtaining its optimal solution, we define the Lagrange multiplier form of L i as: </p><formula xml:id="formula_23">+ p + c ? p + sum ),<label>(15)</label></formula><p>where ? is the Lagrange multiplier. The first order conditions of Eq. <ref type="formula" target="#formula_0">(15)</ref>  </p><p>Thus, when p + sum = 1.0, L i achieves minimum with the optimal value p + i = ? 1 + ?K yi , which is the probability that two samples of the same class are a true positive pair, and the optimal value p + c = 1 1 + ?K yi which is the probability that a sample is closest to its corresponding center c yi among C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">GRADIENT DERIVATION</head><p>In Section 3.4, we analyze PaCo loss under balanced setting, taking full ImageNet as an example. With P sup increases from 0 to 0.71, the intensity of supervised contrastive loss will enlarge. Here we show that more samples will be pulled together with their corresponding centers when P sup increases from 0 to 0.71 from the perspective of gradient derivation.</p><p>?L ?c k = (?K * + 1)p c k x i , y i ? = k; {(?K * + 1)p c k ? 1} x i , y i = k.</p><p>It is worthy to note that when p c k ? (0, 0.71), we have</p><formula xml:id="formula_26">? ? ? ? ? ? ? ?L ?c k &gt; 0, y i ? = k; ?L ?c k &lt; 0, y i = k.<label>(19)</label></formula><p>Eqs. <ref type="bibr" target="#b17">(18)</ref> and <ref type="bibr" target="#b18">(19)</ref> mean that as P sup increases in training process, the probability that a sample is closest to its corresponding center will increase and the probability that a sample is closest to other centers will decrease. Thus, more and more samples will be pulled together with their right centers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">IMPLEMENTATION DETAILS FOR TABLE 1</head><p>We train models with cross-entropy, parametric contrastive loss 400 epochs without RandAugment respectively. For supervised contrastive loss, following the original paper, we firstly train the model 400 epochs. Then we fix the backbone and train a linear classifier 400 epochs.  Inference time is calculated with a batch of 64 images on Nvidia GeForce 2080Ti GPU, Pytorch1.5, Python3.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">MORE EXPERIMENTAL RESULTS ON MANY-SHOT, MEDIUM-SHOT, AND FEW-SHOT.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone Method</head><p>Inference time (ms) Many Medium Few All RIDEResNet</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Comparison on ImageNet-LT for long-tailed image classification. Comparison on ADE20K for semantic segmentation. Models ImageNet(?) ImageNet-C(mCE ?) ImageNet-C(rel. mCE ?) ImageNet-R(?) ImageNet-S(?)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>GPaCo demonstrates impressive performance on imbalanced and balanced data. (a) shows that GPaCo achieves state-of-the-art on long-tailed classification. GPaCo benefits semantic segmentation task and significant improvement is observed on ADE20K in (b). In (c), top-1 accuracy are reported for full ImageNet, ImageNet-R, and ImageNet-S. mCE and rel. mCE are used for ImageNet-C. Compared with MAE models, GPaCo enjoys better generalization ability and stronger robustness on ImageNet and its variants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Rebalance in contrastive learning. We collect the average L2 norm of the gradient of weights in the last classifier layer on ImageNet-LT. Category indices are sorted by their image counts. The gradient norm varying from the most frequent class to the least one is steep for supervised contrastive learning [20]. In particular, the gradient norm dramatically decreases for the top 200 most frequent classes. Trained with PaCo, the gradient norm is better balanced.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>) = ? log(p) ? 0.41 ? log(1 ? p) (b) Curve for Lextra.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .</head><label>3</label><figDesc>Framework of Parametric Contrastive Learning (PaCo) and Lextra curve. (a) shows the detail of PaCo. We introduce a set of parametric class-wise learnable centers for rebalancing in contrastive learning. More analysis is in Section 3.3 for PaCo. (b) plots the curve of Lextra instantiated with full ImageNet. More analysis on balanced data is in Section 3.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>The larger queue size, the better performance. Experimental results with ResNeXt-50 on ImageNet-LT are plotted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 Supervised contrastive learning is more sensitive to data imbalance.</head><label>1</label><figDesc>Top-1 accuracy (%) on ImageNet-LT with ResNet-50 is reported. Implementation details are in supplementary file. " ?" represents model is trained with PaCo loss without center learning rebalance.</figDesc><table><row><cell>Method</cell><cell cols="3">Many Medium Few</cell><cell>All</cell></row><row><cell>Cross-Entropy</cell><cell>67.5</cell><cell>42.6</cell><cell cols="2">13.7 48.4</cell></row><row><cell>SupCon</cell><cell>53.4</cell><cell>2.9</cell><cell>0</cell><cell>22.0</cell></row><row><cell>PaCo (ours)  ?</cell><cell>69.6</cell><cell>45.8</cell><cell cols="2">16.0 51.0</cell></row><row><cell cols="5">key network is driven by a momentum update with the</cell></row><row><cell cols="5">query network in training. For each network, it usually con-</cell></row><row><cell cols="5">tains one encoder CNN and one two-layer MLP transform.</cell></row><row><cell cols="5">During training, for one two-viewed image batch B =</cell></row><row><cell>(B v1 , B v2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 Top-1 accuracy on ImageNet-LT for different backbone architectures.</head><label>2</label><figDesc>" ?" denotes models are trained with RandAugment<ref type="bibr" target="#b59">[60]</ref> in 400 epochs. More comparisons with RIDE<ref type="bibr" target="#b28">[29]</ref> are inFig. 1.</figDesc><table><row><cell>Method</cell><cell cols="3">ResNet-50 ResNeXt-50 ResNeXt-101</cell></row><row><cell>CE(baseline)</cell><cell>41.6</cell><cell>44.4</cell><cell>44.8</cell></row><row><cell>Decouple-cRT</cell><cell>47.3</cell><cell>49.6</cell><cell>49.4</cell></row><row><cell>Decouple-? -norm</cell><cell>46.7</cell><cell>49.4</cell><cell>49.6</cell></row><row><cell>De-confound-TDE</cell><cell>51.7</cell><cell>51.8</cell><cell>53.3</cell></row><row><cell>ResLT</cell><cell>-</cell><cell>52.9</cell><cell>54.1</cell></row><row><cell>MiSLAS</cell><cell>52.7</cell><cell>-</cell><cell>-</cell></row><row><cell>Decouple-? -norm  ?</cell><cell>54.5</cell><cell>56.0</cell><cell>57.9</cell></row><row><cell>Balanced Softmax  ?</cell><cell>55.0</cell><cell>56.2</cell><cell>58.0</cell></row><row><cell>PaCo ?</cell><cell>57.0</cell><cell>58.2</cell><cell>60.0</cell></row><row><cell>GPaCo ?</cell><cell>58.5</cell><cell>58.9</cell><cell>60.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3 Performance on Places-LT [37], starting from an ImageNet pre-trained ResNet-152 provided by torchvision</head><label>3</label><figDesc>. " ?" denotes the model trained with RandAugment<ref type="bibr" target="#b59">[60]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="4">Many Medium Few All</cell></row><row><cell>CE(baseline)</cell><cell>45.7</cell><cell>27.3</cell><cell cols="2">8.2 30.2</cell></row><row><cell>OLTR</cell><cell>44.7</cell><cell>37.0</cell><cell cols="2">25.3 35.9</cell></row><row><cell>Decouple-? -norm</cell><cell>37.8</cell><cell>40.7</cell><cell cols="2">31.8 37.9</cell></row><row><cell>Balanced Softmax</cell><cell>42.0</cell><cell>39.3</cell><cell cols="2">30.5 38.6</cell></row><row><cell>ResLT</cell><cell>39.8</cell><cell>43.6</cell><cell cols="2">31.4 39.8</cell></row><row><cell>MiSLAS</cell><cell>39.6</cell><cell>43.3</cell><cell cols="2">36.1 40.4</cell></row><row><cell>RIDE (2 experts)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PaCo</cell><cell>37.5</cell><cell>47.2</cell><cell cols="2">33.9 41.2</cell></row><row><cell>PaCo  ?</cell><cell>36.1</cell><cell>47.9</cell><cell cols="2">35.3 41.2</cell></row><row><cell>GPaCo</cell><cell>39.5</cell><cell>47.2</cell><cell cols="2">33.0 41.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4 Evaluation with ViT models [64] on full ImageNet and out-of-distribution data.</head><label>4</label><figDesc>GPaCo models enjoy better generalization ability and stronger robustness compared with MAE models. We use their official open-source code for reproduing results of MAE models with 8 Nvidia GeForce RTX3090 GPUs. Top-1 accuracy are reported for full ImageNet, ImageNet-R, and ImageNet-S. mCE and rel. mCE are reported for ImageNet-C.</figDesc><table><row><cell>Method</cell><cell>Model</cell><cell>Full ImageNet (?)</cell><cell>ImageNet-C (mCE ?)</cell><cell>ImageNet-C (rel. mCE ?)</cell><cell>ImageNet-R (?)</cell><cell>ImageNet-S (?)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">w/ Mixup and CutMix</cell><cell></cell><cell></cell></row><row><cell>MAE</cell><cell>ViT-B</cell><cell>83.6</cell><cell>39.1</cell><cell>49.9</cell><cell>49.9</cell><cell>36.1</cell></row><row><cell>MAE</cell><cell>ViT-L</cell><cell>85.7</cell><cell>32.4</cell><cell>41.4</cell><cell>60.3</cell><cell>45.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">w/o Mixup and CutMix</cell><cell></cell><cell></cell></row><row><cell>SupCon</cell><cell>ResNet-50</cell><cell>78.7</cell><cell>67.2</cell><cell>94.6</cell><cell>-</cell><cell>-</cell></row><row><cell>GPaCo</cell><cell>ResNet-50</cell><cell>79.7</cell><cell>50.9</cell><cell>64.4</cell><cell>41.1</cell><cell>30.9</cell></row><row><cell>MAE</cell><cell>ViT-B</cell><cell>82.5</cell><cell>44.9</cell><cell>56.9</cell><cell>44.9</cell><cell>32.9</cell></row><row><cell>MAE</cell><cell>ViT-L</cell><cell>85.2</cell><cell>36.4</cell><cell>46.3</cell><cell>55.3</cell><cell>42.6</cell></row><row><cell>GPaCo</cell><cell>ViT-B</cell><cell>84.0</cell><cell>37.2</cell><cell>47.3</cell><cell>51.7</cell><cell>39.4</cell></row><row><cell>GPaCo</cell><cell>ViT-L</cell><cell>86.0</cell><cell>30.7</cell><cell>39.0</cell><cell>60.3</cell><cell>48.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 5 Top-1 accuracy on full ImageNet with ResNets.</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">"?" denotes</cell></row><row><cell cols="4">supervised contrastive learning with additional operation of image</cell></row><row><cell></cell><cell cols="2">warping before Gaussian blur.</cell><cell></cell></row><row><cell>Method</cell><cell>Model</cell><cell>augmentation</cell><cell>Top-1 Acc</cell></row><row><cell>Supcon</cell><cell>ResNet-50</cell><cell>SimAugment ?</cell><cell>77.9</cell></row><row><cell>Supcon</cell><cell>ResNet-50</cell><cell>RandAugment</cell><cell>78.4</cell></row><row><cell>Supcon</cell><cell>ResNet-101</cell><cell>StackedRandAugment</cell><cell>80.2</cell></row><row><cell cols="2">multi-task RandAugment</cell><cell>ResNet-50</cell><cell>78.1</cell></row><row><cell>PaCo</cell><cell>ResNet-50</cell><cell>SimAugment</cell><cell>78.7</cell></row><row><cell>PaCo</cell><cell>ResNet-50</cell><cell>RandAugment</cell><cell>79.3</cell></row><row><cell>PaCo</cell><cell>ResNet-101</cell><cell>StackedRandAugment</cell><cell>80.9</cell></row><row><cell>PaCo</cell><cell>ResNet-200</cell><cell>StackedRandAugment</cell><cell>81.8</cell></row><row><cell>GPaCo</cell><cell>ResNet-50</cell><cell>RandAugment</cell><cell>79.5</cell></row><row><cell>GPaCo</cell><cell>ResNet-50</cell><cell>StackedRandAugment</cell><cell>79.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>lists experimen-</cell></row><row><cell>tal results on iNaturalist 2018. Under fair training setting,</cell></row><row><cell>PaCo consistently surpasses recent SOTA methods of De-</cell></row><row><cell>couple, Balanced Softmax and RIDE. Our method is 1.4%</cell></row><row><cell>higher than Balanced Softmax. We also apply PaCo on large</cell></row><row><cell>ResNet-152 architecture. And the performance boosts to</cell></row><row><cell>75.3% top-1 accuracy. Surprisingly, With GPaCo, a ResNet-</cell></row><row><cell>50 model achieves 75.4% top-1 accuracy, implying that the</cell></row><row><cell>momentum encoder for PaCo can hurt optimization in</cell></row><row><cell>training. Note that we only transfer the hyper-parameters</cell></row><row><cell>of PaCo on ImageNet-LT to iNaturalist 2018 without any</cell></row><row><cell>change. Tuning hyper-parameters for GPaCo/PaCo will</cell></row><row><cell>bring further improvement.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 6 Top-1 accuracy over all classes on iNaturalist 2018 with ResNet-50.</head><label>6</label><figDesc>Knowledge distillation is not applied for fair comparison. We compare with RIDE under comparable inference latency. " ?" denotes models trained with RandAugment<ref type="bibr" target="#b59">[60]</ref> in 400 epochs.</figDesc><table><row><cell>Method</cell><cell>Top-1 Acc</cell></row><row><cell>LDAM+DRW</cell><cell>68.0</cell></row><row><cell>Decouple-LWS</cell><cell>69.5</cell></row><row><cell>BBN</cell><cell>69.6</cell></row><row><cell>ResLT</cell><cell>70.2</cell></row><row><cell>MiSLAS</cell><cell>71.6</cell></row><row><cell>RIDE (2 experts)  ?</cell><cell>69.5</cell></row><row><cell>Decouple-? -norm  ?</cell><cell>71.5</cell></row><row><cell>Balanced Softmax  ?</cell><cell>71.8</cell></row><row><cell>PaCo  ?</cell><cell>73.2</cell></row><row><cell>GPaCo  ?</cell><cell>75.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 7 Transfering GPaCo to the semantic segmentation.</head><label>7</label><figDesc></figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">mIoU (s.s.) mIoU (m.s.)</cell></row><row><cell></cell><cell cols="2">ADE20K Dataset</cell><cell></cell></row><row><cell></cell><cell>Swin-T</cell><cell>44.5</cell><cell>45.8</cell></row><row><cell>UperNet</cell><cell>Swin-B</cell><cell>50.0</cell><cell>51.7</cell></row><row><cell></cell><cell>Swin-L</cell><cell>52.0</cell><cell>53.5</cell></row><row><cell></cell><cell>Swin-T</cell><cell>45.4</cell><cell>46.8</cell></row><row><cell>UperNet w/ GPaCo</cell><cell>Swin-B</cell><cell>51.6</cell><cell>53.2</cell></row><row><cell></cell><cell>Swin-L</cell><cell>52.8</cell><cell>54.3</cell></row><row><cell cols="3">COCO-Stuff Dataset</cell><cell></cell></row><row><cell>deeplabv3+</cell><cell>ResNet-50 ResNet-101</cell><cell>35.8 38.1</cell><cell>36.8 39.0</cell></row><row><cell>deeplabv3+ w/ GPaCo</cell><cell>ResNet-50 ResNet-101</cell><cell>37.0 38.8</cell><cell>37.9 40.1</cell></row><row><cell cols="3">PASCAL Context Dataset</cell><cell></cell></row><row><cell>deeplabv3+</cell><cell>ResNet-50 ResNet-101</cell><cell>50.5 52.9</cell><cell>52.1 54.5</cell></row><row><cell>deeplabv3+ w/ GPaCo</cell><cell>ResNet-50 ResNet-101</cell><cell>51.9 54.2</cell><cell>53.7 56.2</cell></row><row><cell></cell><cell>Cityscapes</cell><cell></cell><cell></cell></row><row><cell></cell><cell>ResNet-18</cell><cell>76.9</cell><cell>78.8</cell></row><row><cell>deeplabv3+</cell><cell>ResNet-50</cell><cell>80.1</cell><cell>81.1</cell></row><row><cell></cell><cell>ResNet-101</cell><cell>81.0</cell><cell>82.0</cell></row><row><cell></cell><cell>ResNet-18</cell><cell>78.1</cell><cell>79.7</cell></row><row><cell>deeplabv3+ w/ GPaCo</cell><cell>ResNet-50</cell><cell>80.8</cell><cell>82.0</cell></row><row><cell></cell><cell>ResNet-101</cell><cell>81.4</cell><cell>82.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 8 Top-1 accuracy on CIFAR-100-LT with different imbalance factorsTABLE 9 Top-1 accuracy on full CIFAR-100 with ResNet-50.</head><label>89</label><figDesc>. " ?" represents that models are trained in same setting.</figDesc><table><row><cell>Dataset</cell><cell cols="2">CIFAR-100 LT</cell><cell></cell></row><row><cell>Imbalance factor</cell><cell>100</cell><cell>50</cell><cell>10</cell></row><row><cell>Focal Loss</cell><cell>38.4</cell><cell>44.3</cell><cell>55.8</cell></row><row><cell>LDAM+DRW</cell><cell>42.0</cell><cell>46.6</cell><cell>58.7</cell></row><row><cell>BBN</cell><cell>42.6</cell><cell>47.0</cell><cell>59.1</cell></row><row><cell>Causal Norm</cell><cell>44.1</cell><cell>50.3</cell><cell>59.6</cell></row><row><cell>ResLT</cell><cell>45.3</cell><cell>50.0</cell><cell>60.8</cell></row><row><cell>MiSLAS</cell><cell>47.0</cell><cell>52.3</cell><cell>63.2</cell></row><row><cell>Balanced Softmax  ?</cell><cell>50.8</cell><cell>54.2</cell><cell>63.0</cell></row><row><cell>PaCo  ?</cell><cell>52.0</cell><cell>56.0</cell><cell>64.2</cell></row><row><cell>GPaCo  ?</cell><cell>52.3</cell><cell>56.4</cell><cell>65.4</cell></row><row><cell>Method</cell><cell>dataset</cell><cell cols="2">Top-1 Acc</cell></row><row><cell>CE(baseline)</cell><cell>CIFAR-100</cell><cell cols="2">77.9</cell></row><row><cell>multi-task</cell><cell>CIFAR-100</cell><cell cols="2">78.0</cell></row><row><cell>Supcon</cell><cell>CIFAR-100</cell><cell cols="2">76.5</cell></row><row><cell>PaCo</cell><cell>CIFAR-100</cell><cell cols="2">79.1</cell></row><row><cell>GPaCo</cell><cell>CIFAR-100</cell><cell cols="2">80.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 10 Ablation on augmentation strategies for GPaCo/PaCo on ImageNet-LT with ResNet-50.TABLE 11 Comparison with multi-task re-weighting baselines on ImageNet-LT with ResNet-50.</head><label>1011</label><figDesc>The re-weighting strategy is applied to the supervised contrastive loss. Models are all trained without RandAugment.</figDesc><table><row><cell>Methods</cell><cell>View-1</cell><cell>View-2</cell><cell>Top-1 Acc</cell></row><row><cell>PaCo</cell><cell>SimAug</cell><cell>SimAug</cell><cell>55.0</cell></row><row><cell>PaCo</cell><cell>RandAug</cell><cell>SimAug</cell><cell>57.0</cell></row><row><cell>PaCo</cell><cell>RandAug</cell><cell>RandAug</cell><cell>56.5</cell></row><row><cell>GPaCo</cell><cell>RandAug</cell><cell>SimAug</cell><cell>57.9</cell></row><row><cell>GPaCo</cell><cell>RandAug</cell><cell>RandAugStack</cell><cell>58.5</cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell>Top-1 Acc</cell></row><row><cell>CE</cell><cell></cell><cell></cell><cell>48.4</cell></row><row><cell cols="3">multi-task (CE+Re-weighting)</cell><cell>49.0</cell></row><row><cell cols="2">multi-task (CE+BalSfx)</cell><cell></cell><cell>48.6</cell></row><row><cell>PaCo</cell><cell></cell><cell></cell><cell>51.0</cell></row></table><note>Datasets. ADE20K [40] contains 22K densely annotated im- ages with 150 fine-grained semantic concepts. The training and validation sets consist of 20K and 2K images, respec- tively. COCO-Stuff [41] includes 10K images from the COCO training set. The training and validation sets consist of 9K and 1K images, respectively. It covers 171 classes. For PASCAL Context</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 12 Comparison with multi-task re-weighting baselines that perform center learning rebalance on ImageNet-LT.</head><label>12</label><figDesc>Models are all trained with RandAugment in 400 epochs.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Weight</cell><cell>Top-1 Acc</cell></row><row><cell>multi-task (BalSfx + rw)</cell><cell>ResNeXt-50</cell><cell>0.05</cell><cell>57.0</cell></row><row><cell>multi-task (BalSfx + rw)</cell><cell>ResNeXt-50</cell><cell>0.10</cell><cell>57.1</cell></row><row><cell>multi-task (BalSfx + rw)</cell><cell>ResNeXt-50</cell><cell>0.20</cell><cell>57.1</cell></row><row><cell>multi-task (BalSfx + rw)</cell><cell>ResNeXt-50</cell><cell>0.30</cell><cell>57.0</cell></row><row><cell>multi-task (BalSfx + rw)</cell><cell>ResNeXt-50</cell><cell>0.50</cell><cell>57.2</cell></row><row><cell>multi-task (BalSfx + rw)</cell><cell>ResNeXt-50</cell><cell>0.80</cell><cell>57.2</cell></row><row><cell>multi-task (BalSfx + rw)</cell><cell>ResNeXt-50</cell><cell>1.00</cell><cell>56.9</cell></row><row><cell>PaCo</cell><cell>ResNeXt-50</cell><cell>-</cell><cell>58.2</cell></row><row><cell>multi-task* (BalSfx + rw)</cell><cell>ResNeXt-50</cell><cell>0.50</cell><cell>58.3</cell></row><row><cell>multi-task* (BalSfx + rw)</cell><cell>ResNeXt-50</cell><cell>0.80</cell><cell>58.2</cell></row><row><cell>GPaCo</cell><cell>ResNeXt-50</cell><cell>-</cell><cell>58.9</cell></row><row><cell>multi-task* (BalSfx + rw)</cell><cell>ResNet-50</cell><cell>0.50</cell><cell>57.8</cell></row><row><cell>GPaCo</cell><cell>ResNet-50</cell><cell>-</cell><cell>58.5</cell></row><row><cell>multi-task* (BalSfx + rw)</cell><cell>ResNeXt-101</cell><cell>0.50</cell><cell>60.0</cell></row><row><cell>GPaCo</cell><cell>ResNeXt-101</cell><cell>-</cell><cell>60.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 13 Comparison with multi-task baselines on balanced data.</head><label>13</label><figDesc></figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Weight</cell><cell>Top-1 Acc</cell></row><row><cell cols="3">Full CIFAR-100 Dataset</cell><cell></cell></row><row><cell>multi-task (CE + Supcon)</cell><cell>ResNet-50</cell><cell>0.50</cell><cell>78.0</cell></row><row><cell>PaCo</cell><cell>ResNet-50</cell><cell>-</cell><cell>79.1</cell></row><row><cell>multi-task* (CE + Supcon)</cell><cell>ResNet-50</cell><cell>0.10</cell><cell>79.0</cell></row><row><cell>multi-task* (CE + Supcon)</cell><cell>ResNet-50</cell><cell>0.30</cell><cell>79.1</cell></row><row><cell>multi-task* (CE + Supcon)</cell><cell>ResNet-50</cell><cell>0.50</cell><cell>79.1</cell></row><row><cell>multi-task* (CE + Supcon)</cell><cell>ResNet-50</cell><cell>0.80</cell><cell>78.9</cell></row><row><cell>multi-task* (CE + Supcon)</cell><cell>ResNet-50</cell><cell>1.00</cell><cell>79.0</cell></row><row><cell>GPaCo</cell><cell>ResNet-50</cell><cell>-</cell><cell>80.3</cell></row><row><cell cols="2">Full ImageNet Dataset</cell><cell></cell><cell></cell></row><row><cell>multi-task* (CE + Supcon)</cell><cell>ViT-B</cell><cell>0.50</cell><cell>83.4</cell></row><row><cell>GPaCo</cell><cell>ViT-B</cell><cell>-</cell><cell>84.0</cell></row></table><note>? SimAug: an augmentation policy</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 14 Ablation for the necessary of two-view training on full ImageNet with ViT-B.TABLE 15 Ablation for model ensemble on ImageNet-LT.</head><label>1415</label><figDesc>" ?" represents the ensemble model of a ResNeXt-50 and a ResNeXt-101. "*" represents the results are from their original paper<ref type="bibr" target="#b74">[75]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="2">Full ImageNet(?)</cell><cell cols="3">ImageNet-C(rel. mCE ?)</cell></row><row><cell>single-view</cell><cell>82.5</cell><cell></cell><cell></cell><cell>49.1</cell></row><row><cell>two-view</cell><cell>84.0</cell><cell></cell><cell></cell><cell>47.3</cell></row><row><cell>Method</cell><cell></cell><cell>Many</cell><cell>Medium</cell><cell>Few</cell><cell>All</cell></row><row><cell></cell><cell cols="2">Single Model</cell><cell></cell><cell></cell></row><row><cell cols="2">ResLT *(ResNeXt-50)</cell><cell>63.0</cell><cell>50.5</cell><cell>35.5</cell><cell>52.9</cell></row><row><cell>RIDE (ResNeXt-50)</cell><cell></cell><cell>67.2</cell><cell>49.0</cell><cell>28.1</cell><cell>53.2</cell></row><row><cell cols="2">GPaCo (ResNeXt-50)</cell><cell>67.4</cell><cell>57.1</cell><cell>41.2</cell><cell>58.9</cell></row><row><cell cols="2">GPaCo (ResNeXt-101)</cell><cell>68.7</cell><cell>59.5</cell><cell>42.8</cell><cell>60.8</cell></row><row><cell></cell><cell cols="2">Model Ensemble</cell><cell></cell><cell></cell></row><row><cell>ResLT*</cell><cell></cell><cell>64.0</cell><cell>56.6</cell><cell>44.8</cell><cell>57.6</cell></row><row><cell>RIDE</cell><cell></cell><cell>71.8</cell><cell>53.9</cell><cell>32.0</cell><cell>57.8</cell></row><row><cell cols="2">GPaCo (ResNeXt-50 2-experts)</cell><cell>69.9</cell><cell>59.5</cell><cell>43.4</cell><cell>61.3</cell></row><row><cell>GPaCo  ?</cell><cell></cell><cell>70.1</cell><cell>61.2</cell><cell>45.0</cell><cell>62.4</cell></row><row><cell cols="2">GPaCo (ResNeXt-101 2-experts)</cell><cell>70.9</cell><cell>62.0</cell><cell>45.2</cell><cell>63.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>, the improvements coming from model ensemble is much smaller for GPaCo models when compared with RIDE and ResLT (improvements of ResLT v.s. RIDE v.s. GPaCo are 4.7% v.s. 4.6% v.s. 2.4% ), demonstrating that GPaCo training can reduce prediction variance of trained models.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Generalized Parametric Contrastive Learning Supplementary Material 6 PROOF TO REMARK 1</head><label></label><figDesc>Jiequan Cui received the B.Eng. degree from the computer science department of ShanDong University (SDU) in 2018. He is currently a Ph.D. Candidate at the Chinese University of Hong Kong (CUHK), under the supervision of Prof. Jiaya Jia. He serves as a reviewer for TPAMI, IJCV, CVPR, ICCV, ECCV, ICLR, NeurIPS. His research interests include model generalization and robustness, imbalanced learning, adversarial robustness, neural architecture search, and imagesegmentation. Zhuotao Tian received the B.Eng. degree (Honors) in Computer Science from the School of Computer Science and Technology, Harbin Institute of Technology (HIT) in 2018. He is currently a 3rd year Ph.D. student at the Chinese University of Hong Kong (CUHK), under the supervision of Prof. Jiaya Jia. He serves as a reviewer for IJCV, CVPR, ICCV, ECCV, AAAI. His research interests include few-shot learning, semi-supervised learning, semantic segmentation and scene text detection. Liu now serves as Co-Founder and Technical Head in SmartMore. He received the BS degree from Huazhong University of Science and Technology and the PhD degree from the Chinese University of Hong Kong. He was the winner of 2017 COCO Instance Segmentation Competition and received the Outstanding Reviewer of ICCV in 2019. He continuously served as a reviewer for TPAMI, CVPR, ICCV, NeurIPS, ICLR and etc. His research interests lie in deep learning and computer vision. Yu (Member, IEEE) received the Ph.D. degree from The University of Texas at Austin in 2014. He is currently an Associate Professor in the Department of Computer Science and Engineering, The Chinese University of Hong Kong. He has served as TPC Chair of ACM/IEEE Workshop on Machine Learning for CAD, and in many journal editorial boards and conference committees. He is Editor of IEEE TCCPS Newsletter. He received nine Best Paper Awards from DATE 2022, ICCAD 2021 &amp; 2013, ASP-DAC 2021 &amp; 2012, ICTAI 2019, Integration, the VLSI Journal in 2018, ISPD 2017, SPIE Advanced Lithography Conference 2016, and six ICCAD/ISPD contest awards. Jia received the Ph.D. degree in Computer Science from Hong Kong University of Science and Technology in 2004 and is currently a full professor in Department of Computer Science and Engineering at the Chinese University of Hong Kong (CUHK). He assumes the position of Associate Editor-in-Chief of IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) and is in the editorial board of International Journal of Computer Vision (IJCV). He continuously served as area chairs for ICCV, CVPR, AAAI, ECCV, and several other conferences for the organization. He was on program committees of major conferences in graphics and computational imaging, including ICCP, SIGGRAPH, and SIGGRAPH Asia. He is a Fellow of the IEEE. V</figDesc><table><row><cell>Zhisheng Zhong received the B.Eng. Degree in Communication Engineering from Beijing Uni-versity of Posts and Telecommunications (BUPT) in 2016. He received the Master Degree in Com-puter Science from Peking University (PKU) in 2019. Now he is a Ph.D. student at the Depart-ment of Computer Science Engineering (CSE), the Chinese University of Hong Kong (CUHK). He serves as a reviewer for NeurIPS, CVPR, ICCV, ICLR and etc. His research interests lie in deep learning and computer vision. Shu Bei Jiaya</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>l = ? log p + c + ? ? (log p + 1 + log p + 2 + ... + log p +</figDesc><table><row><cell>ky i</cell><cell>) + ?(p + 1 + p + 2 + ... + p + Ky i</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>w.r.t. ?, p + c and p + i can be written as follows:</figDesc><table><row><cell>? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</cell><cell>?l ?p + i ?? ?l ?l ?p + c</cell><cell cols="3">= ? = p + p + ? i 1 + p + + ? = 0; 2 + ... + p + Ky i = ? 1 c p + + ? = 0;</cell><cell>+ p + c ? p + sum = 0.</cell><cell>(16)</cell></row><row><cell cols="4">From Eq. (16), the optimal solution for p + i and p + c will be with a specific p + sum , the minimal loss value of L i is:</cell><cell cols="2">?p + sum 1 + ?K yi</cell><cell>and</cell><cell>p + sum 1 + ?K yi</cell><cell>respectively. Note that p + sum ? [0, 1],</cell></row><row><cell></cell><cell cols="2">L i = ? log</cell><cell>p + sum 1 + ?K yi</cell><cell cols="2">? ?K yi log</cell><cell>?p +</cell></row></table><note>sum 1 + ?K yi .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>TABLE 16 Comprehensive results on ImageNet-LT with different backbone networks (ResNet-50, ResNeXt-50 &amp; ResNeXt-101).TABLE 17 Comprehensive results on ImageNet-LT with RIDE.</head><label>1617</label><figDesc>Models are trained with RandAugment in 400 epochs. Inference time is calculated with a batch of 64 images on Nvidia GeForce 2080Ti GPU, Pytorch1.5, Python3.6. Models are trained with RandAugment in 400 epochs. Inference time is calculated with a batch of 64 images on Nvidia GeForce 2080Ti GPU, Pytorch1.5, Python3.6.</figDesc><table><row><cell>Backbone</cell><cell cols="2">Method</cell><cell cols="5">Inference time (ms) Many Medium Few</cell><cell>All</cell></row><row><cell></cell><cell cols="2">? -normalize</cell><cell></cell><cell>8.3</cell><cell>65.0</cell><cell>52.2</cell><cell>32.3</cell><cell>54.5</cell></row><row><cell>ResNet-50</cell><cell cols="3">Balanced Softmax</cell><cell>8.3</cell><cell>66.7</cell><cell>52.9</cell><cell>33.0</cell><cell>55.0</cell></row><row><cell></cell><cell cols="2">PaCo</cell><cell></cell><cell>8.3</cell><cell>65.0</cell><cell>55.7</cell><cell>38.2</cell><cell>57.0</cell></row><row><cell></cell><cell cols="2">GPaCo</cell><cell></cell><cell>8.3</cell><cell>66.1</cell><cell>57.3</cell><cell>40.2</cell><cell>58.5</cell></row><row><cell></cell><cell cols="2">? -normalize</cell><cell></cell><cell>13.1</cell><cell>66.4</cell><cell>53.4</cell><cell>38.2</cell><cell>56.0</cell></row><row><cell>ResNeXt-50</cell><cell cols="3">Balanced Softmax</cell><cell>13.1</cell><cell>67.7</cell><cell>53.8</cell><cell>34.2</cell><cell>56.2</cell></row><row><cell></cell><cell cols="2">PaCo</cell><cell></cell><cell>13.1</cell><cell>67.5</cell><cell>56.9</cell><cell>36.7</cell><cell>58.2</cell></row><row><cell></cell><cell cols="2">GPaCo</cell><cell></cell><cell>13.1</cell><cell>67.4</cell><cell>57.1</cell><cell>41.2</cell><cell>58.9</cell></row><row><cell></cell><cell cols="2">? -normalize</cell><cell></cell><cell>25.0</cell><cell>69.0</cell><cell>55.1</cell><cell>36.9</cell><cell>57.9</cell></row><row><cell>ResNeXt-101</cell><cell cols="3">Balanced Softmax</cell><cell>25.0</cell><cell>69.2</cell><cell>55.8</cell><cell>36.3</cell><cell>58.0</cell></row><row><cell></cell><cell cols="2">PaCo</cell><cell></cell><cell>25.0</cell><cell>68.2</cell><cell>58.7</cell><cell>41.0</cell><cell>60.0</cell></row><row><cell></cell><cell cols="2">GPaCo</cell><cell></cell><cell>25.0</cell><cell>68.7</cell><cell>59.5</cell><cell>42.8</cell><cell>60.8</cell></row><row><cell>Backbone</cell><cell></cell><cell>Method</cell><cell cols="5">Inference time (ms) Many Medium Few</cell><cell>All</cell></row><row><cell></cell><cell></cell><cell>1 expert</cell><cell>8.2</cell><cell></cell><cell>64.8</cell><cell>49.8</cell><cell>29.6</cell><cell>52.8</cell></row><row><cell cols="2">RIDEResNet</cell><cell>2 experts</cell><cell cols="2">12.0</cell><cell>67.7</cell><cell>53.5</cell><cell>31.5</cell><cell>56.0</cell></row><row><cell></cell><cell></cell><cell>3 experts</cell><cell cols="2">15.3</cell><cell>69.0</cell><cell>54.7</cell><cell>32.5</cell><cell>57.0</cell></row><row><cell></cell><cell></cell><cell>1 expert</cell><cell cols="2">13.0</cell><cell>67.2</cell><cell>49.0</cell><cell>28.1</cell><cell>53.2</cell></row><row><cell cols="2">RIDEResNeXt</cell><cell>2 experts</cell><cell cols="2">19.0</cell><cell>70.4</cell><cell>52.6</cell><cell>30.3</cell><cell>56.4</cell></row><row><cell></cell><cell></cell><cell>3 experts</cell><cell cols="2">26.0</cell><cell>71.8</cell><cell>53.9</cell><cell>32.0</cell><cell>57.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>TABLE 18 Comprehensive results on iNaturalist 2018 with ResNet-50 and ResNet-152TABLE 19 Comprehensive results on iNaturalist 2018 with RIDE.</head><label>1819</label><figDesc>. ?represents the models are trained without RandAugment. Inference time is calculated with a batch of 64 images on Nvidia GeForce 2080Ti GPU, Pytorch1.5, Python3.6. Models are trained with RandAugment in 400 epochs without knowledge distillation.</figDesc><table><row><cell>Backbone</cell><cell>Method</cell><cell cols="4">Inference time (ms) Many Medium Few</cell><cell>All</cell></row><row><cell></cell><cell>? -normalize</cell><cell>8.3</cell><cell>74.1</cell><cell>72.1</cell><cell>70.4</cell><cell>71.5</cell></row><row><cell>ResNet-50</cell><cell>Balanced Softmax</cell><cell>8.3</cell><cell>72.3</cell><cell>72.6</cell><cell>71.7</cell><cell>71.8</cell></row><row><cell></cell><cell>PaCo</cell><cell>8.3</cell><cell>70.3</cell><cell>73.2</cell><cell>73.6</cell><cell>73.2</cell></row><row><cell>ResNet-50  ?</cell><cell>Balanced Softmax PaCo</cell><cell>8.3 8.3</cell><cell>72.5 69.5</cell><cell>72.3 73.4</cell><cell>71.4 73.0</cell><cell>71.7 73.0</cell></row><row><cell></cell><cell>GPaCo</cell><cell>8.3</cell><cell>73.1</cell><cell>75.4</cell><cell>75.9</cell><cell>75.4</cell></row><row><cell>ResNet-152</cell><cell>PaCo</cell><cell>20.1</cell><cell>75.0</cell><cell>75.5</cell><cell>74.7</cell><cell>75.2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DARTS: differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast and practical neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Once-for-all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ImageNet large scale visual recognition challenge,&quot; IJCV</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for ccene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2022</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="0" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent -A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">?</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
		<editor>NeurIPS, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<editor>NeurIPS, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<editor>NeurIPS, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A systematic study of the class imbalance problem in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Buda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Mazurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning deep representation for imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SMOTE: Synthetic minority over-sampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Relay backpropagation for effective learning of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Balanced meta-softmax for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<editor>NeurIPS, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Decoupling representation and classifier for longtailed recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Long-tailed recognition by routing diverse distribution-aware experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/2010.01809</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reslt: Residual learning for long-tailed recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno>abs/2101.10633</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Long-tailed classification by keeping the good and removing the bad momentum causal effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<editor>NeurIPS, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving calibration for longtailed recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="16" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Region rebalance for long-tailed semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.01969</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="page" from="15" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning imbalanced datasets with label-distribution-aware margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ar?chiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<editor>NeurIPS, H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. B. Fox, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Largescale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The iNaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="633" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Coco-stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="1209" to="1218" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="891" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Towards good practices for recognition &amp; detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A systematic study of the class imbalance problem in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Buda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Mazurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">What is the effect of importance weighting in deep learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">in ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">The class imbalance problem: A systematic study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Japkowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stephen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>Intelligent Data Analysis</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning deep representation for imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep imbalanced learning for face recognition and attribute prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Learning to model the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Metaweight-net: Learning an explicit mapping for sample weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Rethinking class-balanced methods for long-tailed visual recognition from a domain adaptation perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Jamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Equalization loss for long-tailed object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Bbn: Bilateralbranch network with cumulative learning for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-M</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02413</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>abs/1807.03748</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Elf: An early-exiting framework for long-tailed classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Duggal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dhamnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Horng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11979</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<editor>NeurIPS, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Learning imbalanced datasets with label-distribution-aware margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Piecewise classifier mappings: Learning fine-grained learners for novel categories with few examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Shufflenet V2: practical guidelines for efficient CNN architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<editor>ECCV, V. Ferrari, M. Hebert, C. Sminchisescu, and Y. Weiss</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno>abs/1708.04552</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Man?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Benchmarking neural network robustness to common corruptions and surface variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01697</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">The many faces of robustness: A critical analysis of out-of-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dorundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parajuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8340" to="8349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Learning robust global representations by penalizing local predictive power</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Contributors</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/mmsegmentation" />
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="12" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="418" to="434" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Factors in finetuning deep model for object detection with long-tail distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Contrastive learning with stronger augmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=KJSCAsN14" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Reslt: Residual learning for long-tailed recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
