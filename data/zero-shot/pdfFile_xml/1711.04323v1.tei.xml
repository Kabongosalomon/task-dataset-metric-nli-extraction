<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">High-Order Attention Models for Visual Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Schwartz</surname></persName>
							<email>idansc@cs.technion.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science Technion</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
							<email>aschwing@illinois.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamir</forename><surname>Hazan</surname></persName>
							<email>tamir.hazan@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Industrial Engineering &amp; Management Technion</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">High-Order Attention Models for Visual Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The quest for algorithms that enable cognitive abilities is an important part of machine learning. A common trait in many recently investigated cognitive-like tasks is that they take into account different data modalities, such as visual and textual input. In this paper we propose a novel and generally applicable form of attention mechanism that learns high-order correlations between various data modalities. We show that high-order correlations effectively direct the appropriate attention to the relevant elements in the different data modalities that are required to solve the joint task. We demonstrate the effectiveness of our high-order attention mechanism on the task of visual question answering (VQA), where we achieve state-of-the-art performance on the standard VQA dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The quest for algorithms which enable cognitive abilities is an important part of machine learning and appears in many facets, e.g., in visual question answering tasks <ref type="bibr" target="#b5">[6]</ref>, image captioning <ref type="bibr" target="#b25">[26]</ref>, visual question generation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b9">10]</ref> and machine comprehension <ref type="bibr" target="#b7">[8]</ref>. A common trait in these recent cognitive-like tasks is that they take into account different data modalities, for example, visual and textual data.</p><p>To address these tasks, recently, attention mechanisms have emerged as a powerful common theme, which provides not only some form of interpretability if applied to deep net models, but also often improves performance <ref type="bibr" target="#b7">[8]</ref>. The latter effect is attributed to more expressive yet concise forms of the various data modalities. Present day attention mechanisms, like for example <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b25">26]</ref>, are however often lacking in two main aspects. First, the systems generally extract abstract representations of data in an ad-hoc and entangled manner. Second, present day attention mechanisms are often geared towards a specific form of input and therefore hand-crafted for a particular task.</p><p>To address both issues, we propose a novel and generally applicable form of attention mechanism that learns high-order correlations between various data modalities. For example, second order correlations can model interactions between two data modalities, e.g., an image and a question, and more generally, k?th order correlations can model interactions between k modalities. Learning these correlations effectively directs the appropriate attention to the relevant elements in the different data modalities that are required to solve the joint task.</p><p>We demonstrate the effectiveness of our novel attention mechanism on the task of visual question answering (VQA), where we achieve state-of-the-art performance on the VQA dataset <ref type="bibr" target="#b1">[2]</ref>. Some</p><p>What does the man have on his head? How many cars are in the picture?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original Image Unary Potentials Pairwise Potentials Final Attention</head><p>What does the man have on his head?</p><p>What does the man have on his head?</p><p>What does the man have on his head? How many cars are in the picture?</p><p>How many cars are in the picture?</p><p>How many cars are in the picture? <ref type="figure">Figure 1</ref>: Results of our multi-modal attention for one image and two different questions (1 st column). The unary image attention is identical by construction. The pairwise potentials differ for both questions and images since both modalities are taken into account (3 rd column). The final attention is illustrated in the 4 th column.</p><p>of our results are visualized in <ref type="figure">Fig. 1</ref>, where we show how the visual attention correlates with the textual attention.</p><p>We begin by reviewing the related work. We subsequently provide details of our proposed technique, focusing on the high-order nature of our attention models. We then conclude by presenting the application of our high-order attention mechanism to VQA and compare it to the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Attention mechanisms have been investigated for both image and textual data. In the following we review mechanisms for both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image attention mechanisms:</head><p>Over the past few years, single image embeddings extracted from a deep net (e.g., <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16]</ref>) have been extended to a variety of image attention modules, when considering VQA. For example, a textual long short term memory net (LSTM) may be augmented with a spatial attention <ref type="bibr" target="#b28">[29]</ref>. Similarly, Andreas et al. <ref type="bibr" target="#b0">[1]</ref> employ a language parser together with a series of neural net modules, one of which attends to regions in an image. The language parser suggests which neural net module to use. Stacking of attention units was also investigated by Yang et al. <ref type="bibr" target="#b26">[27]</ref>. Their stacked attention network predicts the answer successively. Dynamic memory network modules which capture contextual information from neighboring image regions has been considered by Xiong et al. <ref type="bibr" target="#b23">[24]</ref>. Shih et al. <ref type="bibr" target="#b22">[23]</ref> use object proposals and and rank regions according to relevance. The multi-hop attention scheme of Xu et al. <ref type="bibr" target="#b24">[25]</ref> was proposed to extract fine-grained details. A joint attention mechanism was discussed by Lu et al. <ref type="bibr" target="#b14">[15]</ref> and Fukui et al. <ref type="bibr" target="#b6">[7]</ref> suggest an efficient outer product mechanism to combine visual representation and text representation before applying attention over the combined representation. Additionally, they suggested the use of glimpses. Very recently, Kazemi et al. <ref type="bibr" target="#b10">[11]</ref> showed a similar approach using concatenation instead of outer product. Importantly, all of these approaches model attention as a single network. The fact that multiple modalities are involved is often not considered explicitly which contrasts the aforementioned approaches from the technique we present.</p><p>Very recently Kim et al. <ref type="bibr" target="#b13">[14]</ref> presented a technique that also interprets attention as a multi-variate probabilistic model, to incorporate structural dependencies into the deep net. Other recent techniques are work by Nam et al. <ref type="bibr" target="#b18">[19]</ref> on dual attention mechanisms and work by Kim et al. <ref type="bibr" target="#b12">[13]</ref> on bilinear models. In contrast to the latter two models our approach is easy to extend to any number of data modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Textual attention mechanisms:</head><p>We also want to provide a brief review of textual attention. To address some of the challenges, e.g., long sentences, faced by translation models, Hermann et al. <ref type="bibr" target="#b7">[8]</ref> proposed RNNSearch. To address the challenges which arise by fixing the latent dimension of neural nets processing text data, Bahdanau et al. <ref type="bibr" target="#b2">[3]</ref> first encode a document and a query via a bidirectional LSTM which are then used to compute attentions. This mechanism was later refined in <ref type="bibr" target="#b21">[22]</ref> where a word based technique reasons about sentence representations. Joint attention between two CNN hierarchies is discussed by Yin et al. <ref type="bibr" target="#b27">[28]</ref>.</p><p>Among all those attention mechanisms, relevant to our approach is work by Lu et al. <ref type="bibr" target="#b14">[15]</ref> and the approach presented by Xu et al. <ref type="bibr" target="#b24">[25]</ref>. Both discuss attention mechanisms which operate jointly over two modalities. Xu et al. <ref type="bibr" target="#b24">[25]</ref> use pairwise interactions in the form of a similarity matrix, but ignore the attentions on individual data modalities. Lu et al. <ref type="bibr" target="#b14">[15]</ref> suggest an alternating model, that directly combines the features of the modalities before attending. Additionally, they suggested a parallel model which uses a similarity matrix to map features for one modality to the other. It is hard to extend this approach to more than two modalities. In contrast, our model develops a probabilistic model, based on high order potentials and performs mean-field inference to obtain marginal probabilities. This permits trivial extension of the model to any number of modalities.</p><p>Additionally, Jabri et al. <ref type="bibr" target="#b8">[9]</ref> propose a model where answers are also used as inputs. Their approach questions the need of attention mechanisms and develops an alternative solution based on binary classification. In contrast, our approach captures high-order attention correlations, which we found to improve performance significantly.</p><p>Overall, while there is early work that propose a combination of language and image attention for VQA, e.g., <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b11">12]</ref>, attention mechanism with several potentials haven't been discussed in detail yet. In the following we present our approach for joint attention over any number of modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Higher order attention models</head><p>Attention modules are a crucial component for present day decision making systems. Particularly when taking into account more and more data of different modalities, attention mechanisms are able to provide insights into the inner workings of the oftentimes abstract and automatically extracted representations of our systems.</p><p>An example of such a system that captured a lot of research efforts in recent years is Visual Question Answering (VQA). Considering VQA as an example, we immediately note its dependence on two or even three different data modalities, the visual input V , the question Q and the answer A, which get processed simultaneously. More formally, we let</p><formula xml:id="formula_0">V ? R nv?d , Q ? R nq?d , A ? R na?d</formula><p>denote a representation for the visual input, the question and the answer respectively. Hereby, n v , n q and n a are the number of pixels, the number of words in the question, and the number of possible answers. We use d to denote the dimensionality of the data. For simplicity of the exposition we assume d to be identical across all data modalities.</p><p>Due to this dependence on multiple data modalities, present day decision making systems can be decomposed into three major parts: (i) the data embedding; (ii) attention mechanisms; and (iii) the decision making. For a state-of-the-art VQA system such as the one we developed here, those three parts are immediately apparent when considering the high-level system architecture outlined in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data embedding</head><p>Attention modules deliver to the decision making component a succinct representation of the relevant data modalities. As such, their performance depends on how we represent the data modalities themselves. Oftentimes, an attention module tends to use expressive yet concise data embedding algorithms to better capture their correlations and consequently to improve the decision making performance. For example, data embeddings based on convolutional deep nets which constitute the state-of-the-art in many visual recognition and scene understanding tasks. Language embeddings heavily rely on LSTM which are able to capture context in sequential data, such as words, phrases and sentences. We give a detailed account to our data embedding architectures for VQA in Sec. 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ResNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concatenate Word Embedding</head><p>Is the dog trying to catch a frisbee? </p><formula xml:id="formula_1">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? Data embedding (Sec. 3.1) ? ? ? ? ? ? ? ? ? ? ? Attention (Sec. 3.2) Decision (Sec. 3.3) Figure 2:</formula><p>Our state-of-the-art VQA system</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Attention</head><p>As apparent from the aforementioned description, attention is the crucial component connecting data embeddings with decision making modules.</p><p>Subsequently we denote attention over the n q words in the question via</p><formula xml:id="formula_2">P Q (i q ), where i q ? {1, . . . , n q } is the word index. Similarly, attention over the image is referred to via P V (i v ), where i v ? {1, .</formula><p>. . , n v }, and attention over the possible answers are denoted</p><formula xml:id="formula_3">P A (i a ), where i a ? {1, . . . , n a }.</formula><p>We consider the attention mechanism as a probability model, with each attention mechanism computing "potentials." First, unary potentials ? V , ? Q , ? A denote the importance of each feature (e.g., question word representations, multiple choice answers representations, and image patch features) for the VQA task. Second, pairwise potentials, ? V,Q , ? V,A , ? Q,A express correlations between two modalities. Last, third-order potential, ? V,Q,A captures dependencies between the three modalities.</p><p>To obtain marginal probabilities P Q , P V and P A from potentials, our model performs mean-field inference. We combine the unary potential, the marginalized pairwise potential and the marginalized third order potential linearly including a bias term:</p><formula xml:id="formula_4">P V (i v ) = smax(? 1 ? V (i v )+? 2 ? V,Q (i v )+? 3 ? A,V (i v )+? 4 ? V,Q,A (i v ) + ? 5 ), P Q (i q ) = smax(? 1 ? Q (i q )+? 2 ? V,Q (i q )+? 3 ? A,Q (i q )+? 4 ? V,Q,A (i q ) + ? 5 ), (1) P A (i a ) = smax(? 1 ? A (i a )+? 2 ? A,V (i a )+? 3 ? A,Q (i a )+? 4 ? V,Q.A (i a ) + ? 5 ).</formula><p>Hereby ? i , ? i , and ? i are learnable parameters and smax(?) refers to the soft-max operation over</p><formula xml:id="formula_5">i v ? {1, . . . , n v }, i q ? {1, .</formula><p>. . , n q } and i a ? {1, . . . , n a } respectively. The soft-max converts the combined potentials to probability distributions, which corresponds to a single mean-field iteration. Such a linear combination of potentials provides extra flexibility for the model, since it can learn the reliability of the potential from the data. For instance, we observe that question attention relies more on the unary question potential and on pairwise question and answer potentials. In contrast, the image attention relies more on the pairwise question and image potential.</p><p>Given the aforementioned probabilities P V , P Q , and P A , the attended image, question and answer vectors are denoted by a V ? R d , a Q ? R d and a A ? R d . The attended modalities are calculated as the weighted sum of the image features V = [v 1 , . . . , v nv ] T ? R nv?d , the question features Q = [q 1 , . . . , q nq ] T ? R nq?d , and the answer features A = [a 1 , . . . , a na ] T ? R na?d , i.e.,   The attended modalities, which effectively focus on the data relevant for the task, are passed to a classifier for decision making, e.g., the ones discussed in Sec. 3.3. In the following we now describe the attention mechanisms for unary, pairwise and ternary potentials in more detail.</p><formula xml:id="formula_6">a V = nv iv=1 P V (i v )v iv , a Q = nq iq=1 P Q (i q )q iq , and a V = na ia=1 P A (i a )a ia .</formula><formula xml:id="formula_7">Q V ? Q,V (i q ) ? Q,V (i v )</formula><formula xml:id="formula_8">Q V A ? Q,V,A (i q ) ? Q,V,A (i v ) ? Q,V,A (i a ) (a) (b) (c)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Unary potentials</head><p>We illustrate the unary attention schematically in <ref type="figure" target="#fig_0">Fig. 3 (a)</ref>. The input to the unary attention module is a data representation, i.e., either the visual representation V , the question representation Q, or the answer representation A. Using those representations, we obtain the 'unary potentials' ? V , ? Q and ? A using a convolution operation with kernel size 1 ? 1 over the data representation as an additional embedding step, followed by a non-linearity (tanh in our case), followed by another convolution operation with kernel size 1 ? 1 to reduce embedding dimensionality. Since convolutions with kernel size 1 ? 1 are identical to matrix multiplies we formally obtain the unary potentials via</p><formula xml:id="formula_9">? V (i v ) = tanh(V W v2 )W v1 , ? Q (i q ) = tanh(QW q2 )W q1 , ? A (i a ) = tanh(AW a2 )W a1 . where W v1 , W q1 , W a1 ? R d?1 , and W v2 , W q2 , W a2 ? R d?d are trainable parameters.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Pairwise potentials</head><p>Besides the mentioned mechanisms to generate unary potentials, we specifically aim at taking advantage of pairwise attention modules, which are able to capture the correlation between the representation of different modalities. Our approach is illustrated in <ref type="figure" target="#fig_0">Fig. 3 (b)</ref>. We use a similarity matrix between image and question modalities C 2 = QW q (V W v ) . Alternatively, the (i, j)-th entry is the correlation (inner-product) of the i-th column of QW q and the j-th column of V W v :</p><formula xml:id="formula_10">(C 2 ) i,j = corr 2 ((QW q ) :,i , (V W v ) :,j ), corr 2 (q, v) = d l=1 q l v l .</formula><p>where W q , W v ? R d?d are trainable parameters. We consider (C 2 ) i,j as a pairwise potential that represents the correlation of the i-th word in a question and the j-th patch in an image. Therefore, to retrieve the attention for a specific word, we convolve the matrix along the visual dimension using a 1 ? 1 dimensional kernel. Specifically,</p><formula xml:id="formula_11">? V,Q (i q ) = tanh nv iv=1 w iv (C 2 ) iv,iq , and ? V,Q (i v ) = tanh ? ? nq iq=1 w iq (C 2 ) iv,iq ? ? .</formula><p>Similarly, we obtain ? A,V and ? A,Q , which we omit due to space limitations. These potentials are used to compute the attention probabilities as defined in Eq. (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Ternary Potentials</head><p>To capture the dependencies between all three modalities, we consider their high-order correlations.  Where W q , W v , W a ? R d?d are trainable parameters. Similarly to the pairwise potentials, we use the C 3 tensor to obtain correlated attention for each modality:</p><formula xml:id="formula_12">(C 3 ) i,j,k = corr 3 ((QW q ) :,i , (V W v ) :,j , (AW a ) :,k ), corr 3 (q, v, a) = d l=1 q l v l a l .</formula><formula xml:id="formula_13">? V,Q,A (i q ) = tanh nv iv=1 na ia=1 w iv,ia (C 3 ) iq,iv,ia , ? V,Q,A (i v ) = tanh ? ? nq iq=1 na ia=1 w iq,ia (C 3 ) iq,iv,ia ? ? and ? V,Q,A (i a ) = tanh ? ? nv iv=1 nq iq=1 w iq,ia (C 3 ) iq,iv,ia ? ? .</formula><p>These potentials are used to compute the attention probabilities as defined in Eq. (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Decision making</head><p>The decision making component receives as input the attended modalities and predicts the desired output. Each attended modality is a vector that consists of the relevant data for making the decision. While the decision making component can consider the modalities independently, the nature of the task usually requires to take into account correlations between the attended modalities. The correlation of a set of attended modalities are represented by the outer product of their respective vectors, e.g., the correlation of two attended modalities is represented by a matrix and the correlation of k-attended modalities is represented by a k-dimensional tensor.</p><p>Ideally, the attended modalities and their high-order correlation tensors are fed into a deep net which produces the final decision. The number of parameters in such a network grows exponentially in the number of modalities, as seen in <ref type="figure" target="#fig_1">Fig. 4</ref>. To overcome this computational bottleneck, we follow the tensor sketch algorithm of Pham and Pagh <ref type="bibr" target="#b20">[21]</ref>, which was recently applied to attention models by Fukui et al. <ref type="bibr" target="#b6">[7]</ref> via Multimodal Compact Bilinear Pooling (MCB) in the pairwise setting or Multimodal Compact Trilinear Pooling (MCT), an extension of MCB that pools data from three modalities. The tensor sketch algorithm enables us to reduce the dimension of any rank-one tensor while referring to it implicitly. It relies on the count sketch technique <ref type="bibr" target="#b3">[4]</ref> that randomly embeds an attended vector a ? R d1 into another Euclidean space ?(a) ? R d2 . The tensor sketch algorithm then projects the rank-one tensor ? k i=1 a i which consists of attention correlations of order k using the convolution ?(? k i=1 a i ) = * k i=1 ?(a i ). For example, for two attention modalities, the correlation matrix a 1 a 2 = a 1 ?a 2 is randomly projected to R d2 by the convolution ?(a 1 ?a 2 ) = ?(a 1 ) * ?(a 2 ). The attended modalities ?(a i ) and their high-order correlations ?(? k i=1 a i ) are fed into a fully connected neural net to complete decision making.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Visual question answering</head><p>In the following we evaluate our approach qualitatively and quantitatively. Before doing so we describe the data embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data embedding</head><p>The attention module requires the question representation Q ? R nq?d , the image representation V ? R nv?d , and the answer representation A ? R na?d , which are computed as follows.</p><p>Image embedding: To embed the image, we use pre-trained convolutional deep nets (i.e., VGG-19, ResNet). We extract the last layer before the fully connected units. Its dimension in the VGG net case is 512 ? 14 ? 14 and the dimension in the ResNet case is 2048 ? 14 ? 14. Hence we obtain Question embedding: To obtain a question representation, Q ? R nq?d , we first map a 1-hot encoding of each word in the question into a d-dimensional embedding space using a linear transformation plus corresponding bias terms. To obtain a richer representation that accounts for neighboring words, we use a 1-dimensional temporal convolution with filter of size 3. While a combination of multiple sized filters is suggested in the literature <ref type="bibr" target="#b14">[15]</ref>, we didn't find any benefit from using such an approach. Subsequently, to capture long-term dependencies, we used a Long Short Term Memory (LSTM) layer. To reduce overfitting caused by the LSTM units, we used two LSTM layers with d/2 hidden dimension, one uses as input the word embedding representation, and the other one operates on the 1D conv layer output. Their output is then concatenated to obtain Q. We also note that n q is a constant hyperparameter, i.e., questions with more than n q words are cut, while questions with less words are zero-padded.</p><p>Answer embedding: To embed the possible answers we use a regular word embedding. The vocabulary is specified by taking only the most frequent answers in the training set. Answers that are not included in the top answers are embedded to the same vector. Answers containing multiple words are embedded as n-grams to a single vector. We assume there is no real dependency between the answers, therefore there is no need of using additional 1D conv, or LSTM layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Decision making</head><p>For our VQA example we investigate two techniques to combine vectors from three modalities. First, the attended feature representation for each modality, i.e., a V , a A and a Q , are combined using an MCT unit. Each feature element is of the form ((a V ) i ? (a Q ) j ? (a A ) k ). While this first solution is most general, in some cases like VQA, our experiments show that it is better to use our second approach, a 2-layer MCB unit combination. This permits greater expressiveness as we employ features of the form ((a V ) i ? (a Q ) j ? (a Q ) k ? (a A ) t ) therefore also allowing image features to interact with themselves. Note that in terms of parameters both approaches are identical as neither MCB nor MCT are parametric modules.</p><p>Beyond MCB, we tested several other techniques that were suggested in the literature, including element-wise multiplication, element-wise addition and concatenation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b10">11]</ref>, optionally followed by another hidden fully connected layer. The tensor sketching units consistently performed best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Experimental setup: We use the RMSProp optimizer with a base learning rate of 4e ?4 and ? = 0.99 as well as = 1e ?8 . The batch size is set to 300. The dimension d of all hidden layers is set to 512. The MCB unit feature dimension was set to d = 8192. We apply dropout with a rate of 0.5 after the word embeddings, the LSTM layer, and the first conv layer in the unary potential units. Additionally, for the last fully connected layer we use a dropout rate of 0.3. We use the top 3000 most frequent How many glasses are on the <ref type="table">table?   How many glasses are on the table?  How many glasses are on the table?</ref> Is anyone in the scene wearing blue? Is anyone in the scene wearing blue? Is anyone in the scene wearing blue?</p><p>What kind of flooring is in the bathroom? What kind of flooring is in the bathroom? What kind of flooring is in the bathroom? What room is this? What room is this? What room is this?   <ref type="figure">Figure 6</ref>: The attention generated for two different questions over three modalities. We find the attention over multiple choice answers to emphasis the unusual answers. answers as possible outputs, which covers 91% of all answers in the train set. We implemented our models using the Torch framework 1 <ref type="bibr" target="#b4">[5]</ref>.</p><p>As a comparison for our attention mechanism we use the approach of Lu et al. <ref type="bibr" target="#b14">[15]</ref> and the technique of Fukui et al. <ref type="bibr" target="#b6">[7]</ref>. Their methods are based on a hierarchical attention mechanism and multi-modal compact bilinear (MCB) pooling. In contrast to their approach we demonstrate a relatively simple technique based on a probabilistic intuition grounded on potentials. For comparative reasons only, the visualized attention is based on two modalities: image and question.</p><p>We evaluate our attention modules on the VQA real-image test-dev and test-std datasets <ref type="bibr" target="#b1">[2]</ref>. The dataset consists of 123, 287 training images and 81, 434 test set images. Each image comes with 3 questions along with 18 multiple choice answers.</p><p>Quantitative evaluation: We first evaluate the overall performance of our model and compare it to a variety of baselines. Tab. 1 shows the performance of our model and the baselines on the test-dev and the test-standard datasets for multiple choice (MC) questions. To obtain multiple choice results we follow common practice and use the highest scoring answer among the provided ones. Our approach <ref type="figure">(Fig. 2)</ref> for the multiple choice answering task achieved the reported result after 180,000 iterations, which requires about 40 hours of training on the 'train+val' dataset using a TitanX GPU. Despite the fact that our model has only 40 million parameters, while techniques like <ref type="bibr" target="#b6">[7]</ref> use over 70 million parameters, we observe state-of-the-art behavior. Additionally, we employ a 2-modality model having a similar experimental setup. We observe a significant improvement for our 3-modality model, which shows the importance of high-order attention models. Due to the fact that we use a lower embedding dimension of 512 (similar to <ref type="bibr" target="#b14">[15]</ref>) compared to 2048 of existing 2-modality models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b6">7]</ref>, the 2-modality model achieves inferior performance. We believe that higher embedding dimension and proper tuning can improve our 2-modality starting point.</p><p>Additionally, we compared our proposed decision units. MCT, which is a generic extension of MCB for 3-modalities, and 2-layers MCB which has greater expressiveness (Sec. 4.2). Evaluating on the 'val' dataset while training on the 'train' part using the VGG features, the MCT setup yields 63.82% Is she using a battery-operated device?</p><p>Is she using a battery-operated device?</p><p>Is she using a battery device? Ours: yes <ref type="bibr" target="#b14">[15]</ref>: no <ref type="bibr" target="#b6">[7]</ref>  where 2-layer MCB yields 64.57%. We also tested a different ordering of the input to the 2-modality MCB and found them to yield inferior results.</p><p>Qualitative evaluation: Next, we evaluate our technique qualitatively. In <ref type="figure" target="#fig_2">Fig. 5</ref> we illustrate the unary, pairwise and combined attention of our approach based on the two modality architecture, without the multiple choice as input. For each image we show multiple questions. We observe the unary attention usually attends to strong features of the image, while pairwise potentials emphasize areas that correlate with question words. Importantly, the combined result is dependent on the provided question. For instance, in the first row we observe for the question "How many glasses are on the table?," that the pairwise potential reacts to the image area depicting the glass. In contrast, for the question "Is anyone in the scene wearing blue?" the pairwise potentials reacts to the guy with the blue shirt. In <ref type="figure">Fig. 6</ref>, we illustrate the attention for our 3-modality model. We find the attention over multiple choice answers to favor the more unusual results.</p><p>In <ref type="figure">Fig. 7</ref>, we compare the final attention obtained from our approach to the results obtained with techniques discussed in <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b6">[7]</ref>. We observe that our approach attends to reasonable pixel and question locations. For example, considering the first row in <ref type="figure">Fig. 7</ref>, the question refers to the battery operated device. Compared to existing approaches, our technique attends to the laptop, which seems to help in choosing the correct answer. In the second row, the question wonders "Is this a boy or a girl?". Both of the correct answers were produced when the attention focuses on the hair.</p><p>In <ref type="figure" target="#fig_3">Fig. 8</ref>, we illustrate a failure case, where the attention of our approach is identical, despite two different input questions. Our system focuses on the colorful umbrella as opposed to the object queried for in the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper we investigated a series of techniques to design attention for multimodal input data.</p><p>Beyond demonstrating state-of-the-art performance using relatively simple models, we hope that this work inspires researchers to work in this direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of our k?order attention. (a) unary attention module (e.g., visual). (b) pairwise attention module (e.g., visual and question) marginalized over its two data modalities. (c) ternary attention module (e.g., visual, question and answer) marginalized over its three data modalities..</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of correlation units used for decision making. (a) MCB unit approximately sample from outer product space of two attention vectors, (b) MCT unit approximately sample from outer product space of three attention vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>For each image (1 st column) we show the attention generated for two different questions in columns 2-4 and columns 5-7 respectively. The attentions are ordered as unary attention, pairwise attention and combined attention for both the image and the question. We observe the combined attention to significantly depend on the question.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 :</head><label>8</label><figDesc>Failure cases: Unary, pairwise and combined attention of our approach. Our system focuses on the colorful umbrella as opposed to the table in the first row.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Comparison of results on the Multiple-Choice VQA dataset for a variety of methods. We observe the combination of all three unary, pairwise and ternary potentials to yield the best result.</figDesc><table><row><cell>test-dev</cell><cell>test-std</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>The fourth column provides the question and the answer of the different techniques.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>: no</cell></row><row><cell></cell><cell></cell><cell></cell><cell>GT: yes</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Is this boy</cell></row><row><cell></cell><cell></cell><cell></cell><cell>or a girl?</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Ours: girl</cell></row><row><cell></cell><cell></cell><cell></cell><cell>[15]: boy</cell></row><row><cell></cell><cell></cell><cell></cell><cell>[7]: girl</cell></row><row><cell>Is this a boy or a girl?</cell><cell>Is this a boy or a girl?</cell><cell></cell><cell>GT: girl</cell></row><row><cell cols="4">Figure 7: Comparison of our attention results (2 nd column) with attention provided by [15] (3 rd column)</cell></row><row><cell>and [7] (4 th column). What color is the table?</cell><cell>What color is the table?</cell><cell>What color is the table?</cell><cell>What color is the table? GT: brown Ours: blue</cell></row><row><cell></cell><cell></cell><cell></cell><cell>What color is</cell></row><row><cell></cell><cell></cell><cell></cell><cell>the umbrella?</cell></row><row><cell></cell><cell></cell><cell></cell><cell>GT: blue</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Ours: blue</cell></row><row><cell>What color is the umbrella?</cell><cell>What color is the umbrella?</cell><cell>What color is the umbrella?</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/idansc/HighOrderAtten</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: This research was supported in part by The Israel Science Foundation (grant No. 948/15). This material is based upon work supported in part by the National Science Foundation under Grant No. 1718221. We thank Nvidia for providing GPUs used in this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning to compose neural networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.01705</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Finding frequent items in data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moses</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Farach-Colton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICALP</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop, number EPFL-CONF-192376</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Human attention in visual question answering: Do humans and deep networks look at the same regions?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03556</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01847</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Revisiting visual question answering baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Creativity: Generating Diverse Questions using Variational Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2017. * equal contribution</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Show, ask, attend, and answer: A strong baseline for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vahid</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Elqursh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03162</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multimodal residual learning for visual qa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Woo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyun</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Oh</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Kyoung-Woon On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.04325</idno>
		<title level="m">Hadamard product for low-rank bilinear pooling</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luong</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.00887</idno>
		<title level="m">Structured attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning to answer questions from image using convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.00333</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ask your neurons: A neural-based approach to answering questions about images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06059</idno>
		<title level="m">Generating natural questions about an image</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Dual attention networks for multimodal reasoning and matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00471</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Training recurrent answering units with joint loss minimization for vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03647</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast and scalable polynomial kernels via explicit feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ninh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rasmus</forename><surname>Pagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD. ACM</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reasoning about entailment with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom??</forename><surname>Ko?isk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Where to look: Focus regions for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Dynamic memory networks for visual and textual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01417</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ask, attend and answer: Exploring question-guided spatial attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="451" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Abcnn: Attention-based convolutional neural network for modeling sentence pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.05193</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visual7w: Grounded question answering in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
