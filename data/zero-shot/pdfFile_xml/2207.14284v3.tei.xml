<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HorNet: Efficient High-Order Spatial Interactions with Recursive Gated Convolutions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Meta</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HorNet: Efficient High-Order Spatial Interactions with Recursive Gated Convolutions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent progress in vision Transformers exhibits great success in various tasks driven by the new spatial modeling mechanism based on dot-product self-attention. In this paper, we show that the key ingredients behind the vision Transformers, namely input-adaptive, long-range and high-order spatial interactions, can also be efficiently implemented with a convolution-based framework. We present the Recursive Gated Convolution (g n Conv) that performs high-order spatial interactions with gated convolutions and recursive designs. The new operation is highly flexible and customizable, which is compatible with various variants of convolution and extends the two-order interactions in self-attention to arbitrary orders without introducing significant extra computation. g n Conv can serve as a plug-and-play module to improve various vision Transformers and convolution-based models. Based on the operation, we construct a new family of generic vision backbones named HorNet. Extensive experiments on ImageNet classification, COCO object detection and ADE20K semantic segmentation show HorNet outperform Swin Transformers and ConvNeXt by a significant margin with similar overall architecture and training configurations. HorNet also shows favorable scalability to more training data and larger model sizes. Apart from the effectiveness in visual encoders, we also show g n Conv can be applied to task-specific decoders and consistently improve dense prediction performance with less computation. Our results demonstrate that g n Conv can be a new basic module for visual modeling that effectively combines the merits of both vision Transformers and CNNs. Code is available at https://github.com/raoyongming/HorNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convolutional neural networks (CNN) have driven remarkable progress in deep learning and computation vision since the introduction of AlexNet <ref type="bibr" target="#b30">[31]</ref> in the last decade. There are quite a few nice properties of CNNs making them naturally suitable for a wide range of vision applications. Translation equivariance introduces useful inductive biases to major vision tasks and enables transferability across different input resolutions. The highly optimized implementation makes it efficient on both high-performance GPUs and edge devices. The evolution of architectures <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b50">51]</ref> further increases its popularity on various vision tasks.</p><p>The emergence of Transformer-based architectures <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b41">42]</ref> greatly challenges the dominance of CNNs. By combining some successful designs in CNN architectures and the new self-attention mechanism, vision Transformers have shown leading performance on various vision tasks such as image classification <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b47">48]</ref>, object detection <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b40">41]</ref>, semantic segmentation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref> and video understanding <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b17">18]</ref>. What makes vision Transformers more powerful than CNNs? Some efforts have been made to improve the CNN architectures by learning from the new designs in vision  <ref type="figure">Figure 1</ref>: Illustration of our main idea. We show representative spatial modeling operations that perform different orders of interactions. In this paper, we focus on studying explicit spatial interactions between a feature (red) and its neighboring region (light gray). (a) The standard convolution operation does not explicitly consider the spatial interaction. (b) Dynamic convolution <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b3">4]</ref> and SE <ref type="bibr" target="#b24">[25]</ref> introduce the dynamic weights to improve the modeling power of convolutions with extra spatial interactions. (c) The self-attention operation <ref type="bibr" target="#b55">[56]</ref> performs two-order spatial interactions with two successive matrix multiplications. (d) g n Conv realizes arbitrary-order spatial interactions using a highly efficient implementation with gated convolutions and recursive deigns.</p><p>Transformers. <ref type="bibr" target="#b42">[43]</ref> presents a thorough study to adopt the meta architecture of vision Transformer to improve CNNs and proposes to use a large 7?7 kernel to construct a modern CNN. <ref type="bibr" target="#b45">[46]</ref> and <ref type="bibr" target="#b13">[14]</ref> propose to use even larger kernels to learn long-range relations with global filters and up to 31?31 convolutions, respectively. <ref type="bibr" target="#b19">[20]</ref> shows that the input-adaptive weights play a key role in vision Transformers and achieve similar performance with Swin Transformers with dynamic convolutions <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">28]</ref>. However, the effectiveness of dot-product self-attention in vision tasks has not been analyzed from the prospective of high-order spatial interactions.</p><p>While there exists complex and often high-order interactions between two spatial locations in a deep model due to the non-linearity, the success of self-attention and other dynamic networks suggests that the explicit and high-order spatial interactions introduced by the architectural designs are beneficial to improving the modeling power of vision models. As illustrated in <ref type="figure">Figure 1</ref>, the plain convolution operation does not explicitly consider the spatial interactions between a spatial location (i.e., the red feature) and its neighboring region (i.e., the light gray region). Enhanced convolution operations like dynamic convolution <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b19">20]</ref> introduce explicit spatial interaction by generating dynamic weights. The dot-product self-attention operation in Transformers <ref type="bibr" target="#b55">[56]</ref> consists of two successive spatial interactions by performing matrix multiplication among queries, keys and values. The trend of the basic operations for visual modeling indicates that the network capacity can be improved by increasing the order of spatial interactions.</p><p>In this paper, we summarize that the key ingredient behind the success of vision Transformers is the new way of spatial modeling with input-adaptive, long-range and high-order spatial interactions performed by the self-attention operation. While previous work has successfully migrated the meta architecture <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b13">14]</ref>, input-adaptive weight generation strategy <ref type="bibr" target="#b19">[20]</ref> and large-range modeling ability <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b13">14]</ref> of vision Transformers to CNN models, a higher-order spatial interaction mechanism has not been studied. We show that all the three key ingredients can be efficiently implemented using a convolution-based framework. We propose the Recursive Gated Convolution (g n Conv) that performs high-order spatial interactions with gated convolutions and recursive deigns. Instead of simply imitating the successful designs in self-attention, g n Conv has several extra favorable properties: 1) Efficient. The convolution-based implementation avoids the quadratic complexity of self-attention. The design that progressively increases the channel width during performing spatial interactions also enables us to achieve higher-order interactions with bounded complexity; 2) Extendable. We extend the two-order interaction in self-attention to arbitrary orders to further improve the modeling power. Since we do not make assumptions on the type of spatial convolution, g n Conv is compatible with various kernel size and spatial mixing strategies like <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b13">14]</ref>; 3) Translation-equivariant. g n Conv fully inherits the translation equivariance of the standard convolution, which introduces beneficial inductive biases to major vision tasks and avoids the asymmetry brought by local attention <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>Based on g n Conv, we construct a new family of generic vision backbones named HorNet. We conduct extensive experiments on ImageNet classification <ref type="bibr" target="#b12">[13]</ref>, COCO object detection <ref type="bibr" target="#b37">[38]</ref> and ADE20K semantic segmentation <ref type="bibr" target="#b70">[71]</ref> to verify the effectiveness of our models. With the same 7?7 kernel/window and similar overall architecture and training configurations, HorNet outperforms Swin and ConvNeXt by a large margin on all tasks at different levels of complexity. The gap can be further enlarged by using a global kernel size <ref type="bibr" target="#b45">[46]</ref>. HorNet also shows favorable scalability to more training data and larger model size, attaining 87.7% top-1 accuracy on ImageNet, 57.9% mIoU on ADE20K val and 59.2% bounding box AP on COCO val with ImageNet-22K pre-training. Apart from applying g n Conv in visual encoders, we further test the generality of our designs on task-specific decoders. By adding gConv to the widely used feature fusion model FPN <ref type="bibr" target="#b35">[36]</ref>, we develop HorFPN to model the high-order spatial relationships of features from different hierarchical levels. We observe that HorFPN can also consistently improve various dense prediction models with lower computational costs. Our results demonstrate that g n Conv can be a promising alternative to self-attention for visual modeling and effectively combine the merits of both vision Transformers and CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Vision Transformers. The Transformer architecture <ref type="bibr" target="#b55">[56]</ref> is originally designed for the natural language processing tasks. Since Dosovitskiy et al. <ref type="bibr" target="#b15">[16]</ref> show that vision models constructed only by the Transformer blocks and a patch embedding layer can also achieve competitive performance to CNNs, many new models have been proposed to modify the Transformer-based architecture and make it more suitable for various vision tasks <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b54">55]</ref>. Different from the original designs in <ref type="bibr" target="#b15">[16]</ref>, state-of-the-art vision Transformers usually utilize a CNN-like hierarchical architecture and change the global self-attention among all patches to local self-attention to avoid the quadratic complexity. In this paper, we follow the overall architecture of the previous hierarchical vision Transformers <ref type="bibr" target="#b41">[42]</ref> and replace the self-attention sub-layer with our proposed g n Conv to fairly compare with the previous Transformer-based models.</p><p>Convolution-based models. Inspired by the recent success of vision Transformers, several papers propose to adopt the Transformer-style architecture and spatial convolutions with a large kernel size to improve the performance of CNNs. Han et al. <ref type="bibr" target="#b19">[20]</ref> replace the window self-attention in Swin Transformers with large-kernel dynamic convolutions and achieve better performance. GFNet <ref type="bibr" target="#b45">[46]</ref> proposes to perform the global spatial interactions like vision Transformers with global filters in the frequency domain, which are equivalent to depth-wise convolutions with a global kernel size and circular padding. ConvNeXt <ref type="bibr" target="#b42">[43]</ref> thoroughly analyzes the designs in recent vision Transformers and presents a strong convolutional model with 7?7 depth-wise convolutions. RepLKNet <ref type="bibr" target="#b13">[14]</ref> explores CNN models with very large kernels (up to 31?31), showing good scalability as vision Transformers. <ref type="bibr">VAN [19]</ref> and FocalNet <ref type="bibr" target="#b64">[65]</ref> use gated convolutions to perform input-adaptive attention and adopts large-kernel dilated convolutions and multiple successive 3?3 convolutions respectively to produce the weights. Previous work focuses on the meta architecture <ref type="bibr" target="#b66">[67]</ref>, large-kernel designs and inputadaptive weights to improve CNNs by learning from vision Transformers. In this paper, we offer a new perspective of high-order spatial attention to analyze the merits of vision Transformers. We show that the proposed HorNet that combines the advantages of both CNNs and vision Transformers is a better architecture for various vision tasks.</p><p>Hybrid models. Combining vision Transformers and CNNs to develop hybrid architectures is a new direction in various visual recognition problems. Recently, several efforts have been made to integrate the two types of blocks into a unified model with a sequential <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b62">63]</ref> or parallel <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b10">11]</ref> design. Many enhanced vision Transformers also use lightweight convolutions in the basic building block to efficiently capture neighboring patterns <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b16">17]</ref> or relax the quadratic complexity of self-attention <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b17">18]</ref>. Different from these hybrid models, we aim to develop a self-attention free model while combining the favorable properties of both vision Transformers and CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">g n Conv: Recursive Gated Convolutions</head><p>In this section, we will present g n Conv, an efficient operation to achieve long-term and high-order spatial interactions. The g n Conv is built with standard convolutions, linear projections and elementwise multiplications, but has a similar function of input-adaptive spatial mixing to self-attention.</p><p>Input-adaptive interactions with gated convolution. Recent success in vision Transformers mainly depends on the proper modeling of the spatial interactions in visual data. Unlike CNNs that simply use the static convolution kernel to aggregate neighboring features, vision Transformers apply multi-head self-attention to dynamically generate the weights to mix spatial tokens. However, the quadratic complexity w.r.t. the input size of the self-attention largely hinders the application of vision Transformers, especially on downstream tasks including segmentation and detection where higher-resolution feature maps are required. In this work, instead of reducing the complexity of self-attention like previous methods <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b56">57]</ref>, we seek a more efficient and effective way to perform spatial interactions with simple operations like convolution and fully-connected layers.</p><p>The basic operation of our method is the gated convolution (gConv). Let x ? R HW ?C be the input feature, the output of the gated convolution y =gConv(x) can be written as:</p><formula xml:id="formula_0">[p HW ?C 0 , q HW ?C 0 ] = ? in (x) ? R HW ?2C , p 1 = f (q 0 ) p 0 ? R HW ?C , y = ? out (p 1 ) ? R HW ?C ,<label>(3.1)</label></formula><p>where ? in , ? out are linear projection layers to perform channel mixing and f is a depth-wise convolution. Note that p</p><formula xml:id="formula_1">(i,c) 1 = j??i w c i?j q (j,c) 0 p (i,c) 0 ,</formula><p>where ? i is the local window centered at i and w represents the convolution weight of f . Therefore, the above formulation explicitly introduce interactions among the neighboring features p (i) 0 and q (j) 0 through the element-wise multiplication. We consider the interaction in gConv as 1-order interaction as each p (i) 0 has interacted with its neighbor feature q (j) 0 only once. High-order interactions with recursive gating.</p><p>After achieving an efficient 1-order spatial interactions with the gConv, we then design the g n Conv, a recursive gated convolution to further enhance the model capacity by introducing higher-order interactions. Formally, we first use ? in to obtain a set of projected features p 0 and {q k } n?1 k=0 :</p><formula xml:id="formula_2">p HW ?C0 0 , q HW ?C0 0 , . . . , q HW ?Cn?1 n?1 = ? in (x) ? R HW ?(C0+ 0?k?n?1 C k ) . (3.2)</formula><p>We then perform the gated convolution recursively by</p><formula xml:id="formula_3">p k+1 = f k (q k ) g k (p k )/?, k = 0, 1, . . . , n ? 1,<label>(3.3)</label></formula><p>where we scale the output by 1/? to stabilize the training. {f k } are a set of depth-wise convolution layers and {g k } are used to match the dimension in different orders:</p><formula xml:id="formula_4">g k = Identity, k = 0, Linear (C k?1 , C k ) , 1 ? k ? n ? 1. (3.4)</formula><p>Finally, we feed the output of the last recursion step q n to the projection layer ? out to obtain the result of the g n Conv. From the recursive formula Equation (3.3), it is easy to show that the interaction-order of p k will be increased by 1 after each step. As a result, we can see that the g n Conv achieves n-order spatial interactions. It is also worth noting that we need only a single f to perform depthwise convolution to the concatenation of the features {q k } n?1 k=0 together instead of computing the convolution in each recursive step as in Equation (3.3), which can further simplify the implementation and improve the efficiency on GPUs. To ensure that the high-order interactions do not introduce too much computational overhead, we set the channel dimension in each order as:</p><formula xml:id="formula_5">C k = C 2 n?k?1 , 0 ? k ? n ? 1. (3.5)</formula><p>This design indicates that we perform the interactions in a coarse-to-fine manner, where lower orders are computed with fewer channels. Besides, the channel dimension of ? in (x) is exactly 2C and the total FLOPs can be strictly bounded even with n increasing. It can be proved that (see Appendix A):</p><formula xml:id="formula_6">FLOPs(g n Conv) &lt; HW C(2K 2 + 11/3 ? C + 2), (3.6)</formula><p>where K is the kernel size of the depth-wise convolution. Therefore, our g n Conv achieves high-order interactions with a similar computational cost to a convolutional layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Long-term interactions with large kernel convolutions. Another difference between vision</head><p>Transformers and conventional CNNs is the receptive field. Conventional CNNs <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b21">22]</ref> often use 3?3 convolution through the whole network, while vision Transformers calculate self-attention on the whole feature maps <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b51">52]</ref> or inside a relatively large local window (e.g., 7?7). The large g n Conv</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HorBlock</head><p>Layer Norm Layer Norm</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FFN</head><p>Proj, 2C  <ref type="figure">Figure 2</ref>: Overview of the basic building block in HorNet with g n Conv. We adopt the block design of Transformers <ref type="bibr" target="#b55">[56]</ref> and replace the self-attention sub-layer with g n Conv to develop our HorNet (left). We also provide the detailed implementation of g 3 Conv (middle) and the Pytorch-style code for an arbitrary order (right). receptive field in vision Transformers makes it easier to capture long-term dependencies, which is also recognized as one of the key advantages of vision Transformers. Inspired by this design, there are some efforts to introduce large kernel convolutions to CNNs recently <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b45">46]</ref>. To make our g n Conv capable of capturing long-term interactions, we adopt two implementations for the depth-wise convolution f :</p><formula xml:id="formula_7">DWConv, 2C-C/4 Mul Proj, C/2 Mul Proj, C Mul Proj, C (C/4, *) (C/2, *) (C, *) (C, *) (C/4,</formula><formula xml:id="formula_8">for i in range(order-1)]) self.proj_out = nn.Conv2d(dim, dim, 1) def forward(self, x): x = self.proj_in(x) y, x = torch.split(x, (self.dims[0], sum(self.dims)), dim=1) x = self.dwconv(x) x_list = torch.split(x, self.dims, dim=1) x = y * x_list[0] for i in range(self.order -1): x = self.projs[i](x) * x_list[i+1] return self.proj_out(x)</formula><p>? 7?7 Convolution. 7?7 is the default window/kernel size of Swin Transformers <ref type="bibr" target="#b41">[42]</ref> and</p><p>ConvNext <ref type="bibr" target="#b42">[43]</ref>. Studies in <ref type="bibr" target="#b42">[43]</ref> show that the kernel size produces good performance on ImageNet classification and various downstream tasks. We follow this configuration to fairly compare with representative work of vision Transformers and modern CNNs. ? Global Filter (GF). The GF layer <ref type="bibr" target="#b45">[46]</ref> multiplies the frequency domain features with learnable global filters, which is equivalent to a convolution in the spatial domain with a global kernel size and circular padding. We use a modified version of the GF layer by processing half of the channels with the global filter and the other half with 3?3 depth-wise convolutions and only use GF layers in late stages to preserve more local details.</p><p>Spatial interactions in vision models. We review some representative vision model designs from the perspective of spatial interactions, as shown in <ref type="figure">Figure 1</ref>. Specifically, we are interested in the interactions between a feature x i and its neighboring feature x j , j ? ? i . By using the tool designed for explaining the interaction effect (IE) in <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b0">1]</ref>, we provide an intuitive analysis of the order of explicit spatial interactions in Appendix B. Our analysis reveals a key difference between vision Transformers and previous architectures from a new view, i.e., vision Transformers have higher-order spatial interactions in each basic block. The result inspires us to explore an architecture that can realize more efficient and effective spatial interactions with more than two orders. As discussed above, our proposed g n Conv can achieve arbitrary-order interactions with bounded complexity. It is also worth noting that similar to other scaling factors in deep models like width <ref type="bibr" target="#b68">[69]</ref> and depth <ref type="bibr" target="#b21">[22]</ref>, simply increasing the order of spatial interactions without considering the overall model capacity will not lead to a good trade-off <ref type="bibr" target="#b50">[51]</ref>. In this paper, we focus on developing a stronger visual modeling architecture based on the analysis of the spatial interaction orders of well-designed models. We believe a more thorough and formal discussion on the high-order spatial interactions can be an important future direction.</p><p>Relation to dot-product self-attention. Although the computation of our g n Conv largely differs from dot-product self-attention, we will show that g n Conv also accomplishes the goal of inputadaptive spatial mixing. Let M be the attention matrix obtained by multi-head self-attention (MHSA), we write M as (m c ij ) since the mixing weight may vary across the channels. The spatial mixing result (before the final channel mixing projection) of the c-th channel at location i is</p><formula xml:id="formula_9">x (i,c) MHSA = j??i m c ij v (i,j) = j??i C c =1 m c ij w (c ,c) V x (j,c ) ,<label>(3.7)</label></formula><p>where w V is the weight of the V-projection layer. Note that m ij obtained by the dot-product operation contains 1-order interaction. On the other hand, the output of our g n Conv (before the ? out ) can be written as</p><formula xml:id="formula_10">x (i,c) g n Conv = p (i,c) n = j??i C c =1 w c n?1,i?j g (i,c) n?1 w (c ,c) ?in x (j,c ) j??i C c =1 h c ij w (c ,c) ?in x (j,c ) , (3.8)</formula><p>where w n?1 is the convolutional weight for f n?1 , w ?in is the linear weight of ? in , and g n?1 = g n?1 (p n?1 ) is a projection of p n?1 . From the formulation in Equation (3.8) we find our g n Conv also achieves input-adaptive spatial mixing with {h c ij } as the weights. Observing that h ij is computed from p n?1 which contains n ? 1 order interactions, we can regard our g n Conv as an extension of the self-attention in terms of the order of the spatial mixing weight. Therefore, our g n Conv can better model more complex spatial interactions.</p><p>The details of g n Conv and our implementation are summarized in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Architectures</head><p>HorNet. The g n Conv can be a drop-in replacement of the spatial mixing layer in vision Transformers <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b41">42]</ref> or modern CNNs <ref type="bibr" target="#b42">[43]</ref>. We follow the same meta-architecture as <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b41">42]</ref> to construct HorNet, where the basic block contains a spatial mixing layer and a feed-forward network (FFN). Depending on the model size and the implementation of the depth-wise convolution f k in our g n Conv, we have two series of model variants named HorNet-T/S/B/L 7?7 and HorNet-T/S/B/L GF . We consider the popular Swin Transformer <ref type="bibr" target="#b41">[42]</ref> and ConvNeXt <ref type="bibr" target="#b42">[43]</ref> as the vision Transformer and CNN baselines since our models are implemented based on a convolution-based framework while having high-order interactions like vision Transformers. To fairly compare with the baselines, we directly follow the number of blocks of Swin Transformers-S/B/L <ref type="bibr" target="#b41">[42]</ref> but insert an extra block to the stage 2 to make the overall complexity close, resulting in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b1">2]</ref> blocks in each stage in all of the model variants. We simply adjust the base number of channels C to construct models with different sizes and set the number of channels in 4 stages as [C, 2C, 4C, 8C] following common practice. We use C = 64, 96, 128, 192 for HorNet-T/S/B/L, respectively. We set the interaction orders (i.e., the n in g n Conv) for each stage as 2,3,4,5 by default, such that the channels of the coarsest order C 0 is the same across different stages.</p><p>HorFPN. Apart from using g n Conv in visual encoders, we find our g n Conv can be an enhanced alternative for standard convolution that considers higher-order spatial interactions in a wide range of convolution-based models. Thus, we replace spatial convolutions for feature fusion in the FPN <ref type="bibr" target="#b36">[37]</ref> with our g n Conv to improve spatial interactions for downstream tasks. Specifically, we add our g n Conv after the fusion of features from different pyramid levels. For object detection, we replace the 3?3 convolution after the top-down pathway with the g n Conv in each level. For semantic segmentation, we simply replace the 3?3 convolution after the concatenation of the multi-level feature maps with g n Conv since the final results are directly predicted from this concatenated feature. We also have two implementations called HorFPN 7?7 and HorFPN GF decided by the choice of f k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct extensive experiments to verify the effectiveness of our method. We present the main results on ImageNet <ref type="bibr" target="#b12">[13]</ref> and compare them with various architectures. We also test our models on the downstream dense prediction tasks on commonly used semantic segmentation benchmark ADE20K <ref type="bibr" target="#b70">[71]</ref> and object detection dataset COCO <ref type="bibr" target="#b37">[38]</ref>. Lastly, we provide ablation studies of our designs and analyze the effectiveness of g n Conv on a wide range of models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">ImageNet Classification</head><p>Setups. We conduct image classification experiments on the widely used ImageNet <ref type="bibr" target="#b12">[13]</ref> dataset. We train our HorNet-T/S/B models using the standard ImageNet-1K dataset following common practice. To fairly compare with previous work, we directly use the training configurations of <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b51">52]</ref> to train our models. We train the models for 300 epochs with 224 ? 224 input. To evaluate the scaling ability of our designs, we further train the HorNet-L models on the ImageNet-22K dataset that contains over 10? images and more categories. We follow previous practice <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref> to train our models for 90 epochs and use a similar data augmentation strategy as ImageNet-1K experiments. <ref type="table">Table 1</ref>: ImageNet classification results. We compare our models with state-of-the-art vision Transformers and CNNs that have comparable FLOPs and parameters. We report the top-1 accuracy on the validation set of ImageNet as well as the number of parameters and FLOPs. We also show the improvements over Swin Trasnformers that have similar overall architectures and training configurations to our models. "?384" indicates that the model is fine-tuned on 384?384 images for 30 epochs. Our models are highlighted in gray. We fine-tune the models pre-trained on ImageNet-22K or at the 224?224 resolution to ImageNet-1K or/and 384?384 resolution for 30 epochs following <ref type="bibr" target="#b42">[43]</ref>. When adapting the ImageNet-22K models to ImageNet-1K, we initialize the classifier with the pre-trained class centers to stabilize the training process. More details can be found in Appendix C.</p><p>Results. The results of our ImageNet classification experiments are summarized in <ref type="table">Table 1</ref>. We see that our models achieve very competitive performance with state-of-the-art vision Transformers and CNNs. Notably, HorNet surpasses Swin Transformers and ConvNeXt which have similar overall architectures and training configurations by a healthy margin on various model sizes and settings. Our models also generalize well to a larger image resolution, larger model sizes and more training data. These results clearly demonstrate the effectiveness and generality of our designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dense Prediction Tasks</head><p>HorNet for semantic segmentation. We evaluate our HorNet for semantic segmentation task on ADE20K <ref type="bibr" target="#b70">[71]</ref> dataset using the commonly used UperNet <ref type="bibr" target="#b61">[62]</ref> framework. All the models are trained for 160k iterations using AdamW <ref type="bibr" target="#b43">[44]</ref> optimizer with a global batch size of 16. The image size during training is 512 ? 512 for ImagNet-1k (HorNet-T/S/B) pre-trained models and 640 ? 640 for the ImageNet-22K pre-trained models (HorNet-L). The results are summarized in the left part of <ref type="table" target="#tab_2">Table 2</ref>, where we report both the single-scale (SS) and multi-scale (MS) mIoU on the validation set. Both our HorNet 7?7 and HorNet GF models outperform Swin <ref type="bibr" target="#b41">[42]</ref> and ConvNeXt <ref type="bibr" target="#b42">[43]</ref> models with similar model sizes and FLOPs. Specifically, HorNet GF models achieve better results than HorNet 7?7 and ConvNeXt series by large margins in single-scale mIoU, indicating the global interactions captured by the global filter are helpful for semantic segmentation. Notably, we find both our HorNet-L 7?7 and HorNet-L GF even outperform ConvNeXt-XL with ?25% fewer FLOPs. These results clearly demonstrate the effectiveness and scalability of our HorNet on semantic segmentation.</p><p>HorNet for object detection. We also evaluate our models on the COCO <ref type="bibr" target="#b37">[38]</ref> dataset. We adopt the cascade Mask R-CNN framework <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b1">2]</ref> to perform object detection and instance segmentation using HorNet-T/S/B/L backbones. Following Swin <ref type="bibr" target="#b41">[42]</ref> and ConvNeXt <ref type="bibr" target="#b42">[43]</ref>, we use 3? schedule with multi-scale training. The right part of <ref type="table" target="#tab_2">Table 2</ref> compares the box AP and mask AP of our HorNet models and Swin/ConvNeXt models. Similarly, we show our HorNet models achieve consistently and significantly better performance than the Swin/ConvNeXt counterparts, in both box AP and mask AP. The HorNet GF series obtain +1.2?2.0 box AP and +1.0?1.9 mask AP compared with ConvNeXt.  Again, our large model HorNet-L 7?7 and HorNet GF can outperform ConvNeXt-XL, which further validates the favorable transferability with a larger model size and larger pre-trained dataset.  HorFPN for dense prediction. We now show another application of the proposed g n Conv, i.e., to serve as a better fusion module that can better capture the higher-order interactions among different levels of features in dense prediction tasks. Specifically, we directly modify the FPN <ref type="bibr" target="#b36">[37]</ref> as described in Section 3.2 in UperNet <ref type="bibr" target="#b61">[62]</ref> and Mask R-CNN <ref type="bibr" target="#b20">[21]</ref> for semantic segmentation and object detection, respectively.We show the results in <ref type="table" target="#tab_3">Table 3</ref>, where we compare the performance of our HorFPN and standard FPN on different backbones including ResNet-50/101 <ref type="bibr" target="#b21">[22]</ref>, Swin-S <ref type="bibr" target="#b41">[42]</ref> and HorNet-S 7?7 . For semantic segmentation, we find our HorFPN can significantly reduce the FLOPs (?50%) while achieving better validation mIoU. For object detection, <ref type="table">Table 6</ref>: Ablation study and results of applying g n Conv to other models/operations. We provide the ablation study of our designs in (a).</p><p>[*] indicates the baseline of our model. The baseline and our final models are highlighted in gray. In (b) and (c), we apply the proposed g n Conv to isotropic models that have a similar level of complexity with ViT/DeiT-S <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b51">52]</ref> and other spatial mixing operations including the 3?3 depth-wise convolution and 3?3 pooling used in <ref type="bibr" target="#b66">[67]</ref>.</p><p>(a) Ablation study.  our HorFPN can also outperform standard FPN in terms of both box AP and mask AP on different backbones with about 30G fewer FLOPs. Besides, we observe that the HorFPN GF is consistently better than HorFPN 7?7 , indicating that global interactions are also important when fusing hierarchical features.</p><p>Results with state-of-the-art frameworks. To further show the effectiveness our backbone, we conduct experiments to combine our large HorNet model with recent state-of-the-art dense prediction frameworks including HTC++ <ref type="bibr" target="#b2">[3]</ref>, DINO <ref type="bibr" target="#b69">[70]</ref> and Mask2Former <ref type="bibr" target="#b6">[7]</ref>. For HTC++ and DINO, we train our models on COCO for 36 epochs (3? schedule) and does not introduce extra pre-training data like Object365 in <ref type="bibr" target="#b69">[70]</ref>. We report the single-scale performance on the validation set and compared with several state-of-the-art methods in <ref type="table" target="#tab_4">Table 4</ref>. For Mask2Former, we train our models on ADE20K with 640 ? 640. We report the mIoU of both single-scale and multi-scale testing on the validation set in <ref type="table" target="#tab_5">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis</head><p>Ablation study. We provide detailed ablation studies of the g n Conv and our HorNet in <ref type="table">Table 6</ref>. We first study the model designs of our HorNet in <ref type="table">Table 6a</ref>. Our baseline ([*]) is obtained by simply replacing the self-attention with 7?7 depth-wise convolution in Swin-T <ref type="bibr" target="#b41">[42]</ref>. We first show that both SE <ref type="bibr" target="#b24">[25]</ref> and our g n Conv with n = 1 (g {1,1,1,1} Conv) can improve over the baseline model [*], and g {1,1,1,1} Conv is slightly better. We then perform ablations on the interaction order n for each stage and find: (1) if n is shared across the 4 stages, the accuracy will increase with larger n but saturate at 82.5 when n = 4; (2) progressively increased order (g {2,3,4,5} Conv) can further improve the accuracy. Our final models are built on g {2,3,4,5} Conv by adjusting the depth and width of the networks (HorNet-T 7?7 ) and applying Global Filter <ref type="bibr" target="#b45">[46]</ref> for the depth-wise convolution (HorNet-T GF ). These results clearly show that our g n Conv is an efficient and extendable operation that can better capture high-order spatial interactions than both self-attention and depth-wise convolution. <ref type="figure">Figure 4</ref>: Visualization of the adaptive weights generated by g n Conv. We see that the spatial mixing weights of our g n Conv are adaptive both to input samples and spatial locations, which further indicates that g n Conv shares these two desirable characteristics with the self-attention operation.</p><p>g n Conv for isotropic models. We also evaluate g n Conv on isotropic architectures (with constant spatial resolutions). We replace the self-attention in DeiT-S <ref type="bibr" target="#b51">[52]</ref> with our g n Conv and adjust the number of blocks to 13 to obtain the isotropic HorNet-S 7?7 and HorNet-S GF . We compare DeiT-S, isotropic ConvNeXt-S and isotropic HorNet-S in <ref type="table">Table 6b</ref>. While isotropic ConvNeXt-S cannot improve DeiT-S, our isotropic HorNet surpasses DeiT-S by a large margin. These results indicate that our g n Conv can better realize the functions of self-attention compared to plain convolutions and have better ability to model the complex spatial interactions.</p><p>g n Conv for other operations. To further demonstrate the universality of g n Conv, we use 3?3 depth-wise convolution and 3?3 pooling <ref type="bibr" target="#b66">[67]</ref> as the basic operation in the g n Conv. The results in <ref type="table">Table 6c</ref> show that g n Conv can also improve these two operations by large margins, indicating our g n Conv is potentially more powerful when equipped with some better basic operations.</p><p>Accuracy-complexity trade-offs. We visualize accuracy-complexity trade-offs of Swin, ConvNeXt and HorNet series in <ref type="figure" target="#fig_1">Figure 3</ref>. For fair comparisons, we fix the input image size to 224 ? 224 and use HorNet 7?7 such that all the compared models are based on 7?7 local window. We see HorNet can achieve better trade-offs than the representative vision Transformers and modern CNNs with regards to model size, FLOPs and GPU latency.</p><p>Visualization. We provide some visualizations of the adaptive weights learned by g n Conv in <ref type="figure">Figure 4</ref>. For each sample, we show the value of 1 C C c=1 h c ij (see Equation <ref type="bibr">(3.8)</ref> or the definition of h c ij ) for two random spatial locations i from layer {1, 3, 5, 7, 8, 12} of the isotropic HorNet-S model. <ref type="figure">Figure 4</ref> demonstrates that the spatial mixing weights of our g n Conv are adaptive both to input samples and spatial locations, which further indicates that g n Conv shares these two desirable characteristics with the self-attention operation.</p><p>Limitations. While HorNet shows better overall latency-accuracy trade-offs, we notice that HorNet is slower than ConvNeXt with similar FLOPs on GPU, which may be caused by the more complex designs to perform the high-order interactions. We think that developing a more hardware-friendly operation for high-order spatial interactions is an interesting future direction to improve our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented the Recursive Gated Convolution (g n Conv) that performs efficient, extendable, and translation-equivariant high-order spatial interactions with gated convolutions and recursive deigns. g n Conv can serve as a drop-in replace of the spatial mixing layer in various vision Transformers and convolution-based models. Based on the operation, we have constructed a new family of generic vision backbones HorNet. Extensive experiments demonstrate the effectiveness of g n Conv and HorNet on commonly used visual recognition benchmarks. We hope our attempt can inspire future work to further explore the high-order spatial interactions in vision models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A FLOPs of g n Conv</head><p>We will divide the computation of our g n Conv into 3 parts, and calculate the FLOPs for each part.</p><p>? Projection layers. The FLOPs of two projection layers ? in and ? out can be easily derived as:</p><formula xml:id="formula_11">FLOPs(? in ) = 2HW C 2 , FLOPs(? out ) = HW C 2 (A.1)</formula><p>? Depth-wise convolution. We first consider the standard depth-wise convolution (DWConv) with kernel size K. The DWConv is performed for all {q k } n?1 k=1 , where q k ? R HW ?C k and C k = C 2 n?k?1 . Therefore, the FLOPs for DWConv are</p><formula xml:id="formula_12">FLOPs(DWConv) = HW K 2 n?1 k=0 C 2 n?k?1 = 2HW CK 2 1 ? 1 2 n . (A.2)</formula><p>? Recursive Gating. We consider both the flops of the projection layer g k and the elementwise multiplication.</p><formula xml:id="formula_13">FLOPs(RecursiveGating) = HW C 0 + n?1 k=1 (HW C k?1 C k + HW C k ) = HW C 2 3 C 1 ? 1 4 n?1 + 2 ? 1 2 n?1 . (A.3)</formula><p>Therefore, the total FLOPs are:</p><formula xml:id="formula_14">FLOPs(g n Conv) = HW C 2K 2 1 ? 1 2 n + 11 3 ? 2 3 ? 4 n?1 C + 2 ? 1 2 n?1 . (A.4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Spatial Interactions in Vision Models.</head><p>We review some representative vision model designs from the perspective of spatial interactions, as shown in <ref type="figure">Figure 1</ref>. Specifically, we are interested in the interactions between a feature x i and its neighbor feature x j , j ? ? i . Inspired by the interaction effect (IE) <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b0">1]</ref>, we consider that a binary function F (x i , x j ) which directly operates on x i , x j introduces an effective interaction between x i x j , if</p><formula xml:id="formula_15">IE(F ) = ?F ?x i ?x j = 0. (B.1)</formula><p>We now analyze the cases in <ref type="figure">Figure 1</ref> of our main paper using the above rule. (a): Convolution. The output F i = j?? w i?j x j , which leads to IE(F ) = 0. Therefore, standard convolution introduce no interaction between x i and x j and we call it a 0-order interaction. (b): SE Block/Gated Convolution. In this case, we have</p><formula xml:id="formula_16">F i = j?? w i?j x j s i (x), where s i (x) = 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HW</head><p>HW l=1 x l for the SE block and s i (x) = x i for the gated convolution. It is easy to show IE(F ) = 0 because ?si ?xi = 0. Hence, these two operations both introduce 1-order interaction. (c): Self-attention (SA). We first denote the projected query/key/value features as q, k, v. The SA first perform an 1-order interaction by computing the attention with dot-product: a i = q i [k 1 , . . . , k HW ]/ ? C. We then view a i as the feature at location i in the following computation. The normalized? i is then obtained by Softmax, which do not contribute to the order since it can be viewed as an implicit interaction that does not explicitly introduce x j to the computation. The second interaction is performed by x i = j??? i v j . To sum up, the SA is a 2-order interaction. (d): g n Conv. According to Section 3.1, we have already known that g n Conv can achieve n-order interaction with bounded computational cost.</p><p>From the above discussion, we reveal a key difference between ViTs and previous architectures from a new view, i.e., ViTs have higher-order spatial interactions in each basic block. Then it begs the question that whether we can achieve better accuracy-complexity trade-offs viz interactions with more than 2 orders. Our proposed g n Conv exactly targets this question for the first time. First, we can realize arbitrary n-order interaction as long as 1 ? n ? 1 + log 2 C easily. Second, unlike the In our implementation of g n Conv, the higher-order spatial interactions are based on the gating mechanism, which has also been investigated in LSTM <ref type="bibr" target="#b22">[23]</ref> and some vision modules <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b38">39]</ref>. However, these previous methods can only achieve up to 2-order interactions, and did not fully reveal the potential of higher-order interactions. On the contrary, our g n Conv is more extendable to achieve arbitrary higher-order spatial interactions under a controllable computational budget.</p><p>C Implementation Details C.1 Architecture Details.</p><p>To better verify the effectiveness of our new designs, we introduce minimal changes in the overall architecture of Swin Transformers <ref type="bibr" target="#b41">[42]</ref>. Specifically, we make two changes to the overall architecture of Swin Transformers <ref type="bibr" target="#b41">[42]</ref>: 1) We add one block in stage 2 to make the overall computation and parameters close to previous models; 2) We use the LayerScale <ref type="bibr" target="#b52">[53]</ref> techniques to make our models more stable during training following the practice of ConvNeXt <ref type="bibr" target="#b42">[43]</ref>. Note that the two changes have been applied to the baseline model considered in our ablation study to clearly show the effects of our designs. The detailed architectures of ConvNeXt <ref type="bibr" target="#b42">[43]</ref>, Swin Transformers <ref type="bibr" target="#b41">[42]</ref> and HorNet are summarized in <ref type="table" target="#tab_7">Table 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Experimental Settings for Image Classification.</head><p>ImageNet-1K training. ImageNet-1K <ref type="bibr" target="#b12">[13]</ref> is a widely used large-scale benchmark for image classification, which contains around 1.2 million images from 1,000 categories. Following common practice <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b42">43]</ref>, we train our models on the training set of ImageNet and report the single-crop top-1 accuracy on 50,000 validation images. To fairly compare with our baseline methods (i.e., Swin Transformers <ref type="bibr" target="#b41">[42]</ref> and ConvNeXt <ref type="bibr" target="#b42">[43]</ref>), we follow the most training details of ConvNeXt and make several small modifications to make the training configurations suitable for our models. For HorNet with 7?7 convolutions, we find that applying gradient clipping with a maximal norm of 5 will significantly stabilize the training process, which may be due to the large gradients brought by the high-order structures in our models. For HorNet with global filters, we use stronger regularization strategies since we find that larger kernels will improve the model capacity but may also cause more severe overfitting. Specifically, we set the gradient norm to 1 and use more aggressive RandAug <ref type="bibr" target="#b9">[10]</ref> data augmentation strategies (i.e., we adjust the magnitudes for tiny, small and base models to 9, 12 and 15, respectively). We set the stochastic depth coefficient of HorNet-T/S/B models to 0.2, 0.4 and 0.5. The other details are identical to ConvNeXt <ref type="bibr" target="#b42">[43]</ref>. Our models are trained using 32 NVIDIA A100 GPUs with a global batch size of 4096.  <ref type="bibr" target="#b12">[13]</ref> is a larger dataset that contains &gt;21k classes and around 14M images. We use the subset suggested by <ref type="bibr" target="#b46">[47]</ref> since the new winter 2021 release is the accessible version now. We also follow the <ref type="bibr" target="#b46">[47]</ref> to remove categories with few images, resulting in roughly half fewer categories and only 13% fewer images compared to the original dataset. We follow previous practice <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref> to train our models for 90 epochs and use a similar data augmentation strategy as ImageNet-1K experiments. We set the stochastic depth coefficient <ref type="bibr" target="#b25">[26]</ref> to 0.2. We also set the maximal gradient norm to 5 and 1 for our large models with standard 7?7 convolutions and global filters respectively. We also adjust the weight decay to 0.1. The other details are identical to ConvNeXt <ref type="bibr" target="#b42">[43]</ref>. We also fine-tune our best model HorNet-L GF on 384?384 images on ImageNet-22K for 10 epochs compete with state-of-the-art models on downstream tasks. The model is only used in the experiments in Appendix 4.2.</p><p>ImageNet-1K fine-tuning.</p><p>We fine-tune the models pre-trained on ImageNet-22K or at the 224?224 resolution to ImageNet-1K or/and 384?384 resolution for 30 epochs with a batch size of 512 and a cosine learning rate schedule with an initial learning rate of 5e ?5 . We set the weight decay to 1e ?6 and disable MixUp and CutMix following <ref type="bibr" target="#b42">[43]</ref>. We initialize the ImageNet-1K classifier with the corresponding classifier weights for ImageNet-22K classes to further stabilize the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Experimental Settings for Downstream Tasks.</head><p>Object detection and instance segmentation on COCO. We adopt the widely used Cascade Mask R-CNN <ref type="bibr" target="#b1">[2]</ref> framework to perform object detection and instance segmentation on COCO, following Swin <ref type="bibr" target="#b41">[42]</ref> and ConvNeXt <ref type="bibr" target="#b42">[43]</ref>. Our backbones are pre-trained on ImageNet-1K for the HorNet-T/S/B and ImageNet-22K for the HorNet-L. We use the 3? schedule where we train all of our model for 36 epochs with AdamW <ref type="bibr" target="#b43">[44]</ref> optimizer and a global batch size of 16. We set the learning rate of as {2e-4, 2e-4, 2e-4, 1e-4} and the stochastic depth rate as {0.4, 0.6, 0.7, 0.7}for HorNet-T/S/B/L. We set the weight decay as 0.05 for all the models.</p><p>Semantic Segmentation on ADE20K. We use the UperNet 160K <ref type="bibr" target="#b61">[62]</ref> framework for semantic segmentation on ADE20K. We use a global batch size of 16 and train all the models for 160 iterations with the AdamW <ref type="bibr" target="#b43">[44]</ref> optimizer. We use 512?512 image for ImageNet-1K pre-trained HorNet-T/S/B and 640 ? 640 image for ImagNet-22K pre-trained HorNet-L. We set the learning rate as 1e-4 and the weight decay as 0.05 for all the models. We report the mIoU of both single-scale and multi-scale testing on the validation set.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D More Analysis</head><p>Comparisons with state-of-the-art methods on ImageNet. Our experiments are designed to clearly verify the superior of our design over previous basic operations like plain convolution and self-attention. Therefore, we choose to follow the basic architecture and the training configuration of widely used architectures Swin Transformers and ConvNeXt Therefore, there is still substantial room to further improve the performance on ImageNet-1K. We notice that some recent work like Pale Transformers <ref type="bibr" target="#b60">[61]</ref> and Dynamic Group Transformer <ref type="bibr" target="#b39">[40]</ref> with hybrid architectures or more careful designs achieve better performance than HorNet on ImageNet-1K. We think many techniques that have been used in previous work can be useful to further imporve our models, including further optimized overall architectures (e.g., optimized depth/width for each stage), better patch embedding strategies (e.g., overlapping convolutional layers for input embedding and downsampling), more efficient ways to compute adaptive weights (e.g., using downsmapled features to produce attention weights like <ref type="bibr" target="#b34">[35]</ref>), and more advanced training methods and hybrid architectures (e.g., combining g n Conv with self-attention and plain convolutions).</p><p>Throughput analysis. We provide the detailed throughput statistics of our models and several baseline methods in <ref type="table" target="#tab_8">Table 8</ref>. Apart from ConvNeXt and Swin Transformers, we also compare our model with recent MViTv2 <ref type="bibr" target="#b34">[35]</ref> models. The multiple small matrix multiplications introduced by g n Conv will affect the speed of our models on GPU. We observed that our method is slower than ConvNeXt by 7% 15% with similar FLOPs. Meanwhile, thanks to the highly efficient depth-wise convolutions implementation of CuDNN, we also see that our models achieve similar or slightly faster speed than typical vision Transformers with similar FLOPs. Notably, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>(c), the higher classification accuracy helps our models achieve better speed-accuracy trade-offs than ConvNeXt and Swin Transformers. Therefore, we believe the speed of our method is still competitive with these recent models.</p><p>Effects of ?. We find that re-scaling the output of gated convolution will avoid the large values produced by the recursive process and stabilize the training process. We analyze the the effects of ? on our ImageNet experiments based on HorNet-B 7?7 . The results are summarized in <ref type="table" target="#tab_9">Table 9</ref>. We see ? = 3 leads the best performance. Therefore, we set ? to 3 in all our models.</p><p>Effects of activation functions in gated convolutions. The gated convolutions used in our models can be viewed as a type of channel attention that uses different attention weights in different locations and generates weights based on spatial interactions. Previous channel attention methods like SE-Net <ref type="bibr" target="#b24">[25]</ref> usually add a sigmoid function to the attention weights to generate bounded attention. Therefore, we investigate several possible activation functions in our models. The results are presented in <ref type="table" target="#tab_10">Table 10</ref>. We see the version described in our paper (i.e., no activation function) achieves the best performance. The result also suggests that g n Conv exhibits a different behavior from conventional channel attention methods. Since the gating weights are critical components in our models, activation functions that can cause significant information losses like GELU and sigmoid will severely hurt the performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Comparisons of trade-offs of Swin, ConvNeXt and HorNet. We compare the trade-offs of the models via the top-1 accuracy on ImageNet w.r.t. (a) number of parameters; (b) FLOPs; (c) latency. The latency is measured with a single NVIDIA RTX 3090 GPU with a batch size of 128.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Conv class gnconv(nn.Module): def __init__(self, dim, order, dwtype): super().__init__() self.order = order self.dims = [dim // 2 ** i for i in range(order)].reverse() self.proj_in = nn.Conv2d(dim, 2*dim, 1) self.dwconv = dwconv(sum(self.dims), type=dwtype) self.projs = nn.ModuleList( [nn.Conv2d(self.dims[i], self.dims[i+1], 1)</figDesc><table /><note>*) gn Conv PyTorch-style code for g n</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Object detection and semantic segmentation results with different backbones. We use Uper-Net<ref type="bibr" target="#b61">[62]</ref> for semantic segmentation and Cascade Mask R-CNN<ref type="bibr" target="#b1">[2]</ref> for object detection. ? indicates that the model is pre-trained on ImageNet-22K. For semantic segmentation, we report both single-scale (SS) and multiscale (MS) mIoU. The FLOPs are calculated with image size (2048, 512) for ImageNet-1K pre-trained models and (2560, 640) for ImageNet-22K pre-trained models. For object detection, we report the box AP and the mask AP. FLOPs are measured on input sizes of (1280, 800). Our models are highlighted in gray.</figDesc><table><row><cell></cell><cell cols="4">Semantic Segmentation with UperNet 160K</cell><cell cols="4">Object Detection with Cascade Mask R-CNN 3?</cell></row><row><cell>Backbone</cell><cell>mIoU ss</cell><cell>mIoU ms</cell><cell>Params</cell><cell>FLOPs</cell><cell>AP box</cell><cell>AP mask</cell><cell>Params</cell><cell>FLOPs</cell></row><row><cell>Swin-T [42]</cell><cell>44.5</cell><cell>45.8</cell><cell>60M</cell><cell>945G</cell><cell>50.4</cell><cell>43.7</cell><cell>86M</cell><cell>745G</cell></row><row><cell>ConvNeXt-T [43]</cell><cell>46.0</cell><cell>46.7</cell><cell>60M</cell><cell>939G</cell><cell>50.4</cell><cell>43.7</cell><cell>86M</cell><cell>741G</cell></row><row><cell>HorNet-T 7?7</cell><cell>48.1</cell><cell>48.9</cell><cell>52M</cell><cell>926G</cell><cell>51.7</cell><cell>44.8</cell><cell>80M</cell><cell>730G</cell></row><row><cell>HorNet-T GF</cell><cell>49.2</cell><cell>49.3</cell><cell>55M</cell><cell>924G</cell><cell>52.4</cell><cell>45.6</cell><cell>80M</cell><cell>728G</cell></row><row><cell>Swin-S [42]</cell><cell>47.6</cell><cell>49.5</cell><cell>81M</cell><cell>1038G</cell><cell>51.9</cell><cell>45.0</cell><cell>107M</cell><cell>838G</cell></row><row><cell>ConvNeXt-S [43]</cell><cell>48.7</cell><cell>49.6</cell><cell>82M</cell><cell>1027G</cell><cell>51.9</cell><cell>45.0</cell><cell>108M</cell><cell>827G</cell></row><row><cell>HorNet-S 7?7</cell><cell>49.2</cell><cell>49.8</cell><cell>81M</cell><cell>1030G</cell><cell>52.7</cell><cell>45.6</cell><cell>107M</cell><cell>830G</cell></row><row><cell>HorNet-S GF</cell><cell>50.0</cell><cell>50.5</cell><cell>85M</cell><cell>1027G</cell><cell>53.3</cell><cell>46.3</cell><cell>108M</cell><cell>827G</cell></row><row><cell>Swin-B [42]</cell><cell>48.1</cell><cell>49.7</cell><cell>121M</cell><cell>1188G</cell><cell>51.9</cell><cell>45.0</cell><cell>145M</cell><cell>982G</cell></row><row><cell>ConvNeXt-B [43]</cell><cell>49.1</cell><cell>49.9</cell><cell>122M</cell><cell>1170G</cell><cell>52.7</cell><cell>45.6</cell><cell>146M</cell><cell>964G</cell></row><row><cell>HorNet-B 7?7</cell><cell>50.0</cell><cell>50.5</cell><cell>121M</cell><cell>1174G</cell><cell>53.3</cell><cell>46.1</cell><cell>144M</cell><cell>969G</cell></row><row><cell>HorNet-B GF</cell><cell>50.5</cell><cell>50.9</cell><cell>126M</cell><cell>1171G</cell><cell>54.0</cell><cell>46.9</cell><cell>146M</cell><cell>965G</cell></row><row><cell>Swin-L  ? [42]</cell><cell>52.1</cell><cell>53.5</cell><cell>234M</cell><cell>2468G</cell><cell>53.9</cell><cell>46.7</cell><cell>253M</cell><cell>1382G</cell></row><row><cell>ConvNeXt-L  ? [43]</cell><cell>53.2</cell><cell>53.7</cell><cell>235M</cell><cell>2458G</cell><cell>54.8</cell><cell>47.6</cell><cell>255M</cell><cell>1354G</cell></row><row><cell>ConvNeXt-XL  ? [43]</cell><cell>53.6</cell><cell>54.0</cell><cell>391M</cell><cell>3335G</cell><cell>55.2</cell><cell>47.7</cell><cell>407M</cell><cell>1898G</cell></row><row><cell>HorNet-L  ? 7?7 HorNet-L  ? GF</cell><cell>54.1 55.0</cell><cell>54.5 55.2</cell><cell>232M 239M</cell><cell>2473G 2465G</cell><cell>55.4 56.0</cell><cell>48.0 48.6</cell><cell>251M 259M</cell><cell>1363G 1358G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Backbone</cell><cell>Fusion Module</cell><cell cols="4">Semantic Segmentation with UperNet 160K mIoU ss mIoU ms Params FLOPs</cell><cell cols="4">Object Detection with Mask R-CNN 1? AP box AP mask Params FLOPs</cell></row><row><cell></cell><cell>FPN [37]</cell><cell>40.7</cell><cell>41.8</cell><cell>66M</cell><cell>947G</cell><cell>38.2</cell><cell>34.7</cell><cell>44M</cell><cell>260G</cell></row><row><cell>ResNet-50 [22]</cell><cell>HorFPN7?7</cell><cell>41.8</cell><cell>44.1</cell><cell>60M</cell><cell>499G</cell><cell>38.7</cell><cell>35.1</cell><cell>43M</cell><cell>226G</cell></row><row><cell></cell><cell>HorFPNGF</cell><cell>43.2</cell><cell>44.5</cell><cell>60M</cell><cell>497G</cell><cell>39.1</cell><cell>35.5</cell><cell>43M</cell><cell>224G</cell></row><row><cell></cell><cell>FPN [37]</cell><cell>42.9</cell><cell>44.0</cell><cell>85M</cell><cell>1025G</cell><cell>40.0</cell><cell>36.1</cell><cell>63M</cell><cell>336G</cell></row><row><cell>ResNet-101 [22]</cell><cell>HorFPN7?7</cell><cell>44.1</cell><cell>45.5</cell><cell>79M</cell><cell>577G</cell><cell>40.3</cell><cell>36.4</cell><cell>62M</cell><cell>302G</cell></row><row><cell></cell><cell>HorFPNGF</cell><cell>44.5</cell><cell>46.4</cell><cell>79M</cell><cell>574G</cell><cell>40.5</cell><cell>36.7</cell><cell>62M</cell><cell>300G</cell></row><row><cell></cell><cell>FPN [37]</cell><cell>47.6</cell><cell>49.5</cell><cell>81M</cell><cell>1038G</cell><cell>45.5</cell><cell>40.9</cell><cell>69M</cell><cell>354G</cell></row><row><cell>Swin-S [42]</cell><cell>HorFPN7?7</cell><cell>48.0</cell><cell>49.2</cell><cell>74M</cell><cell>580G</cell><cell>46.3</cell><cell>41.1</cell><cell>68M</cell><cell>325G</cell></row><row><cell></cell><cell>HorFPNGF</cell><cell>49.0</cell><cell>49.9</cell><cell>75M</cell><cell>578G</cell><cell>46.8</cell><cell>41.9</cell><cell>69M</cell><cell>323G</cell></row><row><cell></cell><cell>FPN [37]</cell><cell>49.2</cell><cell>49.8</cell><cell>81M</cell><cell>1030G</cell><cell>47.1</cell><cell>42.2</cell><cell>69M</cell><cell>351G</cell></row><row><cell>HorNet-S</cell><cell>HorFPN7?7</cell><cell>49.4</cell><cell>50.1</cell><cell>74M</cell><cell>577G</cell><cell>47.4</cell><cell>42.3</cell><cell>68M</cell><cell>322G</cell></row><row><cell></cell><cell>HorFPNGF</cell><cell>49.7</cell><cell>50.3</cell><cell>75M</cell><cell>575G</cell><cell>47.7</cell><cell>42.4</cell><cell>68M</cell><cell>321G</cell></row></table><note>Comparisons of HorFPN with standard FPN on different backbones. We use UperNet 160K and Mask R-CNN 1? schedule for semantic segmentation and object detection, respectively. We find our HorFPN consistently outperforms standard FPN with various of backbones on both the two tasks.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Object detection results with recent state-of-the-art frameworks. We report the single-scale AP box and AP mask on the validation set of COCO. Our models are highlighted in gray.</figDesc><table><row><cell>Backbone</cell><cell>Framework</cell><cell>AP box</cell><cell>AP mask</cell></row><row><cell>Swin-L [42]</cell><cell>HTC++ [3]</cell><cell>57.1</cell><cell>49.5</cell></row><row><cell>ViT-Adapter-L [5]</cell><cell>HTC++ [3]</cell><cell>57.9</cell><cell>50.2</cell></row><row><cell>HorNet-L GF</cell><cell>HTC++ [3]</cell><cell>58.1</cell><cell>50.5</cell></row><row><cell>Swin-L [42]</cell><cell>DINO [70]</cell><cell>58.5</cell><cell>-</cell></row><row><cell>HorNet-L GF</cell><cell>DINO [70]</cell><cell>59.2</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Semantic Segmentation results with recent state-ofthe-art frameworks. We report the single-scale (SS) and multiscale (MS) mIoU on the validation set of ADE20K. Our models are highlighted in gray.</figDesc><table><row><cell>Backbone</cell><cell>Framework</cell><cell>mIoU ss</cell><cell>mIoU ms</cell></row><row><cell>Swin-L [42]</cell><cell>Mask2Former [7]</cell><cell>56.1</cell><cell>57.3</cell></row><row><cell>Swin-L-FaPN [27]</cell><cell>Mask2Former [7]</cell><cell>56.4</cell><cell>57.7</cell></row><row><cell>HorNet-L GF</cell><cell>Mask2Former [7]</cell><cell>57.5</cell><cell>57.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>The detailed architectures of ConvNeXt<ref type="bibr" target="#b42">[43]</ref>, Swin Transformers<ref type="bibr" target="#b41">[42]</ref>, and HorNet.</figDesc><table><row><cell></cell><cell>Output Size</cell><cell cols="2">ConvNeXt-S/B/L C=96/128/192</cell><cell>Swin-S/B/L C=96/128/192</cell><cell></cell><cell cols="2">HorNet-T/S/B/L C=64/96/128/192</cell></row><row><cell>Stem</cell><cell>56?56</cell><cell cols="2">Conv 4?4 , C, stride 4</cell><cell cols="2">Conv 4?4 , C, stride 4</cell><cell cols="2">Conv 4?4 , C, stride 4</cell></row><row><cell>Stage1</cell><cell>56?56</cell><cell>DWConv 7?7 , C MLP, 4C, C</cell><cell>? 3</cell><cell>MSA [MLP, 4C, C] H=C/32 7?7 , C</cell><cell>? 2</cell><cell>g 2 Conv 7?7/GF , C [MLP, 4C, C]</cell><cell>? 2</cell></row><row><cell>Stage2</cell><cell>28?28</cell><cell>DWConv 7?7 , 2C MLP, 8C, 2C</cell><cell>? 3</cell><cell>MSA [MLP, 8C, 2C] H=C/32 7?7 , 2C</cell><cell>? 2</cell><cell>g 3 Conv 7?7/GF , 2C [MLP, 8C, 2C]</cell><cell>? 3</cell></row><row><cell>Stage3</cell><cell>14?14</cell><cell>DWConv 7?7 , 4C MLP, 16C, 4C</cell><cell>? 27</cell><cell>MSA [MLP, 16C, 4C] H=C/32 7?7 , 4C</cell><cell>? 18</cell><cell>g 4 Conv 7?7/GF , 4C [MLP, 16C, 4C]</cell><cell>? 18</cell></row><row><cell>Stage4</cell><cell>7?7</cell><cell>DWConv 7?7 , 8C MLP, 32C, 8C</cell><cell>? 3</cell><cell>MSA [MLP, 32C, 8C] H=C/32 7?7 , 8C</cell><cell>? 2</cell><cell>g 5 Conv 7?7/GF , 8C [MLP, 32C, 8C]</cell><cell>? 2</cell></row><row><cell>Classifier</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Global Average Pooling, Linear</cell><cell></cell><cell></cell></row><row><cell cols="8">quadratic complexity of self-attention, the computational cost of g n Conv has an upper bound w.r.t.</cell></row><row><cell>the order n.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Throughput analysis. We provide the detailed throughput statistics of our models and several baseline methods. The throughput is measured with a single NVIDIA RTX 3090 GPU with a batch size of 128.</figDesc><table><row><cell>Model</cell><cell>FLOPs (G)</cell><cell cols="2">Throughput Top-1 Acc. (img/s) (%)</cell></row><row><cell>ConvNeXt-T [43]</cell><cell>4.5</cell><cell>1010.3</cell><cell>82.1</cell></row><row><cell>Swin-T [42]</cell><cell>4.5</cell><cell>832.2</cell><cell>81.3</cell></row><row><cell>MViTv2-T [35]</cell><cell>4.7</cell><cell>728.4</cell><cell>82.3</cell></row><row><cell>HorNet-T 7?7</cell><cell>4.0</cell><cell>845.7</cell><cell>82.8</cell></row><row><cell>ConvNeXt-S [43]</cell><cell>8.7</cell><cell>621.5</cell><cell>83.1</cell></row><row><cell>Swin-S [42]</cell><cell>8.7</cell><cell>520.7</cell><cell>83.0</cell></row><row><cell>MViTv2-S [35]</cell><cell>7.0</cell><cell>531.5</cell><cell>83.6</cell></row><row><cell>HorNet-S 7?7</cell><cell>8.8</cell><cell>525.8</cell><cell>83.8</cell></row><row><cell>ConvNeXt-B [43]</cell><cell>15.4</cell><cell>440.8</cell><cell>83.8</cell></row><row><cell>Swin-B [42]</cell><cell>15.4</cell><cell>364.8</cell><cell>83.5</cell></row><row><cell>MViTv2-B [35]</cell><cell>10.2</cell><cell>369.1</cell><cell>84.4</cell></row><row><cell>HorNet-B 7?7</cell><cell>15.6</cell><cell>410.0</cell><cell>84.2</cell></row><row><cell cols="2">ImageNet-22K training. ImageNet-22K</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Effects of ?.</figDesc><table><row><cell>?</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>5</cell><cell>10</cell></row><row><cell>ImageNet Top-1 Acc. (%)</cell><cell>82.71</cell><cell>82.76</cell><cell>82.81</cell><cell>82.74</cell><cell>82.69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Effects of activation functions in gated convolutions.</figDesc><table><row><cell></cell><cell>None</cell><cell>Sigmoid</cell><cell>Tanh</cell><cell>GELU</cell></row><row><cell>ImageNet Top-1 Acc. (%)</cell><cell>82.7</cell><cell>82.3 (-0.4)</cell><cell>82.6 (-0.1)</cell><cell>82.2 (-0.5)</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Interaction terms in logit and probit models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunrong</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Norton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Economics letters</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dynamic convolution: Attention over convolution kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="11030" to="11039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Vision transformer adapter for dense predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.08534</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Maskedattention mask transformer for universal image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.01527</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Maskedattention mask transformer for universal image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Per-pixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Twins: Revisiting the design of spatial attention in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Mixformer: End-to-end tracking with iterative mixed attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
		<idno>2022. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scaling up your kernels to 31x31: Revisiting large kernel design in cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhuang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cswin transformer: A general vision transformer backbone with cross-shaped windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convit: Improving vision transformers with soft convolutional inductive biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>St?phane D&amp;apos;ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulio</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sagun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="2286" to="2296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multiscale vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng-Hao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Ze</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Ning</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.09741</idno>
		<title level="m">Visual attention network</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Demystifying local vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejia</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04263</idno>
	</analytic>
	<monogr>
		<title level="m">Sparse connectivity, weight sharing, and dynamic weight</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fapn: Feature-aligned pyramid network for dense image prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="864" to="873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Token labeling: Training a 85</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10858</idno>
	</analytic>
	<monogr>
		<title level="m">5% top-1 accuracy vision transformer with 56m parameters on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="491" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Imagenet classification with deep convolutional neural networks. NeurIPS</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Explaining local, global, and higherorder interactions in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Venuto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Exploring plain vision transformer backbones for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.16527</idno>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mvitv2: Improved multiscale vision transformers for classification and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pay attention to mlps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dynamic group transformer: A general vision transformer backbone with dynamic group attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09883</idno>
		<title level="m">Swin transformer v2: Scaling up capacity and resolution</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A convnet for the 2020s</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">On the integration of self-attention and convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuran</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjiang</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiji</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanfu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.14556</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Global filter networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10972</idno>
		<title level="m">Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for the masses</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Scaling vision with sparse mixture of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr? Susano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17239</idno>
		<title level="m">Alexandre Sablayrolles, Gabriel Synnaeve, and Herv? J?gou. Going deeper with image transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Maxim: Multi-axis mlp for image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhong</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Milanfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinxiao</forename><surname>Li</surname></persName>
		</author>
		<idno>2022. 15</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="5769" to="5780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Maxvit: Multi-axis vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhong</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Milanfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinxiao</forename><surname>Li</surname></persName>
		</author>
		<idno>2022. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Belinda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768,2020.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">SORT: second-order response transform for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15808</idno>
		<title level="m">Cvt: Introducing convolutions to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Pale transformer: A general vision transformer backbone with pale-shaped attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sitong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoru</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Early convolutions help transformers see better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="30392" to="30400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Multiview transformers for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Shen Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.04288</idno>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Focal modulation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.11926,2022.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Focal attention for long-range interactions in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11418</idno>
		<title level="m">Metaformer is actually what you need for vision</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lionel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.03605</idno>
		<title level="m">Detr with improved denoising anchor boxes for end-to-end object detection</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
