<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GENIE: Higher-Order Denoising Diffusion Solvers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dockhorn</surname></persName>
							<email>tim.dockhorn@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
							<email>avahdat@nvidia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
							<email>kkreis@nvidia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GENIE: Higher-Order Denoising Diffusion Solvers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Denoising diffusion models (DDMs) have emerged as a powerful class of generative models. A forward diffusion process slowly perturbs the data, while a deep model learns to gradually denoise. Synthesis amounts to solving a differential equation (DE)  defined by the learnt model. Solving the DE requires slow iterative solvers for high-quality generation. In this work, we propose Higher-Order Denoising Diffusion Solvers (GENIE): Based on truncated Taylor methods, we derive a novel higher-order solver that significantly accelerates synthesis. Our solver relies on higher-order gradients of the perturbed data distribution, that is, higher-order score functions. In practice, only Jacobian-vector products (JVPs) are required and we propose to extract them from the first-order score network via automatic differentiation. We then distill the JVPs into a separate neural network that allows us to efficiently compute the necessary higher-order terms for our novel sampler during synthesis. We only need to train a small additional head on top of the first-order score network. We validate GENIE on multiple image generation benchmarks and demonstrate that GENIE outperforms all previous solvers. Unlike recent methods that fundamentally alter the generation process in DDMs, our GENIE solves the true generative DE and still enables applications such as encoding and guided sampling. Project page and code: https://nv-tlabs.github.io/GENIE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Denoising diffusion models (DDMs) offer both state-of-the-art synthesis quality and sample diversity in combination with a robust and scalable learning objective. DDMs have been used for image <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> and video <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> synthesis, super-resolution <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, deblurring <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, image editing and inpainting <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>, text-to-image synthesis <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>, conditional and semantic image generation <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>, imageto-image translation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> and for inverse problems in medical imaging <ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref>. They also enable high-quality speech synthesis <ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref>, 3D shape generation <ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref>, molecular modeling <ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref>, maximum likelihood training <ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref>, and more <ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref><ref type="bibr" target="#b52">[53]</ref><ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref><ref type="bibr" target="#b55">[56]</ref>. In DDMs, a diffusion process gradually perturbs the data towards random noise, while a deep neural network learns to denoise. Formally, the problem reduces to learning the score function, i.e., the gradient of the log-density of the perturbed data. The (approximate) inverse of the forward diffusion can be described by an ordinary or a stochastic differential equation (ODE or SDE, respectively), defined by the learned score function, and can therefore be used for generation when starting from random noise <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b56">57]</ref>.</p><p>A crucial drawback of DDMs is that the generative ODE or SDE is typically difficult to solve, due to the complex score function. Therefore, efficient and tailored samplers are required for fast synthesis. In this work, building on the generative ODE <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58]</ref>, we rigorously derive a novel second-order ODE solver using truncated Taylor methods [59]. These higher-order methods require higher-order gradients of the ODE-in our case this includes higher-order gradients of the * Work done during internship at NVIDIA. 36th Conference on Neural Information Processing Systems (NeurIPS 2022).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Our novel Higher-Order Denoising Diffusion Solver (GENIE) relies on the second truncated Taylor method (TTM) to simulate a (re-parametrized) Probability Flow ODE for sampling from denoising diffusion models. The second TTM captures the local curvature of the ODE's gradient field and enables more accurate extrapolation and larger step sizes than the first TTM (Euler's method), which previous methods such as DDIM <ref type="bibr" target="#b57">[58]</ref> utilize. log-density of the perturbed data, i.e., higher-order score functions. Because such higher-order scores are usually not available, existing works typically use simple first-order solvers or samplers with low accuracy <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b59">60]</ref>, higher-order methods that rely on suboptimal finite difference or other approximations <ref type="bibr" target="#b60">[61]</ref><ref type="bibr" target="#b61">[62]</ref><ref type="bibr" target="#b62">[63]</ref>, or alternative approaches <ref type="bibr" target="#b63">[64]</ref><ref type="bibr" target="#b64">[65]</ref><ref type="bibr" target="#b65">[66]</ref> for accelerated sampling. Here, we fundamentally avoid such approximations and directly model the higher-order gradient terms: Importantly, our novel Higher-Order Denoising Diffusion Solver (GENIE) relies on Jacobian-vector products (JVPs) involving second-order scores. We propose to calculate these JVPs by automatic differentiation of the regular learnt first-order scores. For computational efficiency, we then distill the entire higher-order gradient of the ODE, including the JVPs, into a separate neural network. In practice, we only need to add a small head to the first-order score network to predict the components of the higher-order ODE gradient. By directly modeling the JVPs we avoid explicitly forming high-dimensional higher-order scores. Intuitively, the higher-order terms in GENIE capture the local curvature of the ODE and enable larger steps when iteratively solving the generative ODE ( <ref type="figure">Fig. 1</ref>).</p><p>Experimentally, we validate GENIE on multiple image modeling benchmarks and achieve state-ofthe-art performance in solving the generative ODE of DDMs with few synthesis steps. In contrast to recent methods that fundamentally modify the generation process of DDMs by training conditional GANs <ref type="bibr" target="#b66">[67]</ref> or by distilling the full sampling trajectory <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b68">69]</ref>, GENIE solves the true generative ODE. Therefore, we also show that we can still encode images in the DDM's latent space, as required for instance for image interpolation, and use techniques such as guided sampling <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b69">70]</ref>.</p><p>We make the following contributions: (i) We introduce GENIE, a novel second-order ODE solver for fast DDM sampling. (ii) We propose to extract the required higher-order terms from the first-order score model by automatic differentiation. In contrast to existing works, we explicitly work with higher-order scores without finite difference approximations. To the best of our knowledge, GENIE is the first method that explicitly uses higher-order scores for generative modeling with DDMs. (iii) We propose to directly model the necessary JVPs and distill them into a small neural network. (iv) We outperform all previous solvers and samplers for the generative differential equations of DDMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>We consider continuous-time DDMs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b70">71]</ref> whose forward process can be described by p t (x t |x 0 ) = N (x t ; ? t x 0 , ? 2 t I),</p><p>where x 0 ? p 0 (x 0 ) is drawn from the empirical data distribution and x t refers to diffused data samples at time t ? [0, 1] along the diffusion process. The functions ? t and ? t are generally chosen such that the logarithmic signal-to-noise ratio <ref type="bibr" target="#b47">[48]</ref> log</p><formula xml:id="formula_1">? 2 t ? 2 t</formula><p>decreases monotonically with t and the data diffuses towards random noise, i.e., p 1 (x 1 ) ? N (x 1 ; 0, I). We use variance-preserving <ref type="bibr" target="#b56">[57]</ref> diffusion processes for which ? 2 t = 1 ? ? 2 t (however, all methods introduced in this work are applicable to more general DDMs). The diffusion process can then be expressed by the (variance-preserving) SDE</p><formula xml:id="formula_2">dx t = ? 1 2 ? t x t dt + ? t dw t ,<label>(2)</label></formula><p>where ? t = ? d dt log ? 2 t , x 0 ? p 0 (x 0 ) and w t is a standard Wiener process. A corresponding reverse diffusion process that effectively inverts the forward diffusion is given by <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b72">73]</ref> </p><formula xml:id="formula_3">dx t = ? 1 2 ? t [x t + 2? xt log p t (x t )] dt + ? t dw t ,<label>(3)</label></formula><p>and this reverse-time generative SDE is marginally equivalent to the generative ODE <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b56">57]</ref> </p><formula xml:id="formula_4">dx t = ? 1 2 ? t [x t + ? xt log p t (x t )] dt,<label>(4)</label></formula><p>where ? xt log p t (x t ) is the score function. Eq. (4) is referred to as the Probability Flow ODE <ref type="bibr" target="#b56">[57]</ref>, an instance of continuous Normalizing flows <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b74">75]</ref>. To generate samples from the DDM, one can sample x 1 ? N (x 1 ; 0, I) and numerically simulate either the Probability Flow ODE or the generative SDE, replacing the unknown score function by a learned score model s ? (x t , t) ? ? xt log p t (x t ).</p><p>The DDIM solver <ref type="bibr" target="#b57">[58]</ref> has been particularly popular to simulate DDMs due to its speed and simplicity. It has been shown that DDIM is Euler's method applied to an ODE based on a re-parameterization of the Probability Flow ODE <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b68">69]</ref>:</p><formula xml:id="formula_5">Defining ? t = 1?? 2 t ? 2 t andx t = x t 1 + ? 2 t , we have dx t d? t = 1 + ? 2 t dx t dt dt d? t + x t ? t 1 + ? 2 t = ? ? t 1 + ? 2 t ? xt log p t (x t ),<label>(5)</label></formula><p>where we inserted Eq. (4) for dxt dt and used ?(t) dt d?t = 2?t</p><formula xml:id="formula_6">? 2 t +1 . Letting s ? (x t , t) := ? ? (xt,t)</formula><p>?t denote a parameterization of the score model, the approximate generative DDIM ODE is then given by</p><formula xml:id="formula_7">dx t = ? (x t , t) d? t ,<label>(6)</label></formula><p>where we used ? t = 1 ? ? 2 t = ?t ?</p><formula xml:id="formula_8">? 2 t +1</formula><p>(see App. A for a more detailed derivation of Eq. (6)). The model ? (x t , t) can be learned by minimizing the score matching objective <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b75">76]</ref> min ? E t?U [t cutoff ,1],x0?p(x0), ?N (0,I) g(t) ? ? (x t , t) 2 2 , x t = ? t x 0 + ? t ,</p><p>for small 0 &lt; t cutoff 1. As is standard practice, we set g(t) = 1. Other weighting functions g(t) are possible; for example, setting g(t) = ?t 2? 2 t recovers maximum likelihood learning <ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Higher-Order Denoising Diffusion Solver</head><p>As discussed in Sec. 2, the so-known DDIM solver <ref type="bibr" target="#b57">[58]</ref> is simply Euler's method applied to the DDIM ODE (cf. Eq. <ref type="formula" target="#formula_7">(6)</ref>). In this work, we apply a higher-order method to the DDIM ODE, building on the truncated Taylor method (TTM) <ref type="bibr" target="#b58">[59]</ref>. The p-th TTM is simply the p-th order Taylor polynomial applied to an ODE. For example, for the general dy dt = f (y, t), the p-th TTM reads as y tn+1 = y tn + h n dy dt | (yt n ,tn) + ? ? ? + 1 p! h p n d p y dt p | (yt n ,tn) ,</p><p>where h n = t n+1 ? t n (see App. B.1 for a truncation error analysis with respect to the exact ODE solution). Note that the first TTM is simply Euler's method. Applying the second TTM to the DDIM ODE results in the following scheme:</p><p>x tn+1 =x tn + h n ? (x tn , t n ) + 1 2 h 2 n d ? d? t | (xt n ,tn) ,</p><p>where h n = ? tn+1 ? ? tn . Recall that ? t =</p><formula xml:id="formula_12">1?? 2 t ? 2 t ,</formula><p>where the function ? t is a time-dependent hyperparameter of the DDM. The total derivative d ?t ? := d ? d?t can be decomposed as follows</p><formula xml:id="formula_13">d ?t ? (x t , t) = ? ? (x t , t) ?x t dx t d? t + ? ? (x t , t) ?t dt d? t ,<label>(10)</label></formula><p>where ? ? (xt,t) ?xt denotes the Jacobian of ? (x t , t) and</p><formula xml:id="formula_14">dx t d? t = ?x t ?x t dx t d? t + ?x t ?? t = 1 ? 2 t + 1 ? (x t , t) ? ? t 1 + ? 2 t x t .<label>(11)</label></formula><p>If not explicitly stated otherwise, we refer to the second TTM applied to the DDIM ODE, i.e., the scheme in Eq. <ref type="bibr" target="#b8">(9)</ref>, as Higher-Order Denoising Diffusion Solver (GENIE). Intuitively, the higher-order gradient terms used in the second TMM model the local curvature of the ODE. This translates into a Taylor formula-based extrapolation that is quadratic in time (cf. Eqs. <ref type="bibr" target="#b7">(8)</ref> and <ref type="bibr" target="#b8">(9)</ref>) and more accurate than linear extrapolation, as in Euler's method, thereby enabling larger time steps (see <ref type="figure">Fig. 1</ref> for a visualization). In App. B, we also discuss the application of the third TTM to the DDIM ODE. We emphasize that TTMs are not restricted to the DDIM ODE and could just as well be applied to the Probability Flow ODE <ref type="bibr" target="#b56">[57]</ref> (also see App. B) or neural ODEs <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b74">75]</ref> more generally.  The Benefit of Higher-Order Methods: We showcase the benefit of higher-order methods on a 2D toy distribution ( <ref type="figure" target="#fig_0">Fig. 2a</ref>) for which we know the score function as well as all higher-order derivatives necessary for GENIE analytically. We generate 1k different accurate "ground truth" trajectories x t using DDIM with 10k steps. We compare these "ground truth" trajectories to single steps of DDIM and GENIE for varying step sizes ?t. We then measure the mean L 2 -distance of the single stepsx t (?t) to the "ground truth" trajectories x t , and we repeat this experiment for three starting points t ? {0.1, 0.2, 0.5}. We see ( <ref type="figure" target="#fig_1">Fig. 3</ref> (top)) that GENIE can use larger step sizes to stay within a certain error tolerance for all starting points t. We further show samples for DDIM and GENIE, using 25 solver steps, in <ref type="figure" target="#fig_0">Fig. 2</ref>. DDIM has the undesired behavior of sampling low-density regions between modes, whereas GENIE looks like a slightly noisy version of the ground truth distribution ( <ref type="figure" target="#fig_0">Fig. 2a</ref>).</p><formula xml:id="formula_15">GENIE, t = 0.1 GENIE, t = 0.2 GENIE, t = 0.5 DDIM, t = 0.1 DDIM, t = 0.2 DDIM, t = 0.5 0.00 0.01 0.02 0.03 0.04 0.05 Step size ?t 0.0 0.1 0.2 0.3 0.4 0.5 ?t(?t) t = 0.1 t = 0.2 t = 0.5</formula><p>Comparison to Multistep Methods: Linear multistep methods are an alternative higher-order method to solve ODEs. Liu et al. <ref type="bibr" target="#b62">[63]</ref> applied the well-established Adams-Bashforth [AB, 77] method to the DDIM ODE. AB methods can be derived from TTMs by approximating higher-order derivatives d p y dt p using the finite difference method <ref type="bibr" target="#b77">[78]</ref>. For example, the second AB method is obtained from the second TTM by replacing d 2 y dt 2 with the first-order forward difference approximation (f (y tn , t n ) ? f (y tn?1 , t n?1 ))/h n?1 . In <ref type="figure" target="#fig_1">Fig. 3</ref> (bottom), we visualize the mean L 2 -norm of the difference ? t (?t) between the analytical derivative d ?t ? and its first-order forward difference approximation for varying step sizes ?t for the 2D toy distribution. The approximation is especially poor at small t for which the score function becomes complex (App. E for details on all toy experiments).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning Higher-Order Derivatives</head><p>The above observations inspire to apply GENIE to DDMs of more complex and high-dimensional data such as images. Regular DDMs learn a model ? for the first-order score; however, the higher-order gradient terms required for GENIE (cf. Eq. (10)) are not immediately available to us, unlike in the toy example above. Let us insert Eq. (11) into Eq. (10) and analyze the required terms more closely:</p><formula xml:id="formula_16">d ?t ? (x t , t) = 1 ? 2 t + 1 ? ? (x t , t) ?x t ? (x t , t) JVP1 ? ? t 1 + ? 2 t ? ? (x t , t) ?x t x t JVP2 + ? ? (x t , t) ?t dt d? t .<label>(12)</label></formula><p>We see that the full derivative decomposes into two JVP terms and one simpler time derivative term. The term ? ? (xt,t) ?xt plays a crucial role in Eq. <ref type="bibr" target="#b11">(12)</ref>. It can be expressed as</p><formula xml:id="formula_17">? ? (x t , t) ?x t = ?? t ?s ? (x t , t) ?x t ? ?? t ? xt ? xt log p t (x t ),<label>(13)</label></formula><p>which means that GENIE relies on second-order score functions ? xt ? xt log p t (x t ) under the hood.</p><p>Given a DDM, that is, given ? , we could compute the derivative d ?t ? for the GENIE scheme in Eq. (9) using automatic differentiation (AD). This would, however, make a single step of GENIE at least twice as costly as DDIM, because we would need a forward pass through the ? network to compute ? (x t , t) itself, and another pass to compute the JVPs and the time derivative in Eq. <ref type="bibr" target="#b11">(12)</ref>. These forward passes cannot be parallelized, since the vector-part of JVP 1 in Eq. (12) involves ? itself, and needs to be known before computing the JVP. To accelerate sampling, this overhead is too expensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4:</head><p>Our distilled model k ? that predicts the gradient d? t ? is implemented as a small additional output head on top of the first-order score model ? . Purple layers are used both in ? and k ? ; green layers are specific for ? and k ? .</p><p>Gradient Distillation: To avoid this overhead, we propose to first distill d ?t ? into a separate neural network. During distillation training, we can use the slow AD-based calculation of d ?t ? , but during synthesis we call the trained neural network. We build on the observation that the internal representations of the neural network modeling ? (in our case a U-Net <ref type="bibr" target="#b78">[79]</ref> architecture) can be used for downstream tasks <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b80">81]</ref>: specifically, we provide the last feature layer from the ? network together with its time embedding as well as x t and the output ? (x t , t) to a small prediction head k ? (x t , t) that models the different terms in Eq. (12) (see <ref type="figure">Fig. 4</ref>). The overhead generated by k ? is small, for instance less than 2% for our CIFAR-10 model (also see Sec. 5), and we found this approach to provide excellent performance. Note that in principle we could also train an independent deep neural network, which does not make use of the internal representations of ? and could therefore theoretically be run in parallel to the ? model. We justify using small prediction heads over independent neural networks because AD-based distillation training is slow: in each training iteration we first need to call the ? network, then calculate the JVP terms, and only then can we call the distillation model. By modeling d ?t ? via small prediction heads, while reusing the internal representation of the score model, we can make training relatively fast: we only need to train k ? for up to 50k iterations. In contrast, training score models from scratch takes roughly an order of magnitude more iterations. We leave training of independent networks to predict d ?t ? to future work. Mixed Network Parameterization: We found that learning d ?t ? directly as single output of a neural network can be challenging. Assuming a single data point distribution p 0 (x 0 ) = ?(x 0 = 0), for which we know the diffused score function and all higher-order derivatives analytically, we found that the terms in Eq. (12) all behave very differently within the t ? [0, 1] interval (for instance, the prefactor of JVP 1 in Eq. (12) approaches 1 as t ? 0, while JVP 2 's prefactor vanishes). As outlined in detail in App. C.2.3, this simple single data point assumption implies an effective mixed network parameterization, an approach inspired by the "mixed score parametrizations" in Vahdat et al. <ref type="bibr" target="#b48">[49]</ref> and Dockhorn et al. <ref type="bibr" target="#b59">[60]</ref>. In particular, we model</p><formula xml:id="formula_18">k ? = ? 1 ? t k (1) ? + ? t 1 + ? 2 t k (2) ? + 1 ? t (1 + ? 2 t ) k (3) ? ? d ?t ? ,<label>(14)</label></formula><p>where k (i) ? (x t , t), i ? {1, 2, 3}, are different output channels of the neural network (i.e. the additional head on top of the ? network). The three terms in Eq. (14) exactly correspond to the three terms of Eq. (12), in the same order. We show the superior performance of this parametrization in Sec. 5.3. Learning Objective: Ideally, we would like our model k ? to match d ?t ? exactly, for all t ? [0, T ] and x t in the diffused data distribution, which the generative ODE trajectories traverse. This suggests a simple (weighted) L 2 -loss, similar to regular score matching losses for DDMs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b56">57]</ref>:</p><formula xml:id="formula_19">min ? E t?U [t cutoff ,1],x0?p(x0), ?N (0,I) g d (t) k ? (? t x 0 + ? t , t) ? d ?t ? (? t x 0 + ? t , t) 2 2<label>(15)</label></formula><p>for diffused data points ? t x 0 + ? t and g d (t) = ? 2 t to counteract the 1/? t in the first and third terms of Eq. <ref type="bibr" target="#b13">(14)</ref>. This leads to a roughly constant loss over different time values t. During training we compute d ?t ? via AD; however, at inference time we use the learned prediction head k ? to approximate d ?t ? . In App. C.2.4, we provide pseudo code for training and sampling with heads k ? . Note that our distillation objective is consistent and principled: if k ? matches d ?t ? exactly, the resulting GENIE algorithm recovers the second TTM exactly (extended discussion in App. B.4). Alternative Learning Approaches: As shown in Eq. (13), GENIE relies on second-order score functions. Recently, Meng et al. <ref type="bibr" target="#b81">[82]</ref> directly learnt such higher-order scores with higher-order score matching objectives. Directly applying these techniques has the downside that we would need to explicitly form the higher-order score terms ? xt ? (x t , t), which are very high-dimensional for data such as images. Low-rank approximations are possible, but potentially insufficient for high performance. In our approach, we are avoiding this complication by directly modeling the lower-dimensional JVPs. We found that the methods from Meng et al. <ref type="bibr" target="#b81">[82]</ref> can be modified to provide higher-order score matching objectives for the JVP terms required for GENIE and we briefly explored this (see App. D). However, our distillation approach with AD-based higher-order gradients worked much better. Nevertheless, this is an interesting direction for future research. To the best of our knowledge, GENIE is the first solver for the generative differential equations of DDMs that directly uses higher-order scores (in the form of the distilled JVPs) for generative modeling without finite difference or other approximations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Accelerated Sampling from DDMs. Several previous works address the slow sampling of DDMs: One line of work reduces and readjusts the timesteps <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b63">64]</ref> used in time-discretized DDMs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b70">71]</ref>. This can be done systematically by grid search <ref type="bibr" target="#b31">[32]</ref> or dynamic programming <ref type="bibr" target="#b82">[83]</ref>. Bao et al. <ref type="bibr" target="#b64">[65]</ref> speed up sampling by defining a new DDM with optimal reverse variances. DDIM <ref type="bibr" target="#b57">[58]</ref>, discussed in Sec. 2, was also introduced as a method to accelerate DDM synthesis. Further works leverage modern ODE and SDE solvers for fast synthesis from (continuous-time) DDMs: For instance, higher-order Runge-Kutta methods <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b83">84]</ref> and adaptive step size SDE solvers <ref type="bibr" target="#b61">[62]</ref> have been used. These methods are not optimally suited for the few-step synthesis regime, in which GENIE shines; see also Sec. 5. Most closely related to our work is Liu et al. <ref type="bibr" target="#b62">[63]</ref>, which simulates the DDIM ODE <ref type="bibr" target="#b57">[58]</ref> using a higher-order linear multistep method <ref type="bibr" target="#b76">[77]</ref>. As shown in Sec. 3, linear multistep methods can be considered an approximation of the TTMs used in GENIE. Furthermore, Tachibana et al. <ref type="bibr" target="#b60">[61]</ref> solve the generative SDE via a higher-order It?-Taylor method <ref type="bibr" target="#b58">[59]</ref> and in contrast to our work, they propose to use an "ideal derivative trick" to approximate higher-order score functions. In App. B.2, we show that applying this ideal derivative approximation to the DDIM ODE does not have any effect: the "ideal derivatives" are zero by construction. Note that in GENIE, we in fact use the DDIM ODE, rather than, for example, the regular Probability Flow ODE <ref type="bibr" target="#b56">[57]</ref>, as the base ODE for GENIE.</p><p>Alternatively, sampling from DDMs can also be accelerated via learning: For instance, Watson et al. <ref type="bibr" target="#b65">[66]</ref> learn parameters of a generalized family of DDMs by optimizing for perceptual output quality. Luhman and Luhman <ref type="bibr" target="#b67">[68]</ref> and Salimans and Ho <ref type="bibr" target="#b68">[69]</ref> distill a DDIM sampler into a student model, which enables sampling in as few as a single step. Xiao et al. <ref type="bibr" target="#b66">[67]</ref> replace DDMs' Gaussian samplers with expressive generative adversarial networks, similarly allowing for few-step synthesis. GENIE can also be considered a learning-based approach, as we distill a derivative of the generative ODE into a separate neural network. However, in contrast to the mentioned methods, GENIE still solves the true underlying generative ODE, which has major advantages: for instance, it can still be used easily for classifier-guided sampling <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b69">70]</ref> and to efficiently encode data into latent space-a prerequisite for likelihood calculation <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b56">57]</ref> and editing applications <ref type="bibr" target="#b16">[17]</ref>. Note that the learnt sampler <ref type="bibr" target="#b65">[66]</ref> defines a proper probabilistic generalized DDM; however, it isn't clear how it relates to the generative SDE or ODE and therefore how compatible the method is with applications such as classifier guidance.</p><p>Other approaches to accelerate DDM sampling change the diffusion itself <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b85">86]</ref> or train DDMs in the latent space of a Variational Autoencoder <ref type="bibr" target="#b48">[49]</ref>. GENIE is complementary to these methods.</p><p>Higher-Order ODE Gradients beyond DDMs. TTMs <ref type="bibr" target="#b77">[78]</ref> and other methods that leverage higherorder gradients are also applied outside the scope of DDMs. For instance, higher-order derivatives can play a crucial role when developing solvers <ref type="bibr" target="#b86">[87]</ref> and regularization techniques <ref type="bibr" target="#b87">[88,</ref><ref type="bibr" target="#b88">89]</ref> for neural ODEs <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b74">75]</ref>. Outside the field of machine learning, higher-order TTMs have been widely studied, for example, to develop solvers for stiff <ref type="bibr" target="#b89">[90]</ref> and non-stiff <ref type="bibr" target="#b89">[90,</ref><ref type="bibr" target="#b90">91]</ref>   <ref type="bibr" target="#b93">[94]</ref> propose new fast solvers, both deterministic and stochastic, specifically designed for the differential equations arising in DDMs. Both Zhang et al. <ref type="bibr" target="#b94">[95]</ref> and Karras et al. <ref type="bibr" target="#b93">[94]</ref> realize that the DDIM ODE has "straight line solution trajectories" for spherical normal data and single data points-this exactly corresponds to our derivation that the higher-order terms in the DDIM ODE are zero in such a setting (see App. B.2). Bao et al. <ref type="bibr" target="#b95">[96]</ref> learn covariance matrices for DDM sampling using prediction heads somewhat similar to the ones in GENIE; in App. G.1, we thoroughly discuss the differences between GENIE and the method proposed in Bao et al. <ref type="bibr" target="#b95">[96]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Datasets: We run experiments on five datasets: CIFAR-10 [97] (resolution 32), LSUN Bedrooms <ref type="bibr" target="#b97">[98]</ref> (128), LSUN Church-Outdoor <ref type="bibr" target="#b97">[98]</ref> (128), (conditional) ImageNet <ref type="bibr" target="#b98">[99]</ref>  <ref type="bibr" target="#b63">(64)</ref>, and AFHQv2 <ref type="bibr" target="#b99">[100]</ref> (512). On AFHQv2 we only consider the subset of cats; referred to as "Cats" in the remainder of this work.</p><p>Architectures: Except for CIFAR-10 (we use a checkpoint by Song et al. <ref type="bibr" target="#b56">[57]</ref>), we train our own score models using architectures introduced by previous works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>. The architecture of our prediction heads is based on (modified) BigGAN residual blocks <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b100">101]</ref>. To minimize computational overhead, we only use a single residual block. See App. C for training and architecture details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation:</head><p>We measure sample quality via Fr?chet Inception Distance [FID, 102] (see App. F.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synthesis Strategy:</head><p>We simulate the DDIM ODE from t=1 up to t=10 ?3 using evaluation times following a quadratic function (quadratic striding <ref type="bibr" target="#b57">[58]</ref>). For variance-preserving DDMs, it can be beneficial to denoise the ODE solver output at the cutoff t=10 ?3 , i.e., x 0 = xt??t ? (xt,t) ?t <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b103">103]</ref>. Note that the denoising step involves a score model evaluation, and therefore "loses" a function evaluation that could otherwise be used as an additional step in the ODE solver. To this end, denoising the output of the ODE solver is left as a hyperparameter of our synthesis strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analytical First</head><p>Step (AFS): Every additional neural network call becomes crucial in the low number of function evaluations (NFEs) regime. We found that we can improve the performance of GENIE and all other methods evaluated on our checkpoints by replacing the learned score with the (analytical) score of N (0, I) ? p t=1 (x t ) in the first step of the ODE solver. The "gained" function evaluation can then be used as an additional step in the ODE solver. Similarly to the denoising step mentioned above, AFS is treated as a hyperparameter of our Synthesis Strategy. AFS details in App. F.2.</p><p>Accounting for Computational Overhead: GENIE has a slightly increased computational overhead compared to other solvers due to the prediction head k ? . The computational overhead is increased by 1.47%, 2.83%, 14.0%, and 14.4% on CIFAR-10, ImageNet, LSUN Bedrooms, and LSUN Church-Outdoor, respectively (see also App. C.2.5). This additional overhead is always accounted for implicitly: we divide the NFEs by the computational overhead and round to the nearest integer. For example, on LSUN Bedrooms, we compare baselines with 10/15 NFEs to GENIE with 9/13 NFEs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Image Generation</head><p>In <ref type="figure" target="#fig_12">Fig. 5</ref> we compare our method to the most competitive baselines. In particular, on the same score model checkpoints, we compare GENIE with DDIM <ref type="bibr" target="#b57">[58]</ref>, S-PNDM <ref type="bibr" target="#b62">[63]</ref>, and F-PNDM <ref type="bibr" target="#b62">[63]</ref>. For these four methods, we only include the best result over the two hyperparameters discussed above, namely, the denoising step and AFS (see App. F.6 for tables with all results). We also include three competitive results from the literature <ref type="bibr" target="#b63">[64]</ref><ref type="bibr" target="#b64">[65]</ref><ref type="bibr" target="#b65">[66]</ref> that use different checkpoints and sampling strategies: for each method, we include the best result for their respective set of hyperparameters. We do not compare in this <ref type="figure">figure</ref>  For NFEs ? {10, 15, 20, 25}, GENIE outperforms all baselines (on the same checkpoint) on all four datasets (see detailed results in App. F.6 and GENIE image samples in App. F.7). On CIFAR-10 and (conditional) ImageNet, GENIE also outperforms these baselines for NFEs=5, whereas DDIM outperforms GENIE slightly on the LSUN datasets (see tables in App. F.6). GENIE also performs better than the three additional baselines from the literature (which use different checkpoints and sampling strategies) with the exception of the Learned Sampler [LS, 66] on LSUN Bedrooms for NFEs=20. Though LS uses a learned striding schedule on LSUN Bedrooms (whereas GENIE simply uses quadratic striding), the LS's advantage is most likely due to the different checkpoint. In Tab. 1, we investigate the effect of optimizing the striding schedule, via learning (LS) or grid search (DDIM &amp; GENIE), on CIFAR-10 and find that its significance decreases rapidly with increased NFEs (also see App. F.6 for details). In Tab. 1, we also show additional baseline results; however, we do not include commonly-used adaptive step size solvers in <ref type="figure" target="#fig_12">Fig. 5</ref>, as they are arguably not well-suited for this low NFE regime: for example, on the same CIFAR-10 checkpoint we use for GENIE, the adaptive SDE solver introduced in Jolicoeur-Martineau et al. <ref type="bibr" target="#b61">[62]</ref> obtains an FID of 82.4 at 48 NFEs. Also   on the same checkpoint, the adaptive Runge-Kutta 4(5) <ref type="bibr" target="#b83">[84]</ref> method applied to the ProbabilityFlow ODE achieves an FID of 13.1 at 38 NFEs (solver tolerances set to 10 ?2 ).</p><p>The results in <ref type="figure" target="#fig_12">Fig. 5</ref> suggest that higher-order gradient information, as used in GENIE, can be efficiently leveraged for image synthesis. Despite using small prediction heads our distillation seems to be sufficiently accurate: for reference, replacing the distillation heads with the derivatives computed via AD, we obtain FIDs of 9.22, 4.11, 3.54, 3.46 using 10, 20, 30, and 40 NFEs, respectively (NFEs adjusted assuming an additional computational overhead of 100%). As discussed in Sec. 3, linear multistep methods such as S-PNDM <ref type="bibr" target="#b62">[63]</ref> and F-PNDM <ref type="bibr" target="#b62">[63]</ref> can be considered (finite difference) approximations to TTMs as used in GENIE. These approximations can be inaccurate for large timesteps, which potentially explains their inferior performance when compared to GENIE. When compared to DDIM, the superior performance of GENIE seems to become less significant for large NFE: this is in line with the theory, as higher-order gradients contribute less for smaller step sizes (see the GENIE scheme in Eq. <ref type="formula" target="#formula_11">(9)</ref>). Approaches such as FastDDIM <ref type="bibr" target="#b63">[64]</ref> and AnalyticDDIM <ref type="bibr" target="#b64">[65]</ref>, which adapt variances and discretizations of discrete-time DDMs, are useful; however, GENIE suggests that rigorous higher-order ODE solvers leveraging the continuous-time DDM formalism are still more powerful. To the best of our knowledge, the only methods that outperform GENIE abandon this ODE or SDE formulation entirely and train NFE-specific models <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b68">69]</ref> which are optimized for the single use-case of image synthesis. As discussed in Sec. 4, one major drawback of approaches such as KD <ref type="bibr" target="#b67">[68]</ref>, PG <ref type="bibr" target="#b68">[69]</ref> and DDGAN <ref type="bibr" target="#b66">[67]</ref> is that they abandon the ODE/SDE formalism, and cannot easily use methods such as classifier(-free) guidance <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b69">70]</ref> or perform image encoding. However, these techniques can play an important role in synthesizing photorealistic images from DDMs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref>, as well as for image editing tasks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17]</ref>. Classifier-Free Guidance <ref type="bibr" target="#b69">[70]</ref>: We replace the unconditional</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Guidance and Encoding</head><formula xml:id="formula_20">model ? (x t , t) with? ? (x t , t, c, w) = (1 + w) ? (x t , t, c) ? NFEs=5 NFEs=10 NFEs=15 NFEs=5 NFEs=10 NFEs=15 NFEs=5 NFEs=10 NFEs=15</formula><p>Guidance scale 0.0 Guidance scale 1.0 Guidance scale 2.0 L2-distance to reference in Inception feature space <ref type="bibr" target="#b104">[104]</ref>, averaged over 100 images.</p><formula xml:id="formula_21">w ? (x t , t) in the DDIM ODE (cf Eq. (6)), where ? (x t , t, c) is a conditional model and w &gt; 1.0 is the "guidance scale". GENIE then requires the derivative d ?t? ? (x t , t, c, w) = (1 + w)d ?t ? (x t , t, c) ? wd ?t ? (x t , t).<label>(16)</label></formula><p>for guidance. Hence, we need to distill d ?t ? (x t , t, c) and d ?t ? (x t , t), for which we could also share parameters <ref type="bibr" target="#b69">[70]</ref>. We compare GENIE with DDIM on ImageNet in <ref type="figure" target="#fig_4">Fig. 6</ref>. GENIE clearly outperforms DDIM, in particular for few NFEs, and GENIE also synthesizes high-quality images (see <ref type="figure">Fig. 7</ref>). Image Encoding: We can use GENIE also to solve the generative ODE in reverse to encode given images. Therefore, we compare GENIE to DDIM on the "encode-decode" task, analyzing reconstructions for different NFEs (used twice for encoding and decoding): We find that GENIE reconstructs images much more accurately (see <ref type="figure" target="#fig_5">Fig. 8</ref>). For more details on this experiment as well as the guidance experiment above, see App. F.4 and App. F.3, respectively. We also show latent space interpolations for both GENIE and DDIM in App. F.5. We perform ablation studies over architecture and training objective for the prediction heads used in GENIE: In Tab. 2, "No mixed" refers to learning d ?t ? directly as single network output without mixed network parameterization; "No weighting" refers to setting g d (t) = 1 in Eq. (15); "Standard" uses both the mixed network parameterization and the weighting function g d (t) = ? 2 t . We can see that having both the mixed network parametrization and the weighting function is clearly beneficial. We also tested deeper networks in the prediction heads: for "Bigger model" we increased the number of residual blocks from one to two. The performance is roughly on par with "Standard", and we therefore opted for the smaller head due to the lower computational overhead. Cascaded diffusion model pipelines <ref type="bibr" target="#b1">[2]</ref> and DDM-based super-resolution <ref type="bibr" target="#b7">[8]</ref> have become crucial ingredients in DDMs for large-scale image generation <ref type="bibr" target="#b105">[105]</ref>. Hence, we also explore the applicability of GENIE in this setting. We train a 128 ? 128 base model as well as a 128 ? 128 ? 512 ? 512 diffusion upsampler <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8]</ref> on Cats. In Tab. 3, we compare the generative performance of GENIE to other fast samplers for the upsampler (in isolation). We find that GENIE performs very well on this task: with only five NFEs GENIE outperforms all other methods at NFEs=15. We show upsampled samples for GENIE with NFEs=5 in <ref type="figure">Fig. 9</ref>. For more quantitative and qualitative results, we refer to App. F.6 and App. F.7, <ref type="figure">Figure 9</ref>: High-resolution images generated with the 128 ? 128 ? 512 ? 512 GENIE upsampler using only five neural network calls. For the two images at the top, the upsampler is conditioned on test images from the Cats dataset. For the two images at the bottom, the upsampler is conditioned on samples from the 128 ? 128 GENIE base model (generated using 25 NFEs); an upsampler neural network evaluation is roughly four times as expensive as a base model evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Upsampling</head><p>respectively. Training and inference details for the score model and the GENIE prediction head, for both base model and upsampler, can be found in App. C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We introduced GENIE, a higher-order ODE solver for DDMs. GENIE improves upon the commonly used DDIM solver by capturing the local curvature of its ODE's gradient field, which allows for larger step sizes when solving the ODE. We further propose to distill the required higher-order derivatives into a small prediction head-which we can efficiently call during inference-on top of the first-order score network. A limitation of GENIE is that it is still slightly slower than approaches that abandon the differential equation framework of DDMs altogether, which, however, comes at the considerable cost of preventing applications such as guided sampling. To overcome this limitation, future work could leverage even higher-order gradients to accelerate sampling from DDMs even further (also see App. G.2). Broader Impact. Fast synthesis from DDMs, the goal of GENIE, can potentially make DDMs an attractive method for promising interactive generative modeling applications, such as digital content creation or real-time audio synthesis, and also reduce DDMs' environmental footprint by decreasing the computational load during inference. Although we validate GENIE on image synthesis, it could also be utilized for other tasks, which makes its broader societal impact application-dependent. In that context, it is important that practitioners apply an abundance of caution to mitigate impacts given generative modeling can also be used for malicious purposes, discussed for instance in <ref type="bibr">Vaccari</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DDIM ODE</head><p>The DDIM ODE has previously been shown <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b68">69]</ref> to be a re-parameterization of the Probability Flow ODE <ref type="bibr" target="#b56">[57]</ref>. In this section, we show an alternative presentation to the ones given in Song et al. <ref type="bibr" target="#b57">[58]</ref> and Salimans and Ho <ref type="bibr" target="#b68">[69]</ref>. We start from the Probability Flow ODE for variance-preserving continuous-time DDMs <ref type="bibr" target="#b56">[57]</ref>, i.e.,</p><formula xml:id="formula_22">dx t = ? 1 2 ? t [x t + ? xt log p t (x t )] dt,<label>(17)</label></formula><p>where ? t = ? d dt log ? 2 t and ? xt log p t (x t ) is the score function. Replacing the unknown score function with a learned score model s ? (x t , t) ? ? xt log p t (x t ), we obtain the approximate Probability Flow ODE</p><formula xml:id="formula_23">dx t = ? 1 2 ? t [x t + s ? (x t , t)] dt.<label>(18)</label></formula><p>Let us now define</p><formula xml:id="formula_24">? t = 1?? 2 t ? 2 t andx t = x t 1 + ? 2 t ,</formula><p>and take the (total) derivative ofx t with respect to ? t :</p><formula xml:id="formula_25">dx t d? t = ?x t ?x t dx t d? t + ?x t ?? t<label>(19)</label></formula><formula xml:id="formula_26">= 1 + ? 2 t dx t d? t + ? t 1 + ? 2 t x t .<label>(20)</label></formula><p>The derivative dxt d?t can be computed as follows</p><formula xml:id="formula_27">dx t d? t = dx t dt dt d? t (by chain rule) (21) = ? 1 2 ? t [x t + s ? (x t , t)] dt d? t (inserting Eq. (18))<label>(22)</label></formula><formula xml:id="formula_28">= 1 2 d log ? 2 t dt [x t + s ? (x t , t)] dt d? t (by definition of ? t )<label>(23)</label></formula><formula xml:id="formula_29">= 1 2 d log ? 2 t d? t [x t + s ? (x t , t)] (by chain rule)<label>(24)</label></formula><formula xml:id="formula_30">= 1 2 d log ? 2 t d? 2 t d? 2 t d? t [x t + s ? (x t , t)] (by chain rule)<label>(25)</label></formula><formula xml:id="formula_31">= 1 2 1 ? 2 t d? 2 t d? t [x t + s ? (x t , t)] .<label>(26)</label></formula><p>We can write ? 2 t as a function of ? t , i.e., ? 2 t = ? 2 t + 1 ?1 , and therefore</p><formula xml:id="formula_32">d? 2 t d? t = ? 2? t (? 2 t + 1) 2 .<label>(27)</label></formula><p>Inserting Eq. <ref type="formula" target="#formula_2">(27)</ref> into Eq. <ref type="formula" target="#formula_2">(26)</ref>, we obtain</p><formula xml:id="formula_33">dx t d? t = ? ? t ? 2 t + 1 [x t + s ? (x t , t)] .<label>(28)</label></formula><p>Lastly, inserting Eq.</p><formula xml:id="formula_34">(28) into Eq. (20), we have dx t d? t = ? ? t ? 2 t + 1 s ? (x t , t)<label>(29)</label></formula><p>Letting</p><formula xml:id="formula_35">s ? (x t , t) := ? ? (xt,t) ?t , where ? t = 1 ? ? 2 t = ?t ? ? 2 t +1</formula><p>, denote a particular parameterization of the score model, we obtain the approximate generative DDIM ODE as</p><formula xml:id="formula_36">dx t d? t = ? t ? 2 t + 1 ? (x t , t) ? t<label>(30)</label></formula><p>= ? (x t , t).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Synthesis from Denoising Diffusion Models via Truncated Taylor Methods</head><p>In this work, we propose Higher-Order Denoising Diffusion Solvers (GENIE). GENIE is based on the truncated Taylor method (TTM) <ref type="bibr" target="#b77">[78]</ref>. As outlined in Sec. 3, the p-th TTM is simply the p-th order Taylor polynomial applied to an ODE. For example, for the general dy dt = f (y, t), the p-th TTM reads as</p><formula xml:id="formula_38">y tn+1 = y tn + h n dy dt | (yt n ,tn) + ? ? ? + 1 p! h p n d p y dt p | (yt n ,tn) ,<label>(32)</label></formula><p>where h n = t n+1 ? t n . To generate samples from denoising diffusion models, we can, for example, apply the second TTM to the (approximate) Probability Flow ODE or the (approximate) DDIM ODE, resulting in the following respective schemes:</p><formula xml:id="formula_39">x tn+1 = x tn + (t n+1 ? t n )f (x tn , t n ) + 1 2 (t n+1 ? t n ) 2 df dt | (xt n ,tn) ,<label>(33)</label></formula><p>where</p><formula xml:id="formula_40">f (x t , t) = ? 1 2 ?(t) x t ? ? (xt,t) ?t</formula><p>, and</p><formula xml:id="formula_41">x tn+1 =x tn + (? tn+1 ? ? tn ) ? (x tn , t n ) + 1 2 (? tn+1 ? ? tn ) 2 d ? d? t | (xt n ,tn) .<label>(34)</label></formula><p>In this work, we generate samples from DDMs using the scheme in Eq. <ref type="bibr" target="#b33">(34)</ref>. We distill the derivative d ?t ? := d ? d?t into a small neural network k ? . For training, d ?t ? is computed via automatic differentiation, however, during inference, we can efficiently query the trained network k ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Theoretical Bounds for the Truncated Taylor Method</head><p>Consider the p-TTM for a general ODE dy dt = f (y, t):</p><formula xml:id="formula_42">y tn+1 = y tn + h n dy dt | (yt n ,tn) + ? ? ? + 1 p! h p n d p y dt p | (yt n ,tn) .<label>(35)</label></formula><p>We represent, the exact solution y(t n+1 ) using the (p + 2)-th Taylor expansion</p><formula xml:id="formula_43">y(t n+1 ) = y(t n ) + h n dy dt | (yt n ,tn) + ? ? ? + 1 p! h p n d p y dt p | (yt n ,tn) + 1 (p + 1)! h p+1 n d p+1 y dt p+1 | (yt n ,tn) + O(h p+2 n ).<label>(36)</label></formula><p>The local truncation error (LTE) introduced by the p-th TTM is given by the difference between the two equations above</p><formula xml:id="formula_44">y tn+1 ? y(t n+1 ) = 1 (p + 1)! h p+1 n d p+1 y dt p+1 | (yt n ,tn) + O(h p+2 n ) .<label>(37)</label></formula><p>For small h n , the LTE is proportional to h p+1 n . Consequently, using higher orders p implies lower errors, as h n usually is a small time step.</p><p>In conclusion, this demonstrates that it is preferable to use higher-order methods with lower errors when aiming to accurately solve ODEs like the Probability Flow ODE or the DDIM ODE of diffusion models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Approximate Higher-Order Derivatives via the "Ideal Derivative Trick"</head><p>Tachibana et al. <ref type="bibr" target="#b60">[61]</ref> sample from DDMs using (an approximation to) a higher-order It?-Taylor method <ref type="bibr" target="#b58">[59]</ref>. In their scheme, they approximate higher-order score functions with the "ideal derivative trick", essentially assuming simple single-point (x 0 ) data distributions, for which higher-order score functions can be computed analytically (more formally, their approximation corresponds to ignoring the expectation over the full data distribution when learning the score function. They assume that for any x t , there is a single unique x 0 from the input data to be predicted with the score model). In that case, further assuming the score model ? (x t , t) is learnt perfectly (i.e., it perfectly predicts the noise that was used to generate x t from x 0 ), one has</p><formula xml:id="formula_45">? (x t , t) ? x t ? ? t x 0 ? t .<label>(38)</label></formula><p>This expression can now be used to analytically calculate approximate spatial and time derivatives (also see App. F.1 and App. F.2 in Tachibana et al. <ref type="bibr" target="#b60">[61]</ref>):</p><formula xml:id="formula_46">? ? (x t , t) ?x t ? ? ?x t x t ? ? t x 0 ? t = 1 ? t I,<label>(39)</label></formula><p>and</p><formula xml:id="formula_47">? ? (x t , t) ?t ? ? ?t x t ? ? t x 0 ? t = ? x t ? ? t x 0 ? 2 t d? t dt ? x 0 ? t d? t dt .<label>(40)</label></formula><p>Rearranging Eq. (38), we have</p><formula xml:id="formula_48">x 0 ? x t ? ? t ? (x t , t) ? t .<label>(41)</label></formula><p>Inserting this expression, Eq. <ref type="formula" target="#formula_4">(40)</ref> becomes</p><formula xml:id="formula_49">? ? (x t , t) ?t ? d log ? 2 t dt 2? t ? (x t , t) ? t ? x t .<label>(42)</label></formula><p>We will now proceed to show that the "ideal derivative trick", i.e. using the approximations in Eqs. <ref type="bibr" target="#b38">(39)</ref> and <ref type="formula" target="#formula_2">(42)</ref>, results in d ?t ? = 0.</p><p>As in Sec. 3, the total derivative d ?t ? is composed as</p><formula xml:id="formula_50">d ?t ? (x t , t) = ? ? (x t , t) ?x t dx t d? t + ? ? (x t , t) ?t dt d? t .<label>(43)</label></formula><p>Inserting the "ideal derivative trick", the above becomes</p><formula xml:id="formula_51">d ?t ? (x t , t) ? 1 ? t 1 2 1 ? 2 t d? 2 t d? t x t ? ? (x t , t) ? t + d log ? 2 t dt 2? t ? (x t , t) ? t ? x t dt d? t ,<label>(44)</label></formula><p>where we have inserted Eq. d?t , we can see that the right-hand side of Eq. (44) is 0. Hence, applying the second TTM to the DDIM ODE and using the "ideal derivative trick" is equivalent to the first TTM (Euler's method) applied to the DDIM ODE. We believe that this is potentially a reason why the DDIM solver <ref type="bibr" target="#b57">[58]</ref>, Euler's method applied to the DDIM ODE, shows such great empirical performance: it can be interpreted as an approximate ("ideal derivative trick") second order ODE solver. On the other hand, our derivation also implies that the "ideal derivative trick" used in the second TTM for the DDIM ODE does not actually provide any benefit over the standard DDIM solver, because all additional second-order terms vanish. Hence, to improve upon regular DDIM, the "ideal derivative trick" is insufficient and we need to learn the higher-order score terms more accurately without such coarse approximations, as we do in our work. Furthermore, it is interesting to show that we do not obtain the same cancellation effect when applying the "ideal derivative trick" to the Probability Flow ODE in Eq. <ref type="formula" target="#formula_0">(18)</ref></p><formula xml:id="formula_52">: Let f (x t , t) = ? 1 2 ?(t) x t ? ? (xt,t) ?t (right-hand side of Probability Flow ODE), then df dt | (xt,t) = ? (t) ?(t) f (x t , t) ? 1 2 ?(t) d dt x t ? ? (x t , t) ? t (45) = ? (t) ?(t) ? 1 2 ?(t) f (x t , t) + 1 2 ?(t) d ? (xt,t) dt ? t ? ? ?2 t d? t dt ? (x t , t) ,<label>(46)</label></formula><p>where ? (t) := d?(t) dt . Using the "ideal derivative trick", we have d ? dt = d ?t ? d t ? t ? 0, and therefore the above becomes</p><formula xml:id="formula_53">df dt | (xt,t) ? ? (t) ?(t) ? 1 2 ?(t) f (x t , t) ? ?(t) 2? 2 t d? t dt (x t , t).<label>(47)</label></formula><p>The derivative d?t dt can be computed as follows</p><formula xml:id="formula_54">d? t dt = 1 2? t d? 2 t dt (48) = 1 2? t d dt 1 ? e ? t 0 ?(t ) dt (49) = ?(t)e ? t 0 ?(t ) dt 2? t .<label>(50)</label></formula><p>Putting everything back together, we have</p><formula xml:id="formula_55">df dt | (xt,t) = ? (t) 2? t + ? 2 (t) 4? t ? ? 2 (t)e ? t 0 ?(t ) dt 4? 3 t ? (x t , t) + ? ? (t) 2 + ? 2 (t) 4 x t ,<label>(51)</label></formula><p>which is clearly not 0 for all x t and t. Hence, in contrast to the DDIM ODE, applying Euler's method to the Probability Flow ODE does not lead to an approximate (in the sense of the "ideal derivative trick") second order ODE solver.</p><p>Note that very related observations have been made in the concurrent works Karras et al. <ref type="bibr" target="#b93">[94]</ref> and Zhang et al. <ref type="bibr" target="#b94">[95]</ref>. These works notice that when the data distribution consist only of a single data point or a spherical Gaussian distribution, then the solution trajectories of the generative DDIM ODE are straight lines. In fact, this exactly corresponds to our observation that in such a setting we have d ?t ? = 0, as shown above in the analysis of the "ideal derivatives approximation". Note in that context that our above derivation considers the "single data point" distribution assumption, but also applies to the setting where the data is a spherical normal distribution (only ? t would be different, which would not affect the derivation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 3rd TTM Applied to the DDIM ODE</head><p>As promised in Sec. 3, we show here how to apply the third TTM to the DDIM ODE, resulting in the following scheme:</p><formula xml:id="formula_56">x tn+1 =x tn + h n ? (x tn , t n ) + 1 2 h 2 n d ? d? t | (xt n ,tn) + 1 6 h 3 n d 2 ? d? 2 t | (xt n ,tn) ,<label>(52)</label></formula><p>where h n = (? tn+1 ? ? tn ). In the remainder of this section, we derive a computable formula for Using the chain rule, we have</p><formula xml:id="formula_57">d 2 ? d? 2 t | (xt,t) = ?d ? ? (x t , t) ?x t dx t d? t + ?d ? ? (x t , t) ?t dt d? t ,<label>(53)</label></formula><p>where, using Eq. <ref type="formula" target="#formula_3">(43)</ref>,</p><formula xml:id="formula_58">?d ? ? (x t , t) ?x t = ? 2 ? (x t , t) ?x 2 dx t d? t + ? ? (x t , t) ?x t 1 ? 2 t + 1 ? ? (x t , t) ?x t ? ? t 1 + ? 2 t I + ? 2 ? (x t , t) ?t?x t dt d? t ,<label>(54)</label></formula><p>and</p><formula xml:id="formula_59">?d ? ? (x t , t) ?t = ? ?t ? ? (x t , t) ?x t dx t d? t + ? ?t ? ? (x t , t) ?t dt d? t .<label>(55)</label></formula><p>The remaining terms in Eq. (55) can be computed as</p><formula xml:id="formula_60">? ?t ? ? (x t , t) ?t dt d? t = ? 2 ? (x t , t) ?t 2 dt d? t + ? ? (x t , t) ?t d dt d?t dt ,<label>(56)</label></formula><p>and</p><formula xml:id="formula_61">? ?t ? ? (x t , t) ?x t dx t d? t = ? 2 ? (x t , t) ?t ?x t dx t d? t + ? ? (x t , t) ?x t ? dxt d?t ?t<label>(57)</label></formula><p>Step  where, inserting Eq. (28) for dxt d?t as well as using the usual parameterization s ? (x t , t) :</p><formula xml:id="formula_62">= ? ? (xt,t) ?t , ? dxt d?t ?t = ? ?t ? ? t ? 2 t + 1 x t ? ? (xt,t) ?t (58) = ? 1 ? ? 2 t +1 ?t ? (x t , t) + 1 ? 2 t + 1 ? ? (x t , t) ?t ? ? ?t 1+? 2 t ?t x t using ? t = ? t ? 2 t + 1 (59) = ? ? t (? 2 t + 1) 3/2 ? (x t , t) + ? 2 t ? 1 (? 2 t + 1) 2 x t d? t dt + 1 ? 2 t + 1 ? ? (x t , t) ?t .<label>(60)</label></formula><p>We now have a formula for d 2 ? d? 2 t containing only partial derivatives, and therefore we can compute using automatic differentiation. Note that we could follow the same procedure to compute even higher derivatives of ? .</p><p>We repeat the 2D toy distribution single step error experiment from Sec. 3 (see also <ref type="figure" target="#fig_1">Fig. 3 (top)</ref> and App. E for details). As expected, in <ref type="figure" target="#fig_9">Fig. 10</ref> we can clearly see that the third TTM improves upon the second TTM.</p><p>In <ref type="figure">Fig. 11</ref>, we compare the second TTM to the third TTM applied to the DDIM ODE on CIFAR-10. Both for the second and the third TTM, we compute all partial derivatives using automatic differentiation (without distillation). It appears that for using 15 or less steps in the ODE solver, the second TTM performs better than the third TTM. We believe that this could potentially be due to our score model s ? (x t , t) not being accurate enough, in contrast to the above 2D toy distribution experiment, where we have access to the analytical score function. Furthermore, note that when we train s ? (x t , t) via score matching, we never regularize (higher-order) derivatives of the neural network, and therefore there is no incentive for them to be well-behaved. It would be interesting to see if, besides having more accurate score models, regularization techniques such as spectral regularization <ref type="bibr" target="#b109">[109]</ref> could potentially alleviate this issue. Also the higher-order score matching techniques derived by Meng et al. <ref type="bibr" target="#b81">[82]</ref> could help to learn higher-order derivates of the score functions more accurately. We leave this exploration to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 GENIE is Consistent and Principled</head><p>GENIE is a consistent and principled approach to developing a higher-order ODE solver for sampling from diffusion models: GENIE's design consists of two parts: <ref type="bibr" target="#b0">(1)</ref> We are building on the second Truncated Taylor Method (TTM), which is a well-studied ODE solver (see Kloeden and Platen <ref type="bibr" target="#b77">[78]</ref>) <ref type="figure">Figure 11</ref>: Qualitative comparison of the second and the third TTMs applied to the DDIM ODE on CIFAR-10 (all necessary derivatives calculated with automatic differentiation). The number of steps in the ODE solver is denoted as n.</p><p>with provable local and global truncation errors (see also App. B.1). Therefore, if during inference we had access to the ground truth second-order ODE derivatives, which are required for the second TTM, GENIE would simply correspond to the exact second TTM.</p><p>(2) In principle, we could calculate the exact second-order derivatives during inference using automatic differentiation. However, this is too slow for competitive sampling speeds, as it requires additional backward passes through the first-order score network. Therefore, in practice, we use the learned prediction heads k ? (x t , t).</p><p>Consequently, if k ? (x t , t) modeled the ground truth second-order derivatives exactly, i.e. k ? (x t , t) = d ?t ? (x t , t) for all x t and t, we would obtain a rigorous second-order solver based on the TTM, following (1) above.</p><p>In practice, distillation will not be perfect. However, given the above analysis, optimizing a neural network k ? (x t , t) towards d ?t ? (x t , t) is well motivated and theoretically grounded. In particular, during training we are calculating exact ODE gradients using automatic differentiation on the firstorder score model as distillation targets. Therefore, in the limit of infinite neural network capacity and perfect optimization, we could in theory minimize our distillation objective function (Eq. (15)) perfectly and obtain k ? (x t , t) = d ?t ? (x t , t).</p><p>Also recall that regular denoising score matching itself, on which all diffusion models rely, follows the exact same argument. In particular, denoising score matching also minimizes a "simple" (weighted) L 2 -loss between a trainable score model s ? (x t , t) and the spatial derivative of the log-perturbation kernel, i.e., ? xt log p t (x t | x 0 ). From this perspective, denoising score matching itself also simply tries to "distill" (spatial) derivatives into a model. If we perfectly optimized the denoising score matching objective, we would obtain a diffusion model that models the data distribution exactly, but in practice, similar to GENIE, we never achieve that due to imperfect optimization and finite-capacity neural networks. Nevertheless, denoising score matching similarly is a well-defined and principled method, precisely because of that theoretical limit in which the distribution can be reproduced exactly.</p><p>We would also like to point out that other, established higher-order methods for diffusion model sampling with the generative ODE, such as linear multistep methods <ref type="bibr" target="#b62">[63]</ref>, make approximations, too, which can be worse in fact. In particular, multistep methods always approximate higher-order derivatives in the TTM using finite differences which is crude for large step sizes, as can be seen in <ref type="figure" target="#fig_1">Fig. 3 (bottom)</ref>. From this perspective, if our distillation is sufficiently accurate, GENIE can be expected to be more accurate than such multistep methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Model and Implementation Details C.1 Score Models</head><p>We train variance-preserving DDMs <ref type="bibr" target="#b56">[57]</ref> for which ? 2 t = 1 ? ? 2 t . We follow Song et al. <ref type="bibr" target="#b56">[57]</ref> and set ?(t) = 0.1 + 19.9t; note that ? t = e ? 1 2 t 0 ?(t ) dt . All score models are parameterized as either <ref type="bibr" target="#b78">[79]</ref>. The -prediction model is trained using the following score matching objective <ref type="bibr" target="#b0">[1]</ref> min</p><formula xml:id="formula_63">s ? (x t , t) := ? ? (xt,t) ?t ( -prediction) or s ? (x t , t) := ? ?tv ? (xt,t)+?txt ?t (v-prediction), where ? (x t , t) and v ? (x t , t) are U-Nets</formula><formula xml:id="formula_64">? E t?U [t cutoff ,1],x0?p(x0), ?N (0,I) ? ? (x t , t) 2 2 , x t = ? t x 0 + ? t .<label>(61)</label></formula><p>The v-prediction model is trained using the following score matching objective <ref type="bibr" target="#b68">[69]</ref> min</p><formula xml:id="formula_65">? E t?U [t cutoff ,1],x0?p(x0), ?N (0,I) ??txt ?t ? v ? (x t , t) 2 2 , x t = ? t x 0 + ? t ,<label>(62)</label></formula><p>which is referred to as "SNR+1" weighting <ref type="bibr" target="#b68">[69]</ref>. The neural network v ? is now effectively tasked with predicting v := ? t ? ? t x 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10:</head><p>On this dataset, we do not train our own score model, but rather use a checkpoint 2 provided by Song et al. <ref type="bibr" target="#b56">[57]</ref>. The model is based on the DDPM++ architecture introduced in Song et al. <ref type="bibr" target="#b56">[57]</ref> and predicts ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSUN Bedrooms and LSUN Church-Outdoor:</head><p>Both datasets use exactly the same model structure.</p><p>The model structure is based on the DDPM architecture introduced in Ho et al. <ref type="bibr" target="#b0">[1]</ref> and predicts ? .</p><p>ImageNet: This model is based on the architecture introduced in Dhariwal and Nichol <ref type="bibr" target="#b3">[4]</ref>. We make a small change to the architecture and replace its sinusoidal time embedding by a Gaussian Fourier projection time embedding <ref type="bibr" target="#b56">[57]</ref>. The model is class-conditional and we follow Dhariwal and Nichol <ref type="bibr" target="#b3">[4]</ref> and simply add the class embedding to the (Gaussian Fourier projection) time embedding. The model predicts ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cats (Base):</head><p>This model is based on the architecture introduced in Dhariwal and Nichol <ref type="bibr" target="#b3">[4]</ref>. We make a small change to the architecture and replace its sinusoidal time embedding by a Gaussian Fourier projection time embedding <ref type="bibr" target="#b56">[57]</ref>. The model predicts v ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cats (Upsampler):</head><p>This model is based on the architecture introduced in Dhariwal and Nichol <ref type="bibr" target="#b3">[4]</ref>. We make a small change to the architecture and replace its sinusoidal time embedding by a Gaussian Fourier projection time embedding <ref type="bibr" target="#b56">[57]</ref>. The upsampler is conditioned on noisy upscaled lower-resolution images, which are concatenated to the regular channels that form the synthesized outputs of the diffusion model. Therefore, we expand the number of input channels from three to six. We use augmentation conditioning <ref type="bibr" target="#b105">[105]</ref> to noise the lower-resolution image. In particular, we upscale ? t x low + ? t z, where x low is the clean lower-resolution image. During training t is sampled from U[t cutoff , 1]. During inference, t is a hyper-parameter which we set to 0.1 for all experiments.</p><p>We use two-independent Gaussian Fourier projection embeddings for t and t and concatenate them before feeding them into the layers of the U-Net.</p><p>Model Hyperparameters and Training Details: All model hyperparameters and training details can be found in Tab. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Prediction Heads</head><p>We model the derivative d ?t ? using a small prediction head k ? on top of the first-order score model ? . In particular, we provide the last feature layer from the ? network together with its time embedding as well as x t and the output of (x t , t) to the prediction head (see <ref type="figure">Fig. 4</ref> for a visualization). We found modeling d ?t ? to be effective even for our Cats models that learn to predict v = ? t ? ? t x 0 rather than . Directly learning d ?t v ? and adapting the mixed network parameterization (see App. C.2.3) could potentially improve results further. We leave this exploration to future work.</p><p>We provide additional details on our architecture next. <ref type="bibr" target="#b1">2</ref> The checkpoint can be found at https://drive.google.com/file/d/16_ -Ahc6ImZV5ClUc0vM5Iivf8OJ1VSif/view?usp=sharing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.1 Model Architecture</head><p>The architecture of our prediction heads is based on (modified) BigGAN residual blocks <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b100">101]</ref>.</p><p>To minimize computational overhead, we only use a single residual block.</p><p>In particular, we concatenate the last feature layer with x t as well as ? (x t , t) and feed it into a convolutional layer. For the upsampler, we also condition on the noisy up-scaled lower resolution image. We experimented with normalizing the feature layer before concatenation. The output of the convolutional layer as well as the time embedding are then fed to the residual block. Similar to U-Nets used in score models, we normalize the output of the residual block and apply an activation function. Lastly, the signal is fed to another convolutional layer that brings the number of channels to a desired value (in our case nine, three for each k (i) ? , i ? {1, 2, 3}, in Eq. <ref type="formula" target="#formula_7">(66)</ref>). All model hyperparameters can be found in Tab. 5. We also include the additional computational overhead induced by the prediction heads in Tab. 5; see App. C.2.5 for details on how we measured the overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.2 Training Details</head><p>We train for 50k iterations using Adam <ref type="bibr" target="#b110">[110]</ref>. We experimented with two base learning rates: 10 ?4 and 5 ? 10 ?5 . We furthermore tried two "optimization setups": (linearly) warming up the learning rate in the first 10k iterations (score models are often trained by warming up the learning rate in the first 100k iterations) or, following Salimans and Ho <ref type="bibr" target="#b68">[69]</ref>, linearly decaying the learning rate to 0 in the entire 50k iterations of training; we respectively refer to these two setups as "warmup" and "decay". We measure the FID every 5k iterations and use the best checkpoint.</p><p>Note that we have to compute the Jacobian-vector products in Eq. (12) via automatic differentiation during training. We repeatedly found that computing the derivative ? ? (xt,t) ?t via automatic differentiation leads to numerical instability (NaN) for small t when using mixed precision training. For simplicity, we turned off mixed precision training altogether. However, training performance could have been optimized by only turning off mixed precision training for the derivative ? ? (xt,t) ?t .</p><p>All training details can be found in Tab. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.3 Mixed Network Parameterization</head><p>Our mixed network parameterization is derived from a simple single data point assumption, i.e., p t (x t ) = N (x t ; 0, ? 2 t I). This assumption leads to ? (x t , t) ? xt ?t which we can plug into the three 1</p><formula xml:id="formula_66">? 2 t + 1 ? ? (x t , t) ?x t ? (x t , t) ? 1 ? 2 t + 1 x t ? 2 t = 1 ? t x t ? t ,<label>(63)</label></formula><p>and</p><formula xml:id="formula_67">? ? t 1 + ? 2 t ? ? (x t , t) ?x t x t ? ? ? t ? t (1 + ? 2 t ) x t = ? ? t 1 + ? 2 t x t ? t ,<label>(64)</label></formula><p>and finally</p><formula xml:id="formula_68">? ? (x t , t) ?t dt d? t ? ? x t ? 2 t d? t dt dt d? t = ? ? 2 t + 1 ? 2 t x t 1 (? 2 t + 1) 3/2 = ? 1 ? t (1 + ? 2 t ) x t ? t ,<label>(65)</label></formula><p>where we have used ? t = ?t ?</p><formula xml:id="formula_69">? 2 t +1</formula><p>. This derivation therefore implies the following mixed network parameterization</p><formula xml:id="formula_70">k ? = ? 1 ? t k (1) ? + ? t 1 + ? 2 t k (2) ? + 1 ? t (1 + ? 2 t ) k (3) ? ? d ?t ? ,<label>(66)</label></formula><p>where k (i) ? (x t , t), i ? {1, 2, 3}, are different output channels of the neural network (i.e. the additional head on top of the ? network). To provide additional intuition, we basically replaced the ? xt ?t terms in Eqs. (63) to (65) by neural networks. However, we know that for approximately Normal data xt ?t ? ? (x t , t), where ? (x t , t) predicts "noise" values that were drawn from a standard Normal distribution and are therefore varying on a well-behaved scale. Consequently, up to the Normal data assumption, we can also expect our prediction heads k (i) ? (x t , t) in the parameterization in Eq. (66) to predict well-behaved output values, which should make training stable. This mixed network parameterization approach is inspired by the mixed score parameterization from Vahdat et al. <ref type="bibr" target="#b48">[49]</ref> and Dockhorn et al. <ref type="bibr" target="#b59">[60]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.4 Pseudocode</head><p>In this section, we provide pseudocode for training our prediction heads k ? and using them for sampling with GENIE. In Alg. 1, the analytical dt d?t is an implicit hyperparameter of the DDM as it depends on ? t . For our choice of ? t = e ? 1 2 t 0 0.1+19.9t dt (see App. C.1), we have </p><formula xml:id="formula_71">dt d? t =<label>2?t</label></formula><formula xml:id="formula_72">where ? t = 1?? 2 t ? 2 t .<label>(67)</label></formula><p>In Alg. 2, we are free to use any time discretization t 0 = 1.0 &gt; t 1 &gt; ? ? ? &gt; t N = t cutoff . When referring to "linear striding" in this work, we mean the time discretization t n = 1.0 ? (1.0 ? t cutoff ) n N . When referring to "quadratic striding" in this work, we mean the time discretization</p><formula xml:id="formula_73">t n = 1.0 ? (1.0 ? ? t cutoff ) n N 2 .</formula><p>Algorithm 1 Training prediction heads k ? Input: Score model s ? := ? ? (x t ,t) ? t , number of training iterations N . Output: Trained prediction head k ? .</p><formula xml:id="formula_74">for n = 1 to N do Sample x0 ? p0(x0), t ? U [t cutoff , 1], ? N (0, I) Set xt = ?tx0 + ?t Compute ? (xt, t)</formula><p>Compute the exact spatial Jacobian-vector product JVPs = ? ? (x t ,t) Setxt 0 = 1 + ? 2 t 0 xt 0 Note thatxt n = 1 + ? 2 tn xt n for all tn for n = 0 to N ? 1 do if AFS and n = 0 then xt n+1 =xt n + (?t n+1 ? ?t n )xt n elsex t n+1 =xt n + (?t n+1 ? ?t n ) ? (xt n , tn)</p><formula xml:id="formula_75">?x t 1 ? ? 2 t +1 ? (xt, t) ? ? t</formula><formula xml:id="formula_76">+ 1 2 (?t n+1 ? ?t n ) 2 k ? (xt n , tn) end if xt n+1 =x t n+1 1+? 2 t n+1 end for if Denoising then y = x t N ?? t N ? (x t N ,t N ) ? t N else y = xt N end if</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.5 Measuring Computational Overhead</head><p>Our prediction heads induce a slight computational overhead since their forward pass has to occur after the forward pass of the score model. We measure the overhead as follows: first, we measure the inference time of the score model itself. We do five forward passes to "warm-up" the model and then subsequently synchronize via torch.cuda.synchronize(). We then measure the total wall-clock time of 50 forward passes. We then repeat this process using a combined forward pass: first the score model and subsequently the prediction head. We choose the batch size to (almost) fill the entire GPU memory. In particular we chose batch sizes of 512, 128, 128, 64, 64, and 8, for CIFAR-10, LSUN Bedrooms, LSUN Church-Outdoor, ImageNet, Cats (base), and Cats (upsampler), respectively. The computational overhead for each model is reported in Tab. 5. This measurement was carried out on a single NVIDIA 3080 Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Learning Higher-Order Gradients without Automatic Differentiation and Distillation</head><p>In this work, we learn the derivative d ?t ? , which includes a spatial and a temporal Jacobian-vector product, by distillation based on automatic differentiation (AD). We now derive an alternative learning objective for the spatial Jacobian-vector product (JVP) which does not require any AD. We start with the following (conditional) expectation</p><formula xml:id="formula_77">E ? 2 t x 0 x t ? ? t x 0 x t + x t x 0 | x t , t = ?x t x t + ? 4 t S 2 (x t , t) + ? 4 t s 1 (x t , t)s 1 (x t , t) + ? 2 t I,<label>(68)</label></formula><p>where s 1 (x t , t) := ? xt log p t (x t ) and S 2 (x t , t) := ? xt ? xt log p t (x t ). The above formula is derived in Meng et al. [ <ref type="bibr">Theorem 1,</ref><ref type="bibr" target="#b81">82]</ref>. Adding x t x t to Eq. (68) and subsequently dividing by ? 2 t , we have</p><formula xml:id="formula_78">E ? 2 t ? 2 t x 0 x t ? ? t ? 2 t x 0 x t + x t x 0 + 1 ? 2 t x t x t | x t , t = ? 2 t S 2 (x t , t) + ? 2 t s 1 (x t , t)s 1 (x t , t) + I,<label>(69)</label></formula><p>where we could pull the 1</p><formula xml:id="formula_79">? 2 t</formula><p>x t x t term into the expectation because it is conditioned on t and x t . Using x t = ? t x 0 + ? t , we can rewrite the above as</p><formula xml:id="formula_80">E | x t , t = ? 2 t S 2 (x t , t) + ? 2 t s 1 (x t , t)s 1 (x t , t) + I.<label>(70)</label></formula><p>For an arbitrary v := v(x t , t), we then have</p><formula xml:id="formula_81">E v | x t , t = ? 2 t S 2 (x t , t)v + ? 2 t s 1 (x t , t)s 1 (x t , t) v + v.<label>(71)</label></formula><p>Therefore, we can develop a score matching-like learning objective for the (general) spatial JVP</p><formula xml:id="formula_82">o ? (x t , t) ? S 2 (x , t)v as E t?U [t cutoff ,1],x0?p(x0), ?N (0,I) g no?ad (t) o ? (x t , t) + s ? (x t , t)s ? (x t , t) v + 1 ? 2 t v ? v 2 2 ,<label>(72)</label></formula><p>for some weighting function g no?ad (t).</p><p>Setting</p><formula xml:id="formula_83">v(x t , t) = ?? t 1 ? ? 2 t +1 ? (x t , t) ? ?t 1+? 2 t x t ,</formula><p>would recover the spatial JVP needed for the computation of d ?t . In the initial phase of this project, we briefly experimented with learning the spatial JVP using this approach; however, we found that our distillation approach worked significantly better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Toy Experiments</head><p>For all toy experiments in Sec. 3, we consider the following ground truth distribution:</p><formula xml:id="formula_84">p 0 (x 0 ) = 1 8 8 i=1 p (i) 0 (x 0 ),<label>(73)</label></formula><p>where</p><formula xml:id="formula_85">p (i) 0 (x 0 ) = 1 8 8 j=1 N (x 0 , s 1 ? i + s 1 s 2 ? j , ? 2 I).<label>(74)</label></formula><p>We set ? = 10 ?2 , s 1 = 0.9, s 2 = 0.2, and</p><formula xml:id="formula_86">? 1 = 1 0 , ? 2 = ?1 0 , ? 3 = 0 1 , ? 4 = 0 ?1 ? 5 = 1 ? 2 1 ? 2 , ? 6 = 1 ? 2 ? 1 ? 2 , ? 7 = ? 1 ? 2 1 ? 2 , ? 8 = ? 1 ? 2 ? 1 ? 2 .</formula><p>The ground truth distribution is visualized in <ref type="figure" target="#fig_0">Fig. 2a</ref>. Note that we can compute the score functions (and all its derivatives) analytically for Gaussian mixture distributions.</p><p>In <ref type="figure" target="#fig_0">Fig. 2</ref>, we compared DDIM to GENIE for sampling using the analytical score function of the ground truth distribution with 25 solver steps. In <ref type="figure" target="#fig_0">Fig. 12</ref>, we repeated this experiment for 5, 10, 15, and 20 solver steps. We found that in particular for n = 10 both solvers generate samples in interesting patterns.  <ref type="figure" target="#fig_0">Figure 12</ref>: Modeling a complex 2D toy distribution: Samples are generated with DDIM and GENIE with n solver steps using the analytical score function of the ground truth distribution (visualized in <ref type="figure" target="#fig_0">Fig. 2a</ref>). Zoom in for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Image Experiments F.1 Evaluation Metrics, Baselines, and Datasets</head><p>Metrics: We quantitatively measure sample quality via Fr?chet Inception Distance <ref type="bibr">[FID,</ref><ref type="bibr" target="#b101">102]</ref>. It is common practice to use 50k samples from the training set for reference statistics. We follow this practice for all datasets except for ImageNet and Cats. For ImageNet, we follow Dhariwal and Nichol <ref type="bibr" target="#b3">[4]</ref> and use the entire training set for reference statistics. For the small Cats dataset, we use the training as well as the validation set for reference statistics.</p><p>Baselines: We run baseline experiments using two publicly available repositories. The score_sde_pytorch repository is licensed according to the Apache License 2.0; see also their license file here. The CLD-SGM repository is licensed according to the NVIDIA Source Code License; see also their license file here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets:</head><p>We link here the websites of the datasets used in this experiment: CIFAR-10, LSUN datasets, ImageNet, and AFHQv2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Analytical First Step (AFS)</head><p>The forward process of DDMs generally converges to an analytical distribution. This analytical distribution is then used to sample from DDMs, defining the initial condition for the generative ODE/SDE. For example, for variance-preserving DDMs, we have p 1 (x 1 ) ? N (x 1 ; 0, I).</p><p>In this work, we try to minimize the computational complexity of sampling from DDMs, and therefore operate in a low NFE regime. In this regime, every additional function evaluation makes a significant difference. We therefore experimented with replacing the learned score with the (analytical score) of N (0, I) ? p 1 (x 1 ) in the first step of the ODE solver. This "gained" function evaluation can then be used as an additional step in the ODE solver later.</p><p>In particular, we have</p><formula xml:id="formula_87">? (x 1 , 1) ? x 1 ,<label>(75)</label></formula><p>and d ? (x1,1) d?1 ? 0 as shown below:</p><formula xml:id="formula_88">d ? (x 1 , 1) d? 1 ? dx t d? t | t=1 (76) = ? ? t ? 2 t + 1 [x t + s ? (x t , t)]| t=1 (using Eq. (28))<label>(77)</label></formula><p>? 0 (using normal assumption s ? (x t , t) ? ?x t ) (78) Given this, the AFS step becomes identical to the Euler update that uses the Normal score function for x 1 . This step is shown in the pseudocode in Alg. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 Classifier-Free Guidance</head><p>As discussed in Sec. 5.2, to guide diffusion sampling towards particular classes, we replace ? (x t , t) with?</p><formula xml:id="formula_89">? (x t , t, c, w) = (1 + w) ? (x t , t, c) ? w ? (x t , t),<label>(79)</label></formula><p>where w &gt; 1.0 is the "guidance scale", in the DDIM ODE. We experiment with classifier-free guidance on ImageNet. In Eq. (79) we re-use the conditional ImageNet score model ? (x t , t, c) trained before (see App. C.1 for details), and train an additional unconditional ImageNet score model ? (x t , t) using the exact same setup (and simply setting the class embedding to zero). We also re-use the conditional prediction head trained on top of the conditional ImageNet score model and train an additional prediction head for the unconditional model. Note that for both the score models as well as the prediction heads, we could share parameters between the models to reduce computational complexity <ref type="bibr" target="#b69">[70]</ref>. The modified GENIE scheme for classifier-free guidance is then given as</p><formula xml:id="formula_90">x tn+1 =x tn + (? tn+1 ? ? tn )? ? (x tn , t n , c, w) + 1 2 (? tn+1 ? ? tn ) 2k ? (x tn , t n , c, w),<label>(80)</label></formula><p>wherek ? (x tn , t n , c, w) = (1 + w)k ? (x tn , t n , c) ? wk ? (x tn , t n ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4 Encoding</head><p>To encode a data point x 0 into latent space, we first "diffuse" the data point to t = 10 ?3 , i.e., x t = ? t x 0 + ? t , ? N (0, I). We subsequently simulate the generative ODE (backwards) from t = 10 ?3 to t = 1, obtaining the latent point x 1 .</p><p>To decode a latent point x 1 , we simulate the generative ODE (forwards) from t = 1.0 to t = 10 ?3 . We then denoise the data point, i.e., x 0 = xt??t ? (xt,t) ?t . Note that denoising is generally optional to sample from DDMs; however, for our encoding-decoding experiment we always used denoising in the decoding part to match the inital "diffusion" in the encoding part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.5 Latent Space Interpolation</head><p>We can use encoding to perform latent space interpolation of two data points x (0) 0 and x <ref type="bibr" target="#b0">(1)</ref> 0 . We first encode both data points, following the encoding setup from App. F.4, and obtain x (0) 1 and x (1) 1 , respectively. We then perform spherical interpolation of the latent codes:</p><formula xml:id="formula_92">x (b) 1 = x (0) 1 ? 1 ? b + x (1) 2 ? b, b ? [0, 1].<label>(82)</label></formula><p>Subsequently, we decode the latent code x (b) 1 following the decoding setup from App. F.4. In <ref type="figure" target="#fig_1">Fig. 13</ref>, we show latent space interpolations for LSUN Church-Outdoor and LSUN Bedrooms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.6 Extended Quantitative Results</head><p>In this section, we show additional quantitative results not presented in the main paper. In particular, we show results for all four hyperparameter combinations (binary choice of AFS and binary choice of denoising) for methods evaluated by ourselves. For these methods (i.e., GENIE, DDIM, S-PNDM, F-PNDM, Euler-Maruyama), we follow the Synthesis Strategy outlined in Sec. 5, with the exception that we use linear striding instead of quadratic striding for S-PNDM <ref type="bibr" target="#b62">[63]</ref> and F-PNDM <ref type="bibr" target="#b62">[63]</ref>. To apply quadratic striding to these two methods, one would have to derive the Adams-Bashforth methods for non-constant step sizes which is beyond the scope of our work.</p><p>Results can be found in Tabs. 8 to 13. As expected, AFS can considerably improve results for almost all methods, in particular for NFEs ? 15. Denoising, on the other hand, is more important for larger NFEs. For our Cats models, we initially found that denoising hurts performance, and therefore did not further test it in all settings.</p><p>Recall Scores. We quantify the sample diversity of GENIE and other fast samplers using the recall score <ref type="bibr" target="#b111">[111]</ref>. In particular, we follow DDGAN <ref type="bibr" target="#b66">[67]</ref> and use the improved recall score <ref type="bibr" target="#b112">[112]</ref>; results on CIFAR-10 can be found in Tab. 6. As expected, we can see that for all methods recall scores suffer as the NFEs decrease. Compared to the baselines, GENIE achieves excellent recall scores, being on par with F-PNDM for NFE? 15. However, F-PNDM cannot be run for NFE?10 (due to its additional Runge-Kutta warm-up iterations). Overall, these results confirm that GENIE offers strong sample diversity when compared to other common samplers using the same score model checkpoint.</p><p>Striding Schedule Grid Search. As discussed in Sec. 5 the fixed quadratic striding schedule (for choosing the times t for evaluating the model during synthesis under fixed NFE budgets) used in GENIE may be sub-optimal, in particular for small NFEs. To explore this, we did a small grid search over three different striding schedules. As described in App. C.2.4, the quadratic striding schedule can be written as t n = 1.0 ? (1.0 ? ? t cutoff ) n N 2 , and easily be generalized to</p><formula xml:id="formula_93">t n = 1.0 ? (1.0 ? t 1/? cutoff ) n N ? , ? &gt; 1.<label>(83)</label></formula><p>In particular, besides the quadratic schedule ? = 2, we also tested the two additional values ? = 1.5 and ? = 2.5. We tested these schedules on GENIE as well as DDIM <ref type="bibr" target="#b57">[58]</ref>; note that the other two comptetive baselines, S-PNDM <ref type="bibr" target="#b62">[63]</ref> and F-PNDM <ref type="bibr" target="#b62">[63]</ref>, rely on linear striding, and therefore a grid search is not applicable. We show results for GENIE and DDIM in Tab. 7; for each combination of solver and NFE we applied the best synthesis strategy (whether or not we use denoising and/or the analytical first step) of quadratic striding (? = 2.0) also to ? = 1.5 and ? = 2.5. As can be seen in the table, ? = 1.5 improves for both DDIM and GENIE for NFE=5 (over the quadratic schedule   , whereas larger ? are preferred for larger NFE. The improvement of GENIE from 13.9 to 11.2 FID for NFE=5 is significant.</p><p>Discretization Errors of GENIE compared to other Fast Samplers. We compute discretization errors, in particular local and global truncation errors, of GENIE and compare to existing faster solvers. We are using the CIFAR-10 model. We initially sample 100 latent vectors x T ? N (0, I) and then, starting from those latent vectors, synthesize 100 approximate ground truth trajectories (GTTs) using DDIM with 1k NFEs (for that many steps, the discretization error is negligible; hence, we can treat this as a pseudo ground truth).</p><p>We then synthesize 100 sample trajectories for DDIM <ref type="bibr" target="#b57">[58]</ref>, S-PNDM <ref type="bibr" target="#b62">[63]</ref>, F-PNDM <ref type="bibr" target="#b62">[63]</ref>, and GENIE (for NFEs={5, 10, 15, 20, 25}, similar to the main experiments) using the same latent vectors as starting points that were used to generate the GTTs. DDIM, S-PNDM, and F-PNDM are trainingfree methods that can be run on the exact same score model, which also our GENIE relies on. Thereby, we are able to isolate discretization errors from errors in the learnt score function. We then compute the average L 2 -distance (in Inception feature space <ref type="bibr" target="#b104">[104]</ref>) between the output image of the fast samplers and the "output" of the pseudo GTT. As can be seen in <ref type="figure">Fig. 14</ref> (similar to what we did in <ref type="figure" target="#fig_1">Fig. 3</ref>). For each t, we then compare one step predictions for different step sizes ?t against the ground truth trajectory (L 2 -distance in data space averaged over 100 predictions; since we are not operating directly in image space at these intermediate t, using inception feature would not make sense here). As expected, we can see in <ref type="figure" target="#fig_12">Fig. 15</ref> that GENIE has smaller LTE than DDIM for all starting times t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.7 Extended Qualitative Results</head><p>In this section, we show additional qualitative comparisons of DDIM and GENIE on LSUN Church-Outdoor ( <ref type="figure" target="#fig_4">Fig. 16</ref>), ImageNet <ref type="figure" target="#fig_16">(Fig. 17)</ref>, and Cats (upsampler conditioned on test set images) ( <ref type="figure" target="#fig_5">Fig. 18</ref> and <ref type="figure" target="#fig_18">Fig. 19</ref>). In all figures, we can see that samples generated with GENIE generally exhibit finer details as well as sharper contrast and are less blurry compared to standard DDIM.</p><p>In <ref type="figure" target="#fig_0">Fig. 20</ref> and <ref type="figure" target="#fig_0">Fig. 21</ref>, we show additional high-resolution images generated with the GENIE Cats upsampler using base model samples and test set samples, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.8 Computational Resources</head><p>The total amount of compute used in this research project is roughly 163k GPU hours. We used an in-house GPU cluster of V100 NVIDIA GPUs. Step size ?t <ref type="figure" target="#fig_12">Figure 15</ref>: Local Truncation Error: Single step (local discretization) error, measured in L 2 -distance to (approximate) ground truth (computed using DDIM with 1k NFEs) in data space and averaged over 100 samples, for GENIE and DDIM for three starting time points t ? {0.1, 0.2, 0.5} (this is, the t from which a small step with size ?t is taken).     <ref type="table" target="#tab_2">Table 12</ref>: Cats (base model) generative performance (measured in FID). Method AFS NFEs=10 NFEs=15 NFEs=20 NFEs=25     The concurrent Bao et al. <ref type="bibr" target="#b95">[96]</ref> learn covariance matrices for diffusion model sampling using prediction heads somewhat similar to the ones in GENIE. Specifically, both Bao et al. <ref type="bibr" target="#b95">[96]</ref> and GENIE use small prediction heads that operate on top of the large first-order score predictor. However, we would like to stress multiple differences: (i) Bao et al. <ref type="bibr" target="#b95">[96]</ref> learn the DDM's sampling covariance matrices, while we learn higher-order ODE gradients. More generally, Bao et al. <ref type="bibr" target="#b95">[96]</ref> rely on stochastic diffusion model sampling, while we use the ODE formulation. (ii) Most importantly, in our case we can resort to directly learning the low-dimensional JVPs without low-rank or diagonal matrix approximations or other assumptions. Similar techniques are not directly applicable in Bao et al. <ref type="bibr" target="#b95">[96]</ref>'s setting. In detail, this is because in their case the relevant matrices (obtained after Cholesky or another applicable decomposition of the covariance) do not act on regular vectors but random noise variables. In other words, instead of using a deterministic JVP predictor (which takes x t and t as inputs), as in GENIE, Bao et al. <ref type="bibr" target="#b95">[96]</ref> would require to model an entire distribution for each x t and t without explicitly forming high-dimensional Cholesky decomposition-based matrices, if they wanted to do something somewhat analogous to GENIE's novel JVP-based approach. As a consequence, Bao et al. <ref type="bibr" target="#b95">[96]</ref> take another route to keeping the dimensionality of the additional network outputs manageable in practice.</p><formula xml:id="formula_94">x t ?x t (? t ) GENIE, t = 0.1 GENIE, t = 0.2 GENIE, t = 0.5 DDIM, t = 0.1 DDIM, t = 0.2 DDIM, t = 0.5</formula><p>In particular, they resort to assuming a diagonal covariance matrix in their experiments. By directly learning JVPs, we never have to rely on such potentially limiting assumptions. (iii) Experimentally, Bao et al. <ref type="bibr" target="#b95">[96]</ref> also consider fast sampling with few neural network calls. However, GENIE generally outperforms them (see, for example, their CIFAR10 results in their <ref type="table" target="#tab_3">Table 2</ref> for 10 and 25 NFE). This might indeed be due to the assumptions made by Bao et al. <ref type="bibr" target="#b95">[96]</ref>, which we avoid. Furthermore, their stochastic vs. our deterministic sampling may play a role, too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 Combining GENIE with Progressive Distillation</head><p>We speculate that GENIE could potentially be combined with Progressive Distillation <ref type="bibr" target="#b68">[69]</ref>: In every distillation stage of <ref type="bibr" target="#b68">[69]</ref>, one could quickly train a small GENIE prediction head to model higher-order ODE gradients. This would then allow for larger and/or more accurate steps, whose results represent the distillation target (teacher) in the progressive distillation protocol. This may also reduce the number of required distillation stages. Overall, this could potentially speed up the cumbersome stagewise distillation and maybe also lead to an accuracy and performance improvement. In particular, we could replace the DDIM predictions in Algorithm 2 of <ref type="bibr" target="#b68">[69]</ref> with improved GENIE predictions.</p><p>Note that this approach would not be possible with multistep methods as proposed by Liu et al. <ref type="bibr" target="#b62">[63]</ref>. Such techniques could not be used here, because they require the history of previous predictions, which are not available in the progressive distillation training scheme.</p><p>We leave exploration of this direction to future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Modeling a complex 2D toy distribution: Samples in (b) and (c) are generated via DDIM and GENIE, respectively, with 25 solver steps using the analytical score function of the ground truth distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Top: Single step error using analytical score function. Bottom: Norm of difference ?t(?t) between analytical and approximate derivative computed via finite difference method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>with Knowledge Distillation [KD, 68], Progressive Distillation [PG, 69] and Denoising Diffusion GANs [DDGAN, 67] as they do not solve the generative ODE/SDE and use fundamentally different sampling approaches with drawbacks discussed in Sec. 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Sample quality as a function of guidance scale on ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Encoding and subsequent decoding on LSUN Church-Outdoor. Left: Visual reconstruction. Right:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc><ref type="bibr" target="#b25">(26)</ref> for dxt d?t and used the usual parameterization s ? (x t , t) := ? ? (xt,t)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>d 2 ? d? 2 t,</head><label>2</label><figDesc>only containing partial derivatives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Single step error using analytical score function. See alsoFig. 3 (top).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Compute the exact temporal Jacobian-vector product JVPt = ? ? (x t ,t) ?t dt d? t via automatic differentiation ( dt d? t can be computed analytically) Compute k ? (xt, t) using the mixed parameterization in Eq. (66) Update weights ? to minimize ? 2 t k ? (xt, t) ? d? t ? (xt, t) 2 2 , where d? t ? (xt, t) = JVPs ? JVPt end for Algorithm 2 GENIE sampling Input: Score model s ? := ? ? (x t ,t) ? t , prediction head k ? , number of sampler steps N , time discretization {tn} N n=0 . Output: Generated GENIE output sample y. Sample xt 0 ? N (0, I)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>(a) DDIM, n = 5 (</head><label>5</label><figDesc>b) GENIE, n = 5 (c) DDIM, n = 10 (d) GENIE, n = 10 (e) DDIM, n = 15 (f) GENIE, n = 15 (g) DDIM, n = 20 (h) GENIE, n = 20</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 :</head><label>13</label><figDesc>Latent space interpolations for LSUN Church-Outdoor (Top) and LSUN Bedrooms (Bottom). Note that b = 0 and b = 1 correspond to the decodings of the encoded reference images. Since this encode-decode loop is itself not perfect, the references are not perfectly reproduced at b = 0 and b = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 16 :</head><label>16</label><figDesc>Additional samples on LSUN Church-Outdoor with zoom-in on details. GENIE often results in sharper and higher contrast samples compared to DDIM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 17 :</head><label>17</label><figDesc>Additional samples on ImageNet with zoom-in on details. GENIE often results in sharper and higher contrast samples compared to DDIM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 18 :</head><label>18</label><figDesc>Additional samples on Cats with zoom-in on details. GENIE often results in sharper and higher contrast samples compared to DDIM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 19 :</head><label>19</label><figDesc>Additional samples on Cats with zoom-in on details. GENIE often results in sharper and higher contrast samples compared to DDIM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 20 :</head><label>20</label><figDesc>End-to-end samples on Cats. The GENIE base model uses 25 function evaluations and the GENIE upsampler only uses five function evaluations. An upsampler evaluation is roughly four times as expensive as a base model evaluation.G MiscellaneousG.1 Connection to Bao et al.<ref type="bibr" target="#b95">[96]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 21 :</head><label>21</label><figDesc>Upsampling 128 ? 128 test set images using the GENIE upsampler with only five function evaluations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>systems. Concurrent Works. Zhang and Chen [92] motivate the DDIM ODE from an exponential integrator perspective applied to the Probability Flow ODE and propose to apply existing solvers from the numerical ODE literature, namely, Runge-Kutta and linear multistepping, to the DDIM ODE directly. Lu et al. [93] similarly recognize the semi-linear structure of the Probability Flow ODE, derive dedicated solvers, and introduce new step size schedulers to accelerate DDM sampling. Karras et al.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Unconditional CIFAR-10 generative performance (measured in FID). Methods above the mid- dle line use the same score model checkpoint; methods below all use different ones. ( ?): numbers are taken from literature. ( * ): meth- ods either learn an optimal striding schedule (Learned Sampler) or do a small grid search over striding schedules (DDIM &amp; GENIE); also see App. F.6</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>CIFAR-10 ablation studies (measured in FID).</figDesc><table><row><cell>Ablation</cell><cell cols="5">NFEs=5 NFEs=10 NFEs=15 NFEs=20 NFEs=25</cell></row><row><cell>Standard</cell><cell>13.9</cell><cell>6.04</cell><cell>4.49</cell><cell>3.94</cell><cell>3.67</cell></row><row><cell>No mixed</cell><cell>14.7</cell><cell>6.32</cell><cell>4.82</cell><cell>4.31</cell><cell>4.10</cell></row><row><cell>No weighting</cell><cell>14.8</cell><cell>7.45</cell><cell>5.89</cell><cell>5.17</cell><cell>4.80</cell></row><row><cell>Bigger model</cell><cell>13.7</cell><cell>5.58</cell><cell>4.46</cell><cell>4.05</cell><cell>3.77</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Cats (upsampler) generative performance (measured in FID).</figDesc><table><row><cell>Method</cell><cell cols="3">NFEs=5 NFEs=10 NFEs=15</cell></row><row><cell cols="2">GENIE (ours) 5.53</cell><cell>4.90</cell><cell>4.83</cell></row><row><cell>DDIM [58]</cell><cell>9.47</cell><cell>6.64</cell><cell>5.85</cell></row><row><cell cols="2">S-PNDM [63] 14.6</cell><cell>11.0</cell><cell>8.83</cell></row><row><cell cols="2">F-PNDM [63] N/A</cell><cell>N/A</cell><cell>11.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Learning Higher-Order Derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Image Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 5.2 Guidance and Encoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 5.3 Ablation Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 5.4 Upsampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 Theoretical Bounds for the Truncated Taylor Method . . . . . . . . . . . . . . . . 20 B.2 Approximate Higher-Order Derivatives via the "Ideal Derivative Trick" . . . . . . 20 B.3 3rd TTM Applied to the DDIM ODE . . . . . . . . . . . . . . . . . . . . . . . . . 22 B.4 GENIE is Consistent and Principled . . . . . . . . . . . . . . . . . . . . . . . . . 23 Score Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 C.2 Prediction Heads . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 C.2.1 Model Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 C.2.2 Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 C.2.3 Mixed Network Parameterization . . . . . . . . . . . . . . . . . . . . . . 27 C.2.4 Pseudocode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 C.2.5 Measuring Computational Overhead . . . . . . . . . . . . . . . . . . . . . 29 Evaluation Metrics, Baselines, and Datasets . . . . . . . . . . . . . . . . . . . . . 33 F.2 Analytical First Step (AFS) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 F.3 Classifier-Free Guidance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 F.4 Encoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 F.5 Latent Space Interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 F.6 Extended Quantitative Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 F.7 Extended Qualitative Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 F.8 Computational Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 Connection to Bao et al. [96] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 G.2 Combining GENIE with Progressive Distillation . . . . . . . . . . . . . . . . . . . 46</figDesc><table><row><cell>Contents</cell><cell></cell></row><row><cell>1 Introduction</cell><cell>1</cell></row><row><cell>2 Background</cell><cell>2</cell></row><row><cell>3 Higher-Order Denoising Diffusion Solver</cell><cell>3</cell></row><row><cell cols="2">3.1 4 Related Work 5 Experiments 5.1 6 Conclusions References A DDIM ODE B Synthesis from Denoising Diffusion Models via Truncated Taylor Methods B.1 C Model and Implementation Details C.1 D Learning Higher-Order Gradients without Automatic Differentiation and Distillation 30 6 7 10 11 19 20 26 E Toy Experiments 31 F Image Experiments 33 45 F.1 G Miscellaneous G.1</cell></row></table><note>and Chadwick [106], Nguyen et al. [107], Mirsky and Lee [108].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Model hyperparameters and training details. The CIFAR-10 model is taken from Song et al.<ref type="bibr" target="#b56">[57]</ref>; all other models are trained by ourselves.</figDesc><table><row><cell>Hyperparameter</cell><cell>CIFAR-10</cell><cell cols="5">LSUN Bedrooms LSUN Church-Outdoor ImageNet Cats (Base) Cats (Upsampler)</cell></row><row><cell>Model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Data dimensionality (in pixels)</cell><cell>32</cell><cell>128</cell><cell>128</cell><cell>64</cell><cell>128</cell><cell>512</cell></row><row><cell>Residual blocks per resolution</cell><cell>8</cell><cell>2</cell><cell>2</cell><cell>3</cell><cell>2</cell><cell>2</cell></row><row><cell>Attention resolutions</cell><cell>16</cell><cell>16</cell><cell>16</cell><cell>8</cell><cell>(8, 16)</cell><cell>(8, 16)</cell></row><row><cell>Base channels</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>192</cell><cell>96</cell><cell>192</cell></row><row><cell>Channel multipliers</cell><cell>1,2,3,4</cell><cell>1,1,2,2,4,4,4</cell><cell>1,1,2,2,4,4,4</cell><cell>1,2,3,4</cell><cell>1,2,2,3,3</cell><cell>1,1,2,2,3,3,4</cell></row><row><cell>EMA rate</cell><cell>0.9999</cell><cell>0.9999</cell><cell>0.9999</cell><cell>0.9999</cell><cell>0.9999</cell><cell>0.9999</cell></row><row><cell># of head channels</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>64</cell><cell>64</cell><cell>64</cell></row><row><cell># of parameters</cell><cell>107M</cell><cell>148M</cell><cell>148M</cell><cell>283M</cell><cell>200M</cell><cell>80.2M</cell></row><row><cell>Base architecture</cell><cell>DDPM++ [57]</cell><cell>DDPM [1]</cell><cell>DDPM [1]</cell><cell>[4]</cell><cell>[4]</cell><cell>[4]</cell></row><row><cell>Prediction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>v</cell><cell>v</cell></row><row><cell>Training</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell># of iterations</cell><cell>400k</cell><cell>300k</cell><cell>300k</cell><cell>400k</cell><cell>400k</cell><cell>150k</cell></row><row><cell># of learning rate warmup iterations</cell><cell>100k</cell><cell>100k</cell><cell>100k</cell><cell>100k</cell><cell>100k</cell><cell>100k</cell></row><row><cell>Optimizer</cell><cell>Adam</cell><cell>Adam</cell><cell>Adam</cell><cell>Adam</cell><cell>Adam</cell><cell>Adam</cell></row><row><cell>Mixed precision training</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Learning rate Gradient norm clipping</cell><cell>10 ?4 1.0</cell><cell>3 ? 10 ?4 1.0</cell><cell>3 ? 10 ?4 1.0</cell><cell>2 ? 10 ?4 1.0</cell><cell>10 ?4 1.0</cell><cell>10 ?4 1.0</cell></row><row><cell>Dropout</cell><cell>0.1</cell><cell>0.0</cell><cell>0.0</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>Batch size</cell><cell>128</cell><cell>256</cell><cell>256</cell><cell>1024</cell><cell>128</cell><cell>64</cell></row><row><cell>tcutoff</cell><cell>10 ?5</cell><cell>10 ?3</cell><cell>10 ?3</cell><cell>10 ?3</cell><cell>10 ?3</cell><cell>10 ?3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Model hyperparameters and training details for the prediction heads.</figDesc><table><row><cell>Hyperparameter</cell><cell cols="6">CIFAR-10 LSUN Bedrooms LSUN Church-Outdoor ImageNet Cats (Base) Cats (Upsampler)</cell></row><row><cell>Model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Data dimensionality</cell><cell>32</cell><cell>128</cell><cell>128</cell><cell>64</cell><cell>128</cell><cell>512</cell></row><row><cell>EMA rate</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Number of channels</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>196</cell><cell>196</cell><cell>92</cell></row><row><cell># of parameters</cell><cell>526k</cell><cell>526k</cell><cell>526k</cell><cell>1.17M</cell><cell>1.17M</cell><cell>302k</cell></row><row><cell>Normalize x embed</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Training</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell># of iterations</cell><cell>20k</cell><cell>40k</cell><cell>35k</cell><cell>15k</cell><cell>20k</cell><cell>20k</cell></row><row><cell>Optimizer</cell><cell>Adam</cell><cell>Adam</cell><cell>Adam</cell><cell>Adam</cell><cell>Adam</cell><cell>Adam</cell></row><row><cell>Optimization setup</cell><cell>Decay</cell><cell>Warmup</cell><cell>Warmup</cell><cell>Warmup</cell><cell>Warmup</cell><cell>Warmup</cell></row><row><cell>Mixed precision training</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Learning rate Gradient norm clipping</cell><cell>5 ? 10 ?5 1.0</cell><cell>10 ?4 1.0</cell><cell>10 ?4 1.0</cell><cell>10 ?4 1.0</cell><cell>10 ?4 1.0</cell><cell>10 ?4 1.0</cell></row><row><cell>Dropout</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell>Batch size</cell><cell>128</cell><cell>256</cell><cell>256</cell><cell>256</cell><cell>64</cell><cell>16</cell></row><row><cell>t cutoff</cell><cell>10 ?3</cell><cell>10 ?3</cell><cell>10 ?3</cell><cell>10 ?3</cell><cell>10 ?3</cell><cell>10 ?3</cell></row><row><cell>Inference</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Add. comp. overhead</cell><cell>1.47%</cell><cell>14.0%</cell><cell>14.4%</cell><cell>2.83%</cell><cell>7.55%</cell><cell>13.3%</cell></row><row><cell>terms of Eq. (12):</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Unconditional CIFAR-10 generative performance, measured in Recall (higher values are better). All methods use the same score model checkpoint.</figDesc><table><row><cell>Method</cell><cell cols="5">AFS Denoising NFEs=5 NFEs=10 NFEs=15 NFEs=20 NFEs=25</cell></row><row><cell></cell><cell>0.28</cell><cell>0.48</cell><cell>0.54</cell><cell>0.56</cell><cell>0.56</cell></row><row><cell>GENIE (ours)</cell><cell>0.21 0.27</cell><cell>0.45 0.47</cell><cell>0.52 0.53</cell><cell>0.56 0.56</cell><cell>0.57 0.56</cell></row><row><cell></cell><cell>0.19</cell><cell>0.46</cell><cell>0.53</cell><cell>0.55</cell><cell>0.56</cell></row><row><cell></cell><cell>0.10</cell><cell>0.27</cell><cell>0.38</cell><cell>0.43</cell><cell>0.46</cell></row><row><cell>DDIM [58]</cell><cell>0.07 0.08</cell><cell>0.24 0.27</cell><cell>0.35 0.38</cell><cell>0.42 0.43</cell><cell>0.46 0.46</cell></row><row><cell></cell><cell>0.04</cell><cell>0.24</cell><cell>0.36</cell><cell>0.42</cell><cell>0.45</cell></row><row><cell></cell><cell>0.06</cell><cell>0.30</cell><cell>0.43</cell><cell>0.49</cell><cell>0.52</cell></row><row><cell>S-PNDM [63]</cell><cell>0.02 0.11</cell><cell>0.25 0.33</cell><cell>0.39 0.45</cell><cell>0.46 0.50</cell><cell>0.50 0.53</cell></row><row><cell></cell><cell>0.06</cell><cell>0.29</cell><cell>0.41</cell><cell>0.47</cell><cell>0.51</cell></row><row><cell></cell><cell>N/A</cell><cell>N/A</cell><cell>0.55</cell><cell>0.57</cell><cell>0.58</cell></row><row><cell>F-PNDM [63]</cell><cell>N/A N/A</cell><cell>N/A N/A</cell><cell>0.52 0.55</cell><cell>0.56 0.58</cell><cell>0.57 0.59</cell></row><row><cell></cell><cell>N/A</cell><cell>N/A</cell><cell>0.54</cell><cell>0.56</cell><cell>0.57</cell></row><row><cell></cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.02</cell><cell>0.08</cell></row><row><cell>Euler-Maruyama</cell><cell>0.00 0.00</cell><cell>0.00 0.00</cell><cell>0.00 0.00</cell><cell>0.03 0.03</cell><cell>0.06 0.09</cell></row><row><cell></cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.03</cell><cell>0.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Unconditional CIFAR-10 generative performance (measured in FID) using our GENIE and DDIM<ref type="bibr" target="#b57">[58]</ref> with different striding schedules using exponents ? ? {1.5, 2.0, 2.5}.</figDesc><table><row><cell>Method</cell><cell>?</cell><cell cols="5">NFEs=5 NFEs=10 NFEs=15 NFEs=20 NFEs=25</cell></row><row><cell></cell><cell>1.5</cell><cell>11.2</cell><cell>5.28</cell><cell>5.03</cell><cell>4.35</cell><cell>3.97</cell></row><row><cell cols="2">GENIE 2.0</cell><cell>13.9</cell><cell>5.97</cell><cell>4.49</cell><cell>3.94</cell><cell>3.67</cell></row><row><cell></cell><cell>2.5</cell><cell>17.8</cell><cell>7.19</cell><cell>4.57</cell><cell>3.94</cell><cell>3.64</cell></row><row><cell></cell><cell>1.5</cell><cell>27.6</cell><cell>13.5</cell><cell>8.97</cell><cell>7.20</cell><cell>6.15</cell></row><row><cell>DDIM</cell><cell>2.0</cell><cell>29.7</cell><cell>11.2</cell><cell>7.35</cell><cell>5.87</cell><cell>5.16</cell></row><row><cell></cell><cell>2.5</cell><cell>33.2</cell><cell>13.4</cell><cell>8.28</cell><cell>6.36</cell><cell>5.39</cell></row><row><cell>? = 2)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>, GENIE outperforms the three other methods on all NFEs. Global Truncation Error: L 2 -distance of generated outputs by the fast samplers to the (approximate) ground truth (computed using DDIM with 1k NFEs) in Inception feature space<ref type="bibr" target="#b104">[104]</ref>. Results are averaged over 100 samples.GENIE to the LTE of DDIM. In particular, we compute LTEs at three starting times t ? {0.1, 0.2, .5}</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GENIE</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DDIM [58]</cell></row><row><cell>L 2 -distance</cell><cell>15</cell><cell></cell><cell></cell><cell></cell><cell cols="2">S-PNDM [63] F-PNDM [63]</cell></row><row><cell>Mean</cell><cell>8 10</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>4</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>25</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>NFEs</cell><cell></cell></row><row><cell>Figure 14:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>of</cell></row></table><note>Comparing the local truncation error (LTE) of different higher-order solvers can unfortunately not be done in a fair manner. Similar to DDIM, GENIE only needs the current value and a single NFE to predict the next step. In contrast, multistep methods rely on a history of predictions and Runge-Kutta methods rely on multiple NFEs to predict the next step. Thus, we can only fairly compare the LTE</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Unconditional CIFAR-10 generative performance (measured in FID). Methods above the middle line use the same score model checkpoint; methods below all use different ones. ( ?): numbers are taken from literature. This table is an extension of Tab. 1.</figDesc><table><row><cell>Method</cell><cell cols="5">AFS Denoising NFEs=5 NFEs=10 NFEs=15 NFEs=20 NFEs=25</cell></row><row><cell></cell><cell>15.4</cell><cell>5.97</cell><cell>4.70</cell><cell>4.30</cell><cell>4.10</cell></row><row><cell>GENIE (ours)</cell><cell>23.5 13.9</cell><cell>6.91 6.04</cell><cell>4.74 4.76</cell><cell>4.02 4.33</cell><cell>3.72 4.18</cell></row><row><cell></cell><cell>17.9</cell><cell>6.27</cell><cell>4.49</cell><cell>3.94</cell><cell>3.67</cell></row><row><cell></cell><cell>30.1</cell><cell>11.6</cell><cell>7.56</cell><cell>6.00</cell><cell>5.27</cell></row><row><cell>DDIM [58]</cell><cell>37.9 29.7</cell><cell>13.9 11.2</cell><cell>8.76 7.35</cell><cell>6.77 5.87</cell><cell>5.76 5.16</cell></row><row><cell></cell><cell>35.2</cell><cell>12.8</cell><cell>8.17</cell><cell>6.39</cell><cell>5.49</cell></row><row><cell></cell><cell>60.2</cell><cell>12.1</cell><cell>7.16</cell><cell>5.48</cell><cell>4.62</cell></row><row><cell>S-PNDM [63]</cell><cell>101 35.9</cell><cell>17.2 10.3</cell><cell>10.8 6.61</cell><cell>8.74 5.20</cell><cell>7.62 4.51</cell></row><row><cell></cell><cell>56.8</cell><cell>14.9</cell><cell>10.2</cell><cell>8.37</cell><cell>7.35</cell></row><row><cell></cell><cell>N/A</cell><cell>N/A</cell><cell>12.1</cell><cell>6.58</cell><cell>4.89</cell></row><row><cell>F-PNDM [63]</cell><cell>N/A N/A</cell><cell>N/A N/A</cell><cell>19.5 10.3</cell><cell>10.6 5.96</cell><cell>8.43 4.73</cell></row><row><cell></cell><cell>N/A</cell><cell>N/A</cell><cell>15.7</cell><cell>10.9</cell><cell>8.52</cell></row><row><cell></cell><cell>364</cell><cell>236</cell><cell>178</cell><cell>121</cell><cell>85.0</cell></row><row><cell>Euler-Maruyama</cell><cell>391 325</cell><cell>235 230</cell><cell>191 164</cell><cell>129 112</cell><cell>89.9 80.3</cell></row><row><cell></cell><cell>364</cell><cell>235</cell><cell>176</cell><cell>120</cell><cell>83.6</cell></row><row><cell>FastDDIM [64] ( ?)</cell><cell>-</cell><cell>9.90</cell><cell>-</cell><cell>5.05</cell><cell>-</cell></row><row><cell>Learned Sampler [66] ( ?)</cell><cell>12.4</cell><cell>7.86</cell><cell>5.90</cell><cell>4.72</cell><cell>4.25</cell></row><row><cell>Analytic DDIM (LS) [65] ( ?)</cell><cell>-</cell><cell>14.0</cell><cell>-</cell><cell>-</cell><cell>5.71</cell></row><row><cell>CLD-SGM [60]</cell><cell>334</cell><cell>306</cell><cell>236</cell><cell>162</cell><cell>106</cell></row><row><cell>VESDE-PC [57]</cell><cell>461</cell><cell>461</cell><cell>461</cell><cell>461</cell><cell>462</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Conditional ImageNet generative performance (measured in FID).</figDesc><table><row><cell>Method</cell><cell cols="5">AFS Denoising NFEs=5 NFEs=10 NFEs=15 NFEs=20 NFEs=25</cell></row><row><cell></cell><cell>23.4</cell><cell>8.35</cell><cell>6.13</cell><cell>5.36</cell><cell>5.00</cell></row><row><cell>GENIE (ours)</cell><cell>35.4 21.6</cell><cell>7.59 8.92</cell><cell>5.23 6.59</cell><cell>4.48 5.73</cell><cell>4.13 5.27</cell></row><row><cell></cell><cell>20.2</cell><cell>7.41</cell><cell>5.36</cell><cell>4.68</cell><cell>4.27</cell></row><row><cell></cell><cell>39.0</cell><cell>14.5</cell><cell>9.47</cell><cell>7.57</cell><cell>6.64</cell></row><row><cell>DDIM [58]</cell><cell>39.8 37.4</cell><cell>11.1 14.7</cell><cell>7.17 9.73</cell><cell>5.83 7.86</cell><cell>5.19 6.92</cell></row><row><cell></cell><cell>30.0</cell><cell>10.7</cell><cell>7.14</cell><cell>5.93</cell><cell>5.35</cell></row><row><cell></cell><cell>57.9</cell><cell>15.2</cell><cell>10.0</cell><cell>8.12</cell><cell>7.20</cell></row><row><cell>S-PNDM [63]</cell><cell>60.6 39.0</cell><cell>12.2 13.7</cell><cell>8.69 9.75</cell><cell>7.59 8.08</cell><cell>6.94 7.22</cell></row><row><cell></cell><cell>35.5</cell><cell>11.2</cell><cell>8.54</cell><cell>7.52</cell><cell>6.94</cell></row><row><cell></cell><cell>N/A</cell><cell>N/A</cell><cell>13.9</cell><cell>9.45</cell><cell>7.87</cell></row><row><cell>F-PNDM [63]</cell><cell>N/A N/A</cell><cell>N/A N/A</cell><cell>14.5 12.5</cell><cell>9.45 9.01</cell><cell>8.05 7.74</cell></row><row><cell></cell><cell>N/A</cell><cell>N/A</cell><cell>12.3</cell><cell>9.26</cell><cell>7.86</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>Unconditional LSUN Bedrooms generative performance (measured in FID). Methods above the middle line use the same score model checkpoint; Learned Sampler uses a different one. ( ?): numbers are taken from literature.</figDesc><table><row><cell>Method</cell><cell cols="5">AFS Denoising NFEs=5 NFEs=10 NFEs=15 NFEs=20 NFEs=25</cell></row><row><cell></cell><cell>74.1</cell><cell>17.1</cell><cell>13.3</cell><cell>11.6</cell><cell>11.1</cell></row><row><cell>GENIE (ours)</cell><cell>115 55.9</cell><cell>11.4 18.4</cell><cell>7.18 14.1</cell><cell>5.80 12.3</cell><cell>5.35 11.6</cell></row><row><cell></cell><cell>47.3</cell><cell>9.29</cell><cell>6.83</cell><cell>5.79</cell><cell>5.40</cell></row><row><cell></cell><cell>69.6</cell><cell>27.1</cell><cell>19.0</cell><cell>15.8</cell><cell>14.2</cell></row><row><cell>DDIM [58]</cell><cell>81.0 62.1</cell><cell>16.3 27.1</cell><cell>9.18 19.3</cell><cell>7.12 16.3</cell><cell>6.20 14.6</cell></row><row><cell></cell><cell>42.5</cell><cell>12.5</cell><cell>8.21</cell><cell>6.77</cell><cell>6.05</cell></row><row><cell></cell><cell>70.4</cell><cell>22.1</cell><cell>15.7</cell><cell>13.5</cell><cell>12.4</cell></row><row><cell>S-PNDM [63]</cell><cell>88.9 48.0</cell><cell>12.2 20.2</cell><cell>8.40 15.2</cell><cell>7.33 13.4</cell><cell>6.80 12.4</cell></row><row><cell></cell><cell>45.0</cell><cell>10.8</cell><cell>8.14</cell><cell>7.23</cell><cell>6.71</cell></row><row><cell></cell><cell>N/A</cell><cell>N/A</cell><cell>36.1</cell><cell>18.5</cell><cell>14.6</cell></row><row><cell>F-PNDM [63]</cell><cell>N/A N/A</cell><cell>N/A N/A</cell><cell>26.8 29.4</cell><cell>9.85 17.5</cell><cell>7.86 14.3</cell></row><row><cell></cell><cell>N/A</cell><cell>N/A</cell><cell>18.9</cell><cell>9.27</cell><cell>7.69</cell></row><row><cell>Learned Sampler [66] ( ?)</cell><cell>29.2</cell><cell>11.0</cell><cell>-</cell><cell>4.82</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11 :</head><label>11</label><figDesc>Unconditional LSUN Church-Outdoor generative performance (measured in FID). Methods above the middle line use the same score model checkpoint; Learned Sampler uses a different one. ( ?): numbers are taken from literature.</figDesc><table><row><cell>Method</cell><cell cols="5">AFS Denoising NFEs=5 NFEs=10 NFEs=15 NFEs=20 NFEs=25</cell></row><row><cell></cell><cell>97.2</cell><cell>25.4</cell><cell>15.9</cell><cell>11.6</cell><cell>9.57</cell></row><row><cell>GENIE (ours)</cell><cell>147 47.8</cell><cell>13.7 13.6</cell><cell>11.7 10.6</cell><cell>8.52 9.17</cell><cell>7.28 8.28</cell></row><row><cell></cell><cell>60.3</cell><cell>10.5</cell><cell>7.44</cell><cell>6.38</cell><cell>5.84</cell></row><row><cell></cell><cell>81.5</cell><cell>28.5</cell><cell>16.7</cell><cell>11.9</cell><cell>9.9</cell></row><row><cell>DDIM [58]</cell><cell>110 44.0</cell><cell>25.3 17.4</cell><cell>11.5 12.5</cell><cell>8.53 10.2</cell><cell>7.35 9.07</cell></row><row><cell></cell><cell>45.8</cell><cell>12.8</cell><cell>8.44</cell><cell>6.97</cell><cell>6.28</cell></row><row><cell></cell><cell>59.4</cell><cell>18.7</cell><cell>13.3</cell><cell>11.4</cell><cell>10.4</cell></row><row><cell>S-PNDM [63]</cell><cell>87.5 40.7</cell><cell>14.8 17.0</cell><cell>9.54 12.8</cell><cell>7.98 11.2</cell><cell>7.21 10.3</cell></row><row><cell></cell><cell>48.8</cell><cell>12.9</cell><cell>9.10</cell><cell>7.82</cell><cell>7.12</cell></row><row><cell></cell><cell>N/A</cell><cell>N/A</cell><cell>15.5</cell><cell>12.0</cell><cell>10.6</cell></row><row><cell>F-PNDM [63]</cell><cell>N/A N/A</cell><cell>N/A N/A</cell><cell>15.7 15.2</cell><cell>9.78 11.8</cell><cell>7.99 10.4</cell></row><row><cell></cell><cell>N/A</cell><cell>N/A</cell><cell>12.6</cell><cell>9.29</cell><cell>7.83</cell></row><row><cell>Learned Sampler [66] ( ?)</cell><cell>30.2</cell><cell>11.6</cell><cell>-</cell><cell>6.74</cell><cell>-</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Yaoliang Yu for early discussions. Tim Dockhorn acknowledges additional funding from the Vector Institute Research Grant, which is not in direct support of this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Denoising Diffusion Probabilistic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.15282</idno>
		<title level="m">Cascaded Diffusion Models for High Fidelity Image Generation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improved Denoising Diffusion Probabilistic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nichol</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Diffusion Models Beat GANs on Image Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10752</idno>
		<title level="m">High-Resolution Image Synthesis with Latent Diffusion Models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Diffusion Probabilistic Modeling for Video Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakhar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Mandt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.09481</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.03458</idno>
		<title level="m">Video Diffusion Models</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Image Super-Resolution via Iterative Refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Norouzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07636</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srdiff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14951</idno>
		<title level="m">Single Image Super-Resolution with Diffusion Probabilistic Models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Milanfar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.02475</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Deblurring via Stochastic Refinement</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bahjat</forename><surname>Kawar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11793</idno>
		<title level="m">Stefano Ermon, and Jiaming Song. Denoising Diffusion Restoration Models</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.01073</idno>
		<title level="m">SDEdit: Image Synthesis and Editing with Stochastic Differential Equations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.09865</idno>
		<title level="m">RePaint: Inpainting using Denoising Diffusion Probabilistic Models</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiwen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.05826</idno>
		<title level="m">Palette: Image-to-Image Diffusion Models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10741</idno>
		<title level="m">GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Avrahami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ohad</forename><surname>Fried</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.14818</idno>
		<title level="m">lended Diffusion for Text-driven Editing of Natural Images</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casey</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06125</idno>
		<title level="m">Hierarchical Text-Conditional Image Generation with CLIP Latents</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jooyoung</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghyun</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjune</forename><surname>Gwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kushagra</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avideep</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.00308</idno>
		<title level="m">DiffuseVAE: Efficient, Controllable and High-Fidelity Generation from Low-Dimensional Latents</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konpat</forename><surname>Preechakul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nattanat</forename><surname>Chatthee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.15640</idno>
		<title level="m">Suttisak Wizadwongsa, and Supasorn Suwajanakorn. Diffusion Autoencoders: Toward a Meaningful and Decodable Representation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">More Control for Free! Image Synthesis with Semantic Diffusion Guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samaneh</forename><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Chopikyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.05744</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Batzolis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Stanczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carola-Bibiane</forename><surname>Sch?nlieb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Etmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.13606</idno>
		<title level="m">Conditional Image Generation with Score-Based Diffusion Models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Sasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">G</forename><surname>Willcocks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05358</idno>
		<title level="m">UNIT-DDPM: UNpaired Image Translation with Denoising Diffusion Probabilistic Models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.08382</idno>
		<title level="m">Dual Diffusion Implicit Bridges for Image-to-Image Translation</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Solving Inverse Problems in Medical Imaging with Score-Based Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Towards performant and reliable undersampled MR reconstruction via diffusion model sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Kevin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishal</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.04292</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Measurement-conditioned Denoising Diffusion Probabilistic Model for Under-sampled Medical Image Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanzheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.03623</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanxiong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Uecker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.01479</idno>
		<title level="m">MRI Reconstruction via Data Driven Markov Chain with Joint Uncertainty Estimation</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Score-based diffusion models for accelerated MRI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyungjin</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><surname>Chul Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.05243</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dewei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuankai</forename><forename type="middle">K</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ipek</forename><surname>Oguz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11760</idno>
		<title level="m">Unsupervised Denoising of Retinal OCT with Diffusion Probabilistic Model</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyungjin</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongsu</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong Chul</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.05146</idno>
		<title level="m">Come-Closer-Diffuse-Faster: Accelerating Conditional Diffusion Models for Inverse Problems through Stochastic Contraction</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">WaveGrad: Estimating Gradients for Waveform Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">DiffWave: A Versatile Diffusion Model for Audio Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaji</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myeonghun</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Jun</forename><surname>Cheon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Byoung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Soo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01409</idno>
		<title level="m">Diff-TTS: A Denoising Diffusion Model for Text-to-Speech</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">WaveGrad 2: Iterative Refinement for Text-to-Speech Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Najim</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09660</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vovk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Gogoryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tasnima</forename><surname>Sadekova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Kudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songxiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11972</idno>
		<title level="m">DiffGAN-TTS: High-Fidelity and Efficient Text-to-Speech with Denoising Diffusion GANs</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Diffusion Probabilistic Models for 3D Point Cloud Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shitong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning Gradient Fields for Shape Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruojin</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guandao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadar</forename><surname>Averbuch-Elor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/chapter/10.1007/978-3-030-58580-8_22</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">3D Shape Generation and Completion through Point-Voxel Diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A Conditional Point Diffusion-Refinement Paradigm for 3D Point Cloud Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">U</forename><surname>Xudong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">From data to functa: Your data point is a function and you should treat it like one</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilien</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Ali Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Rosenbaum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.12204</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning gradient fields for molecular conformation generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chence</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shitong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minkai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">GeoDiff: A Geometric Diffusion Model for Molecular Conformation Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minkai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chence</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Crystal Diffusion Variational Autoencoder for Periodic Material Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Octavian-Eugen</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyeong</forename><surname>Jo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.02514</idno>
		<title level="m">Seul Lee, and Sung Ju Hwang. Score-based Generative Modeling of Graphs via the System of Stochastic Differential Equations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Maximum Likelihood Training of Score-Based Diffusion Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Conor</forename><surname>Durkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Variational Diffusion Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Score-based Generative Modeling in Latent Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A Variational Perspective on Diffusion-Based Generative Models and Score Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Hyun</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Permutation invariant graph generation via score-based generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhao</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Adversarial Purification with Score-based Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Tashiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emile</forename><surname>Valentin De Bortoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Thornton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doucet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.02763</idno>
		<title level="m">Riemannian Score-Based Generative Modeling</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Diffusion Causal Models for Counterfactual Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sotirios</forename><forename type="middle">A</forename><surname>Tsaftaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First Conference on Causal Learning and Reasoning</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Diffusion Models for Adversarial Purification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weili</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaowei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Score-Based Generative Modeling through Stochastic Differential Equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Denoising Diffusion Implicit Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Numerical Solution of Stochastic Differential Equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eckhard</forename><surname>Kloeden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Score-Based Generative Modeling with Critically-Damped Langevin Diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">It?-Taylor Sampling Scheme for Denoising Diffusion Probabilistic Models using Ideal Derivatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideyuki</forename><surname>Tachibana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mocho</forename><surname>Go</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muneyoshi</forename><surname>Inahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yotaro</forename><surname>Katayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yotaro</forename><surname>Watanabe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.13339</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Pich?-Taillefer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.14080</idno>
		<title level="m">Tal Kachman, and Ioannis Mitliagkas. Gotta Go Fast When Generating Data with Score-Based Models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Pseudo Numerical Methods for Diffusion Models on Manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">On Fast Sampling of Diffusion Probabilistic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.00132</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Tackling the Generative Learning Trilemma with Denoising Diffusion GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Knowledge Distillation in Iterative Generative Models for Improved Sampling Speed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Luhman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Troy</forename><surname>Luhman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.02388</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Progressive Distillation for Fast Sampling of Diffusion Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Classifier-Free Diffusion Guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Deep Unsupervised Learning using Nonequilibrium Thermodynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Reverse-time diffusion equation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename><surname>Brian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stochastic Processes and their Applications</title>
		<imprint>
			<date type="published" when="1982" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="313" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Time Reversal of Diffusions. The Annals of Probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Haussmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pardoux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="page" from="1188" to="1205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Neural Ordinary Differential Equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<title level="m">FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">A Connection Between Score Matching and Denoising Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1661" to="1674" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Numerical Methods for Ordinary Differential Equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John Charles</forename><surname>Butcher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Numerical Solution of Stochastic Differential Equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eckhard</forename><surname>Kloeden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/chapter/10.1007/978-3-319-24574-4_28</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jooyoung</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungbeom</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaehun</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.00227</idno>
		<title level="m">Perception Prioritized Training of Diffusion Models</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Label-Efficient Semantic Segmentation with Diffusion Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Baranchuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Voynov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Rubachev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Khrulkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Babenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Estimating High Order Gradients of the Data Distribution by Denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Learning to Efficiently Sample from Diffusion Probabilistic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03802</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">A family of embedded Runge-Kutta formulae</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Dormand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="26" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliya</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>San Roman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07582</idno>
		<title level="m">Non Gaussian Denoising Diffusion Models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Max</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.11514</idno>
		<title level="m">Bilateral Denoising Diffusion Models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Djeumou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Neary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Goubault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvie</forename><surname>Putot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ufuk</forename><surname>Topcu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.05715</idno>
		<title level="m">Taylor-Lagrange Neural Ordinary Differential Equations: Toward Fast Training and Evaluation of Neural ODEs</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Learning differential equations that are easy to solve</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4370" to="4380" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">How to train your neural ODE: the world of Jacobian and kinetic regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Finlay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levon</forename><surname>Nurbekyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Oberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3154" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">ATOMFT: solving ODEs and DAEs using Taylor series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Corliss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Mathematics with Applications</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="209" to="233" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Solving ordinary differential equations using Taylor series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Corliss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/10.1145/355993.355995</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Mathematical Software (TOMS)</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="114" to="144" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Fast Sampling of Diffusion Models with Exponential Integrator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinsheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.13902</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.00927</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Elucidating the Design Space of Diffusion-Based Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.00364</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinsheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Molei</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.05564</idno>
		<title level="m">gDDIM: Generalized denoising diffusion implicit models</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Estimating the Optimal Covariance with Imperfect Mean in Diffusion Probabilistic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.07309</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Learning Multiple Layers of Features from Tiny Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lsun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">StarGAN v2: Diverse Image Synthesis for Multiple Domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaejun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8188" to="8197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Large Scale GAN Training for High Fidelity Natural Image Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<editor>I. Guyon, U. V</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Adversarial score matching and improved sampling for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Pich?-Taillefer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Ioannis Mitliagkas, and Remi Tachet des Combes</note>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Seyed Kamyar Seyed Ghasemipour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lala</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; S Sara</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rapha Gontijo</forename><surname>Lopes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.11487</idno>
	</analytic>
	<monogr>
		<title level="m">Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Deepfakes and Disinformation: Exploring the Impact of Synthetic Political Video on Deception, Uncertainty, and Trust in News</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Vaccari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chadwick</surname></persName>
		</author>
		<idno type="DOI">https:/journals.sagepub.com/doi/full/10.1177/2056305120903408</idno>
	</analytic>
	<monogr>
		<title level="j">Social Media + Society</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2056305120903408</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><forename type="middle">Thi</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11573</idno>
		<title level="m">Quoc Viet Hung Nguyen, Cuong M. Nguyen, Dung Nguyen, Duc Thanh Nguyen, and Saeid Nahavandi. Deep Learning for Deepfakes Creation and Detection: A Survey</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">The Creation and Detection of Deepfakes: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisroel</forename><surname>Mirsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenke</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/abs/10.1145/3425780?casa_token=dNM1WT1Tk5cAAAAA:EY6oH11-RXcmnv683wpcTq_LnG_M6CpJzlCYiJYVcIle8DQM6fuZGMuVyrKNw6-nEy9gsgRO9Atibw</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Spectral Normalization for Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Assessing Generative Models via Precision and Recall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Improved Precision and Recall Metric for Assessing Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Kynk??nniemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">GENIE (ours)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Cats (upsampler) generative performance (measured in FID)</title>
	</analytic>
	<monogr>
		<title level="m">Method AFS NFEs=5 NFEs=10 NFEs=15</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
