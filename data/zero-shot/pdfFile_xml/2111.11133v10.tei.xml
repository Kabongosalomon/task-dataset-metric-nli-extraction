<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">L-Verse: Bidirectional Generation Between Image and Text</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taehoon</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">LG AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gwangmo</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">LG AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sihaeng</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">LG AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangyun</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">LG AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yewon</forename><surname>Seo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">LG AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soonyoung</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">LG AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung</forename><forename type="middle">Hwan</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">LG AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">LG AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghoon</forename><surname>Bae</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">LG AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">L-Verse: Bidirectional Generation Between Image and Text</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Far beyond learning long-range interactions of natural language, transformers are becoming the de-facto standard for many vision tasks with their power and scalability. Especially with cross-modal tasks between image and text, vector quantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB image into a sequence of feature vectors. To better leverage the correlation between image and text, we propose L-Verse, a novel architecture consisting of feature-augmented variational autoencoder (AugVAE) and bidirectional auto-regressive transformer (BiART) for image-to-text and text-to-image generation. Our AugVAE shows the state-of-the-art reconstruction performance on ImageNet1K validation set, along with the robustness to unseen images in the wild. Unlike other models, BiART can distinguish between image (or text) as a conditional reference and a generation target. L-Verse can be directly used for image-to-text or text-to-image generation without any finetuning or extra object detection framework. In quantitative and qualitative experiments, L-Verse shows impressive results against previous methods in both image-to-text and text-to-image generation on MS-COCO Captions. We furthermore assess the scalability of L-Verse architecture on Conceptual Captions and present the initial result of bidirectional vision-language representation learning on general domain.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image-to-text and text-to-image generation and can be summarized as a task of learning cross-modal representations of image and text. Recent studies <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b31">32]</ref> on vision-language tasks have highly improved the performance of each target task, in particular with various transformer architectures <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b45">45]</ref>. Initially designed to understand natural language, the dot-product multi-head attention mechanism <ref type="bibr" target="#b45">[45]</ref> effectively learns long-range interactions of sequential data. To leverage transformer archi-* Correspondence to: taehoon.kim@lgresearch.ai a sunset view with the river a full moon behind buildings lone astronomer in empty planet GT: A young boy in the park throwing a frisbee.</p><p>L-Verse: A young boy throwing a green frisbee in a lush green park.</p><p>GT: A laptop and a cell phone on a table.</p><p>L-Verse: A collection of electronic devices and cords sitting on top of a table.</p><p>GT: A small bathroom is shown from a door.</p><p>L-Verse: A bathroom with a shower curtain over the bathtub next to a toilet. tectures <ref type="bibr" target="#b45">[45]</ref> also in vision domains, an input image is factorized into a sequence of latent feature vectors. To encode an image into a sequence of latent feature vectors, vector quantized variational autoencoder (VQ-VAE) <ref type="bibr" target="#b43">[44]</ref> can be used to learn a discrete latent representation with quantized embedding vectors from the visual codebook. VQ-VAE is a simple and powerful representation learning method to make image sequential and is widely used in conditional image generation tasks with auto-regressive pairs like RNNs <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b43">44]</ref> or transformers <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b31">32]</ref>. Improving the reconstruction quality of VQ-VAE is also an active area of research <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>Combining an auto-regressive transformer <ref type="bibr" target="#b2">[3]</ref> with a feature extractor like VQ-VAEs or other deep convolutional neural networks (CNNs) is becoming a popular approach for various vision-language tasks. However, training a model for unidirectional image-to-text <ref type="bibr" target="#b6">[7]</ref> or text-to-image <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b31">32]</ref> generation task still requires a large amount of data or an extra object detection framework. We hypothesize that learning bidirectional cross-modal representation of image and text can alleviate this problem via better data efficiency. This paper proposes an approach, L-Verse (latent verse), for learning a bidirectional vision-language cross-modal representation. The key idea of L-Verse is two-fold: (i) augment a visual codebook with diverse features and (ii) enable an auto-regressive transformer to learn bidirectional imagetext generation. Our novel cross-level feature augmentation technique effectively increase the diversity of a visual codebook with unique feature embedding vectors. We furthermore add a segment embedding to an auto-regressive transformer <ref type="bibr" target="#b2">[3]</ref> to teach the difference between image (or text) as given condition or generation target. Specifically, our contribution for vision-language cross-modal representation learning are summarized as follows:</p><p>? We introduce a feature-augmented variational autoencoder (AugVAE), a VQ-VAE trained with cross-level feature augmentation. With the feature-augmented visual codebook, AugVAE shows the state-of-the-art reconstruction performance on both in-domain Im-ageNet1K <ref type="bibr" target="#b7">[8]</ref> validation set ( <ref type="figure" target="#fig_1">Figure 2</ref>) and out-ofdomain image datasets ( <ref type="figure" target="#fig_5">Figure 5</ref>).</p><p>? We propose a bidirectional auto-regressive transformer (BiART) for bidirectional image-text generation. We index each token with two different embedding vectors according to its role as a conditional reference <ref type="table">([REF]</ref>) or a generation target ([GEN]). With this segment embedding, our BiART can both generate corresponding images to given texts or meaningful captions to given images without any finetuning.</p><p>? L-Verse, consisting of AugVAE and BiART, outperforms previously proposed image captioning models in most of the machine evaluation metrics on MS-COCO Captions <ref type="bibr" target="#b23">[24]</ref> Karpathy test split. It is also notable that L-Verse does not require any object-detection framework, such as Faster-RCNN <ref type="bibr" target="#b33">[34]</ref>.</p><p>? L-Verse shows comparable text-to-image generation results to other generative models on MS-COCO Captions <ref type="bibr" target="#b23">[24]</ref>. We also assess the scalability of L-Verse for zero-shot text-to-image generation by training on Conceptual Captions <ref type="bibr" target="#b38">[39]</ref>.</p><p>Section 2 briefly reviews previous works on VQ-VAE and cross-modal vision-language tasks. Section 3 explains how we design AugVAE and BiART to learn the bidirectional cross-modal representation between image and text. Section 4 shows quantitative and qualitative results on image reconstruction, image-to-text generation, and text-toimage generation. Section 5 summarizes our paper with conclusion and discussion for future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Adapting transformer architectures <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b45">45]</ref> for various vision-language tasks has been an active research area in the recent years. Since an image is a matrix of RGB pixel values, it should be first factorized into a sequence of feature vectors. Recent auto-regressive transformer based generative models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b31">32]</ref> utilize different variants of VQ-VAE <ref type="bibr" target="#b43">[44]</ref> to compress and reconstruct images. In this section, we introduce the main concept of VQ-VAE and its variants. We also explain how VQ-VAE or other CNN architectures are combined with auto-regressive transformers to solve image-to-text or text-to-image generation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Vector Quantized Variational Autoencoder</head><p>Vector quantized variational autoencoder, VQ-VAE <ref type="bibr" target="#b43">[44]</ref>, is a set of an encoder E, a decoder G, and a visual codebook Z for learning discrete representations of images. The CNN encoder E factorizes the continuous representation of an image? into series of discrete vectors z q , each selected from visual codebook Z. The CNN decoder G is used to reconstruct any z q sampled from Z. Razavi et al. <ref type="bibr" target="#b32">[33]</ref> extend this approach to use hierarchical feature representation and apply exponential-moving-average (EMA) weight update to codebook Z. To better optimize the training of VQ-VAE, Ramesh et al. <ref type="bibr" target="#b31">[32]</ref> use the gumbel-softmax relaxation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27]</ref>. Esser et al. <ref type="bibr" target="#b11">[12]</ref> further improve the quality of image reconstruction with additional CNN discriminator, originated from generative adversarial network (GAN) <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Image-to-Text Generation</head><p>As the dot-product multi-head attention <ref type="bibr" target="#b45">[45]</ref> was initially designed for language tasks, transformers have achieved new state-of-the-art results in generating natural and detailed captions corresponding to an input image. Previous works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23]</ref> utilize region features extracted using Faster R-CNN <ref type="bibr" target="#b33">[34]</ref> to generate captions for each image. While visual semantics of each region improves the quality, objects outside detection target classes (80 classes for MS-COCO Detection <ref type="bibr" target="#b23">[24]</ref>) get ignored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Text-to-Image Generation</head><p>Generative adversarial networks (GANs) <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr">53]</ref> have been traditionally used for text-conditional image generation tasks. GAN based models focus on finding better modeling assumptions for specific data domains like CUB-200 <ref type="bibr" target="#b47">[47]</ref> or MS-COCO Captions <ref type="bibr" target="#b23">[24]</ref>. Ramesh et al. <ref type="bibr" target="#b31">[32]</ref> first trained a 12-billion parameter transformer <ref type="bibr" target="#b3">[4]</ref> on 250-million image-text pairs for text-to-image generation in the general domain. Ding et al. <ref type="bibr" target="#b9">[10]</ref> proposed a 4billion parameter transformer, CogView, with stable training techniques and finetuning strategies for various downstream tasks. </p><formula xml:id="formula_0">f'=2 f'=4 f'=2 f'=2 f=2 f=4 f=2 f=2</formula><p>A bench sitting at the end of wide countryside road.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generated Caption</head><p>Overcast skies and mountains. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Text</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminary</head><p>Ramesh et al. <ref type="bibr" target="#b31">[32]</ref> proposed a two-stage training procedure for text-to-image generation with an auto-regressive transformer <ref type="bibr" target="#b2">[3]</ref> :</p><p>? Stage 1: Train a discrete variational autoencoder (dVAE) <ref type="bibr" target="#b31">[32]</ref> to compress each 256 ? 256 RGB into a 32 ? 32 grid of image tokens with each element of 8192 (d Z ) possible values .</p><p>? Stage 2: Concatenate up to 256 BPE-encoded text tokens with the 32 ? 32 = 1024 image tokens, and train an auto-regressive transformer <ref type="bibr" target="#b2">[3]</ref> to model the joint distribution over text and image tokens.</p><p>The approach maximizes the evidence lower bound <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36]</ref> on the joint likelihood of the model distribution over the image x, the caption y, and the tokens z for the encoded RGB image. From the factorization p ?,? (x, y, z) = p ? (x|y, z)p ? (y, z), the lower bound is yielded as</p><formula xml:id="formula_1">ln p ?,? (x, y) ? E z?q ? (z|x) (ln p ? (x|y, z) ? D KL (q ? (y, z|x), p ? (y, z)))<label>(1)</label></formula><p>where:</p><p>? q ? denotes the distribution over the 32 ? 32 encoded tokens generated by dVAE encoder from the image x.</p><p>? p ? denotes the distribution over the reconstructed imagex from dVAE decoder.</p><p>? p ? denotes the joint distribution over the text and image tokens modeled by the transformer.</p><p>In Stage 1, dVAE (or other VQ-VAE variants) learns to minimize the reconstruction loss between x andx. In Stage 2, an auto-regressive transformer optimizes two negative loglikelihood (NLL) losses: (i) for caption y and (ii) for encoded image tokens z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Proposed Approach: L-Verse Framework</head><p>Inspired by DALL-E [32], we propose two major improvements for high-fidelity image reconstruction and bidirectional image-text generation:</p><p>? We improve the diversity of a visual codebook Z with cross-level feature augmentation. We first train multilevel (hierarchical) VQ-VAE (blue in <ref type="figure" target="#fig_1">Figure 2</ref>) and apply weight-sharing to vector quantizers <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b43">44]</ref> in each feature-level. The hierarchical VQ-VAE is then finetuned to a VQ-VAE with codebook size N = 32 ? 32. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Feature-Augmented Variational Autoencoder</head><p>Razavi et al. <ref type="bibr" target="#b32">[33]</ref> states that increasing the number of latent feature map adds extra details to the reconstruction. However, increasing the number of latent map also increases the total codebook size N , from 32 ? 32 = 1024 <ref type="bibr" target="#b43">[44]</ref> to 32 ? 32 + 64 ? 64 = 5120 <ref type="bibr" target="#b32">[33]</ref>.</p><p>For the high-quality image reconstruction at low-cost, we choose to use the single 32 ? 32 latent map and augment the visual codebook Z instead. From the example in <ref type="figure" target="#fig_3">Figure 4</ref>, similar patterns in various patch sizes can appear both in one image (blue) and across different images (red). As the distance between similar patterns gets closer after vector quantization (VQ) <ref type="bibr" target="#b43">[44]</ref>, extracting patches from different latent maps and storing them in one place removes duplicates and fills the codebook with unique 8192 (d Z ) possible values.</p><p>We optimize the encoder -vector quantizer -decoder architecture of VQ-VAE <ref type="bibr" target="#b43">[44]</ref> for cross-level feature augmentation:</p><p>? We define the encoder as z = E(x, f, d out ), where x is an n ? n ? d in tensor and f is a downsampling factor. E(f, d out ) downsamples a tensor x into an n f ? n f ? d out tensor z. ? We define the vector quantizer as</p><formula xml:id="formula_2">z q = V Q(z, d Z ),</formula><p>where z is an n ? n ? d tensor with continuous dsize vectors. z q is a quantized version of z with d Z possible values for each d-size feature vector. We use exponential-moving-average (EMA) vector quantizer <ref type="bibr" target="#b32">[33]</ref>. All vector quantizers in AugVAE shares weight parameters.</p><p>? We define the decoder asx = G(?, f, d out ), where? is an n ? n ? d in tensor and f is a upsampling factor.</p><formula xml:id="formula_3">G(f, d out ) upsamples an n ? n ? d in tensor? into an nf ? nf ? d out tensorx.</formula><p>Hierarchical AugVAE (AugVAE-ML) consists of one E(4, 256) and three E(2, 256), four V Q(8192) with shared weights, and three G(2, 256) and one G(4, 3). As shown in <ref type="figure" target="#fig_1">Figure 2</ref> with blue dotted and connected lines, To reduce the overall codebook size N , we finetune the AugVAE-ML into a single-level AugVAE (AugVAE-SL) of 32 ? 32 latent map. We remove encoders and decoders with 16 ? 16 and 8 ? 8 latent map and replace the concatenation before each decoder with a 1 ? 1 convolution to expand the last channel of previous latent tensor by 2. This modification to AugVAE-ML effectively stabilizes the finetuning process. The final architecture of AugVAE is depicted in <ref type="figure" target="#fig_1">Figure 2</ref> with blue connected lines. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, AugVAEs can compress and reconstruct images with high-fidelity. Implementation details of AugVAE architecture and training hyperparameters are provided in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Bidirectional Auto-Regressive Transformer</head><p>With the masked dot-product multi-head attention, the conventional auto-regressive transformer <ref type="bibr" target="#b2">[3]</ref> can only understand a given sequence from left to right. Bidirectional generation between text and image doesn't require a transformer to be fully-bidirectional: learning how to distinguish an image ? text sequence and a text ? image sequence is enough.</p><p>We just tell our bidirectional auto-regressive transformer (BiART) whether the given text (or image) is a conditional reference For training, we feed the input sequence in text ? image or image ? text order alternately for each iteration. In each iteration, BiART optimizes two negative log-likelihood (NLL) losses: (i) for the conditional reference y indexed as <ref type="bibr">[REF]</ref> and (ii) for the generation target x indexed as <ref type="bibr">[GEN]</ref>. When converges, BiART performs image-to-text (dotted red line in <ref type="figure" target="#fig_1">Figure 2</ref>) and text-to-image (connected red line in <ref type="figure" target="#fig_1">Figure 2</ref>) generations without any finetuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training Details</head><p>Architecture Overview We first train 100-million parameter AugVAE-SL on ImageNet1K <ref type="bibr" target="#b7">[8]</ref>. <ref type="table" target="#tab_5">From results in Figure 3, 5 and Table 1</ref>, our AugVAE-SL shows impressive reconstruction results with both in-domain and out-of-domain images. We use ImageNet1K-trained AugVAE-SL as encoder and decoder of L-Verse and pair encoded tokens with corresponding text tokens. BiART in L-Verse is 500-million parameter GPT <ref type="bibr" target="#b2">[3]</ref> transformer. While DALL-E <ref type="bibr" target="#b31">[32]</ref> and CogView <ref type="bibr" target="#b9">[10]</ref> use a sparse-transformer <ref type="bibr" target="#b3">[4]</ref> with custom attention masks for fast training and sampling, we use a GPTstyle <ref type="bibr" target="#b2">[3]</ref> full-transformer to model the bidirectional crossmodal representation between image and text. We use 64 BPE-encoded <ref type="bibr" target="#b37">[38]</ref> text tokens with 49808 possibilities and 1024 encoded image tokens with 8192 possibilities. More details are provided in Appendix B.</p><p>Mixed Precision Training To save computational cost and sampling time, BiART is trained with FP16(O2) mixed-precision training without inefficient stabilization methods like PB-relaxation <ref type="bibr" target="#b9">[10]</ref> or Sandwich-LayerNorm <ref type="bibr" target="#b9">[10]</ref>. These techniques are designed to eliminate the overflow in forward pass, but computationally inefficient. We instead inference AugVAE in FP32 to prevent the underflow caused by the vector quantizer.</p><p>Ding et al. <ref type="bibr" target="#b9">[10]</ref> states that the precision problem in language-only training is not so significant as in text-toimage training. They hypothesize the heterogeneity of data as a cause. We found that training a transformer in bidirectional manner relieves the heterogeneity between image and text, and leads to stable training. In our toy experiments with smaller parameter sizes, BiART converged faster and showed better performance compared to previous imageto-text or text-to-image auto-regressive transformers. This states that bidirectional training approach with segment embedding is not only useful in the application-level, but also can be a new fundamental to find the cross-modal representation between different data domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Sampling Details</head><p>Image Sampling Similar to Ramesh et al. <ref type="bibr" target="#b31">[32]</ref>, we rerank samples drawn from BiART using a pretrained contrastive model, CLIP <ref type="bibr" target="#b30">[31]</ref>. CLIP assigns a score (clip-score) based on how well the image and text match with each other. For text-to-image generation, we make 64 samples from trained L-Verse model and calculate the clip-score to select a Top 1 image. We repeat this process k times with different random seeds to sample k images in total.</p><p>Text Sampling Our L-Verse auto-regressively generates a sequence of tokens. To generate an RGB image, 1024 (32 ? 32) tokens should be generated one-by-one. However, the length of text may vary depending on its reference image. For this reason, generating full 64 tokens doesn't always guarantee the quality of sampled text. In worst case, the result caption can be just a repeated sequence of same sentence and [PAD] tokens. From the statistics of MS-COCO Captions <ref type="bibr" target="#b23">[24]</ref>, each caption contains average 16 words. We first sample 32 text tokens for each reference image and split the result caption by the full stop (.) token. We only use the first split to calculate the clip-score for reranking. This process dramatically saves computation time to generate 64 samples and select Top 1.</p><p>From machine evaluation metrics in <ref type="table" target="#tab_4">Table 2</ref>, truncated captions from 32 tokens achieve new state-of-the-art in all metrics except CIDEr <ref type="bibr" target="#b46">[46]</ref> among the peers trained only on MS-COCO Captions. L-Verse also shows comparable performance to OSCAR <ref type="bibr" target="#b22">[23]</ref>, which is pretrained on 6.5million image-text pairs. While full 64 token captions score 181.6 in CIDEr and 28.9 in SPICE <ref type="bibr" target="#b0">[1]</ref>, we figured out that scores are high just because each caption has more meaningful words. In our inner-group examination between full and truncated captions, we have agreed that each truncated version is more concise and accurate. We further investigate the quality of L-Verse generated captions with human evaluation, in comparison with human labeled ground-truths.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we demonstrate the performance of proposed L-Verse in every aspect with both quantitative and qualitative experiments. We mainly discuss reconstruction performance on ImageNet1K <ref type="bibr" target="#b7">[8]</ref> and out-of-domain unseen images, image-to-text generation (image captioning) results on MS-COCO Captions <ref type="bibr" target="#b23">[24]</ref>, text-to-image generation results on MS-COCO Captions. For MS-COCO, we trained L-Verse on MS-COCO Captions 2014 Karpathy splits for fair evaluation with previous methods. We also include results of L-Verse trained on Conceptual Captions <ref type="bibr" target="#b38">[39]</ref> to further discuss the scalability of L-Verse architecture for zero-shot text-to-image generation. The FID can change depending on calculation tools. For fair comparison, we compute the Reconstruction FID with torch-fidelity <ref type="bibr" target="#b28">[29]</ref>, caption evaluation metrics in <ref type="table" target="#tab_4">Table 2</ref> with nlg-eval <ref type="bibr" target="#b39">[40]</ref>, and FIDs in <ref type="table" target="#tab_5">Table 3</ref> with the DM-GAN code [53], available at https://github.com/MinfengZhu/ DM-GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Image Reconstruction</head><p>As Esser et al. <ref type="bibr" target="#b11">[12]</ref> stated, the reconstruction Fr?chet Inception Distance (FID) <ref type="bibr" target="#b15">[16]</ref> of a VQ-VAE provide a lower bound on the achievable FID of the generative model trained on it. From the results on ImageNet1K validation set in <ref type="table">Table 1</ref>, our AugVAE-ML trained with novel crosslevel feature augmentation achieves FID of 1.04, meaning AugVAE-ML can compress and reconstruct image without nearly any information loss. Reconstruction examples on <ref type="figure" target="#fig_2">Figure 3</ref> also demonstrates AugVAE-ML's qualitative performance. Finetuned from AugVAE-ML, our AugVAE-SL also achieves new state-of-the-art FID of 3.28 among its single-level peers.</p><p>In a more difficult setting, we evaluate AugVAE-SL on reconstructing out-of-domain unseen images. From the examples in <ref type="figure" target="#fig_5">Figure 5</ref>, AugVAE-SL trained on ImageNet1K shows impressive reconstruction fidelity for all validation input images without extra finetuning. From this result,we believe that our AugVAE-SL can work as a new "imagenetbackbone" for various vision tasks. Detailed examination with more examples for each dataset in <ref type="figure" target="#fig_5">Figure 5</ref> can be found in Appendix C.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Image-to-Text Generation</head><p>We evaluate the image-to-text generation (image captioning) performance of L-Verse with (i) machine evaluation metrics against previous MS-COCO trained state-ofthe-arts and (ii) human evaluation against corresponding ground-truth (reference) captions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Machine Evaluation</head><p>We first compare the performance of our model with MS-COCO trained image captioning models in <ref type="table" target="#tab_4">Table 2</ref>. We also include OSCAR <ref type="bibr" target="#b22">[23]</ref>, which is finetuned from a pretrained model with 6.5-million imagetext pairs, to assess the scalability of our model with larger dataset. With proposed sampling method in Section 3.6, L-Verse surpasses all the other methods in terms of BLEU-4, METEOR, ROUGE, and SPICE without any object detection framework or other extra information. L-Verse also shows comparable performance to OSCAR, showing that pretraining L-Verse on a larger set of image-text pairs is a promising direction for future work.</p><p>Human Evaluation Without caption truncation, L-Verse achieves the highest score in CIDEr and SPICE. As we stated in Section 3.6, machine evaluation metrics don't always guarantee the qualitative performance of generated captions. We further conduct a human evaluation similar to the one used in Li et al. <ref type="bibr" target="#b21">[22]</ref>. We directly evaluate L-Verse generated captions with human-labeled ground-truth captions, which is the theoretical upper-bound of L-Verse GT: A small yellow bird on a small branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L-Verse: A yellow bird sitting on a branch of a tree.</head><p>GT: The man in the business suit takes a video of city buildings. L-Verse: A person walking through the city and taking a picture of buildings.</p><p>GT: Two men playing a game of frisbee on a lush green field. L-Verse: A man is throwing a frisbee in a field.  in image-to-text generation. We randomly sample 500 sets of images, corresponding ground-truth caption (GT), and L-Verse generated caption (Pred) from MS-COCO 2014 minival split for the evaluation pool. 150 anonymous people participated for the evaluation. For each participant, we show randomly sampled 50 sets of image, GT, and Pred from the pool and ask to choose the best caption for each set. To cope with tie situation, we also allow each participant to choose "Both captions well describe the image". We provide more details on human evaluation in Appendix D. Results in <ref type="figure" target="#fig_7">Figure 7</ref> show that L-Verse can generate a detailed explanation of a given image, receiving 30.4% of votes (Pred + Both) in total. Examples in <ref type="figure" target="#fig_6">Figure 6</ref> also demonstrate that L-Verse doesn't miss the detail of each image.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Text-to-Image Generation</head><p>Following Ramesh et al. <ref type="bibr" target="#b31">[32]</ref> and Ding et al. <ref type="bibr" target="#b9">[10]</ref>, we evaluate the text-to-image generation performance of L-Verse by comparing it to prior approaches. We compute FIDs in <ref type="table" target="#tab_5">Table 3</ref> after applying a Gaussian filter with varying radii to both validation images and samples from L-Verse. We use the image sampling process explained in Section 3.6. Generated samples with corresponding captions from MS-COCO are provided in Appendix E.</p><p>According to Ramesh et al. <ref type="bibr" target="#b31">[32]</ref>, training a transformer on tokens from a VQ-VAE encoder disadvantages model since it generates an image in low-frequency domain. Trained on same MS-COCO training set, L-Verse achieves best FID among previous approaches by a large margin with a slight blur of radius 2. The gap tends to increase as the blur radius is increased. We also compare L-Verse-CC, L-Verse trained on Conceptual Captions <ref type="bibr" target="#b38">[39]</ref>, with DALL-E <ref type="bibr" target="#b31">[32]</ref> and CogView <ref type="bibr" target="#b9">[10]</ref>. Considering the size of training data, L-Verse shows comparable text-to-image generation performance to other large-scale transformers as blur radius increases.</p><p>It is interesting that L-Verse shows decreasing FID with increasing blur radius, while other models show increasing FID. We hypothesize that L-Verse focuses on objects in the reference text, showing lower FIDs when high-frequency details are lost. This finding also corresponds with imageto-text generation results in Section 4.2. We also provide initial zero-shot text-to-image generation results with L-Verse in <ref type="figure" target="#fig_9">Figure 8</ref>. Trained on Conceptual Captions <ref type="bibr" target="#b38">[39]</ref>, L-Verse generates detailed images with objects in reference texts. We believe that L-Verse will also be able to generate realistic images in zero-shot fashion when trained with sufficient data and scale. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper presents L-Verse, a novel framework for bidirectional generation between image and text. Our featureaugmented variational autoencoder (AugVAE) achieves new state-of-the-art reconstruction FID and shows its potential as an universal backbone encoder-decoder for generative models. We also enable bidirectional training of auto-regressive transformer with segment embedding. Proposed bidirectional auto-regressive transformer (BiART) learns both image-to-text and text-to-image as a whole. Experimental results demonstrate that our L-Verse framework shows remarkable performance in both image-to-text and text-to-image generation. The AugVAE encoder and decoder are ResNet <ref type="bibr" target="#b13">[14]</ref> with bottleneck-style Resblocks. Our AugVAE is specifically based on the encoder-decoder from official VQGAN <ref type="bibr" target="#b11">[12]</ref> implementation available at https://github. com/CompVis/taming-transformers. From VQ-GAN implementation, we removed the attention block and applied the modification we describe in 3.3. The high-level architecture of our AugVAE is depicted in <ref type="figure" target="#fig_10">Figure 9</ref>. Before we start AugVAE-SL fine-tuning, we change the model architecture by removing 16 ? 16 and 8 ? 8 latent map from AugVAE-ML and replacing concatenation with 1 ? 1 convolution for channel upsampling. Precise details for the architecture are given in files latent-verse/models/vqvae.py and latent-verse/modules/vqvae/vae.py of our source code available at: https://github.com/ tgisaturday/L-Verse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Training</head><p>Our AugVAE is trained on ImageNet1K <ref type="bibr" target="#b7">[8]</ref>. We resize each image into 256 ? 256 ? 3 and apply random crop with 0.75 crop ratio for training. We train both AugVAE-ML and AugVAE-SL using AdamW <ref type="bibr" target="#b25">[26]</ref> optimizer with ? 1 = 0.9, ? 2 = 0.999, ? = 10e ? 8, weight decay multiplier 1e ? 5, and the learning rate 4.5e ? 6 multiplied by the batch size. We half the learning rate each time the training loss appeared to plateau. For the loss term, we use a combination of mean-squared-error (MSE) and LPIPS <ref type="bibr" target="#b52">[52]</ref> losses between the input and the reconstructed image. For stable training, we multiply the LPIPS loss by 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Details for BiART</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Architecture</head><p>Our BiART is similar to the GPT architecture <ref type="bibr" target="#b2">[3]</ref>. We utilize the minGPT implementation of GPT architectures available at https://github.com/karpathy/ minGPT. We only add segment embedding with dimension size 256 for <ref type="bibr">[REF]</ref> and <ref type="bibr">[GEN]</ref>. Each segment embedding is added to the positional encoding of an input token. We use a 32-layer decoder-only transformer with 1024 dimensional states and 16 masked selfattention heads. While BiART uses an integrated embedding matrix for image and text tokens, each token groups are separately indexed from 0 to 8191 and from 8192 to 57999. Special tokens [PAD] (padding), [SOC] (start-oftext), and [SOI] (start-of-image) are indexed from 58000 to 58002.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset FID</head><p>CelebA-HQ <ref type="bibr" target="#b24">[25]</ref> 7.24 FFHQ <ref type="bibr" target="#b18">[19]</ref> 4.92 AFHQ <ref type="bibr" target="#b4">[5]</ref> 4.36 MS-COCO <ref type="bibr" target="#b23">[24]</ref> 4.77 OpenImages V6 <ref type="bibr" target="#b20">[21]</ref> 3.15 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Training</head><p>BiART is trained on MS-COCO Captions <ref type="bibr" target="#b23">[24]</ref> and Conceptual Captions <ref type="bibr" target="#b38">[39]</ref>. We resize each image into 256 ? 256 ? 3 and apply random crop with 0.75 crop ratio for training. We apply BPE dropout <ref type="bibr" target="#b29">[30]</ref> with a rate of 0.1 to our byte-pair encoder. We also apply residual, embedding, and attention dropouts <ref type="bibr" target="#b41">[42]</ref> with a rate of 0.1. We train BiART using AdamW <ref type="bibr" target="#b25">[26]</ref> optimizer with ? 1 = 0.9, ? 2 = 0.95, ? = 1e ? 8, weight decay multiplier 1e ? 2, and the learning rate 4.5e ? 7 multiplied by the batch size. We don't apply weight decay to embedding parameters. We half the learning rate each time the training loss appeared to plateau.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Examples for Image Reconstruction</head><p>We provide more examples of in-domain image reconstruction in <ref type="figure" target="#fig_0">Figure 11</ref> and out-of domain in <ref type="figure" target="#fig_0">Figure 12</ref>. We also provide the reconstruction FID of AugVAE-SL on various datasets in <ref type="table" target="#tab_7">Table 4</ref> as a reference for future works. AugVAE-SL trained on ImageNet1K shows "? 8" FID for all data domain without extra finetuning. The resolution of each reconstructed image is 256 ? 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Examples for Image-to-Text Generation</head><p>We provide an example task interface of our human evaluation we mentioned in Section 4.2 in <ref type="figure" target="#fig_0">Figure 10</ref>. We also provide more examples of image-to-text generation on MS-COCO Captions in <ref type="figure" target="#fig_0">Figure 13</ref>. All examples in <ref type="figure" target="#fig_0">Figure 13</ref> received "Both captions well describe the image" in our human evaluation. The resolution of each input image is 256 ? 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Examples for Text-to-Image Generation</head><p>We provide examples of zero-shot text-to-image generation with L-Verse-CC in <ref type="figure" target="#fig_0">Figure 14</ref>. Captions are randomly sampled from MS-COCO Captions 2017 validation set. The resolution of each generated image is 256 ? 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Discussion</head><p>Bidirectional Learning L-Verse internally learns a reversible and densely connected mapping between images and texts. From this, L-Verse can generate a text or an image in accordance with the given condition without any finetuning or extra object detection framework. Bidirectional learning not only saves time and computational cost for training and application. As we mentioned in Section 3.5, our bidirectional approach also mitigates the heterogeneity of data and enables stable FP16(O2) mixed-precision training.</p><p>Efficiency The bidirectional training enables L-Verse to efficiently learn the vision-language cross-modal representation with smaller dataset and model size. L-Verse requires 97.6% less data (compared to OSCAR <ref type="bibr" target="#b22">[23]</ref>) for imageto-text and 98.8% less data (compared to DALL-E <ref type="bibr" target="#b31">[32]</ref>) for text-to-image generation to achieve comparable performances. L-Verse also has 95% less parameters compared to DALL-E , which makes L-Verse more suitable to the environment with limited computing resources.</p><p>Vision-Language Pre-Training Vision-Language (VL) pre-training from OSCAR surely brings positive effects in learning the cross-modal representation. This also follows the current trend of large scale model training: pre-training with a large data set on a general task and fine-tuning with smaller set to solve downstream tasks. Since we mainly focus on the efficiency over the amount of training data and computing resources, VL pre-training is out-of-scope of this work. However, we also believe that combining VL pretraining with bidirectional training will further improve the performance of L-Verse.</p><p>Large Scale Training With limited amount of training data and computational resources, we couldn't consider training L-Verse in larger scale like OSCAR, DALL-E or CogView <ref type="bibr" target="#b9">[10]</ref>. Nevertheless, our bidirectionally trained L-Verse shows competitive results to other large scale models. As 400M well-filtered text-image dataset <ref type="bibr" target="#b36">[37]</ref> has been released recently, we are optimistic about training L-Verse in larger scales.</p><p>Zero-Shot Image Captioning L-Verse also has an ability to perform zero-shot image captioning when trained on Conceptual Captions (CC) <ref type="bibr" target="#b38">[39]</ref>. Unlike MS-COCO Captions <ref type="bibr" target="#b23">[24]</ref> which is carefully annotated by humans, images and their raw descriptions in CC are harvested from the web. While texts in CC represent a wider variety of styles, its diversity also adds noise to the caption that L-Verse generates. For this reason, we mainly use L-Verse trained with MS-COCO for the experiment on image captioning. Potential Negative Impact Our findings show excellent performance in both image-to-text and text-to-image generation. L-Verse has a wide range of beneficial applications for society, including image captioning, visual question answering, and text visualization. However, there are still potential malicious or unintended uses of L-Verse including image-to-text or text-to-image generation with social bias. To prevent potential negative impact to our society, we provide open access only to AugVAEs for now.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AugVAE-ML</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Examples of L-Verse on zero-shot text-to-image generation (256 ? 256 pixels) on Conceptual Captions (top) and imageto-text generation on MS-COCO Captions (bottom). Trained in bidirectional manner, L-verse can both generate well-conditioned synthetic images and detailed captions without any finetuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Proposed L-Verse framework. [SOC]: Start of Caption (text) token. [SOI]: Start of Image token. Feature-augmented variational autoencoder (AugVAE) in blue. Bidirectional auto-regressive transformer (BiART) in red. AugVAE encoder E encodes an image x into tokens z. Segment embedding indicates each token as a conditional reference (REF), or a generation target (GEN). BiART T either can generate image tokens T (y) from text tokens y or text tokens T (z) from z. AugVAE decoder G decodes z and T (y) into RGB images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>?Figure 3 .</head><label>3</label><figDesc>We use segment embedding to indicate whether each token is given as a conditional reference ([REF]) or a generation target ([GEN]). For example, [REF] is added to each text token and [GEN] is added to each image token for text-to-image generation. Comparison of input images (top), reconstructions from multi-level (hierarchical) feature-augmented variational autoencoder (AugVAE-ML) (middle), and reconstructions from singlelevel feature-augmented variational autoencoder (AugVAE-SL) (bottom) on Imagenet1K validation set. The resolution of each image is 256 ? 256 pixels. Following subsections describe the training and sampling procedure of L-Verse in detail. The overview of L-Verse framework with actual reconstruction and generation examples are shown in Figure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Cross-level patch similarity inside an image (blue) and across images (red). Our feature-augmented variational autoencoder (AugVAE) utilizes the cross-level patch similarity to diversify the feature codebook.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>([REF]) or a generation target ([GEN]). We feed BiART with an extra sequence of segment indexes for each token. A learnable embedding vector is assigned to each segment index ([REF]) and ([GEN]) and added to the input sequence. This simple idea enables the training and sampling of bidirectional image-text generation with BiART.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative evaluation on the reconstruction performance of different VQVAEs with unseen image domains. For all settings, we use ImageNet1K trained models without any finetuning. Images are resized to 256 ? 256 with LANCZOS<ref type="bibr" target="#b5">[6]</ref> filter. Cross-level feature augmentation allows AugVAE-SL to express out-of-domain unseen images in high-fidelity. Please zoom in for the detailed comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Examples of captions generated by L-Verse with corresponding ground-truths. Examples are sampled from conducted human evaluation results which received "Both captions well describe the image".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Human evaluation results on MS-COCO Captions minival split. With question "Which caption well describes the given image?", L-Verse generated captions received 30.4% of votes (Pred + Both) in total.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>ModelFID- 0 FID- 1 FID- 2 FID- 4 FID- 8 AttnGAN [ 48 6 -</head><label>01248486</label><figDesc>] 35.2 44.0 72.0 108.0 100.0 DM-GAN [53] 26.0 39.0 73.0 119.0 112.3 DF-GAN [43] 26.0 33.8 55.9 91.0 FID-k: FID of images blurred by radius k Gaussian filter. * L-Verse trained on Conceptual Captions. ? Models trained on over 30 million image-text pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Examples of zero-shot text-to-image generation. Results are sampled from L-Verse-CC, which is trained on 3-million image-text pairs from Conceptual Captions. The resolution of each image is 256 ? 256 pixels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 .</head><label>9</label><figDesc>Proposed AugVAE. Trained with cross-level feature augmentation, AugVAE-ML is finetuned into AugVAE-SL to reduce the length of encoded image sequence. We remove unnecessary encoders and decoders from AugVAE-ML and replace the concatenation operation with a 1 ? 1 convolution which expands the last dimension of the input tensor by two.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 .</head><label>10</label><figDesc>Example interface for human evaluation. Random sampled 30 examples are shown to each participant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 .Figure 12 .Figure 14 .</head><label>111214</label><figDesc>More examples of input images (top) and reconstructions from AugVAE-SL (bottom) on Imagenet1K validation set. The resolution of each image is 256 ? 256 pixels. More examples of input images (top) and reconstructions from AugVAE-SL (bottom) with unseen image domains (256 ? 256 pixels).some brown kitchen cabinets in a kitchen and a coffee maker a cow is standing in the grass near a fence . a kitchen area with toilet and various cleaning appliances a table filled with different types of vegetables an empty kitchen and living room decorated in white and black a close up of a plate of food on a table with pizza a very clean sink in a bathroom and a towel a few animals standing behind a fence in the grass there is a messy bedroom with a bed, desk and chair a surferboarder is surfing on a small wave a giraffe walking through the trees , brush and large boulders the small bathroom has wooden cabinets around the sink a living room area with a number of couches photograph of the inside of a home with unique decorations a man wearing a hat standing next to a pile of produce commercial white jet airliner sitting on tarmac area the plane is flying high in the sky several people swim in the ocean at a beach a large bathroom with a vanity, mirror, sink and deep tub a pan pizza with pepperonis and a spatula a man is holding an umbrella and walking down the sidewalk a scenic view of a grassland with mountains in the background a hotel room features two large beds with red blankets a young man reaches to hit a tennis ball while others watch a lot of zebras standing in the field on a hot summer day kitchen with silver appliances sun shining in the room a group of kites are being flown in the air a couple of giraffes that are walking in the grass many people are skiing down a hill that is full of snow a man standing in a kitchen while closing a cupboard door Examples of text-to-image generation on MS-COCO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>(4, 256) first downsamples a 256 ? 256 ? 3 RGB image into a 64 ? 64 ? 256 latent feature tensor. Each E(2, 256) downsamples the previous tensor by 2. In total, four latent feature tensors (64 ? 64 ? 256, 32 ? 32 ? 256, 16 ? 16 ? 256, and 8 ? 8 ? 256) are extracted. These four tensors are quantized with a V Q(8192) for each latent map. During the training of AugVAE-ML, each codebook with 8192 values gains diversity via weight sharing. Each G(2, 256) upsamples the concatenation of previous tensor (if exists) and? of each level by 2.G(4, 3)reconstructs the original input from the last latent tensor and the quantized vector.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table /><note>Comparison with state-of-the-arts on MS-COCO Cap- tions Karpathy test split. We mainly compare results with models trained only on MS-COCO. Results from OSCAR (which requires additional fine-tuning) is given as a reference.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table><row><cell>Fr?chet Inception Distance (FID) on a subset of 30,000</cell></row><row><cell>captions sampled from MS-COCO Captions validation set. We</cell></row><row><cell>mainly compare results with models trained only on MS-COCO.</cell></row><row><cell>In the bottom part of the table, we provide results from DALL-</cell></row><row><cell>E, Cogview, and L-Verse-CC (which are trained from much larger</cell></row><row><cell>datasets) as references.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>of the International Conference on Computer Vision, 2015. deep features as a perceptual metric. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018. 12 [53] Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. Dm-gan: Dynamic memory generative adversarial networks for textto-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019.</figDesc><table><row><cell>A. Details for AugVAE</cell></row><row><cell>A.1. Architecture</cell></row><row><cell>2, 6, 8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>Reconstruction Fr?chet Inception Distance (FID) of Aug-VAE on various datasets. For all settings, we use ImageNet1K trained AugVAE-SL without any finetuning on each dataset. Images are resized to 256 ? 256 with LANCZOS<ref type="bibr" target="#b5">[6]</ref> filter.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would first like to thank Yountae Jung and LG AI Research AI Platform Sector for providing technical support and computing resources for training and evaluation. We would also like to thank Jinsu Yoo, Daechul Ahn, Janghyeon Lee, Yeonsik Jo, and other members of LG AI Research Vision Lab for helpful discussions and feedback on the paper. Finally, we would like to thank all members of LG AI Research, without whom this work would not have been possible.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>GT: Skier performing jump on high altitude ski slope. L-Verse: A young person wearing a yellow snow suit skiing down a white hill.</p><p>GT: A man is flying a kite at on the shore at the beach. L-Verse: A person flying a large rainbow kite on a beach.</p><p>GT: A woman holding an umbrella while walking in the rain. L-Verse: A woman is standing under an umbrella in the rain.</p><p>GT: This person is riding a big wave on the water. L-Verse: A surfer is riding a very large wave.</p><p>GT: The man is skiing down the snowy hill. L-Verse: A person in a red ski suit skis down the mountain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT:</head><p>A pizza with lots of cheese on a flat brown surface. L-Verse: The homemade pizza is freshly baked and ready to be eaten.</p><p>GT: A colorful assortment of different fruits and vegetables. L-Verse: A plastic container filled with different types of food.</p><p>GT: A woman tennis player serving a tennis ball.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L-Verse: A woman playing tennis on a tennis court.</head><p>GT: The large living room has been decorated in modern furnishings. L-Verse: A well furnished living room with modern furniture. GT: A group of zebras grazing in field next to a body of water. L-Verse: A group of zebras grazing in field next to a body of water.</p><p>GT: A black woman wearing white attire and shoes running on a court. L-Verse: A woman swinging a teninis racquet towards a tennis ball.</p><p>GT: A black and white picture of people walking in the rain under an umbrella. L-Verse: A group of people with umbrellas standing in front of a bus.</p><p>GT: A blue fire hydrant standing close to a tree. L-Verse: A blue fire hydrant sitting in the middle of a park. a <ref type="figure">Figure 13</ref>. More examples of image-to-text generation on MS-COCO with corresponding ground-truths.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spice: Semantic propositional image caption evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ilya Sutskever, and Dario Amodei. Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stargan v2: Diverse image synthesis for multiple domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaejun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multigrid lanczos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chulwoo</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lehner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EPJ Web of Conferences</title>
		<imprint>
			<biblScope unit="volume">175</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Meshed-memory transformer for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcella</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Stefanini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cogview: Mastering text-toimage generation via transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagebart: Bidirectional context with multinomial diffusion for autoregressive image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image captioning: Transforming objects into words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simao</forename><surname>Herdade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armin</forename><surname>Kappeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kofi</forename><surname>Boakye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Soares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention on attention for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yong</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The open images dataset v4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<idno>2020. 12</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Oscar: Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings</title>
		<meeting>null</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Encoding sentences with graph convolutional networks for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Highfidelity performance metrics for generative models in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Obukhov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Seitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Semen</forename><surname>Zhydenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Kyl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elvis Yu-Jing</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">BPEdropout: Simple and effective subword regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Provilkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Emelianenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Voita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generating diverse high-fidelity images with vq-vae-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mroueh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Jarret Ross, and Vaibhava Goel</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Laion-400m: Open dataset of clip-filtered 400 million image-text pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Vencu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Kaczmarczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clayton</forename><surname>Mullis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aarush</forename><surname>Katta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Coombes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenia</forename><surname>Jitsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aran</forename><surname>Komatsuzaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Relevance of unsupervised metrics in task-oriented dialogue for evaluating natural language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Layla</forename><forename type="middle">El</forename><surname>Asri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremie</forename><surname>Zumer</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rf-net: An end-to-end image matching network based on receptive field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zenglei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglu</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Df-gan: Deep fusion generative adversarial networks for text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songsong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yuan</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingkun</forename><surname>Bao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<title level="m">Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Ramakrishna Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010-001</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Attngan: Finegrained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Auto-encoding scene graphs for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Hierarchy parsing for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Cross-modal contrastive learning for text-toimage generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yu Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">The unreasonable effectiveness of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
