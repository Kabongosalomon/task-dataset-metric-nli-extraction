<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LION: Latent Point Diffusion Models for 3D Shape Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Zeng</surname></persName>
							<email>xzeng@nvidia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
							<email>avahdat@nvidia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Williams</surname></persName>
							<email>fwilliams@nvidia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zan</forename><surname>Gojcic</surname></persName>
							<email>zgojcic@nvidia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Litany</surname></persName>
							<email>olitany@nvidia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
							<email>sfidler@nvidia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
							<email>kkreis@nvidia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LION: Latent Point Diffusion Models for 3D Shape Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Denoising diffusion models (DDMs) have shown promising results in 3D point cloud synthesis. To advance 3D DDMs and make them useful for digital artists, we require (i) high generation quality, (ii) flexibility for manipulation and applications such as conditional synthesis and shape interpolation, and (iii) the ability to output smooth surfaces or meshes. To this end, we introduce the hierarchical Latent Point Diffusion Model (LION) for 3D shape generation. LION is set up as a variational autoencoder (VAE) with a hierarchical latent space that combines a global shape latent representation with a point-structured latent space. For generation, we train two hierarchical DDMs in these latent spaces. The hierarchical VAE approach boosts performance compared to DDMs that operate on point clouds directly, while the point-structured latents are still ideally suited for DDM-based modeling. Experimentally, LION achieves state-of-the-art generation performance on multiple ShapeNet benchmarks. Furthermore, our VAE framework allows us to easily use LION for different relevant tasks: LION excels at multimodal shape denoising and voxel-conditioned synthesis, and it can be adapted for text-and image-driven 3D generation. We also demonstrate shape autoencoding and latent shape interpolation, and we augment LION with modern surface reconstruction techniques to generate smooth 3D meshes. We hope that LION provides a powerful tool for artists working with 3D shapes due to its high-quality generation, flexibility, and surface reconstruction. Project page and code: https://nv-tlabs.github.io/LION.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generative modeling of 3D shapes has extensive applications in 3D content creation and has become an active area of research . However, to be useful as a tool for digital artists, generative models of 3D shapes have to fulfill several criteria: (i) Generated shapes need to be realistic and of highquality without artifacts.</p><p>(ii) The model should enable flexible and interactive use and refinement: For example, a user may want to refine a generated shape and synthesize versions with varying details. Or an artist may provide a coarse or noisy input shape, thereby guiding the model to produce multiple realistic high-quality outputs. Similarly, a user may want to interpolate different shapes. (iii) The model should output smooth meshes, which are the standard representation in most graphics software.</p><p>Existing 3D generative models build on various frameworks, including generative adversarial networks (GANs) , variational autoencoders (VAEs) <ref type="bibr">[24]</ref><ref type="bibr">[25]</ref><ref type="bibr">[26]</ref><ref type="bibr">[27]</ref><ref type="bibr">[28]</ref><ref type="bibr">[29]</ref><ref type="bibr">[30]</ref>, normalizing flows [31-34], autoregressive models <ref type="bibr">[35]</ref><ref type="bibr">[36]</ref><ref type="bibr">[37]</ref><ref type="bibr">[38]</ref>, and more <ref type="bibr">[39]</ref><ref type="bibr">[40]</ref><ref type="bibr">[41]</ref><ref type="bibr">[42]</ref><ref type="bibr">[43]</ref><ref type="bibr">[44]</ref>. Most recently, denoising diffusion models (DDMs) <ref type="figure">Figure 1</ref>: LION is set up as a hierarchical point cloud VAE with denoising diffusion models over the shape latent and latent point distributions. Point-Voxel CNNs (PVCNN) with adaptive Group Normalization (Ada. GN) are used as neural networks. The latent points can be interpreted as a smoothed version of the input point cloud. Shape As Points (SAP) is optionally used for mesh reconstruction.</p><p>have emerged as powerful generative models, achieving outstanding results not only on image synthesis <ref type="bibr" target="#b28">[53]</ref><ref type="bibr" target="#b29">[54]</ref><ref type="bibr" target="#b30">[55]</ref><ref type="bibr" target="#b31">[56]</ref><ref type="bibr" target="#b32">[57]</ref><ref type="bibr" target="#b33">[58]</ref><ref type="bibr" target="#b34">[59]</ref><ref type="bibr" target="#b35">[60]</ref><ref type="bibr" target="#b36">[61]</ref><ref type="bibr" target="#b37">[62]</ref><ref type="bibr" target="#b38">[63]</ref><ref type="bibr" target="#b39">[64]</ref> but also for point cloud-based 3D shape generation <ref type="bibr" target="#b20">[45]</ref><ref type="bibr" target="#b21">[46]</ref><ref type="bibr" target="#b22">[47]</ref>. In DDMs, the data is gradually perturbed by a diffusion process, while a deep neural network is trained to denoise. This network can then be used to synthesize novel data in an iterative fashion when initialized from random noise <ref type="bibr" target="#b28">[53,</ref><ref type="bibr" target="#b40">[65]</ref><ref type="bibr" target="#b41">[66]</ref><ref type="bibr" target="#b42">[67]</ref>. However, existing DDMs for 3D shape synthesis struggle with simultaneously satisfying all criteria discussed above for practically useful 3D generative models.</p><p>Here, we aim to develop a DDM-based generative model of 3D shapes overcoming these limitations. We introduce the Latent Point Diffusion Model (LION) for 3D shape generation (see <ref type="figure">Fig. 1</ref>). Similar to previous 3D DDMs, LION operates on point clouds, but it is constructed as a VAE with DDMs in latent space. LION comprises a hierarchical latent space with a vector-valued global shape latent and another point-structured latent space. The latent representations are predicted with point cloud processing encoders, and two latent DDMs are trained in these latent spaces. Synthesis in LION proceeds by drawing novel latent samples from the hierarchical latent DDMs and decoding back to the original point cloud space. Importantly, we also demonstrate how to augment LION with modern surface reconstruction methods <ref type="bibr" target="#b43">[68]</ref> to synthesize smooth shapes as desired by artists. LION has multiple advantages:</p><p>Expressivity: By mapping point clouds into regularized latent spaces, the DDMs in latent space are effectively tasked with learning a smoothed distribution. This is easier than training on potentially complex point clouds directly <ref type="bibr" target="#b33">[58]</ref>, thereby improving expressivity. However, point clouds are, in principle, an ideal representation for DDMs. Because of that, we use latent points, this is, we keep a point cloud structure for our main latent representation. Augmenting the model with an additional global shape latent variable in a hierarchical manner further boosts expressivity. We validate LION on several popular ShapeNet benchmarks and achieve state-of-the-art synthesis performance.</p><p>Varying Output Types: Extending LION with Shape As Points (SAP) <ref type="bibr" target="#b43">[68]</ref> geometry reconstruction allows us to also output smooth meshes. Fine-tuning SAP on data generated by LION's autoencoder reduces synthesis noise and enables us to generate high-quality geometry. LION combines (latent) point cloud-based modeling, ideal for DDMs, with surface reconstruction, desired by artists.</p><p>Flexibility: Since LION is set up as a VAE, it can be easily adapted for different tasks without retraining the latent DDMs: We can efficiently fine-tune LION's encoders on voxelized or noisy inputs, which a user can provide for guidance. This enables multimodal voxel-guided synthesis and shape denoising. We also leverage LION's latent spaces for shape interpolation and autoencoding. Optionally training the DDMs conditioned on CLIP embeddings enables image-and text-driven 3D generation.</p><p>In summary, we make the following contributions: (i) We introduce LION, a novel generative model for 3D shape synthesis, which operates on point clouds and is built on a hierarchical VAE framework with two latent DDMs. (ii) We validate LION's high synthesis quality by reaching state-of-the-art performance on widely used ShapeNet benchmarks. (iii) We achieve high-quality and diverse 3D shape synthesis with LION even when trained jointly over many classes without conditioning. (iv) We propose to combine LION with SAP-based surface reconstruction. (v) We demonstrate the flexibility of our framework by adapting it to relevant tasks such as multimodal voxel-guided synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Traditionally, DDMs were introduced in a discrete-step fashion: Given samples x 0 ? q(x 0 ) from a data distribution, DDMs use a Markovian fixed forward diffusion process defined as <ref type="bibr" target="#b40">[65,</ref><ref type="bibr" target="#b28">53]</ref>  q(x 1:</p><formula xml:id="formula_0">T |x 0 ) := T t=1 q(x t |x t?1 ), q(x t |x t?1 ) := N (x t ; 1 ? ? t x t?1 , ? t I),<label>(1)</label></formula><p>where T denotes the number of steps and q(x t |x t?1 ) is a Gaussian transition kernel, which gradually adds noise to the input with a variance schedule ? 1 , ..., ? T . The ? t are chosen such that the chain approximately converges to a standard Gaussian distribution after T steps, q(x T )?N (x T ; 0, I).</p><p>DDMs learn a parametrized reverse process (model parameters ?) that inverts the forward diffusion:</p><formula xml:id="formula_1">p ? (x 0:T ) := p(x T ) T t=1 p ? (x t?1 |x t ), p ? (x t?1 |x t ) := N (x t?1 ; ? ? (x t , t), ? 2 t I).<label>(2)</label></formula><p>This generative reverse process is also Markovian with Gaussian transition kernels, which use fixed variances ? 2 t . DDMs can be interpreted as latent variable models, where x 1 , ..., x T are latents, and the forward process q(x 1:T |x 0 ) acts as a fixed approximate posterior, to which the generative p ? (x 0:T ) is fit. DDMs are trained by minimizing the variational upper bound on the negative log-likelihood of the data x 0 under p ? (x 0:T ). Up to irrelevant constant terms, this objective can be expressed as <ref type="bibr" target="#b28">[53]</ref> min ? E t?U {1,T },x 0 ?p(x 0 ), ?N (0,I) w(t)|| ? ? (?tx0 + ?t , t)|| <ref type="bibr" target="#b1">2</ref> 2 , w(t) =</p><formula xml:id="formula_2">? 2 t 2? 2 t (1 ? ?t)(1 ? ? 2 t ) ,<label>(3)</label></formula><p>where ? t = t s=1 (1 ? ? s ) and ? t = 1 ? ? 2 t are the parameters of the tractable diffused distribution after t steps q(x t |x 0 ) = N (x t ; ? t x 0 , ? 2 t I). Furthermore, Eq. (3) employs the widely used parametrization ? ? (x t , t) :</p><formula xml:id="formula_3">= 1 ? 1??t x t ? ?t ? 1?? 2 t ? (x t , t)</formula><p>. It is common practice to set w(t) = 1, instead of the one in Eq. (3), which often promotes perceptual quality of the generated output. In the objective of Eq. (3), the model ? is, for all possible steps t along the diffusion process, effectively trained to predict the noise vector that is necessary to denoise an observed diffused sample x t . After training, the DDM can be sampled with ancestral sampling in an iterative fashion:</p><formula xml:id="formula_4">x t?1 = 1 ? 1??t (x t ? ?t ? 1?? 2 t ? (x t , t)) + ? t ?,<label>(4)</label></formula><p>where ? ? N (?; 0, I). This sampling chain is initialized from a random sample x T ? N (x T ; 0, I). Furthermore, the noise injection in Eq. 4 is usually omitted in the last sampling step.</p><p>DDMs can also be expressed with a continuous-time framework <ref type="bibr" target="#b42">[67,</ref><ref type="bibr" target="#b44">69]</ref>. In this formulation, the diffusion and reverse generative processes are described by differential equations. This approach allows for deterministic sampling and encoding schemes based on ordinary differential equations (ODEs). We make use of this framework in Sec. 3.1 and we review this approach in more detail in App. B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hierarchical Latent Point Diffusion Models</head><p>We first formally introduce LION, then discuss various applications and extensions in Sec. 3.1, and finally recapitulate its unique advantages in Sec. 3.2. See <ref type="figure">Fig. 1</ref> for a visualization of LION.</p><p>We are modeling point clouds x ? R 3?N , consisting of N points with xyz-coordinates in R 3 . LION is set up as a hierarchical VAE with DDMs in latent space. It uses a vector-valued global shape latent z 0 ? R Dz and a point cloud-structured latent h 0 ? R (3+D h )?N . Specifically, h 0 is a latent point cloud consisting of N points with xyz-coordinates in R 3 . In addition, each latent point can carry additional D h latent features. Training of LION is then performed in two stages-first, we train it as a regular VAE with standard Gaussian priors; then, we train the latent DDMs on the latent encodings.</p><p>First Stage Training. Initially, LION is trained by maximizing a modified variational lower bound on the data log-likelihood (ELBO) with respect to the encoder and decoder parameters ? and ? <ref type="bibr" target="#b45">[70,</ref><ref type="bibr" target="#b46">71]</ref>: L ELBO (?, ?) = E p(x),q ? (z0|x),q ? (h0|x,z0) log p ? (x|h 0 , z 0 ) ? ? z D KL (q ? (z 0 |x)|p(z 0 )) ? ? h D KL (q ? (h 0 |x, z 0 )|p(h 0 )) . Here, the global shape latent z 0 is sampled from the posterior distribution q ? (z 0 |x), which is parametrized by factorial Gaussians, whose means and variances are predicted via an encoder network. The point cloud latent h 0 is sampled from a similarly parametrized posterior q ? (h 0 |x, z 0 ), while also conditioning on z 0 (? denotes the parameters of both encoders). Furthermore, p ? (x|h 0 , z 0 ) denotes the decoder, parametrized as a factorial Laplace distribution with predicted means and fixed unit scale parameter (corresponding to an L 1 reconstruction loss). ? z and ? h are hyperparameters balancing reconstruction accuracy and Kullback-Leibler regularization (note that only for ? z = ? h = 1 we are optimizing a rigorous ELBO). The priors p(z 0 ) and p(h 0 ) are N (0, I). Also see <ref type="figure">Fig. 1</ref> again.</p><p>Second Stage Training. In principle, we could use the VAE's priors to sample encodings and generate new shapes. However, the simple Gaussian priors will not accurately match the encoding distribution from the training data and therefore produce poor samples (prior hole problem <ref type="bibr" target="#b33">[58,</ref><ref type="bibr" target="#b47">[72]</ref><ref type="bibr" target="#b48">[73]</ref><ref type="bibr" target="#b49">[74]</ref><ref type="bibr" target="#b50">[75]</ref><ref type="bibr" target="#b51">[76]</ref><ref type="bibr" target="#b52">[77]</ref><ref type="bibr" target="#b53">[78]</ref><ref type="bibr" target="#b54">[79]</ref>). This motivates training highly expressive latent DDMs. In particular, in the second stage we freeze the VAE's encoder and decoder networks and train two latent DDMs on the encodings z 0 and h 0 sampled from q ? (z 0 |x) and q ? (h 0 |x, z 0 ), minimizing score matching (SM) objectives similar to Eq. <ref type="formula" target="#formula_1">(2)</ref>:</p><formula xml:id="formula_6">L SM z (?) = E t?U {1,T },p(x),q ? (z0|x), ?N (0,I) || ? ? (z t , t)|| 2 2 ,<label>(6)</label></formula><formula xml:id="formula_7">L SM h (?) = E t?U {1,T },p(x),q ? (z0|x),q ? (h0|x,z0), ?N (0,I) || ? ? (h t , z 0 , t)|| 2 2 ,<label>(7)</label></formula><p>where z t = ? t z 0 + ? t and h t = ? t h 0 + ? t are the diffused latent encodings. Furthermore, ? denotes the parameters of the global shape latent DDM ? (z t , t), and ? refers to the parameters of the conditional DDM ? (h t , z 0 , t) trained over the latent point cloud (note the conditioning on z 0 ).</p><p>Generation. With the latent DDMs, we can formally define a hierarchical generative model p ?,?,? (x, h 0 , z 0 ) = p ? (x|h 0 , z 0 )p ? (h 0 |z 0 )p ? (z 0 ), where p ? (z 0 ) denotes the distribution of the global shape latent DDM, p ? (h 0 |z 0 ) refers to the DDM modeling the point cloud-structured latents, and p ? (x|h 0 , z 0 ) is LION's decoder. We can hierarchically sample the latent DDMs following Eq. <ref type="bibr" target="#b3">(4)</ref> and then translate the latent points back to the original point cloud space with the decoder.</p><p>Network Architectures and DDM Parametrization. Let us briefly summarize key implementation choices. The encoder networks, as well as the decoder and the latent point DDM, operating on point clouds x, are all implemented based on Point-Voxel CNNs (PVCNNs) <ref type="bibr" target="#b55">[80]</ref>, following Zhou et al. <ref type="bibr" target="#b21">[46]</ref>. PVCNNs efficiently combine the point-based processing of PointNets <ref type="bibr" target="#b56">[81,</ref><ref type="bibr" target="#b57">82]</ref> with the strong spatial inductive bias of convolutions. The DDM modeling the global shape latent uses a ResNet <ref type="bibr" target="#b58">[83]</ref> structure with fully-connected layers (implemented as 1?1-convolutions). All conditionings on the global shape latent are implemented via adaptive Group Normalization <ref type="bibr" target="#b59">[84]</ref> in the PVCNN layers. Furthermore, following Vahdat et al. <ref type="bibr" target="#b33">[58]</ref> we use a mixed score parametrization in both latent DDMs. This means that the score models are parametrized to predict a residual correction to an analytic standard Gaussian score. This is beneficial since the latent encodings are regularized towards a standard Gaussian distribution during the first training stage (see App. D for all details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Applications and Extensions</head><p>Here, we discuss how LION can be used and extended for different relevant applications.</p><p>Multimodal Generation. We can synthesize different variations of a given shape, enabling multimodal generation in a controlled manner: Given a shape, i.e., its point cloud x, we encode it into latent space. Then, we diffuse its encodings z 0 and h 0 for a small number of steps ? &lt; T towards intermediate z ? and h ? along the diffusion process such that only local details are destroyed. Running the reverse generation process from this intermediate ? , starting at z ? and h ? , leads to variations of the original shape with different details (see, for instance, <ref type="figure">Fig. 2</ref>). We refer to this procedure as diffuse-denoise (details in App. C.1). Similar techniques have been used for image editing <ref type="bibr" target="#b60">[85]</ref>.</p><p>Encoder Fine-tuning for Voxel-Conditioned Synthesis and Denoising. In practice, an artist using a 3D generative model may have a rough idea of the desired shape. For instance, they may be able to quickly construct a coarse voxelized shape, to which the generative model then adds realistic details. In LION, we can support such applications: using a similar ELBO as in Eq. (5), but with a frozen decoder, we can fine-tune LION's encoder networks to take voxelized shapes as input (we simply place points at the voxelized shape's surface) and map them to the corresponding latent encodings z 0 and h 0 that reconstruct the original non-voxelized point cloud. Now, a user can utilize the fine-tuned encoders to encode voxelized shapes and generate plausible detailed shapes. Importantly, this can be naturally combined with the diffuse-denoise procedure to clean up imperfect encodings and to generate different possible detailed shapes (see <ref type="figure">Fig. 4</ref>).</p><p>Furthermore, this approach is general. Instead of voxel-conditioned synthesis, we can also fine-tune the encoder networks on noisy shapes to perform multimodal shape denoising, also potentially combined with diffuse-denoise. LION supports these applications easily without re-training the latent DDMs due to its VAE framework with additional encoders and decoders, in contrast to previous works that train DDMs on point clouds directly <ref type="bibr" target="#b21">[46,</ref><ref type="bibr" target="#b22">47]</ref>. See App. C.2 for technical details.</p><p>Shape Interpolation. LION also enables shape interpolation: We can encode different point clouds into LION's hierarchical latent space and use the probability flow ODE (see App. B) to further encode into the latent DDMs' Gaussian priors, where we can safely perform spherical interpolation and expect valid shapes along the interpolation path. We can use the intermediate encodings to generate the interpolated shapes (see <ref type="figure" target="#fig_3">Fig. 7</ref>; details in App. C.3). Surface Reconstruction. While point clouds are an ideal 3D representation for DDMs, artists may prefer meshed outputs. Hence, we propose to combine LION with modern geometry reconstruction methods (see Figs. 2, 4 and 5). We use Shape As Points (SAP) <ref type="bibr" target="#b43">[68]</ref>, which is based on differentiable Poisson surface reconstruction and can be trained to extract smooth meshes from noisy point clouds. Moreover, we fine-tune SAP on training data generated by LION's autoencoder to better adjust SAP to the noise distribution in point clouds generated by LION. Specifically, we take clean shapes, encode them into latent space, run a few steps of diffuse-denoise that only slightly modify some details, and decode back. The diffuse-denoise in latent space results in noise in the generated point clouds similar to what is observed during unconditional synthesis (details in App. C.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">LION's Advantages</head><p>We now recapitulate LION's unique advantages. LION's structure as a hierarchical VAE with latent DDMs is inspired by latent DDMs on images <ref type="bibr" target="#b32">[57,</ref><ref type="bibr" target="#b33">58,</ref><ref type="bibr" target="#b52">77]</ref>. This framework has key benefits:</p><p>(i) Expressivity: First training a VAE that regularizes the latent encodings to approximately fall under standard Gaussian distributions, which are also the DDMs' equilibrium distributions towards which the diffusion processes converge, results in an easier modeling task for the DDMs: They have to model only the remaining mismatch between the actual encoding distributions and their own Gaussian priors <ref type="bibr" target="#b33">[58]</ref>. This translates into improved expressivity, which is further enhanced by the additional decoder network. However, point clouds are, in principle, an ideal representation for the DDM framework, because they can be diffused and denoised easily and powerful point cloud processing architectures exist. Therefore, LION uses point cloud latents that combine the advantages of both latent DDMs and 3D point clouds. Our point cloud latents can be interpreted as smoothed versions of the original point clouds that are easier to model (see <ref type="figure">Fig. 1</ref>). Moreover, the hierarchical VAE setup with an additional global shape latent increases LION's expressivity even further and results in natural disentanglement between overall shape and local details captured by the shape latents and latent points (Sec. 5.2).</p><p>(ii) Flexibility: Another advantage of LION's VAE framework is that its encoders can be fine-tuned for various relevant tasks, as discussed previously, and it also enables easy shape interpolation. Other 3D point cloud DDMs operating on point clouds directly <ref type="bibr" target="#b22">[47,</ref><ref type="bibr" target="#b21">46]</ref> do not offer simultaneously as much flexibility and expressivity out-of-the-box (see quantitative comparisons in Secs. 5.1 and 5.4).  (iii) Mesh Reconstruction: As discussed, while point clouds are ideal for DDMs, artists likely prefer meshed outputs. As explained above, we propose to use LION together with modern surface reconstruction techniques <ref type="bibr" target="#b43">[68]</ref>, again combining the best of both worlds-a point cloud-based VAE backbone ideal for DDMs, and smooth geometry reconstruction methods operating on the synthesized point clouds to generate practically useful smooth surfaces, which can be easily transformed into meshes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>We are building on DDMs <ref type="bibr" target="#b28">[53,</ref><ref type="bibr" target="#b40">[65]</ref><ref type="bibr" target="#b41">[66]</ref><ref type="bibr" target="#b42">[67]</ref>, which have been used most prominently for image <ref type="bibr" target="#b28">[53]</ref><ref type="bibr" target="#b29">[54]</ref><ref type="bibr" target="#b30">[55]</ref><ref type="bibr" target="#b31">[56]</ref><ref type="bibr" target="#b32">[57]</ref><ref type="bibr" target="#b33">[58]</ref><ref type="bibr" target="#b34">[59]</ref><ref type="bibr" target="#b35">[60]</ref><ref type="bibr" target="#b36">[61]</ref><ref type="bibr" target="#b37">[62]</ref><ref type="bibr" target="#b38">[63]</ref> and speech synthesis <ref type="bibr" target="#b61">[86]</ref><ref type="bibr" target="#b62">[87]</ref><ref type="bibr" target="#b63">[88]</ref><ref type="bibr" target="#b64">[89]</ref><ref type="bibr" target="#b65">[90]</ref><ref type="bibr" target="#b66">[91]</ref>. We train DDMs in latent space, an idea that has been explored for image <ref type="bibr" target="#b32">[57,</ref><ref type="bibr" target="#b33">58,</ref><ref type="bibr" target="#b52">77]</ref> and music <ref type="bibr" target="#b67">[92]</ref> generation, too. However, these works did not train separate conditional DDMs. Hierarchical DDM training has been used for generative image upsampling <ref type="bibr" target="#b29">[54]</ref>, text-to-image generation <ref type="bibr" target="#b38">[63,</ref><ref type="bibr" target="#b39">64]</ref>, and semantic image modeling <ref type="bibr" target="#b35">[60]</ref>. Most relevant among these works is Preechakul et al. <ref type="bibr" target="#b35">[60]</ref>, which extracts a high-level semantic representation of an image with an auxiliary encoder and then trains a DDM that adds details directly in image space. We are the first to explore related concepts for 3D shape synthesis and we also train both DDMs in latent space. Furthermore, DDMs and VAEs have also been combined in such a way that the DDM improves the output of the VAE <ref type="bibr" target="#b68">[93]</ref>.</p><p>Most related to LION are "Point-Voxel Diffusion" (PVD) <ref type="bibr" target="#b21">[46]</ref> and "Diffusion Probabilistic Models for 3D Point Cloud Generation" (DPM) <ref type="bibr" target="#b22">[47]</ref>. PVD trains a DDM directly on point clouds, and our decision to use PVCNNs is inspired by this work. DPM, like LION, uses a shape latent variable, but models its distribution with Normalizing Flows <ref type="bibr" target="#b69">[94,</ref><ref type="bibr" target="#b70">95]</ref>, and then trains a weaker point-wise conditional DDM directly on the point cloud data (this allows DPM to learn useful representations in its latent variable, but sacrifices generation quality). As we show below, neither PVD nor DPM easily enables applications such as multimodal voxel-conditioned synthesis and denoising. Furthermore, LION achieves significantly stronger generation performance. Finally, neither PVD nor DPM reconstructs meshes from the generated point clouds. Point cloud and 3D shape generation have also been explored with other generative models: PointFlow [31], DPF-Net [33] and SoftFlow [32] rely on Normalizing Flows <ref type="bibr" target="#b69">[94]</ref><ref type="bibr" target="#b70">[95]</ref><ref type="bibr" target="#b71">[96]</ref><ref type="bibr" target="#b72">[97]</ref>. SetVAE [29] treats point cloud synthesis as set generation and uses VAEs. ShapeGF <ref type="bibr" target="#b20">[45]</ref> learns distributions over gradient fields that model shape surfaces. Both IM-GAN <ref type="bibr" target="#b6">[7]</ref>, which models shapes as neural fields, and l-GAN <ref type="bibr" target="#b1">[2]</ref> train GANs over latent variables that encode the shapes, similar to other works <ref type="bibr" target="#b2">[3]</ref>, while r-GAN <ref type="bibr" target="#b1">[2]</ref> generates point clouds directly. PDGN <ref type="bibr" target="#b27">[52]</ref> proposes progressive deconvolutional networks within a point cloud GAN. SP-GAN <ref type="bibr" target="#b18">[19]</ref> uses a spherical point cloud prior. Other progressive <ref type="bibr">[22,</ref><ref type="bibr">37]</ref> and graph-based architectures <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref> have been used, too. Also generative cellular automata (GCAs) can be employed for voxel-based 3D shape generation <ref type="bibr">[43]</ref>. In orthogonal work, point cloud DDMs have been used for generative shape completion <ref type="bibr" target="#b21">[46,</ref><ref type="bibr" target="#b73">98]</ref>.</p><p>Recently, image-driven <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr">44]</ref> training of 3D generative models as well as text-driven 3D generation [34, <ref type="bibr" target="#b24">[49]</ref><ref type="bibr" target="#b25">[50]</ref><ref type="bibr" target="#b26">[51]</ref> have received much attention. These are complementary directions to ours; in fact, augmenting LION with additional image-based training or including text-guidance are promising future directions. Finally, we are relying on SAP <ref type="bibr" target="#b43">[68]</ref> for mesh generation. Strong alternative approaches for reconstructing smooth surfaces from point clouds exist <ref type="bibr" target="#b74">[99]</ref><ref type="bibr" target="#b75">[100]</ref><ref type="bibr" target="#b76">[101]</ref><ref type="bibr" target="#b77">[102]</ref><ref type="bibr" target="#b78">[103]</ref>.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We provide an overview of our most interesting experimental results in the main paper. All experiment details and extensive additional experiments can be found in App. E and App. F, respectively. Datasets. To compare LION against existing methods, we use ShapeNet <ref type="bibr" target="#b79">[104]</ref>, the most widely used dataset to benchmark 3D shape generative models. Following previous works [31, <ref type="bibr" target="#b21">46,</ref><ref type="bibr" target="#b22">47]</ref>, we train on three categories: airplane, chair, car. Also like previous methods, we primarily rely on PointFlow's [31] dataset splits and preprocssing. It normalizes the data globally across the whole dataset. However, some baselines require per-shape normalization <ref type="bibr" target="#b18">[19,</ref><ref type="bibr">43,</ref><ref type="bibr" target="#b20">45,</ref><ref type="bibr" target="#b27">52]</ref>; hence, we also train on such data. Furthermore, training SAP requires signed distance fields (SDFs) for volumetric supervision, which the PointFlow data does not offer. Hence, for simplicity we follow Peng et al. <ref type="bibr" target="#b43">[68,</ref><ref type="bibr" target="#b76">101]</ref> and also use their data splits and preprocessing, which includes SDFs.We train LION, DPM, PVD, and IM-GAN (which synthesizes shapes as SDFs) also on this dataset version (denoted as ShapeNet-vol here). This data is also per-shape normalized. Dataset details in App. E.1.   Mesh Reconstruction. As explained in Sec. 3.1, we combine LION with mesh reconstruction, to directly synthesize practically useful meshes. We show generated meshes in <ref type="figure">Fig. 2</ref>, which look smooth and of high quality. In <ref type="figure">Fig. 2</ref>, we also visually demonstrate how we can vary the local details of synthesized shapes while preserving the overall shape with our diffuse-denoise technique (Sec. 3.1). Details about the number of diffusion steps for all diffuse-denoise experiments are in App. E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Single-Class 3D Shape Generation</head><p>Shape Interpolation. As discussed in Sec. 3.1, LION also enables shape interpolation, potentially useful for shape editing applications. We show this in <ref type="figure" target="#fig_3">Fig. 7</ref>, combined with mesh reconstruction. The generated shapes are clean and semantically plausible along the entire interpolation path. In App. F.12.1, we also show interpolations from PVD <ref type="bibr" target="#b21">[46]</ref> and DPM <ref type="bibr" target="#b22">[47]</ref> for comparison. . Training a single model without conditioning over such diverse shapes is challenging, as the data distribution is highly complex and multimodal. We show LION's generated samples in <ref type="figure" target="#fig_0">Fig. 3</ref>, including meshes: LION synthesizes high-quality and diverse plausible shapes even when trained on such complex data. We report the model's quantitative generation performance in Tab. 4, and we also trained various strong baseline methods under the same setting for comparison. We find that LION significantly outperforms all baselines by a large margin. We further observe that the hierarchical VAE architecture of LION becomes crucial: The shape latent variable z 0 captures global shape, while the latent points h 0 model details. This can be seen in <ref type="figure" target="#fig_4">Fig. 8</ref>: we show samples when fixing the global shape latent z 0 and only sample h 0 (details in App. F.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Many-class</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>55-Class LION Model.</head><p>Encouraged by these results, we also trained a LION model again jointly without any class conditioning on all 55 different categories from ShapeNet. Note that we did on purpose not use class-conditioning in these experiments to create a difficult 3D generation task and thereby explore LION's scalability to highly complex and multimodal datasets. We show generated point cloud samples in <ref type="figure" target="#fig_5">Fig. 9</ref> (we did not train an SAP model on the 55 classes data): LION synthesizes high-quality and diverse shapes. It can even generate samples from the cap class, which contributes with only 39 training data samples, indicating that LION has an excellent mode coverage that even includes the very rare classes. To the best of our knowledge no previous 3D shape generative models have demonstrated satisfactory generation performance for such diverse and multimodal 3D data without relying on conditioning information (details in App. F.4). In conclusion, we observe that LION out-of-the-box easily scales to highly complex multi-category shape generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Training LION on Small Datasets</head><p>Next, we explore whether LION can also be trained successfully on very small datasets. To this end, we train models on the Mug and Bottle ShapeNet classes. The number of training samples</p><formula xml:id="formula_8">DPM-0 PVD-0 LION-0 (ours)</formula><p>Input Voxels DPM-50 PVD-50 LION-50 (ours) <ref type="figure">Figure 11</ref>: Voxel-guided synthesis. We show different methods with 0 and 50 steps of diffuse-denoise. Voxelizations of generated points are also shown: Yellow boxes indicate generated points correctly fill input voxels, green boxes indicate voxels should be filled but are left empty, red boxes indicate extra voxels.  <ref type="figure">Figure 12</ref>: Reconstruction metrics with respect to clean inputs for airplane category (lower is better) when guiding synthesis with voxelized or noisy inputs (using uniform, outlier, and normal noise, see App. F.7).</p><p>x-axes denote number of diffuse-denoise steps.  <ref type="figure" target="#fig_0">Figure 13</ref>: Voxel-guided generation. Quality metrics for output points (lower is better) and voxel IOU with respect to input (higher is better). xaxes denote diffuse-denoise steps.</p><p>is 149 and 340, respectively, which is much smaller than the common classes like chair, car and airplane. Furthermore, we also train LION on 553 animal assets from the TurboSquid data repository. Generated shapes from the three models are shown in <ref type="figure" target="#fig_6">Fig. 10</ref>. LION is able to generate correct mugs and bottles as well as diverse and high-quality animal shapes. We conclude that LION also performs well even when training in the challenging low-data setting (details in Apps. F.5 and F.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Voxel-guided Shape Synthesis and Denoising with Fine-tuned Encoders</head><p>Next, we test our strategy for multimodal voxel-guided shape synthesis (see Sec. 3.1) using the airplane class LION model (experiment details in App. E, more experiments in App. F.7). We first voxelize our training set and fine-tune our encoder networks to produce the correct encodings to decode back the original shapes. When processing voxelized shapes with our point-cloud networks, we sample points on the surface of the voxels. As discussed, we can use different numbers of diffusedenoise steps in latent space to generate various plausible shapes and correct for poor encodings. Instead of voxelizations, we can also consider different noisy inputs (we use normal, uniform, and outlier noise, see App. F.7) and achieve multimodal denoising with the same approach. The same tasks can be attempted with the important DDM-based baselines PVD and DPM, by directly-not in a latent space-diffusing and denoising voxelized (converted to point clouds) or noisy point clouds. <ref type="figure">Fig. 12</ref> shows the reconstruction performance of LION, DPM and PVD for different numbers of diffuse-denoise steps (we voxelized or noised the validation set to measure this). We see that for almost all inputs-voxelized or different noises-LION performs best. PVD and DPM perform acceptably for normal and uniform noise, which is similar to the noise injected during training of their DDMs, but perform very poorly for outlier noise or voxel inputs, which is the most relevant case to us, because voxels can be easily placed by users. It is LION's unique framework with additional fine-tuned encoders in its VAE and only latent DDMs that makes this possible. Performing more diffuse-denoise steps means that more independent, novel shapes are generated. These will be cleaner and of higher quality, but also correspond less to the noisy or voxel inputs used for guidance. In <ref type="figure" target="#fig_0">Fig. 13</ref>, we show this trade-off for the voxel-guidance experiment (other experiments in App. F.7), where (top) we measured the outputs' synthesis quality by calculating 1-NNA with respect to the validation set, and (bottom) the average intersection over union (IOU) between the input voxels and the voxelized outputs. We gener-  <ref type="figure">Figure 14</ref>: We apply Text2Mesh <ref type="bibr" target="#b24">[49]</ref> on meshes generated by LION. In Text2Mesh, textures are generated and meshes refined such that rendered images of the 3D objects are aligned with user-provided text prompts <ref type="bibr" target="#b80">[105]</ref>. ally see a trade-off: More diffuse-denoise steps result in lower 1-NNA (better quality), but also lower IOU. LION strikes the best balance by a large gap: Its additional encoder network directly generates plausible latent encodings from the perturbed inputs that are both high quality and also correspond well to the input. This trade-off is visualized in <ref type="figure">Fig. 11</ref> for LION, DPM, and PVD, where we show generated point clouds and voxelizations (note that performing no diffuse-denoise at all for PVD and DPM corresponds to simply keeping the input, as these models' DDMs operate directly on point clouds). We see that running 50 diffuse-denoise steps to generate diverse outputs for DPM and especially PVD results in a significant violation of the input voxelization. In contrast, LION generates realistic outputs that also obey the driving voxels. Overall, LION wins out both in this task and also in unconditional generation with large gaps over these previous DDM-based point cloud generative models. We conclude that LION does not only offer state-of-the-art 3D shape generation quality, but is also very versatile. Note that guided synthesis can also be combined with mesh reconstruction, as shown in <ref type="figure">Fig. 4</ref>. While our main experiments use 1,000-step DDPM-based synthesis, which takes ? 27.12 seconds, we can significantly accelerate generation without significant loss in quality. Using DDIM-based sampling <ref type="bibr" target="#b81">[106]</ref>, we can generate high quality shapes in under one second ( <ref type="figure" target="#fig_1">Fig. 15</ref>), which would enable real-time interactive applications. More analyses in App. F.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Sampling Time</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.6</head><p>Overview of Additional Experiments in Appendix (i) In App. F.1, we perform various ablation studies. The experiments quantitatively validate LION's architecture choices and the advantage of our hierarchical VAE setup with conditional latent DDMs. (ii) In App. F.8, we measure LION's autoencoding performance. (iii) To demonstrate the value of directly outputting meshes, in App. F.10 we use Text2Mesh <ref type="bibr" target="#b24">[49]</ref> to generate textures based on text prompts for synthesized LION samples ( <ref type="figure">Fig. 14)</ref>. This would not be possible, if we only generated point clouds. (iv) To qualitatively show that LION can be adapted easily to other relevant tasks, in App. F.11 we condition LION on CLIP embeddings of the shapes' rendered images, following CLIP-Forge [34] ( <ref type="figure" target="#fig_10">Fig. 16</ref>). This enables text-driven 3D shape generation and single view 3D reconstruction ( <ref type="figure" target="#fig_3">Fig. 17</ref>). (v) We also show many more samples (Apps. F.2-F.6) and shape interpolations (App. F.12) from our models, more examples of voxel-guided and noise-guided synthesis (App. F.7), and we further analyze our 13-class LION model (App. F.3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We introduced LION, a novel generative model of 3D shapes. LION uses a VAE framework with hierarchical DDMs in latent space and can be combined with SAP for mesh generation. LION achieves state-of-the-art shape generation performance and enables applications such as voxel-conditioned synthesis, multimodal shape denoising, and shape interpolation. LION is currently trained on 3D point clouds only and can not directly generate textured shapes. A promising extension would be to include image-based training by incorporating neural or differentiable rendering <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b82">[107]</ref><ref type="bibr" target="#b83">[108]</ref><ref type="bibr" target="#b84">[109]</ref><ref type="bibr" target="#b85">[110]</ref><ref type="bibr" target="#b86">[111]</ref> and to also synthesize textures <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b87">[112]</ref><ref type="bibr" target="#b88">[113]</ref><ref type="bibr" target="#b89">[114]</ref>. Furthermore, LION currently focuses on single object generation only. It would be interesting to extend it to full 3D scene synthesis. Moreover, synthesis could be further accelerated by building on works on accelerated sampling from DDMs <ref type="bibr" target="#b36">[61,</ref><ref type="bibr" target="#b37">62,</ref><ref type="bibr" target="#b42">67,</ref><ref type="bibr" target="#b81">106,</ref><ref type="bibr" target="#b90">[115]</ref><ref type="bibr" target="#b91">[116]</ref><ref type="bibr" target="#b92">[117]</ref><ref type="bibr" target="#b93">[118]</ref><ref type="bibr" target="#b94">[119]</ref><ref type="bibr" target="#b95">[120]</ref><ref type="bibr" target="#b96">[121]</ref>.</p><p>Broader Impact. We believe that LION can potentially improve 3D content creation and assist the workflow of digital artists. We designed LION with such applications in mind and hope that it can grow into a practical tool enhancing artists' creativity. Although we do not see any immediate negative use-cases for LION, it is important that practitioners apply an abundance of caution to mitigate impacts given generative modeling more generally can also be used for malicious purposes, discussed for instance in Vaccari and Chadwick <ref type="bibr" target="#b97">[122]</ref>, Nguyen et al. <ref type="bibr" target="#b98">[123]</ref>, Mirsky and Lee <ref type="bibr" target="#b99">[124]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Funding Disclosure</head><p>This work was fully funded by NVIDIA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Continuous-Time Diffusion Models and Probability Flow ODE Sampling</head><p>Here, we are providing additional background on denoising diffusion models (DDMs). In Sec. 2, we have introduced DDMs in the "discrete-time" setting, where we have a fixed number T of diffusion and denoising steps <ref type="bibr" target="#b28">[53,</ref><ref type="bibr" target="#b40">65]</ref>. However, DDMs can also be expressed in a continuous-time framework, in which the fixed forward diffusion and the generative denoising process in a continuous manner gradually perturb and denoise, respectively <ref type="bibr" target="#b42">[67]</ref>. In this formulation, these processes can be described by stochastic differential equations (SDEs). In particular, the fixed forward diffusion process is given by (for the "variance-preserving" SDE <ref type="bibr" target="#b42">[67]</ref>, which we use. Other diffusion processes are possible <ref type="bibr" target="#b42">[67,</ref><ref type="bibr" target="#b33">58,</ref><ref type="bibr" target="#b36">61]</ref>):</p><formula xml:id="formula_9">dx t = ? 1 2 ? t x t dt + ? t dw t ,<label>(8)</label></formula><p>where time t ? [0, 1] and w t is a standard Wiener process. In the continuous-time formulation, we usually consider times t ? [0, 1], while in the discrete-time setting it is common to consider discrete time values t ? {0, ..., T } (with t = 0 corresponding to no diffusion at all). This is just a convention and we can easily translate between them as t cont. = tdisc. T . We always take care of these conversions here when appropriate without explicitly noting this to keep the notation concise. The function ? t in Eq. (8) above is a continuous-time generalization of the set of ? t 's used in the discrete formulation (denoted as variance schedule in Sec. 2). Usually, the ? t 's in the discrete-time setting are generated by discretizing an underlying continuous function ? t -in our case ? t is simply a linear function of t-, which is now used in Eq. (8) above directly.</p><p>It can be shown that a corresponding reverse diffusion process exists that effectively inverts the forward diffusion from Eq. (8) <ref type="bibr" target="#b42">[67,</ref><ref type="bibr" target="#b100">125,</ref><ref type="bibr" target="#b101">126]</ref>:</p><formula xml:id="formula_10">dx t = ? 1 2 ? t [x t + 2? xt log q t (x t )] dt + ? t dw t .<label>(9)</label></formula><p>Here, q t (x t ) is the marginal diffused data distribution after time t, and ? xt log q t (x t ) is the score function. Hence, if we had access to this score function, we could simulate this reverse SDE in reverse time direction, starting from random noise x 1 ? N (x 1 ; 0, I), and thereby invert the forward diffusion process and generate novel data. Consequently, the problem reduces to learning a model for the usually intractable score function. This is where the discrete-time and continuous-time frameworks connect: Indeed, the objective in Eq. <ref type="formula" target="#formula_2">(3)</ref> for training the denoising model also corresponds to denoising score matching <ref type="bibr" target="#b102">[127,</ref><ref type="bibr" target="#b41">66,</ref><ref type="bibr" target="#b28">53]</ref>, i.e., it represents an objective to learn a model for the score function. We have</p><formula xml:id="formula_11">? xt log q t (x t ) ? ? ? (x t , t) ? t .<label>(10)</label></formula><p>However, we trained ? (x t , t) for T discrete steps only, rather than for continuous times t. In principle, the objective in Eq. <ref type="formula" target="#formula_2">(3)</ref> can be easily adapted to the continuous-time setting by simply sampling continuous time values rather than discrete ones. In practice, T = 1000 steps, as used in our models, represents a fine discretization of the full integration interval and the model generalizes well when queried at continuous t "between" steps, due to the smooth cosine-based time step embeddings.</p><p>A unique advantage of the continuous-time framework based on differential equations is that it allows us to construct an ordinary differential equation (ODE), which, when simulated with samples from the same random noise distribution x 1 ? N (x 1 ; 0, I) as inputs (where t = 1, with x t=1 , denotes the end of the diffusion for continuous t ? [0, 1]), leads to the same marginal distributions along the reverse diffusion process and can therefore also be used for synthesis <ref type="bibr" target="#b42">[67]</ref>:</p><formula xml:id="formula_12">dx t = ? 1 2 ? t [x t + ? xt log p t (x t )] dt.<label>(11)</label></formula><p>This is an instance of continuous Normalizing flows <ref type="bibr" target="#b71">[96,</ref><ref type="bibr" target="#b72">97]</ref> and often called probability flow ODE. Plugging in our score function estimate, we have</p><formula xml:id="formula_13">dx t = ? 1 2 ? t x t ? ? (x t , t) ? t dt,<label>(12)</label></formula><p>which we refer to as the generative ODE. Given a sample from x 1 ? N (x 1 ; 0, I), the generative process of this generative ODE is fully deterministic. Similarly, we can also use this ODE to encode given data into the DDM's own prior distribution x 1 ? N (x 1 ; 0, I) by simulating the ODE in the other direction.</p><p>These properties allow us to perform interpolation: Due to the deterministic generation process with the generative ODE, smoothly changing an encoding x 1 will result in a similarly smoothly changing generated output x 0 . We are using this for our interpolation experiments (see Sec. 3.1 and App. C.3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Technical Details on LION's Applications and Extensions</head><p>In this section, we provide additional methodological details on the different applications and extensions of LION that we discussed in Sec. 3.1 and demonstrated in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Diffuse-Denoise</head><p>Our diffuse-denoise technique is essentially a tool to inject diversity into the generation process in a controlled manner and to "clean up" imperfect encodings when working with encoders operating on noisy or voxelized data (see Sec. 3.1 and App. C.2). It is related to similar methods that have been used for image editing <ref type="bibr" target="#b60">[85]</ref>.</p><p>Specifically, assume we are given an input shape x in the form of a point cloud. We can now use LION's encoder networks to encode it into the latent spaces of LION's autoencoder and obtain the shape latent encoding z 0 and the latent points h 0 . Now, we can diffuse those encodings for ? &lt; T steps (using the Gaussian transition kernel defined in Eq. <ref type="formula" target="#formula_0">(1)</ref>) to obtain intermediate z ? and h ? along the diffusion process. Next, we can denoise them back to newz 0 andh 0 using the generative stochastic sampling defined in Eq. (4), starting from the intermediate z ? and h ? . Note that we first need to generate the newz 0 , since denoising h ? is conditioned onz 0 according to LION's hierarchical latent DDM setup.</p><p>The forward diffusion of DDMs progressively destroys more and more details of the input data. Hence, diffusing LION's latent encodings only for small ? , and then denoising again, results in new z 0 andh 0 that have only changed slightly compared to the original z 0 and h 0 . In other words for small ? , the diffuse-denoisedz 0 andh 0 will be close to the original z 0 and h 0 . This observation was also made by Meng et al. <ref type="bibr" target="#b60">[85]</ref>. Similarly, we find that whenz 0 andh 0 are sent through LION's decoder network the corresponding point cloudx resembles the input point cloud x in overall shape well, and only has different details. Diffusing for more steps, i.e., larger ? , corresponds to resampling the shape also more globally (with ? = T meaning that an entirely new shape is generated), while using smaller ? implies that the original shape is preserved more faithfully (with ? = 0 meaning that the original shape is preserved entirely). Hence, we can use this technique to inject diversity into any given shape and resample different details in a controlled manner (as shown, for instance, in <ref type="figure">Fig. 2</ref>).</p><p>We can use this diffuse-denoise approach not only for resampling different details from clean shapes, but also to "clean up" poor encodings. For instance, when LION's encoders operate on very noisy or coarsely voxelized input point clouds (see Sec. 3.1 and App. C.2), the predicted shape encodings may be poor. The encoder networks may roughly recognize the overall shape but not capture any details due to the noise or voxelizations. Hence, we can perform some diffuse-denoise to essentially partially discard the poor encodings and regenerate them from the DDMs, which have learnt a model of clean detailed shapes, while preserving the overall shape. This allows us to perform multimodal generation when using voxelized or noisy input point clouds as guidance, because we can sample various different plausible versions using diffuse-denoise, while always approximately preserving the overall input shape (see examples in Figs. 4, 29, 30, and 31).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Encoder Fine-Tuning for Voxel-Conditioned Synthesis and Denoising</head><p>A crucial advantage of LION's underlying VAE framework with latent DDMs is that we can adapt the encoder neural networks for different relevant tasks, as discussed in Sec. 3.1 and demonstrated in our experiments. For instance, a digital artist may have a rough idea about the shape they desire to synthesize and they may be able to quickly put together a coarse voxelization according to whatever they imagine. Or similarly, a noisy version of a shape may be available and the user may want to guide LION's synthesis accordingly.</p><p>To this end, we propose to fine-tune LION's encoder neural networks for such tasks: In particular, we take clean shapes x from the training data and voxelize them or add noise to them. Specifically, we test three different noise types as well as voxelization (see Figs. 31 and 30): we either perturb the point cloud with uniform noise, Gaussian noise, outlier noise, or we voxelize it. We denote the resulting coarse or noisy shapes asx here. Given a fully trained LION model, we can fine-tune its encoder networks to ingest the perturbedx, instead of the clean x. For that, we are using the following ELBO-like (maximization) objective:</p><formula xml:id="formula_14">L finetune (?) = L reconst (?)?E p(x),q ? (z0|x) [? z D KL (q ? (z 0 |x)|p(z 0 )) + ? h D KL (q ? (h 0 |x, z 0 )|p(h 0 ))] .<label>(13)</label></formula><p>When training the encoder to denoise uniform or Gaussian noise added to the point cloud, we use the same reconstruction objective as during original LION training, i.e.,</p><formula xml:id="formula_15">L L1 reconst (?) = E p(x),q ? (z0|x),q ? (h0|x,z0) log p ? (x|h 0 , z 0 ).<label>(14)</label></formula><p>However, when training with voxelized inputs or outlier noise, there is no good corresponce to define the point-wise reconstruction loss with the Laplace distribution (corresponding to an L 1 loss). Therefore, in these cases we instead rely on Chamfer Distance (CD) and Earth Mover Distance (EMD) for the reconstruction term:</p><formula xml:id="formula_16">L CD/EMD reconst (?) = E p(x),q ? (z0|x),q ? (h0|x,z0) L CD (? ? (h 0 , z 0 ), x) + L EMD (? ? (h 0 , z 0 ), x)<label>(15)</label></formula><p>Here, L CD and L EMD denote CD and EMD losses:</p><formula xml:id="formula_17">L CD (x, y) = x?x min y?y ||x ? y|| 1 + y?y min x?x ||x ? y|| 1 ,<label>(16)</label></formula><formula xml:id="formula_18">L EMD (x, y) = min ?:x?y x?x ||x ? ?(x)|| 2 ,<label>(17)</label></formula><p>where ? denotes a bijection between the point clouds x and y (with the same number of points). Note that we are using an L 1 loss for the distance calculation in the CD, which we found to work well and corresponds to the L 1 loss we are relying on during original LION training. Furthermore, ? ? (h 0 , z 0 ) in Eq. <ref type="formula" target="#formula_0">(15)</ref> formally denotes the deterministic decoder output given the latent encodings z 0 and h 0 , this is, the mean of the Laplace distribution p ? (x|h 0 , z 0 ). The weights for the Kullback-Leibler (KL) terms in Eq. <ref type="formula" target="#formula_0">(13)</ref>, ? z and ? h , are generally kept the same as during original LION training. Training with the above objectives ensures that the encoder maps perturbed inputsx to latent encodings that will decode to the original clean shapes x. This is because maximizing the ELBO (or an adaptation like above) with respect to the encoders q ? (z 0 |x) and q ? (h 0 |x, z 0 ) while keeping the decoder fixed is equivalent to minimizing the KL divergences D KL (q ? (z 0 |x)|p ? (z 0 |x)) and</p><formula xml:id="formula_19">E q ? (z0|x) [D KL (q ? (h 0 |x, z 0 )|p ? (h 0 |x, z 0 ))]</formula><p>with respect to the encoders, where p ? (z 0 |x) and p ? (h 0 |x, z 0 ) are the true posterior distributions given clean shapes x <ref type="bibr" target="#b45">[70,</ref><ref type="bibr" target="#b103">128,</ref><ref type="bibr" target="#b104">129]</ref>. Consequently, the fine-tuned encoders are trained to use the noisy or voxel inputsx to predict the true posterior distributions over the latent variables z 0 and h 0 given the clean shapes x. A user can therefore utilize these fine-tuned encoders to reconstruct clean shapes from noisy or voxelized inputs. Importantly, once the fine-tuned encoder predicts an encoding we can further refine it, clean up imperfect encodings, and sample different shape variations by a few steps of diffuse-denoise in latent space (see previous App. C.1). This allows for multimodal denoising and voxel-driven synthesis (also see <ref type="figure">Fig. 4</ref>).</p><p>One question that naturally arises is regarding the processing of the noisy or voxelized input shapes. Our PVCNN-based encoder networks can easily process noisy point clouds, but not voxels. Therefore, given a voxelized shape, we uniformly distribute points over the voxelized shape's surface, such that it can be consumed by LION's point cloud processing networks (see details in App. E.4).</p><p>We would like to emphasize that LION supports these applications easily without re-training the latent DDMs due to its VAE framework with additional encoders and decoders, in contrast to previous works that train DDMs on point clouds directly <ref type="bibr" target="#b21">[46,</ref><ref type="bibr" target="#b22">47]</ref>. For instance, PVD <ref type="bibr" target="#b21">[46]</ref> operates directly on the voxelized or perturbed point clouds with its DDM. Because of that PVD needs to perform many steps of diffuse-denoise to remove all the noise from the input-there is no encoder that can help with that. However, this has the drawback that this induces significant shape variations that do not well correspond to the original noisy or voxelized inputs (see experiments and discussion in Sec. 5.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Shape Interpolation</head><p>Here, we explain in detail how exactly we perform shape interpolation. It may be instructive to take a step back first and motivate our approach. Of course, we cannot simply linearly interpolate two point clouds, this is, the points' xyz-coordinates, directly. This would result in unrealistic outputs along the interpolation path. Rather, we should perform interpolation in a space where semantically similar point clouds are mapped near each other. One option that comes to mind is to use the latent space, this is, both the shape latent space and the latent points, of LION's point cloud VAE. We could interpolate two point clouds' encodings, and then decode back to point cloud space. However, we also do not have any guarantees in this situation, either, due to the VAE's prior hole problem <ref type="bibr" target="#b33">[58,</ref><ref type="bibr" target="#b47">[72]</ref><ref type="bibr" target="#b48">[73]</ref><ref type="bibr" target="#b49">[74]</ref><ref type="bibr" target="#b50">[75]</ref><ref type="bibr" target="#b51">[76]</ref><ref type="bibr" target="#b52">[77]</ref><ref type="bibr" target="#b53">[78]</ref><ref type="bibr" target="#b54">[79]</ref>, this is, the problem that the distribution of all encodings of the training data won't perfectly form a Gaussian, which it was regularized towards during VAE training (see Eq. <ref type="formula" target="#formula_5">(5)</ref>). Hence, when simply interpolating directly in the VAE's latent space, we would pass regions in latent space for which the decoder does not produce a realistic sample. This would result in poor outputs.</p><p>Therefore, we rather interpolate in the prior spaces of our latent DDMs themselves, this is, the spaces that emerge at the end of the forward diffusion processes. Since the diffusion process of DDMs by construction perturbs all data points into almost perfectly Gaussian x 1 ? N (x 1 ; 0, I) (where t = 1 denotes the end of the diffusion for continuous t ? [0, 1]), DDMs do not suffer from any prior hole challenges-the denoising model is essentially well trained for all possible x 1 ? N (x 1 ; 0, I). Hence, given two x A 1 and x B 1 , in DDMs we can safely interpolate them according to</p><formula xml:id="formula_20">x s 1 = ? s x A 1 + ? 1 ? s x B 1<label>(18)</label></formula><p>for s ? [0, 1] and expect meaningful outputs when generating the corresponding denoised samples.</p><p>But why do we choose the square root-based interpolation? Since we are working in a very highdimensional space, we know that according to the Gaussian annulus theorem both x A 1 and x B 1 are almost certainly lying on a thin (high-dimensional) spherical shell that supports almost all probability mass of p 1 (x 1 ) ? N (x 1 ; 0, I). Furthermore, since x A 1 and x B 1 are almost certainly orthogonal to each other, again due to the high dimensionality, our above interpolation in Eq. (18) between x A 1 and x B 1 corresponds to performing spherical interpolation along the spherical shell where almost all probability mass concentrates. In contrast, linear interpolation would leave this shell, which resulted in poorer results, because the model wasn't well trained for denoising samples outside the typical set. Note that we found spherical interpolation to be crucial (in DDMs of images, linear interpolation tends to still work decently; for our latent point DDM, however, linear interpolation performed very poorly).</p><p>In LION, we have two DDMs operating on the shape latent variables z 0 and the latent points h 0 . Concretely, for interpolating two shapes Note that instead of using given shapes and encoding them into the VAE's latent space and further into the DDMs' prior, we can also directly sample novel encodings in the DDM priors and interpolate those.</p><p>In practice, to solve the generative ODE both for encoding and generation, we are using an adaptive step size Runge-Kutta4(5) <ref type="bibr" target="#b42">[67,</ref><ref type="bibr" target="#b105">130]</ref> solver with error tolerances 10 ?5 . Furthermore, we don't actually solve the ODE all the way to exactly 0, but only up to a small time 10 ?5 for numerical reasons (hence, the actual integration interval for the ODE solver is [10 ?5 , 1]). We are generally relying on our LION models whose latent DDMs were trained with 1000 discrete time steps (see objectives Eqs. <ref type="formula" target="#formula_6">(6)</ref> and <ref type="formula" target="#formula_7">(7)</ref>) and found them to generalize well to the continuous-time setting where the model is also queried for intermediate times t (see discussion in App. B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Mesh Reconstruction with Shape As Points</head><p>Before explaining in App. C.4.2 how we incorporate Shape As Points <ref type="bibr" target="#b43">[68]</ref> into LION to reconstruct smooth surfaces, we first provide background on Shape As Points in App. C.4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4.1 Background on Shape As Points</head><p>Shape As Points (SAP) <ref type="bibr" target="#b43">[68]</ref> reconstructs 3D surfaces from points by finding an indicator function ? : R 3 ? R whose zero level set corresponds to the reconstructed surface. To recover ?, SAP first densifies the input point cloud X = {x i ? R 3 } N i=1 by predicting k offsetted points and normals for each input point-such that in total we have additional points X = {x i } kN i=1 and normals N = {n i } kN i=1 -using a neural network f ? (X) with parameters ? conditioned on the input point cloud X.</p><p>After upsampling the point cloud and predicting normals, SAP solves a Poisson partial differential equation (PDE) to recover the function ? from the densified point cloud. Casting surface reconstruction as a Poisson problem is a widely used approach first introduced by Kazhdan et al. <ref type="bibr" target="#b106">[131]</ref>. Unlike Kazhdan et al. <ref type="bibr" target="#b106">[131]</ref>, which encodes ? as a linear combination of sparse basis functions and solves the PDE using a finite element solver on an octree, SAP represents ? in a discrete Fourier basis on a dense grid and solves the problem using a spectral solver. This spectral approach has the benefits of being fast and differentiable, at the expense of cubic (with respect to the grid size) memory consumption.</p><p>To train the upsampling network f , SAP minimizes the L 2 distance between the predicted indicator function ? (sampled on a dense, regular grid) and a pseudo-ground-truth indicator function ? gt recovered by solving the same Poisson PDE on a dense set of points and normals. Denoting the differentiable Poisson solve as ? = Poisson(X , N ), we can write the loss minimized by SAP as</p><formula xml:id="formula_21">L(?) = E {Xi,?i?D} Poisson(f ? (X i )) ? ? i 2 2<label>(19)</label></formula><p>where D is the training data distribution of indicator functions ? i for shapes and point samples on the surface of those shapes X i .</p><p>Since the ideas from Poisson Surface Reconstruction (PSR) <ref type="bibr" target="#b106">[131]</ref> lie at the core of Shape As Points, we give a brief overview of the Poisson formulation for surface reconstruction: Given input points</p><formula xml:id="formula_22">X = {x i ? R 3 } N i=1 and normals N = {n i ? R 3 } N i=1</formula><p>, we aim to recover an indicator function ? : R 3 ? R such that the reconstructed surface S is the zero level set of ?, i.e., S = {x : ?(x) = 0}.</p><p>Intuitively, we would like the recovered ? to change sharply between a positive value and a negative value at the surface boundary along the direction orthogonal to the surface. Thus, PSR treats the surface normals N as noisy samples of the gradient of ?. In practice, PSR first constructs a smoothed vector field V from N by convolving these with a filter (e.g. a Gaussian), and recovers ? by minimizing min </p><formula xml:id="formula_23">? ?? ? V 2 2<label>(20)</label></formula><formula xml:id="formula_24">?? = ? ? V ,<label>(21)</label></formula><p>which can be solved using standard numerical methods for solving Elliptic PDEs. Since PSR is effectively integrating V to recover ?, the solution is ambiguous up to an additive constant. To remedy this, PSR subtracts the mean value of ? at the input points, i.e., 1 N N i=1 ?(x i ), yielding a unique solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4.2 Incorporating Shape As Points in LION</head><p>LION is primarily set up as a point cloud generative model. However, an artist may prefer a mesh as output of the model, because meshes are still the most commonly used shape representation in graphics software. Therefore, we are augmenting LION with mesh reconstruction, leveraging SAP. In particular, given a generated point cloud from LION, we use SAP to predict an indicator function ? defining a smooth shape surface in R 3 as its zero level set, as explained in detail in the previous section. Then, we extract polygonal meshes from ? via marching cubes <ref type="bibr" target="#b107">[132]</ref>.</p><p>SAP is commonly trained using slightly noisy perturbed point clouds as input to its neural network f ? <ref type="bibr" target="#b43">[68]</ref>. This results in robustness and generalization to noisy shapes during inference. Also, the point clouds generated by LION are not perfectly clean and smooth but subject to some noise. In principle, to make our SAP model ideally suited for reconstructing surfaces from LION's generated point clouds, it would be best to train SAP using inputs that are subject to the same noise as generated by LION. Although we do not know the exact form of LION's noise, we propose to nevertheless specialize the SAP model for LION: Specifically, we take SAP's clean training data (i.e. densely sampled point clouds from which accurate pseudo-ground-truth indicator functions can be calculated via PSR; see previous App. C.4.1) and encode it into LION's latent spaces z 0 and h 0 . Then, we perform a few diffuse-denoise steps in latent space (see App. C.1) that create small shape variations of the input shapes when decoded back to point clouds. However, when doing these diffuse-denoise steps, we are exactly using LION's generation mechanism, i.e., the stochastic sampling in Eq. (4), to generate the slightly perturbed encodings. Hence, we are injecting the same noise that is also seen in generation. Therefore, the correspondingly generated point clouds can serve as slightly noisy versions of the original clean point clouds before encoding, diffuse-denoise, and decoding, and we can use this data to train SAP. We found experimentally that this LION-specific training of SAP can indeed improve SAP's performance when reconstructing meshes from LION's generated point clouds. We investigate this experimentally in App. F.1.4.</p><p>Note that in principle an even tighter integration of SAP with LION would be possible. In future versions of LION, it would be interesting to study joint end-to-end LION and SAP training, where LION's decoder directly predicts a dense set of points with normals that is then matched to a pseudoground-truth indicator function using differentiable PSR. However, we are leaving this to future research. To the best of our knowledge, LION is the first point cloud generative model that directly incorporates modern surface and mesh reconstruction at all. In conclusion, using SAP we can convert LION into a mesh generation model, while under the hood still leveraging point clouds, which are ideal for DDM-based modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Implementation</head><p>In <ref type="figure" target="#fig_5">Fig. 19</ref>, we plot the building blocks used in LION: ? PVCNN visualizes a typical network used in LION. Both the latent points encoder, decoder and the latent point prior share this high-level architecture design, which is modified from the base network of PVD 2 <ref type="bibr" target="#b21">[46]</ref>. It consists of some set abstraction levels and feature propagation levels. The details of these levels can be found in PointNet++ <ref type="bibr" target="#b57">[82]</ref>.</p><p>? ResSE denotes a ResNet block with squeeze-and-excitation (SE) <ref type="bibr" target="#b108">[133]</ref> layers.  <ref type="figure" target="#fig_5">Figure 19</ref>: Building blocks of LION. We denote the voxel grid size as r, and the hidden dimension as d. They are hyperparameters of our model. All GroupNorm layers use 8 groups.</p><p>? AdaGN is the adaptive group normalization (GN) layer that is used for conditioning on the shape latent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 VAE Backbone</head><p>Our VAE backbone consists of two encoder networks, and a decoder network. The PVCNNs we used are based on PointNet++ <ref type="bibr" target="#b57">[82]</ref> with point-voxel convolutions <ref type="bibr" target="#b55">[80]</ref>.</p><p>We show the details of the shape latent encoder in Tab. 5, the latent points encoder in Tab. 6, and the details of the decoder in Tab. 7.</p><p>We use a dropout probability of 0.1 for all dropout layers in the VAE. All group normalization layers in the latent points encoder as well as in the decoder are replaced by adaptive group normalization (AdaGN) layers to condition on the shape latent. For the AdaGN layers, we initialized the weight of the linear layer with scale at 0.1. The bias for the output factor is set as 1.0 and the bias for the output bias is set as 0.0. The AdaGN is also plot in <ref type="figure" target="#fig_5">Fig. 19</ref>.</p><p>Model Initialization. We initialize our VAE model such that it acts as an identity mapping between the input, the latent space, and reconstructed points at the beginning of training. We achieve this by scaling down the variances of encoders and by weighting the skip connections accordingly.</p><p>Weighted Skip Connection. We add skip connections in different places to improve information propagation. In the latent points encoder, the clean point cloud coordinates (in 3 channels) are added to the mean of the predicted latent point coordinates (in 3 channels), which is multiplied by 0.01 before the addition. In the decoder, the sampled latent points coordinates are added to the output point coordinates (in 3 channels). The predicted output point coordinates are multiplied by 0.01 before the addition.</p><p>Variance Scaling. We subtract the log of the standard deviation of the posterior Normal distribution with a constant value. The constant value helps pushing the variance of the posterior towards zero when the LION model is initialized. In our experiments, we set this offset value as 6.</p><p>With the above techniques, at the beginning of training the input point cloud is effectively copied into the latent point cloud and then directly decoded back to point cloud space, and the shape latent variables are not active. This prevents diverging reconstruction losses at the beginning of training.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Shape Latent DDM Prior</head><p>We show the details of the shape latent DDM prior in Tab. 8. We use a dropout probability of 0.3, 0.3, and 0.4 for the airplane, car, and chair category, respectively. The time embeddings are added to the features of each ResSE layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Latent Points DDM Prior</head><p>We show the details of the latent points DDM prior in Tab. 9. We use a dropout probability of 0.1 for all dropout layers in this DDM prior. All group normalization layers are replaced by adaptive group normalization layers to condition on the shape latent variable. The time embeddings are concatenated with the point features for the inputs of each SA and FP layer.</p><p>Note that both latent DDMs use a mixed denoising score network parametrization, directly following Vahdat et al. <ref type="bibr" target="#b33">[58]</ref>. In short, the DDM's denoising model is parametrized as the analytically ideal denoising network assuming a normal data distribution plus a neural network-based correction. This can be advantageous, if the distribution that is modeled by the DDM is close to normal. This is indeed the case in our situation, because during the first training stage all latent encodings were regularized to fall under a standard normal distribution due to the VAE objective's Kullback-Leibler regularization. Our implementation of the mixed denoising score network technique directly follows Vahdat et al. <ref type="bibr" target="#b33">[58]</ref> and we refer the reader there for further details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Two-stage Training</head><p>The training of LION consists of two stages:</p><p>First Stage Training. LION optimizes the modified ELBO of Eq. (5) with respect to the two encoders and the decoder as shown in the main paper. We use the same value for ? z and ? h . These KL weights, starting at 10 ?7 , are annealed linearly for the first 50% of the maximum number of epochs. Their final value is set to 0.5 at the end of the annealing process.</p><p>Second Stage Training. In this stage, the encoders and the decoder are frozen, and only the two DDM prior networks are trained using the objectives in Eqs. (6) and <ref type="bibr" target="#b6">(7)</ref>. During training, we first  encode the clean point clouds x with the encoders and sample z 0 ? q ? (z 0 |x), h 0 ? q ? (h 0 |x, z 0 ). We then draw the time steps t uniformly from U {1, ..., T }, then sample the diffused shape latent z t and latent points h t . Our shape latent DDM prior takes z t with t as input, and the latent points DDM prior takes (z 0 , t, h t ) as input. We use the un-weighted training objective (i.e., w(t) = 1).</p><p>During second stage training, we regularize the prior DDM neural networks by adding spectral normaliation (SN) <ref type="bibr" target="#b109">[134]</ref> and a group normalization (GN) loss similar to Vahdat et al. <ref type="bibr" target="#b33">[58]</ref>. Furthermore, we record the exponential moving average (EMA) of the latent DDMs' weight parameters, and use the parameter EMAs during inference when calling the DDM priors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Experiment Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Different Datasets</head><p>For the unconditional 3D point cloud generation task, we follow previous works and use the ShapeNet <ref type="bibr" target="#b79">[104]</ref> dataset, as pre-processed and released by PointFlow [31]. Also following previous works [31, <ref type="bibr" target="#b21">46,</ref><ref type="bibr" target="#b22">47]</ref> and to be able to compare with many different baseline methods, we train on three categories: airplane, chair and car. The ShapeNet dataset released by PointFlow consists of 15k points for each shape. During training, 2,048 points are randomly sampled from the 15k points at each iteration. The training set consists of 2,832, 4,612, and 2,458 shapes for airplane, chair and car, respectively. The sample quality metrics are reported with respect to the standard reference set, which consists of 405, 662, and 352 shapes for airplane, chair and car, respectively. During training, we use the same normalization as in PointFlow [31] and PVD <ref type="bibr" target="#b21">[46]</ref>, where the data is normalized globally across all shapes. We compute the means for each axis across the whole training set, and one standard deviation across all axes and the whole training set. Note that there is a typo in the caption   When reproducing the baselines on the ShapeNet dataset released by PointFlow [31], we found that some methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr">43,</ref><ref type="bibr" target="#b20">45,</ref><ref type="bibr" target="#b27">52]</ref> require per-shape normalization, where the mean is computed for each axis for each shape, and the scale is computed as the maximum length across all axes for each shape. As a result, the xyz-values of the point coordinates will be bounded within [?1, 1]. We train and evaluate LION following this convention <ref type="bibr" target="#b20">[45]</ref> when comparing it to these methods. Note that these different normalizations imply different generative modeling problems. Therefore, it is important to carefully distinguish these different setups for fair comparisons.  When training the SAP model, we follow Peng et al. <ref type="bibr" target="#b43">[68]</ref>, Mescheder et al. <ref type="bibr" target="#b75">[100]</ref> and also use their data splits and data pre-processing to get watertight meshes. Watertight meshes are required to properly determine whether points are in the interior of the meshes or not, and to define signed distance fields (SDFs) for volumetric supervision, which the PointFlow data does not offer. More details of the data processing can be found in Mescheder et al. <ref type="bibr">[</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Evaluation Metrics</head><p>Different metrics to quantitatively evaluate the generation performance of point cloud generative models have been proposed, and some of them suffer from certain drawbacks. Given a generated set of point clouds S g and a reference set S r , the most popular metrics are (we are following Yang et al. <ref type="bibr">[31]</ref>):</p><p>? Coverage (COV):</p><formula xml:id="formula_25">COV(S g , S r ) = |{arg min Y ?Sr D(X, Y )|X ? S g }| |S r | ,<label>(22)</label></formula><p>where D(?, ?) is either the Chamfer distance (CD) or earth mover distance (EMD). COV measures the number of reference point clouds that are matched to at least one generated shape. COV can quantify diversity and is sensitive to mode dropping, but it does not quantify the quality of the generated point clouds. Also low quality but diverse generated point clouds can achieve high coverage scores. ? Minimum matching distance (MMD):</p><formula xml:id="formula_26">MMD(S g , S r ) = 1 |S r | Y ?Sr min X?Sg D(X, Y ),<label>(23)</label></formula><p>where again D(?, ?) is again either CD or EMD. The idea behind MMD is to calculate the average distance between the point clouds in the reference set and their closest neighbors in the generated set. However, MMD is not sensitive to low quality points clouds in S g , as they are most likely not matched to any shapes in S r . Therefore, it is also not a reliable metric to measure overall generation quality, and it also does not quantify diversity or mode coverage. </p><formula xml:id="formula_27">1-NNA(S g , S r ) = X?Sg I[N X ? S g ] + Y ?Sr I[N Y ? S r ] |S g | + |S r | ,<label>(24)</label></formula><p>where I[?] is the indicator function and N X is the nearest neighbor of X in the set S r ? S g ? {X} (i.e., the union of the sets S r and S g , but without the particular point cloud X). Hence, 1-NNA represents the leave-one-out accuracy of the 1-NN classifier defined in Eq. (24). More specifically, this 1-NN classifier classifies each sample as belonging to either S r or S g based on its nearest neighbor sample within N X (nearest neighbors can again be computed based on either CD or EMD). If the generated S g matches the reference S r well, this classification accuracy will be close to 50%. Hence, this accuracy can be used as a metric to quantify point cloud generation performance. Importantly, 1-NNA directly quantifies distribution similarity between S r and S g and measures both quality and diversity.</p><p>Following Yang et al.</p><p>[31], we can conclude that COV and MMD are potentially unreliable metrics to quantify point cloud generation performance and 1-NNA seems like a more suitable evaluation metric. Also the more recent and very relevant PVD <ref type="bibr" target="#b21">[46]</ref> follows this and uses 1-NNA as its primary evaluation metric. Note that also Jensen-Shannon Divergence (JSD) is sometimes used to quantify point cloud generation performance. However, it measures only the "average shape" similarity by marginalizing over all point clouds from the generated and reference set, respectively. This makes it an almost meaningless metric to quantify individual shape quality (see discussion in Yang et al. <ref type="bibr">[31]</ref>).</p><p>In conclusion, we are following Yang et al.</p><p>[31] and Zhou et al. <ref type="bibr" target="#b21">[46]</ref> and use 1-NNA as our primary evaluation metric to quantify point cloud generation performance and we evaluate it generally both using CD and EMD distances, according to the following standard definitions:</p><formula xml:id="formula_28">CD(X, Y ) = x?X min y?Y ||x ? y|| 2 2 + y?Y min x?X ||x ? y|| 2 2 ,<label>(25)</label></formula><formula xml:id="formula_29">EMD(X, Y ) = min ?:X?Y x?X ||x ? ?(x)|| 2 ,<label>(26)</label></formula><p>where ? is a bijection between point clouds X and Y with the same number of points. We use released codes to compute CD 3 and EMD 4 .</p><p>Since COV and MMD are still widely used in the literature, though, we are also reporting COV and MMD for all our models in App. F, even though they may be unreliable as metrics for generation quality. Note that for the more meaningful 1-NNA metric, LION generally outperforms all baselines in all experiments.</p><p>For fair comparisons and to quantify LION's performance in isolation without SAP-based mesh reconstruction, all metrics are computed directly on LION's generated point clouds, not meshed outputs. However, we also do calculate generation performance after the SAP-based mesh reconstruction in a separate ablation study (see App. F.1.4). In those cases, we sample points from the SAP-generated surface to create the point clouds for evaluation metric calculation. Similarly, when calculating metrics for the IM-GAN <ref type="bibr" target="#b6">[7]</ref> baseline we sample points from the implicitly defined surfaces generated by IM-GAN. Analogously, for the GCA [43] baseline we sample points from the generated voxels' surfaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Details for Unconditional Generation</head><p>We list the hyperparameters used for training the unconditional generative LION models in Tab. 10. The hyperparameters are the same for both the single class model and many-class model. Notice that we do not perform any hyperparameter tuning on the many-class model, i.e., it is likely that the many-class LION can be further improved with some tuning of the hyperparameters.  When tuning the model for unconditional generation, we found that the dropout probability and the hidden dimension for the shape latent DDM prior have the largest impact on the model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Keys Values</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VAE Training</head><p>The other hyperparameters, such as the size of the encoders and decoder, matter less.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 Details for Voxel-guided Synthesis</head><p>Setup. We use a voxel size of 0.6 for both training and testing. During training, the training data (after normalization) are first voxelized, and the six faces of all voxels are collected. The faces that are shared by two or more voxels are discarded. To create point clouds from the voxels, we sample the voxels' faces and then randomly sample points within the faces. In our experiments, 2,048 points are sampled from the voxel surfaces for each shape. We randomly sample a similar number of points at each face.</p><p>Encoder Fine-Tuning. For encoder fine-tuning, we initialize the model weights from the LION model trained on the same categories with clean data. Both the shape latent encoder and the latent points encoder are fine-tuned on the voxel inputs, while the decoder and the latent DDMs are frozen. We set the maximum training epochs as 10,000 and perform early-stopping when the reconstruction loss on the validation set reaches a minimum value. In our experiments, training is usually stopped early after around 500 epochs. For example, our model on airplane, chair, and car category are stopped at 129, 470, and 189 epochs, respectively. All other hyperparameters are the same as for the unconditional generation experiments. The training objective can be found in Eq. (13) and Eq. (15).</p><p>In <ref type="figure">Fig. 12</ref> and <ref type="figure" target="#fig_0">Fig. 13</ref>, we report the reconstruction of input points and IOU of the voxels on the test set. We also evaluate the output shape quality by having the models encode and decode the whole training set, and compute the sample quality metrics with respect to the reference set.</p><p>Note that we also tried fine-tuning the encoder of the DPM baseline <ref type="bibr" target="#b22">[47]</ref>; however, the results did not substantially change. Hence, we kept using standard DPM models.</p><p>Multimodal Generation. When performing multimodal generation for the voxel-guided synthesis experiments, we encode the voxel inputs into the shape latent z 0 and the latent points h 0 , and run the forward diffusion process for a few steps to obtain their diffused versions. The diffused shape latent (z ? ) is then denoised by the shape latent DDM. The diffused latent points h ? are denoised by the latent points DDM, conditioned on the shape latent generated by the shape latent DDM (also see App. C.1). Thee number of diffuse-denoise steps can be found in <ref type="figure">Figs. 11, 12</ref>, and 13.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5 Details for Denoising Experiments</head><p>Setup. We perturb the input data using different types of noise and show how well different methods denoise the inputs. The experimental setting for each noise type is listed below:</p><p>? Normal Noise: for each coordinate of a point, we first sample the standard deviation value of the noise uniformly from 0 to 0.25; then, we perturb the point with the noise, sampled from a normal distribution with zero mean and the sampled standard deviation value.</p><p>? Uniform Noise: for each coordinate of a point, we add noise sampled from the uniform distribution U (0, 0.25).</p><p>? Outlier Noise: for a shape consisting of N points, we replace 50% of its points with points drawn uniformly from the 3D bounding box of the original shape. The remaining 50% of the points are kept at their original location.</p><p>Similar to the encoder fine-tuning for voxel-guided synthesis (App. E.4), when fine-tuning LION's encoder networks for the different denoising experiments, we freeze the latent DDMs and the decoder and only update the weights of the shape latent encoder and the latent points encoder. The maximum number of epochs is set to 4,000 and the training process is stopped early based on the reconstruction loss on the validation set. The other hyperparameters are the same as for the unconditional generation experiments. To get different generations from the same noisy inputs, we again diffuse and denoise in the latent space. The operations are the same as for the multimodal generation during voxel-guided synthesis (App. E.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.6 Details for Fine-tuning SAP on LION</head><p>Training the Original SAP. We first train the SAP model on the clean data with normal noise injected, following the practice in SAP <ref type="bibr" target="#b43">[68]</ref>. We set the standard deviation of the noise to 0.005.</p><p>Data Preparation. The training data for SAP fine-tuning is obtained by having LION encode the whole training set, diffuse and denoise in the latent space for some steps, and then decode the point cloud using the decoder. We ablate the number of steps for the diffuse-denoise process in App. F.1.4.</p><p>In our experiments, we randomly sample the number of steps from {20, 30, 35, 40, 50}. The number of points used in this preparation process is 3,000, since the SAP model takes 3,000 points as input (since LION is constructed only from PointNet-based and convolutional networks, it can be run with any number of points). To prevent SAP from overfitting to the sampled points, we generate 4 different samples for each shape, with the same number of diffuse-denoise steps. During fine-tuning, SAP randomly draws one sample as input.</p><p>Fine-Tuning. When fine-tuning SAP, we use the same learning rate, batch size, and other hyperparameters as during training of the original SAP model, except that we change the input and reduce the maximum number of epochs to 1,000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.7 Training Times</head><p>For single-class LION models, the total training time is ? 550 GPU hours (? 110 GPU hours for training the backbone VAE; ? 440 GPU hours for training the two latent diffusion models). Sampling time analyses can be found in App. F.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.8 Used Codebases</head><p>Here, we list all external codebases and datasets we use in our project.</p><p>To compare to baselines, we use the following codes: We use further codebases in other places:</p><p>? We use the MitSuba renderer for visualizations <ref type="bibr" target="#b110">[135]</ref>: https://github.com/ mitsuba-renderer/mitsuba2 (License: https://github.com/mitsuba-renderer/ mitsuba2/blob/master/LICENSE), and the code to generate the scene discription files for MitSuba [31]: https://github.com/zekunhao1995/PointFlowRenderer.</p><p>? We rely on SAP <ref type="bibr" target="#b43">[68]</ref> for mesh generation with the code at https://github.com/ autonomousvision/shape_as_points (MIT License). ? For calculating the evaluation metrics, we use the implementation for CD at https: //github.com/ThibaultGROUEIX/ChamferDistancePytorch (MIT License) and for EMD at https://github.com/daerduoCarey/PyTorchEMD.</p><p>? We use Text2Mesh <ref type="bibr" target="#b24">[49]</ref> for per-sample text-driven texture synthesis: https://github. com/threedle/text2mesh (MIT License)</p><p>We also rely on the following datasets:</p><p>? ShapeNet <ref type="bibr" target="#b79">[104]</ref>. Its terms of use can be found at https://shapenet.org/terms.</p><p>? The Cars dataset <ref type="bibr" target="#b111">[136]</ref> from http://ai.stanford.edu/~jkrause/cars/car_ dataset.html with ImageNet License: https://image-net.org/download.php.</p><p>? The TurboSquid data repository, https://www.turbosquid.com. We obtained a custom license from TurboSquid to use this data.</p><p>? Redwood 3DScan Dataset <ref type="bibr" target="#b112">[137]</ref>: https://github.com/isl-org/redwood-3dscan (Public Domain)</p><p>? Pix3D <ref type="bibr" target="#b113">[138]</ref>: https://github.com/xingyuansun/pix3d. (Creative Commons Attribution 4.0 International License).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.9 Computational Resources</head><p>The total amount of compute used in this research project is roughly 340,000 GPU hours. We used an in-house GPU cluster of V100 NVIDIA GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Additional Experimental Results</head><p>Overview:</p><p>? In App. F.1.1, we present an ablation study on LION's hierarchical architecture.</p><p>? In App. F.1.2, we present an ablation study on the point cloud processing backbone neural network architecture.</p><p>? In App. F.1.3, we present an ablation study on the extra dimensions of the latent points.</p><p>? In App. F.1.4, we show an ablation study on the number of diffuse-denoise steps used during SAP fune-tuning.</p><p>? In App. F.2, we provide additional experimental results on single-class unconditional generation. We show MMD and COV metrics, and also incorporate additional baselines in the extended tables. Furthermore, in App. F.2.1 we visualize additional samples from the LION models.</p><p>? In App. F.3, we provide additional experimental results for the 13-class unconditional generation LION model. In App. F.3.1 we show more samples from our many-class LION model. Additionally, in App. F.3.2 we analyze LION's shape latent space via a two-dimensional t-SNE projection <ref type="bibr" target="#b114">[139]</ref>.</p><p>? In App. F.4, we provide additional experimental results for the 55-class unconditional generation LION model.</p><p>? In App. F.5, we provide additional experimental results for the LION models trained on ShapeNet's Mug and Bottle classes.</p><p>? In App. F.6, we provide additional experimental results for the LION model trained on 3D animal shapes.</p><p>? In App. F.7, we provide additional results on voxel-guided synthesis and denoising for the chair and car categories.</p><p>? In App. F.8, we quantify LION's autoencoding performance and compare to various baselines, which we all outperform.</p><p>? In App. F.9, we provide additional results on significantly accelerated DDIM-based synthesis in LION <ref type="bibr" target="#b81">[106]</ref>.</p><p>? In App. F.10, we use Text2Mesh <ref type="bibr" target="#b24">[49]</ref> to generate textures based on text prompts for synthesized LION samples.</p><p>? In App. F.11, we condition LION on CLIP embeddings of the shapes' rendered images, following CLIP-Forge <ref type="bibr">[34]</ref>. This allows us to perform text-driven 3D shape generation and single view 3D reconstruction. ? In App. F.12, we demonstrate more shape interpolations using the three single-class and also the 13-class LION models and we also show shape interpolations of the relevant PVD <ref type="bibr" target="#b21">[46]</ref> and DPM <ref type="bibr" target="#b22">[47]</ref> baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1.1 Ablation Study on LION's Hierarchical Architecture</head><p>We perform an ablation experiment with the car category over the different components of LION's architecture. We consider three settings:</p><p>? LION model without shape latents. But it still has latent points and a corresponding latent points DDM prior. ? LION model without latent points. But it still has the shape latents and a corresponding shape latent DDM. ? LION model without any latent variables at all, i.e., a DDM operates on the point clouds directly (this is somewhat similar to PVD <ref type="bibr" target="#b21">[46]</ref>).</p><p>When simply dropping the different architecture components, the model "loses" parameters. Hence, a decrease in performance could also simply be due to the smaller model rather than an inferior architecture. Therefore, we also increase the model sizes in the above ablation study (by scaling up the channel dimensions of all networks), such that all models have approximately the same number of parameters as our main LION model that has all components. The results on the car category can be found in Tab. 11. The results show that the full LION setup with both shape latents and latent points performs best on all metrics, sometimes by a large margin. Furthermore, for the models with no or only one type of latent variables, increasing model size does not compensate for the loss of performance due to the different architectures. This ablation study demonstrates the unique advantage of the hierarchical setup with both shape latent variables and latent points, and two latent DDMs. We believe that the different latent variables complement each other-the shape latent variables model overall global shape, while the latent points capture details. This interpretation is supported by the experiments in which we keep the shape latent fixed and only observe small shape variations due to different local point latent configurations (Sec. 5.2 and <ref type="figure" target="#fig_4">Fig. 8</ref>).   <ref type="bibr" target="#b115">[140]</ref> and PointTransformer <ref type="bibr" target="#b116">[141]</ref>. For the ablation on the encoder and decoder backbones, we train LION's VAE model (without prior) with different backbones, and compare the reconstruction performance for different backbones. We select the PVCNN as it provides Method CD? EMD? PVCNN 0.006 0.009 DGCNN (knn=20) 0.068 0.231 DGCNN (knn=10) 0.081 0.331 PointTransformer 0.015 0.029 <ref type="table" target="#tab_3">Table 12</ref>: Ablation on the backbone of the encoder and decoder of LION's VAE. The point cloud auto-encoding performance on the car category is reported. Both CD and EMD reconstruction values are multiplied with 1 ? 10 ?2 . The same KL weights are applied in all experiments. For DGCNN, we try different numbers of top-k nearest neighbors (knn). the strongest performance (Tab. 12). For the ablation on the prior backbone, we first train the VAE model with the PVCNN architecture, as in all of our main experiments, and then train the prior with different backbones and compare the generation performance. Again, PVCNN performs best as network to implement the latent points diffusion model (Tab. 13). In conclusion, these experiments support choosing PVCNN as our point cloud neural network backbone architecture for implementing LION.</p><p>Note that all ablations were run with similar hyperparameters and the neural networks were generally set up in such a way that different architectures consumed the same GPU memory.  <ref type="table" target="#tab_3">Table 13</ref>: Ablation on the backbone of the latent points prior, on the car category. For DGCNN, we try different numbers of top-k nearest neighbors (knn).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1.3 Ablation Study on Extra Dimensions for Latent Points</head><p>Next, we ablate the extra dimension D h for the latent points in Tab. 14, again using LION models on the car category. We see that D h = 1 provides the overall best performance. With a relatively large number of extra dimensions, it is observed that the 1-NNA scores are getting worse in general. We use D h = 1 for all other experiments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1.4 Ablation Study on SAP Fine-Tuning</head><p>After applying SAP to extract meshes from the generated point clouds, it is possible to again sample points from the meshed surface and evaluate the points' quality with the generation metrics that we used for unconditional generation. We call this process resampling.</p><p>See Tab. 15 for an ablation over the results of resampling from SAP with or without fine-tuning. It also contains the ablation over different numbers of diffuse-denoise steps used to generate the training data for the SAP fine-tuning. Without fine-tuning, the reconstructed mesh has slightly lower quality according to 1-NNA, presumably since the noise within the generated points is different from the noise which the SAP model is trained on. For the "mixed" number of steps entry in the table, SAP randomly chooses one number of diffuse-denoise steps from the above five values at each iteration when producing the training shapes. This setting tends to give an overall good sample quality in terms of the 1-NNA evaluation metrics. We use this setting in all experiments.</p><p>To visually demonstrate the improvement of SAP's mesh reconstruction performance with and without fine-tuning, we show the reconstructed meshes before and after finetuning in <ref type="figure" target="#fig_17">Fig. 20</ref>. The original SAP is trained with clean point clouds augmented with small Gaussian noise. As a result, SAP can handle small scale Gaussian noise in the point clouds. However, it is less robust to the generated points where the noise is different from the Gaussian noise which SAP is trained with. With our proposed fine-tuning, SAP produces smoother surfaces and becomes more robust to the noise distribution in the point clouds generated by LION.   <ref type="table" target="#tab_3">Table 16</ref>: Generation performance metrics on Airplane, Chair, Car. MMD-CD is multiplied with 1 ? 10 3 , MMD-EMD is multiplied with 1 ? 10 2 .</p><p>the MMD and COV metrics, which, however, can be unreliable with respect to quality (see discussion in App. E.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2.1 More Visualizations of the Generated Shapes</head><p>More visualizations of the generated shapes from the LION models trained on airplane, chair and car classes can be found in <ref type="figure" target="#fig_0">Fig. 32, Fig. 33</ref> and <ref type="figure" target="#fig_0">Fig. 34</ref>. LION is generating high quality samples with high diversity. We visualize both point clouds and meshes generated with the SAP that is fine-tuned on the VAE-encoded training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 Unconditional Generation of 13 ShapeNet Classes</head><p>See Tab. <ref type="bibr" target="#b18">19</ref> for the evaluation metrics of the sample quality of LION and other baselines, trained on the 13-class dataset. To evaluate the models, we sub-sample 1,000 shapes from the reference set and sample 1,000 shapes from the models. We can see that LION is better than all baselines under this challenging setting. The results are also consistent with our observations on the single-class models. For baseline comparisons, we picked PVD <ref type="bibr" target="#b21">[46]</ref> and DPM <ref type="bibr" target="#b22">[47]</ref>, because they are also DDM-based  and most relevant. We also picked TreeGAN <ref type="bibr" target="#b5">[6]</ref>, as it is also trained on diverse data in their original paper, and DPF-Net [33], as it represents a modern competitive flow-based method that we could train relatively quickly. We did not run all other baselines that we ran for the single-class models due to limited compute resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3.1 More Visualizations of the Generated Shapes</head><p>See <ref type="figure" target="#fig_0">Fig. 35</ref> for more visualizations of the generated shapes from LION trained on the 13-class data. We visualize both point clouds and meshes generated with the SAP that is fine-tuned on the VAE-encoded training set. LION is again able to generate diverse and high quality shapes even when training in the challenging 13-class setting. We also show in <ref type="figure" target="#fig_0">Fig. 36</ref> additional samples from the 13-class LION model with fixed shape latent variables, where only the latent points are sampled, similar to the experiments in Sec. 5.2 and <ref type="figure" target="#fig_4">Fig. 8</ref>. We again see that the shape latent variables seem to capture overall shape, while the latent points are responsible for generating different details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3.2 Shape Latent Space Visualization</head><p>We project the shape latent variables learned by LION's 13-classes VAE into the 2D plane and create a t-SNE <ref type="bibr" target="#b114">[139]</ref> plot in <ref type="figure" target="#fig_0">Fig. 37</ref>. It can be seen that many categories are separated, such as the rifle, car, watercraft, airplane, telephone, lamp, and display classes. The other categories that are hard to distinguish such as bench and table are mixing a bit, which is also reasonable. This indicates that LION's shape latent is learning to represent the category information, presumably capturing overall shape information, as also supported by our experiments in Sec. 5.2 and <ref type="figure" target="#fig_4">Fig. 8</ref>. Potentially, this also means that the representation learnt by the shape latents could be leveraged for downstream tasks, such as shape classification, similar to Luo and Hu <ref type="bibr" target="#b22">[47]</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4 Unconditional Generation of all 55 ShapeNet Classes</head><p>We train a LION model jointly without any class conditioning on all 55 5 different categories from ShapeNet. The total number of training data is 35,708. Training a single model without conditioning over such a large number of categories is challenging, as the data distribution is highly complex and multimodal. Note that we did on purpose not use class-conditioning to explore LION's scalability to such complex and multimodal datasets. Furthermore, the number of training samples across different categories is imbalanced in this setting: 15 categories have less than 100 training samples and 5 categories have more than 2,000 training samples. We adopt the same model hyperparameters as for the single class LION models here without any tuning.</p><p>We show LION's generated samples in <ref type="figure" target="#fig_18">Fig. 21</ref>: LION synthesizes high-quality and diverse shapes. It can even generate samples from the cap class, which contributes with only 39 training samples, indicating that LION has an excellent mode coverage that even includes the very rare classes. Note that we did not train an SAP model on the 55 classes data. Hence, we only show the generated point clouds in <ref type="figure" target="#fig_18">Fig. 21</ref>.</p><p>This experiment is run primarily as a qualitative scalability test of LION and due to limited compute resources, we did not train baselines here. Furthermore, to the best of our knowledge no previous 3D shape generative models have demonstrated satisfactory generation performance for such diverse and multimodal 3D data without relying on conditioning information. That said, to make sure future works can compare to LION, we report the generation performance over 1,000 samples in Tab. 20.</p><p>We would like to emphasize that hyperparameter tuning and using larger LION models with more parameters will almost certainly significantly improve the results even further. We simply used the single-class training settings out of the box.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.5 Unconditional Generation of ShapeNet's Mug and Bottle Classes</head><p>Next, we explore whether LION can also be trained successfully on very small datasets. To this end, we train LION on the Mug and Bottle classes in ShapeNet. The number of training samples is 149 and 340, respectively, which is much smaller than the common classes like chair, car and airplane. All the hyperparameters are the same as for the models trained on single classes. We show generated shapes in <ref type="figure" target="#fig_19">Fig. 22</ref> and <ref type="figure" target="#fig_0">Fig. 23</ref> (to extract meshes from the generated point clouds, for convenience we are using the SAP model that was trained for the 13-class LION experiment). We find that LION is also able to generate correct mugs and bottles in this very small training data set situation. We report the performance of the generated samples in <ref type="table">Tab</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.6 Unconditional Generation of Animal Shapes</head><p>Furthermore, we also train LION on 553 animal assets from the TurboSquid data repository. <ref type="bibr" target="#b5">6</ref> . The animal data includes shapes of cats, bears, goats, etc. All the hyperparameters are again the same as for the models trained on single classes. See <ref type="figure" target="#fig_21">Fig. 24</ref> for visualizations of the generated shapes from LION trained on the animal data. We visualize both point clouds and meshes. For simplicity, the meshes are generated again with the SAP model that was trained on the ShapeNet 13-classes data. LION is again able to generate diverse and high quality shapes even when training in the challenging low-data setting.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.7 Voxel-guided Synthesis and Denoising</head><p>We additionally add the results for voxel-guided synthesis and denoising experiments on the chair and car categories. In <ref type="figure" target="#fig_1">Fig. 25 and Fig. 27</ref>, we show the reconstruction metrics for different types of input: voxelized input, input with outlier noise, input with uniform noise, and input with normal noise. LION outperforms the other two baselines (PVD and DPM), especially for the voxelized input and the input with outliers, similar to the results presented in the main paper on the airplane class (Sec. 5.4). In <ref type="figure" target="#fig_4">Fig. 26 and Fig. 28, we</ref> show the output quality metrics and the voxel IOU for voxel-guided synthesis on chair, and car category, respectively. LION achieves high output quality while obeying the voxel input constraint well.</p><p>More on Multimodal Visualization. In <ref type="figure" target="#fig_0">Fig. 30</ref>, we show visualizations of multimodal voxel-guided synthesis on different classes. As discussed, we generate various plausible shapes using different numbers of diffuse-denoise steps. We show two different plausible shapes (with the corresponding latent points, and reconstructed meshes) given the same input at each row under different settings. LION is able to capture the structure indicated by the voxel grids: the shapes obey the voxel grid constraints. For example, the tail of the airplane, the legs of the chair, and the back part of the car are consistent with the input. Meanwhile, LION generates diverse and reasonable details in the output shapes.</p><p>See <ref type="figure" target="#fig_5">Fig. 29</ref> for denoising experiments, with comparisons to other baselines. In <ref type="figure" target="#fig_0">Fig. 31</ref>, we also show the visualizations for different classes. LION handles different types of input noises and generates reasonable and diverse details given the same input. See the car examples in the first column for the normal noise, uniform noise and the outlier noise.</p><p>Notice that we applied the SAP model here only for visualizing the meshed output shapes. The SAP model is not fine-tuned on voxel or noisy input data. This is potentially one reason why some reconstructed meshes do not have high quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.7.1 LION vs. Deep Matching Tetrahedra on Voxel-guided Synthesis</head><p>We additionally compare LION's performance on voxel-guided synthesis to Deep Marching Tetrahedra [41] (DMTet) on the airplane category (see Tab. 22). We train and evaluate DMTet with the same data as was used in our voxel-guided shape synthesis experiments (see Sec. 5.4 and Apps. C.2 and E.4). To compute the evaluation metrics on the DMTet output, we randomly sample points on DMTet's output meshes. LION achieves reconstruction results of similar or slighty better quality than DMTet. However, note that DMTet was specifically designed for such reconstruction tasks and is not a general generative model that could synthesize novel shapes from scratch without any guidance chair category (lower is better) when guiding synthesis with voxelized or noisy inputs (using uniform, outlier, and normal noise, see App. F.7).</p><p>x-axes denote number of diffuse-denoise steps.  <ref type="figure">Figure 26</ref>: Voxel-guided generation for chair category. Quality metrics for output points (lower is better) and voxel IOU with respect to input (higher is better). x-axes denote diffuse-denoise steps. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.8 Autoencoding</head><p>We report the auto-encoding performance of LION and other baselines in Tab. 23 for single-class models. We are calculating the reconstruction performance of LION's VAE component. Additional results for the LION model trained on many classes can be found in Tab. 24. LION achieves much better reconstruction performance compared to all other baselines. The hierarchical latent space is expressive enough for the model to perform high quality reconstruction. At the same time, as we have shown above, LION also achieves state-of-the-art generation quality. Moreover, as shown in App. F.3.2, its shape latent space is also still semantically meaningful.  <ref type="figure" target="#fig_3">Figure 27</ref>: Reconstruction metrics with respect to clean inputs for car category (lower is better) when guiding synthesis with voxelized or noisy inputs (using uniform, outlier, and normal noise, see App. F.7).</p><p>x-axes denote number of diffuse-denoise steps.  <ref type="figure" target="#fig_4">Figure 28</ref>: Voxel-guided generation for car category. Quality metrics for output points (lower is better) and voxel IOU with respect to input (higher is better). x-axes denote diffuse-denoise steps.  <ref type="figure" target="#fig_0">Figure 31</ref>: Denoising experiments, on different categories. We run diffuse-denoise in latent space with varying numbers of steps to generate diverse plausible clean shapes.</p><p>in the latent points diffusion model. Optionally running SAP for mesh reconstruction requires an additional ? 2.57 seconds.</p><p>A simple and popular way to accelerate sampling in diffusion models is based on the Denoising Diffusion Implicit Models-framework (DDIM). We show DDIM-sampled shapes in <ref type="figure" target="#fig_0">Fig. 38</ref> and the generation performance in Tab. 25. For all DDIM sampling, we use ? = 0.5 as stochasticity hyperparameter and the quadratic time schedule as also proposed in DDIM <ref type="bibr" target="#b81">[106]</ref>. We also tried deterministic DDIM sampling, but it performed worse (for 50-step sampling). We find that we can produce good-looking shapes in under one second with only 25 synthesis steps. Performance significantly degrades when using ? 10 steps.      To demonstrate the value to artists of being able to synthesize meshes and not just point clouds, we consider a downstream application: We apply Text2Mesh 7 <ref type="bibr" target="#b24">[49]</ref> on some generated meshes from LION to additionally synthesize textures in a text-driven manner, leveraging CLIP <ref type="bibr" target="#b80">[105]</ref>. Optionally, Text2Mesh can also locally refine the mesh and displace vertices for enhanced visual effects. See <ref type="figure" target="#fig_0">Fig. 39</ref> for results where we show different objects made of snow and potato chips, respectively. In <ref type="figure">Fig. 40</ref>, we apply different text prompts on the same generated airplane. We show more diverse   Figure 38: DDIM samples from LION trained on different data. The top two rows show the number of steps and the wall-clock time required for drawing one sample. In general, DDIM sampling with 5 steps fails to generate reasonable shapes and DDIM sampling with more than 10 steps can generate high-quality shapes. With DDIM sampling, we can reduce the time to generate an object from 27.09 seconds (1,000 steps) to less than 1 second (25 steps). The sampling time is computed by calling the prior model and the decoder of LION with batch size as 1, number of points as 2,048.</p><p>results on other categories in <ref type="figure" target="#fig_34">Fig. 41</ref>. Note that this is only possible because of our SAP-based mesh reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.11 Single View Reconstruction and Text-driven Shape Synthesis</head><p>Although our main goal in this work was to develop a strong generative model of 3D shapes, here we qualitatively show how to extend LION to also allow for single view reconstruction (SVR) from RGB data. We render 2D images from the 3D ShapeNet shapes, extracted the images' CLIP <ref type="bibr" target="#b80">[105]</ref> image embeddings, and trained LION's latent diffusion models while conditioning on the shapes' CLIP image embeddings. At test time, we then take a single view 2D image, extract the CLIP image embedding, and generate corresponding 3D shapes, thereby effectively performing SVR. We show SVR results from real RGB data in <ref type="figure" target="#fig_1">Fig. 42, Fig. 45</ref> and <ref type="figure" target="#fig_37">Fig. 46</ref>  <ref type="table" target="#tab_4">Table 25</ref>: Generation performance metrics for our LION model that was trained jointly on all 13 ShapeNet classes without class-conditioning. The performance with different numbers of steps used in the DDIM sampler is reported, as well as the 1,000-step DDPM sampling that was used in the main experiments. We also report the required wall-clock time for the sampling when synthesizing one shape. The results are consistent with our visualization <ref type="figure" target="#fig_0">(Fig. 38)</ref>: The DDIM sampler with 5 steps fails, and with more than 10 steps, it starts to generate good shapes.</p><p>a __ made of snow:</p><p>a __ made of potato chips: from Pix3D 8 <ref type="bibr" target="#b113">[138]</ref> and Redwood 3DScan dataset 9 <ref type="bibr" target="#b112">[137]</ref>, respectively. The RGB images of the cars are from the Cars dataset 10 <ref type="bibr" target="#b111">[136]</ref>. For each input image, LION is able to generate different feasible <ref type="bibr" target="#b7">8</ref> We downloaded the data from https://github.com/xingyuansun/pix3d. The Pix3D dataset is licensed under a Creative Commons Attribution 4.0 International License. <ref type="bibr" target="#b8">9</ref> We downloaded the Redwood 3DScan dataset (public domain) from https://github.com/isl-org/ redwood-3dscan. <ref type="bibr" target="#b9">10</ref> We shapes, showing LION's ability to perform multi-modal generation. Qualitatively, our results appear to be of similar quality as the results of PVD <ref type="bibr" target="#b21">[46]</ref> for that task, and at least as good or better than the results of AutoSDF <ref type="bibr">[30]</ref>. Note that this approach only requires RGB images. In contrast, PVD requires RGB-D images, including depth. Hence, our approach can be considered more flexible. Using CLIP's text encoder, our method additionally allows for text-guided generation as demonstrated in <ref type="figure" target="#fig_0">Fig. 43</ref> and <ref type="figure">Fig. 44</ref>. Overall, this approach is inspired by CLIP-Forge <ref type="bibr">[34]</ref>. Note that this is a simple qualitative demonstration of LION's extendibility. We did not perform any hyperparameter tuning here and believe that these results could be improved with more careful tuning and training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.12 More Shape Interpolations</head><p>We show more shape interpolation results for single-class LION models in Figs. <ref type="bibr" target="#b22">47,</ref><ref type="bibr" target="#b23">48,</ref><ref type="bibr" target="#b24">49</ref>, and the many-class LION model in Figs. 50, 51. We can see that LION is able to interpolate two shapes from different classes smoothly. For example, when it tries to interpolate a chair and a table, it starts to make the chair wider and wider, and gradually removes the back of the chair. When it tries to interpolate an airplane and a chair, it starts with making the wings more chair-like, and reduces the size of the rest of the body. The shapes in the middle of the interpolation provide a smooth and reasonable transition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.12.1 Shape Interpolation with PVD and DPM</head><p>To be able to better judge the performance of LION's shape interpolation results, we now also show shape interpolations with PVD <ref type="bibr" target="#b21">[46]</ref> and DPM <ref type="bibr" target="#b22">[47]</ref> in <ref type="figure" target="#fig_1">Fig. 52</ref> and <ref type="figure" target="#fig_0">Fig. 53</ref>, respectively. We apply the spherical interpolation (see Sec. C.3) on the noise inputs for both PVD and DPM. DPM leverages a Normalizing Flow, which already offers deterministic generation given the noise inputs of the Flow's normal prior. For PVD, just like for LION, we again use the diffusion model's ODE formulation to obtain deterministic generation paths. In other words, to avoid confusion, in both cases we are interpolating in the normal prior distribution, just like for LION.</p><p>Although PVD is also able to interpolate two shapes, the transition from the source shapes to the target shapes appear less smooth than for LION; see, for example, the chair interpolation results of PVD. Furthermore, DPM's generated shape interpolations appear fairly noisy. When interpolating        </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3</head><label>3</label><figDesc>Figure 3: Generated shapes (top: point clouds, bottom: corresponding meshes) from LION trained jointly over 13 classes of ShapeNet-vol without conditioning (Sec. 5.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Reconstructing a mesh from LION's generated points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Interpolating different shapes by interpolating their encodings in the standard Gaussian priors of LION's latent DDMs (details in App. C.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Samples from our unconditional 13-class model:In each column, we use the same global shape latent z0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Generated point clouds from LION trained jointly over 55 classes of ShapeNet-vol (no conditioning).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :</head><label>10</label><figDesc>Samples from LION trained on ShapeNet's Mug and Bottle classes, and on Turbosquid animals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>an airplane made of strawberries an airplane made of fabric leather a chair made of wood a car made of rusty metal a car made of brick a denim fabric animal</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 16 :</head><label>16</label><figDesc>Text-driven shape generation of chairs and cars with LION. Bottom row is the text prompt used as input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 15 :</head><label>15</label><figDesc>25-step DDIM<ref type="bibr" target="#b81">[106]</ref> samples (0.89 seconds per shape).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 17 :</head><label>17</label><figDesc>Single view 3D reconstructions of a car from an RGB image. LION can generate multiple plausible outputs using our diffuse-denoise technique.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 18 :</head><label>18</label><figDesc>LION's encoder networks can be fine-tuned to process voxel or noisy point cloud inputs, which can provide guidance to the generative model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>?</head><label></label><figDesc>Multilayer perceptron (MLP), point-voxel convolution (PVC), set abstraction (SA), and feature propagation (FP) represent the building modules for our PVCNNs. The Grouper block (in SA) consists of the sampling layer and grouping layer introduced by PointNet++ [82].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>? 1-nearest neighbor accuracy (1-NNA): To overcome the drawbacks of COV and MMD, Yang et al. [31] proposed to use 1-NNA as a metric to evaluate point cloud generative models:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>? r-GAN, l-GAN<ref type="bibr" target="#b1">[2]</ref>: https://github.com/optas/latent_3d_points (MIT License)? PointFlow [31]: https://github.com/stevenygd/PointFlow (MIT License) ? SoftFlow [32]: https://github.com/ANLGBOY/SoftFlow ? Set-VAE [29]: https://github.com/jw9730/setvae (MIT License) ? DPF-NET [33]: https://github.com/Regenerator/dpf-nets ? DPM [47]: https://github.com/luost26/diffusion-point-cloud (MIT License) ? PVD [46]: https://github.com/alexzhou907/PVD (MIT License) ? ShapeGF [45]: https://github.com/RuojinCai/ShapeGF (MIT License) ? SP-GAN [19]: https://github.com/liruihui/sp-gan (MIT License) ? PDGN [52]: https://github.com/fpthink/PDGN (MIT License) ? IM-GAN [7]: https://github.com/czq142857/implicit-decoder (MIT license) and https://github.com/czq142857/IM-NET-pytorch (MIT license) ? GCA [43]: https://github.com/96lives/gca (MIT license)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 20 :</head><label>20</label><figDesc>Mesh reconstruction results from SAP with the same point cloud inputs. Top: without fine-tuning. Bottom: with fine-tuning on LION. We see that fine-tuning significantly improves the meshes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 21 :</head><label>21</label><figDesc>Generated shapes from our LION model that was trained jointly on all 55 ShapeNet classes without class-conditioning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 22 :</head><label>22</label><figDesc>Generated shapes from the LION model trained on ShapeNet's Mug category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 23 :</head><label>23</label><figDesc>Generated shapes from the LION model trained on ShapeNet's Bottle category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 24 :</head><label>24</label><figDesc>Generated shapes from the animal class LION model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>3 )Figure 25 :</head><label>325</label><figDesc>Reconstruction metrics with respect to clean inputs for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 32 :</head><label>32</label><figDesc>Generated shapes from the airplane class LION model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure 33 :</head><label>33</label><figDesc>Generated shapes from the chair class LION model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Figure 34 :</head><label>34</label><figDesc>Generated shapes from the car class LION model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Figure 35 :</head><label>35</label><figDesc>Generated shapes from the many-class LION model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Figure 36 :</head><label>36</label><figDesc>For each group of shapes, we freeze the shape latent variables, and sample different latent points to generate the other shapes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>F. 10</head><label>10</label><figDesc>Per-sample Text-driven Texture Synthesis</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>Figure 37 :</head><label>37</label><figDesc>Shape latent t-SNE plot for the many-class LION model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>Figure 39 :</head><label>39</label><figDesc>Text2Mesh results of airplane, chair, car, animal. The original mesh is generated by LION.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head>Figure 41 :</head><label>41</label><figDesc>downloaded the Cars dataset from http://ai.stanford.edu/~jkrause/cars/car_dataset. html. The Cars dataset is licensed under the ImageNet License: https://image-net.org/download.php strawberries watermelon glod fabric leather floor tiles jacobean fabric Figure 40: Text2Mesh results with text prompt an airplane made of __. All prompts are applied on the same generated shapes. The original mesh is generated by LION. Text2Mesh results with text prompt: a chair made of __ (top row), a car made of __ (middle row) and an __ animal (bottom row). Each prompt is applied on different generated shapes. The original mesh is generated by LION.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head>Figure 42 :Figure 44 :</head><label>4244</label><figDesc>Single view reconstructions of chairs from RGB images. For each input image, LION can generate multiple plausible outputs. comfortable sofa narrow chair office chair armchair Figure 43: Text-driven shape generation of chairs with LION. Bottom row is the text input.very different shapes using the 13-classes models, both PVD and DPM essentially break down and do not produce sensible outputs anymore. All shapes along the interpolation paths appear noisy.In contrast, LION generally produces coherent interpolations, even when using the multimodal model that was trained on 13 ShapeNet classes (see Figs. 47, 48, 49 and 50 for LION interpolations for reference). Text-driven shape generation of cars with LION. Bottom row is the text input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head>Figure 45 :</head><label>45</label><figDesc>Single view reconstructions of cars from RGB images. For each input image, LION can generate multiple plausible outputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><head>Figure 46 :</head><label>46</label><figDesc>More single view reconstructions of cars from RGB images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_38"><head>Figure 47 :</head><label>47</label><figDesc>More interpolation results from LION models trained on the airplane class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_39"><head>Figure 48 :</head><label>48</label><figDesc>More interpolation results from LION models trained on the chair class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_40"><head>Figure 49 :Figure 50 :Figure 51 :</head><label>495051</label><figDesc>More interpolation results from LION models trained on the car class. More interpolation results from LION trained on many classes. More interpolation results from LION trained on many classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_41"><head>Figure 52 :</head><label>52</label><figDesc>Shape interpolation results of PVD<ref type="bibr" target="#b21">[46]</ref>. From top to bottom: PVD trained on airplane, chair, car, 13 classes of ShapeNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_42"><head>Figure 53 :</head><label>53</label><figDesc>Shape interpolation results of DPM<ref type="bibr" target="#b22">[47]</ref>. From top to bottom: DPM trained on airplane, chair, car, 13 classes of ShapeNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Generation metrics (1-NNA?) on airplane, chair, car categories from ShapeNet dataset from PointFlow [31]. Training and test data normalized globally into [-1, 1].</figDesc><table><row><cell></cell><cell>Airplane</cell><cell>Chair</cell><cell>Car</cell></row><row><cell></cell><cell cols="3">CD EMD CD EMD CD EMD</cell></row><row><cell>r-GAN [2]</cell><cell cols="3">98.40 96.79 83.69 99.70 94.46 99.01</cell></row><row><cell cols="4">l-GAN (CD) [2] 87.30 93.95 68.58 83.84 66.49 88.78</cell></row><row><cell cols="4">l-GAN (EMD) [2] 89.49 76.91 71.90 64.65 71.16 66.19</cell></row><row><cell>PointFlow [31]</cell><cell cols="3">75.68 70.74 62.84 60.57 58.10 56.25</cell></row><row><cell>SoftFlow [32]</cell><cell cols="3">76.05 65.80 59.21 60.05 64.77 60.09</cell></row><row><cell>SetVAE [29]</cell><cell cols="3">76.54 67.65 58.84 60.57 59.94 59.94</cell></row><row><cell>DPF-Net [33]</cell><cell cols="3">75.18 65.55 62.00 58.53 62.35 54.48</cell></row><row><cell>DPM [47]</cell><cell cols="3">76.42 86.91 60.05 74.77 68.89 79.97</cell></row><row><cell>PVD [46]</cell><cell cols="3">73.82 64.81 56.26 53.32 54.55 53.83</cell></row><row><cell>LION (ours)</cell><cell cols="3">67.41 61.23 53.70 52.34 53.41 51.14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Generation results (1-NNA?) on ShapeNet dataset from PointFlow [31]. All data normalized individually into [-1, 1]. ] 97.53 99.88 88.37 96.37 89.77 94.89 ShapeGF [45] 81.23 80.86 58.01 61.25 61.79 57.24 SP-GAN [19] 94.69 93.95 72.58 83.69 87.36 85.94 PDGN [52] 94.94 91.73 71.83 79.00 89.35 87.22 GCA [43] 88.15 85.93 64.27 64.50 70.45 64.20 LION (ours) 76.30 67.04 56.50 53.85 59.52 49.29</figDesc><table><row><cell>Airplane</cell><cell>Chair</cell><cell>Car</cell></row><row><cell cols="3">CD EMD CD EMD CD EMD</cell></row><row><cell>TreeGAN [6</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results (1-NNA?) on ShapeNet-vol.</figDesc><table><row><cell>Airplane</cell><cell>Chair</cell><cell>Car</cell></row><row><cell cols="3">CD EMD CD EMD CD EMD</cell></row><row><cell cols="3">IM-GAN [7] 79.70 77.85 57.09 58.20 88.92 84.58</cell></row><row><cell cols="3">DPM [47] 83.04 96.04 61.96 74.96 77.30 87.12</cell></row><row><cell cols="3">PVD [46] 66.46 56.06 61.89 57.90 64.49 55.74</cell></row><row><cell cols="3">LION (ours) 53.47 53.84 52.07 48.67 54.81 50.53</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell cols="3">Generation results (1-</cell></row><row><cell cols="3">NNA?) of LION trained jointly</cell></row><row><cell cols="3">on 13 classes of ShapeNet-vol.</cell></row><row><cell>Model</cell><cell>CD</cell><cell>EMD</cell></row><row><cell>TreeGAN [6]</cell><cell cols="2">96.80 96.60</cell></row><row><cell cols="3">PointFlow [31] 63.25 66.05</cell></row><row><cell>ShapeGF [45]</cell><cell cols="2">55.65 59.00</cell></row><row><cell>SetVAE [29]</cell><cell cols="2">79.25 95.25</cell></row><row><cell>PDGN [52]</cell><cell cols="2">71.05 86.00</cell></row><row><cell>DPF-Net [33]</cell><cell cols="2">67.10 64.75</cell></row><row><cell>DPM [47]</cell><cell cols="2">62.30 86.50</cell></row><row><cell>PVD [46]</cell><cell cols="2">58.65 57.85</cell></row><row><cell>LION (ours)</cell><cell cols="2">51.85 48.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>[ 21 ]</head><label>21</label><figDesc>Zhiqin Chen, Vladimir G. Kim, Matthew Fisher, Noam Aigerman, Hao Zhang, and Siddhartha Chaudhuri. Decor-gan: 3d shape detailization by conditional refinement. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. [22] Cheng Wen, Baosheng Yu, and Dacheng Tao. Learning progressive point embeddings for 3d point cloud generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10266-10275, 2021. Yu Liu, and Sanja Fidler. Deep marching tetrahedra: a hybrid representation for high-resolution 3d shape synthesis. In Advances in Neural Information Processing Systems (NeurIPS), 2021. [42] Kangxue Yin, Jun Gao, Maria Shugrina, Sameh Khamis, and Sanja Fidler. 3dstylenet: Creating 3d shapes with geometric and texture style variations. In Proceedings of International Conference on Computer Vision (ICCV), 2021. Shen Liu, and Matthias Zwicker. Unsupervised learning of fine structure generation for 3d point clouds by 2d projection matching. arXiv preprint arXiv:2108.03746, 2021. Applications and Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 3.2 LION's Advantages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Single-Class 3D Shape Generation . . . . . . . . . . . . . . . . . . . . . . . . . . Shape Interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 C.4 Mesh Reconstruction with Shape As Points . . . . . . . . . . . . . . . . . . . . . 24 C.4.1 Background on Shape As Points . . . . . . . . . . . . . . . . . . . . . . . 24 C.4.2 Incorporating Shape As Points in LION . . . . . . . . . . . . . . . . . . . 25 VAE Backbone . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 D.2 Shape Latent DDM Prior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 D.3 Latent Points DDM Prior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 E.3 Details for Unconditional Generation . . . . . . . . . . . . . . . . . . . . . . . . . 32 E.4 Details for Voxel-guided Synthesis . . . . . . . . . . . . . . . . . . . . . . . . . . 33 E.5 Details for Denoising Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . 33 E.6 Details for Fine-tuning SAP on LION . . . . . . . . . . . . . . . . . . . . . . . . 34 E.7 Training Times . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 E.8 Used Codebases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 E.9 Computational Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 Ablation Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 F.1.1 Ablation Study on LION's Hierarchical Architecture . . . . . . . . . . . . 36 More Visualizations of the Generated Shapes . . . . . . . . . . . . . . . . 40 F.3.2 Shape Latent Space Visualization . . . . . . . . . . . . . . . . . . . . . . 40 F.4 Unconditional Generation of all 55 ShapeNet Classes . . . . . . . . . . . . . . . . 41 F.5 Unconditional Generation of ShapeNet's Mug and Bottle Classes . . . . . . . . . . 42 F.6 Unconditional Generation of Animal Shapes . . . . . . . . . . . . . . . . . . . . . 42 F.7 Voxel-guided Synthesis and Denoising . . . . . . . . . . . . . . . . . . . . . . . . 45 F.7.1 LION vs. Deep Matching Tetrahedra on Voxel-guided Synthesis . . . . . . 45 F.8 Autoencoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 F.9 Synthesis Time and DDIM Sampling . . . . . . . . . . . . . . . . . . . . . . . . . 47 F.10 Per-sample Text-driven Texture Synthesis . . . . . . . . . . . . . . . . . . . . . . 53 F.11 Single View Reconstruction and Text-driven Shape Synthesis . . . . . . . . . . . . 54 F.12 More Shape Interpolations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 F.12.1 Shape Interpolation with PVD and DPM . . . . . . . . . . . . . . . . . . . 56</figDesc><table><row><cell>Contents</cell><cell></cell></row><row><cell>1 Introduction</cell><cell>1</cell></row><row><cell>2 Background</cell><cell>2</cell></row><row><cell>3 Hierarchical Latent Point Diffusion Models</cell><cell>3</cell></row><row><cell>3.1 4 Related Work 5 Experiments 5.1 6 Conclusions References A Funding Disclosure D Implementation D.1 F Additional Experimental Results F.1</cell><cell>6 7 10 11 20 25 35</cell></row></table><note>[23] Zekun Hao, Arun Mallya, Serge Belongie, and Ming-Yu Liu. GANcraft: Unsupervised 3D Neural Rendering of Minecraft Worlds. In ICCV, 2021.[24] Or Litany, Alex Bronstein, Michael Bronstein, and Ameesh Makadia. Deformable shape completion with graph convolutional autoencoders. CVPR, 2018.[25] Qingyang Tan, Lin Gao, Yu-Kun Lai, and Shihong Xia. Variational autoencoders for deforming 3d mesh models. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5841-5850, 2018.[26] Kaichun Mo, Paul Guerrero, Li Yi, Hao Su, Peter Wonka, Niloy Mitra, and Leonidas J Guibas. Struc- turenet: Hierarchical graph networks for 3d shape generation. arXiv preprint arXiv:1908.00575, 2019.[27] Lin Gao, Jie Yang, Tong Wu, Yu-Jie Yuan, Hongbo Fu, Yu-Kun Lai, and Hao(Richard) Zhang. SDM-NET: Deep generative network for structured deformable mesh. ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH Asia 2019), 38(6):243:1-243:15, 2019.[28] Lin Gao, Tong Wu, Yu-Jie Yuan, Ming-Xian Lin, Yu-Kun Lai, and Hao Zhang. Tm-net: Deep generative networks for textured meshes. ACM Transactions on Graphics (TOG), 40(6):263:1-263:15, 2021.[29] Jinwoo Kim, Jaehoon Yoo, Juho Lee, and Seunghoon Hong. Setvae: Learning hierarchical composition for generative modeling of set-structured data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 15059-15068, June 2021.[30] Paritosh Mittal, Yen-Chi Cheng, Maneesh Singh, and Shubham Tulsiani. AutoSDF: Shape priors for 3d completion, reconstruction and generation. In CVPR, 2022.[31] Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge Belongie, and Bharath Hariharan. PointFlow: 3D point cloud generation with continuous normalizing flows. In ICCV, 2019.[32] Hyeongju Kim, Hyeonseung Lee, Woo Hyun Kang, Joun Yeop Lee, and Nam Soo Kim. SoftFlow: Probabilistic framework for normalizing flow on manifolds. In NeurIPS, 2020.[33] Roman Klokov, Edmond Boyer, and Jakob Verbeek. Discrete point flow networks for efficient point cloud generation. In ECCV, 2020.[34] Aditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang, Chin-Yi Cheng, and Marco Fumero. Clip- forge: Towards zero-shot text-to-shape generation. arXiv preprint arXiv:2110.02624, 2021.[35] Yongbin Sun, Yue Wang, Ziwei Liu, Joshua E Siegel, and Sanjay E Sarma. Pointgrow: Autoregressively learned point cloud generation with self-attention. In Winter Conference on Applications of Computer Vision, 2020.[36] Charlie Nash, Yaroslav Ganin, S. M. Ali Eslami, and Peter W. Battaglia. Polygen: An autoregressive generative model of 3d meshes. ICML, 2020.[37] Wei-Jan Ko, Hui-Yu Huang, Yu-Liang Kuo, Chen-Yi Chiu, Li-Heng Wang, and Wei-Chen Chiu. Rpg: Learning recursive point cloud generation. arXiv preprint arXiv:2105.14322, 2021.[38] Moritz Ibing, Gregor Kobsik, and Leif Kobbelt. Octree transformer: Autoregressive 3d shape generation on hierarchically structured sequences. arXiv preprint arXiv:2111.12480, 2021.[39] Jianwen Xie, Zilong Zheng, Ruiqi Gao, Wenguan Wang, Zhu Song-Chun, and Ying Nian Wu. Learning descriptor networks for 3d shape synthesis and analysis. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.[40] Jianwen Xie, Yifei Xu, Zilong Zheng, Ruiqi Gao, Wenguan Wang, Zhu Song-Chun, and Ying Nian Wu. Generative pointnet: Deep energy-based learning on unordered point sets for 3d generation, reconstruction and classification. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.[41] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-[43] Dongsu Zhang, Changwoon Choi, Jeonghwan Kim, and Young Min Kim. Learning to generate 3d shapes with generative cellular automata. In International Conference on Learning Representations, 2021.[44] Chen Chao, Zhizhong Han, Yu-7 5.2 Many-class Unconditional 3D Shape Generation . . . . . . . . . . . . . . . . . . . 8 5.3 Training LION on Small Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . 8 5.4 Voxel-guided Shape Synthesis and Denoising with Fine-tuned Encoders . . . . . . 9 5.5 Sampling Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 5.6 Overview of Additional Experiments in Appendix . . . . . . . . . . . . . . . . . . 10B Continuous-Time Diffusion Models and Probability Flow ODE Sampling 20 C Technical Details on LION's Applications and Extensions 21 C.1 Diffuse-Denoise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 C.2 Encoder Fine-Tuning for Voxel-Conditioned Synthesis and Denoising . . . . . . . 21 C.3D.4 Two-stage Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 E Experiment Details 28 E.1 Different Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 E.2F.1.2 Ablation Study on the Backbone Point Cloud Processing Network Architecture 36 F.1.3 Ablation Study on Extra Dimensions for Latent Points . . . . . . . . . . . 37 F.1.4 Ablation Study on SAP Fine-Tuning . . . . . . . . . . . . . . . . . . . . . 37 F.2 Single-Class Unconditional Generation . . . . . . . . . . . . . . . . . . . . . . . 38 F.2.1 More Visualizations of the Generated Shapes . . . . . . . . . . . . . . . . 39 F.3 Unconditional Generation of 13 ShapeNet Classes . . . . . . . . . . . . . . . . . . 39 F.3.1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>x A and x B in LION, we first encode them into z A 0 and h A 0 , as well as z B 0 and h B 0 . Now, using the generative ODE (see App. B) we further encode these latents into the DDMs' prior distributions, resulting in encodings z A 1 and h A 1 , as well as z B 1 and h B ? [0, 1] back to point cloud space and obtain the interpolated point clouds x s , which we can optionally convert into meshes with SAP.</figDesc><table><row><cell>?</cell><cell>sz A 1 +</cell><cell>?</cell><cell>1 ? sz B</cell></row></table><note>1 (note that we need to correctly capture the conditioning when using ? (h t , z 0 , t) in the generative ODE for h t ). Next, we first interpolate the shape latent DDM encodings z s 1 =1 and use the generative ODE to deterministically generate all z s 0 along the interpolation path. Then, we also interpolate the latent point DDM encodings h s 1 = ? sh A 1 + ? 1 ? sh B 1 and, conditioned on the corresponding z s 0 along the interpolation path, also generate deterministically all h s 0 along the interpolation path using the generative ODE. Finally, we can decode all z s 0 and h s 0 along the interpolation s</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Shape Latent Encoder Architecture Hyperparameters.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Input: point clouds (2048 ? 3), shape latent (1 ? 128) Output: latent points (2048 ? 2 ? (3 + D h ))</figDesc><table><row><cell></cell><cell>SA 1</cell><cell>SA 2</cell><cell>SA 3</cell><cell>SA 4</cell></row><row><cell># PVC layers</cell><cell>2</cell><cell>1</cell><cell>1</cell><cell>-</cell></row><row><cell># PVC hidden dimension</cell><cell>32</cell><cell>64</cell><cell>128</cell><cell>-</cell></row><row><cell># PVC voxel grid size</cell><cell>32</cell><cell>16</cell><cell>8</cell><cell>-</cell></row><row><cell># Grouper center</cell><cell>1024</cell><cell>256</cell><cell>64</cell><cell>16</cell></row><row><cell># Grouper radius</cell><cell>0.1</cell><cell>0.2</cell><cell>0.4</cell><cell>0.8</cell></row><row><cell># Grouper neighbors</cell><cell>32</cell><cell>32</cell><cell>32</cell><cell>32</cell></row><row><cell># MLP layers</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>3</cell></row><row><cell># MLP output dimension</cell><cell>32,32</cell><cell cols="3">64,128 128,256 128,128,128</cell></row><row><cell>Use attention</cell><cell>False</cell><cell>True</cell><cell>False</cell><cell>False</cell></row><row><cell># Attention dimension</cell><cell>-</cell><cell>128</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">Global attention layer, hidden dimension: 256</cell><cell></cell></row><row><cell></cell><cell>FP 1</cell><cell>FP 2</cell><cell>FP 3</cell><cell>FP 4</cell></row><row><cell># MLP layers</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>3</cell></row><row><cell cols="5"># MLP output dimension 128,128 128,128 128,128 128,128,64</cell></row><row><cell># PVC layers</cell><cell>3</cell><cell>3</cell><cell>2</cell><cell>2</cell></row><row><cell># PVC hidden dimension</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>64</cell></row><row><cell># PVC voxel grid size</cell><cell>8</cell><cell>8</cell><cell>16</cell><cell>32</cell></row><row><cell>Use attention</cell><cell>False</cell><cell>True</cell><cell>False</cell><cell>False</cell></row><row><cell># Attention dimension</cell><cell>-</cell><cell>128</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">MLP: (64, 128)</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Dropout</cell><cell></cell><cell></cell></row><row><cell cols="3">Linear: (128, 2 ? (3 + D h ))</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 6 :</head><label>6</label><figDesc>Latent Point Encoder Architecture Hyperparameters.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>Input feature size: latent points (2048 ? (3 + D h )), shape latent(1 ? 128)    </figDesc><table><row><cell cols="4">Output feature size: point clouds (2048 ? 3)</cell><cell></cell></row><row><cell></cell><cell>SA 1</cell><cell>SA 2</cell><cell>SA 3</cell><cell>SA 4</cell></row><row><cell># PVC layers</cell><cell>2</cell><cell>1</cell><cell>1</cell><cell>-</cell></row><row><cell># PVC hidden dimension</cell><cell>32</cell><cell>64</cell><cell>128</cell><cell>-</cell></row><row><cell># PVC voxel grid size</cell><cell>32</cell><cell>16</cell><cell>8</cell><cell>-</cell></row><row><cell># Grouper center</cell><cell>1024</cell><cell>256</cell><cell>64</cell><cell>16</cell></row><row><cell># Grouper radius</cell><cell>0.1</cell><cell>0.2</cell><cell>0.4</cell><cell>0.8</cell></row><row><cell># Grouper neighbors</cell><cell>32</cell><cell>32</cell><cell>32</cell><cell>32</cell></row><row><cell># MLP layers</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>3</cell></row><row><cell># MLP output dimension</cell><cell>32,64</cell><cell cols="3">64,128 128,256 128,128,128</cell></row><row><cell>Use attention</cell><cell>False</cell><cell>True</cell><cell>False</cell><cell>False</cell></row><row><cell># Attention dimension</cell><cell>-</cell><cell>128</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">Global attention layer, hidden dimension: 256</cell><cell></cell></row><row><cell></cell><cell>FP 1</cell><cell>FP 2</cell><cell>FP 3</cell><cell>FP 4</cell></row><row><cell># MLP layers</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>3</cell></row><row><cell cols="4"># MLP output dimension 128,128 128,128 128,128</cell><cell>128,128,64</cell></row><row><cell># PVC layers</cell><cell>3</cell><cell>3</cell><cell>2</cell><cell>2</cell></row><row><cell># PVC hidden dimension</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>64</cell></row><row><cell># PVC voxel grid size</cell><cell>8</cell><cell>8</cell><cell>16</cell><cell>32</cell></row><row><cell>Use attention</cell><cell>False</cell><cell>False</cell><cell>False</cell><cell>False</cell></row><row><cell></cell><cell cols="2">MLP: (64, 128)</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Dropout</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Linear: (128, 3)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 7 :</head><label>7</label><figDesc>Decoder Architecture Hyperparameters.</figDesc><table><row><cell>Input: latent points (2048 ? (3 + D h )) at t</cell></row><row><cell>shape latent (1 ? 128)</cell></row><row><cell>Output: 2048 ? (3 + D h )</cell></row><row><cell>Time embedding layer:</cell></row><row><cell>Sinusoidal embedding dimension = 128</cell></row><row><cell>Linear (128, 512)</cell></row><row><cell>LeakyReLU (0.1)</cell></row><row><cell>Linear (2048)</cell></row><row><cell>Linear (128, 2048)</cell></row><row><cell>Addition (linear output, time embedding)</cell></row><row><cell>ResSE (2048, 2048) x 8</cell></row><row><cell>Linear (2048, 128)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 8 :</head><label>8</label><figDesc>Shape Latent DDM Architecture Hyperparameters.of Tab. 1 in the main text: In fact, this kind of global normalization using standard deviation does not result in [?1, 1] point coordinate bounds, but the coordinate values usually extend beyond that.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>Input: latent points (2048 ? (3 + D h )) at t, shape latent (1 ? 128)Output: 2048 ? (3 + D h )</figDesc><table><row><cell cols="3">Time embedding Layer:</cell><cell></cell><cell></cell></row><row><cell cols="4">Sinusoidal embedding dimension = 64</cell><cell></cell></row><row><cell></cell><cell cols="2">Linear (64, 64)</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">LeakyReLU(0.1)</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Linear (64, 64)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>SA 1</cell><cell>SA 2</cell><cell>SA 3</cell><cell>SA 4</cell></row><row><cell># PVC layers</cell><cell>2</cell><cell>1</cell><cell>1</cell><cell>-</cell></row><row><cell># PVC hidden dimension</cell><cell>32</cell><cell>64</cell><cell>128</cell><cell>-</cell></row><row><cell># PVC voxel grid size</cell><cell>32</cell><cell>16</cell><cell>8</cell><cell>-</cell></row><row><cell># Grouper center</cell><cell>1024</cell><cell>256</cell><cell>64</cell><cell>16</cell></row><row><cell># Grouper radius</cell><cell>0.1</cell><cell>0.2</cell><cell>0.4</cell><cell>0.8</cell></row><row><cell># Grouper neighbors</cell><cell>32</cell><cell>32</cell><cell>32</cell><cell>32</cell></row><row><cell># MLP layers</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>3</cell></row><row><cell># MLP output dimension</cell><cell>32,64</cell><cell cols="3">64,128 128,128 128,128,128</cell></row><row><cell>Use attention</cell><cell>False</cell><cell>True</cell><cell>False</cell><cell>False</cell></row><row><cell># Attention dimension</cell><cell>-</cell><cell>128</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">Global Attention Layer, hidden dimension: 256</cell><cell></cell></row><row><cell></cell><cell>FP 1</cell><cell>FP 2</cell><cell>FP 3</cell><cell>FP 4</cell></row><row><cell># MLP layers</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>3</cell></row><row><cell cols="5"># MLP output dimension 128,128 128,128 128,128 128,128,64</cell></row><row><cell># PVC layers</cell><cell>3</cell><cell>3</cell><cell>2</cell><cell>2</cell></row><row><cell># PVC hidden dimension</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>64</cell></row><row><cell># PVC voxel grid size</cell><cell>8</cell><cell>8</cell><cell>16</cell><cell>32</cell></row><row><cell>Use attention</cell><cell>False</cell><cell>False</cell><cell>False</cell><cell>False</cell></row><row><cell></cell><cell cols="2">MLP: (64, 128)</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Dropout</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Linear: (128, 3)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 9 :</head><label>9</label><figDesc>Latent Point DDM Architecture Hyperparameters.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head></head><label></label><figDesc>100] (Sec. 1.2 in the Supplementary Material). This dataset variant is denoted as ShapeNet-vol. This data is per-shape normalized, i.e., the points' coordinates are bounded by [?1, 1]. To combine LION and SAP, we also train LION on the same data used by the SAP model. Therefore, we report sample quality of LION as well as the most relevant baselines DPM, PVD, and also IM-GAN (which synthesizes shapes as SDFs) also on this dataset variant. The number of training shapes is 2,832, 1,272, 1,101, 5,248, 4,746, 767, 1,624, 1,134, 1,661, 2,222, 5,958, 737, and 1,359 for airplane, bench, cabinet, car, chair, display, lamp, loudspeaker, rifle, sofa, table, telephone, and watercraft, respectively. The number of shapes in the reference set is 404, 181, 157, 749, 677, 109, 231, 161, 237, 317, 850, 105, and 193 for airplane, bench, cabinet, car, chair, display, lamp, loudspeaker, rifle, sofa, table, telephone, and watercraft, respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 10 :</head><label>10</label><figDesc></figDesc><table /><note>LION's Training and Model Hyperparameters.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 11 :</head><label>11</label><figDesc>Ablation study over LION's hierarchical architecture, on the car category.F.1.2 Ablation Study on the Backbone Point Cloud Processing Network ArchitectureWe ablate different point cloud processing neural network architectures used for implementing LION's encoder, decoder and the latent points prior. Results are shown in Tab.12 and Tab. 13, using   </figDesc><table /><note>the LION model on the car category as in the other ablation studies. We choose three different popular backbones used in the point cloud processing literature: Point-Voxel CNN (PVCNN) [80], Dynamic Graph CNN (DGCNN)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head></head><label></label><figDesc>.75 50.00 56.53 53.41 51.14 DGCNN (knn=20) 1.05 0.80 41.19 52.27 66.48 55.82 DGCNN (knn=10) 1.02 0.80 42.33 50.57 66.34 53.41 PointTransformer 3.67 3.02 10.51 10.51 99.72 99.86</figDesc><table><row><cell>Backbone</cell><cell>MMD?</cell><cell>COV? (%)</cell><cell>1-NNA?</cell></row><row><cell></cell><cell cols="3">CD EMD CD EMD CD EMD</cell></row><row><cell>PVCNN</cell><cell>0.91 0</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 14 :</head><label>14</label><figDesc>Ablation on the number of extra dimensions D h of the latent points, on the car category.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_30"><head>Table 15 :</head><label>15</label><figDesc>Ablation over number of steps used to generate the training data for SAP, on the chair class. The model is trained on the ShapeNet-vol dataset. 3,000 points are sampled from LION to generate meshes with SAP. However, during evaluation after resampling, 2,048 points are used as in all other experiments where we calculate quantitative performance metrics.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">MMD?</cell><cell cols="2">COV? (%)</cell><cell cols="2">1-NNA? (%)</cell></row><row><cell>Category</cell><cell>Model</cell><cell>CD</cell><cell>EMD</cell><cell>CD</cell><cell>EMD</cell><cell>CD</cell><cell>EMD</cell></row><row><cell></cell><cell>train set</cell><cell cols="6">0.218 0.373 46.91 52.10 64.44 64.07</cell></row><row><cell></cell><cell>r-GAN [2]</cell><cell cols="6">0.447 2.309 30.12 14.32 98.40 96.79</cell></row><row><cell>Airplane</cell><cell>l-GAN (CD) [2]</cell><cell cols="6">0.340 0.583 38.52 21.23 87.30 93.95</cell></row><row><cell></cell><cell cols="7">l-GAN (EMD) [2] 0.397 0.417 38.27 38.52 89.49 76.91</cell></row><row><cell></cell><cell>PointFlow [31]</cell><cell cols="6">0.224 0.390 47.90 46.41 75.68 70.74</cell></row><row><cell></cell><cell>SoftFlow [32]</cell><cell cols="6">0.231 0.375 46.91 47.90 76.05 65.80</cell></row><row><cell></cell><cell>SetVAE [29]</cell><cell cols="6">0.200 0.367 43.70 48.40 76.54 67.65</cell></row><row><cell></cell><cell>DPF-Net [33]</cell><cell cols="6">0.264 0.409 46.17 48.89 75.18 65.55</cell></row><row><cell></cell><cell>DPM [47]</cell><cell cols="6">0.213 0.572 48.64 33.83 76.42 86.91</cell></row><row><cell></cell><cell>PVD [46]</cell><cell cols="6">0.224 0.370 48.88 52.09 73.82 64.81</cell></row><row><cell></cell><cell>LION (ours)</cell><cell cols="6">0.219 0.372 47.16 49.63 67.41 61.23</cell></row><row><cell></cell><cell>train set</cell><cell cols="6">2.618 1.555 53.02 51.21 51.28 54.76</cell></row><row><cell></cell><cell>r-GAN [2]</cell><cell cols="6">5.151 8.312 24.27 15.13 83.69 99.70</cell></row><row><cell>Chair</cell><cell>l-GAN (CD) [2]</cell><cell cols="6">2.589 2.007 41.99 29.31 68.58 83.84</cell></row><row><cell></cell><cell cols="7">l-GAN (EMD) [2] 2.811 1.619 38.07 44.86 71.90 64.65</cell></row><row><cell></cell><cell>PointFlow [31]</cell><cell cols="6">2.409 1.595 42.90 50.00 62.84 60.57</cell></row><row><cell></cell><cell>SoftFlow [32]</cell><cell cols="6">2.528 1.682 41.39 47.43 59.21 60.05</cell></row><row><cell></cell><cell>SetVAE [29]</cell><cell cols="6">2.545 1.585 46.83 44.26 58.84 60.57</cell></row><row><cell></cell><cell>DPF-Net [33]</cell><cell cols="6">2.536 1.632 44.71 48.79 62.00 58.53</cell></row><row><cell></cell><cell>DPM [47]</cell><cell cols="6">2.399 2.066 44.86 35.50 60.05 74.77</cell></row><row><cell></cell><cell>PVD [46]</cell><cell cols="6">2.622 1.556 49.84 50.60 56.26 53.32</cell></row><row><cell></cell><cell>LION (ours)</cell><cell cols="6">2.640 1.550 48.94 52.11 53.70 52.34</cell></row><row><cell></cell><cell>train set</cell><cell cols="6">0.938 0.791 50.85 55.68 51.70 50.00</cell></row><row><cell></cell><cell>r-GAN [2]</cell><cell cols="6">1.446 2.133 19.03 6.539 94.46 99.01</cell></row><row><cell>Car</cell><cell>l-GAN (CD) [2]</cell><cell cols="6">1.532 1.226 38.92 23.58 66.49 88.78</cell></row><row><cell></cell><cell cols="7">l-GAN (EMD) [2] 1.408 0.899 37.78 45.17 71.16 66.19</cell></row><row><cell></cell><cell>PointFlow [31]</cell><cell cols="6">0.901 0.807 46.88 50.00 58.10 56.25</cell></row><row><cell></cell><cell>SoftFlow [32]</cell><cell cols="6">1.187 0.859 42.90 44.60 64.77 60.09</cell></row><row><cell></cell><cell>SetVAE [29]</cell><cell cols="6">0.882 0.733 49.15 46.59 59.94 59.94</cell></row><row><cell></cell><cell>DPF-Net [33]</cell><cell cols="6">1.129 0.853 45.74 49.43 62.35 54.48</cell></row><row><cell></cell><cell>DPM [47]</cell><cell cols="6">0.902 1.140 44.03 34.94 68.89 79.97</cell></row><row><cell></cell><cell>PVD [46]</cell><cell cols="6">1.077 0.794 41.19 50.56 54.55 53.83</cell></row><row><cell></cell><cell>LION (ours)</cell><cell cols="4">0.913 0.752 50.00 56.53</cell><cell cols="2">53.41 51.14</cell></row></table><note>F.2 Single-Class Unconditional Generation For our three single-class LION models, we show the full evaluation metrics for different dataset splits, and different data normalizations, in Tab. 16, Tab. 17 and Tab. 18. Under all settings and datasets, LION achieves state-of-the-art performance on the 1-NNA metrics, and is competitive on</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_31"><head></head><label></label><figDesc>.5408 43.<ref type="bibr" target="#b45">70</ref> 45.93 66.42 65.06 TreeGAN [6] 0.5581 1.4602 31.85 17.78 97.53 99.88 ShapeGF [45] 0.3130 0.6365 45.19 40.25 81.23 80.86</figDesc><table><row><cell></cell><cell></cell><cell cols="2">MMD?</cell><cell cols="2">COV? (%)</cell><cell cols="2">1-NNA? (%)</cell></row><row><cell>Category</cell><cell>Model</cell><cell>CD</cell><cell>EMD</cell><cell>CD</cell><cell>EMD</cell><cell>CD</cell><cell>EMD</cell></row><row><cell></cell><cell cols="7">train set 0.2953 0SP-GAN [19] 0.4035 0.7658 26.42 24.44 94.69 93.95</cell></row><row><cell>Airplane</cell><cell>PDGN [52]</cell><cell cols="6">0.4087 0.7011 38.77 36.54 94.94 91.73</cell></row><row><cell></cell><cell>GCA [43]</cell><cell cols="3">0.3586 0.7651 38.02</cell><cell>36.3</cell><cell cols="2">88.15 85.93</cell></row><row><cell></cell><cell>LION (ours)</cell><cell cols="6">0.3564 0.5935 42.96 47.90 76.30 67.04</cell></row><row><cell></cell><cell>train set</cell><cell cols="6">3.8440 2.3209 49.55 53.63 53.17 52.19</cell></row><row><cell></cell><cell cols="7">TreeGAN [6] 4.8409 3.5047 39.88 26.59 88.37 96.37</cell></row><row><cell></cell><cell cols="7">ShapeGF [45] 3.7243 2.3944 48.34 44.26 58.01 61.25</cell></row><row><cell></cell><cell cols="7">SP-GAN [19] 4.2084 2.6202 40.03 32.93 72.58 83.69</cell></row><row><cell>Chair</cell><cell>PDGN [52]</cell><cell cols="6">4.2242 2.5766 43.20 36.71 71.83 79.00</cell></row><row><cell></cell><cell>GCA [43]</cell><cell>4.4035</cell><cell>2.582</cell><cell cols="4">45.92 47.89 64.27 64.50</cell></row><row><cell></cell><cell>LION (ours)</cell><cell cols="6">3.8458 2.3086 46.37 50.15 56.50 53.85</cell></row><row><cell></cell><cell>train set</cell><cell cols="6">1.0400 0.8111 50.28 53.69 55.11 49.86</cell></row><row><cell></cell><cell cols="7">TreeGAN [6] 1.1418 1.0632 40.06 31.53 89.77 94.89</cell></row><row><cell></cell><cell cols="7">ShapeGF [45] 1.0200 0.8239 44.03 47.16 61.79 57.24</cell></row><row><cell></cell><cell cols="7">SP-GAN [19] 1.1676 1.0211 34.94 31.82 87.36 85.94</cell></row><row><cell>Car</cell><cell>PDGN [52]</cell><cell cols="6">1.1837 1.0626 31.25 25.00 89.35 87.22</cell></row><row><cell></cell><cell>GCA [43]</cell><cell cols="6">1.0744 0.8666 42.05 48.58 70.45 64.20</cell></row><row><cell></cell><cell>LION (ours)</cell><cell cols="6">1.0635 0.8075 42.90 50.85 59.52 49.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_32"><head>Table 17 :</head><label>17</label><figDesc>Generation performance metrics on Airplane, Chair, Car. Trained on ShapeNet dataset from PointFlow. Both the training and testing data are normalized individually into range [-1, 1].</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_33"><head></head><label></label><figDesc>.6968 52.48 53.47 50.87 50.99 IM-GAN [7] 0.9047 0.8205 45.54 40.10 79.70 77.85</figDesc><table><row><cell></cell><cell></cell><cell cols="2">MMD?</cell><cell cols="2">COV? (%)</cell><cell cols="2">1-NNA? (%)</cell></row><row><cell>Category</cell><cell>Model</cell><cell>CD</cell><cell>EMD</cell><cell>CD</cell><cell>EMD</cell><cell>CD</cell><cell>EMD</cell></row><row><cell cols="8">train set 0.8462 0Airplane DPM [47] 0.9191 1.3975 38.86 12.13 83.04 96.04</cell></row><row><cell></cell><cell>PVD [46]</cell><cell cols="6">0.8513 0.7198 47.52 51.73 66.46 56.06</cell></row><row><cell></cell><cell cols="2">LION (ours) 0.8317</cell><cell cols="5">0.6964 53.47 53.96 53.47 53.84</cell></row><row><cell></cell><cell>train set</cell><cell cols="6">2.8793 1.6867 55.10 56.13 49.11 48.97</cell></row><row><cell></cell><cell cols="7">IM-GAN [7] 2.8935 1.7320 50.96 50.81 57.09 58.20</cell></row><row><cell>Chair</cell><cell>DPM [47]</cell><cell cols="6">2.5337 1.9746 47.42 35.01 61.96 74.96</cell></row><row><cell></cell><cell>PVD [46]</cell><cell cols="6">2.9024 1.7144 46.23 50.22 61.89 57.90</cell></row><row><cell></cell><cell cols="7">LION (ours) 2.8561 1.6898 49.78 54.51 52.07 48.67</cell></row><row><cell></cell><cell>train set</cell><cell cols="6">0.8643 0.6116 52.47 52.34 49.40 51.34</cell></row><row><cell></cell><cell cols="7">IM-GAN [7] 1.0843 0.7829 21.23 27.77 88.92 84.58</cell></row><row><cell>Car</cell><cell>DPM [47]</cell><cell cols="6">0.8880 0.8633 33.38 22.43 77.30 87.12</cell></row><row><cell></cell><cell>PVD [46]</cell><cell cols="6">0.9041 0.6140 46.33 51.00 64.49 55.74</cell></row><row><cell></cell><cell cols="7">LION (ours) 0.8687 0.6092 48.20 52.60 54.81 50.53</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_34"><head>Table 18 :</head><label>18</label><figDesc>Generation performance metrics on Airplane, Chair, Car; trained on ShapeNet-vol dataset version, the same data used by SAP.</figDesc><table><row><cell></cell><cell cols="2">MMD?</cell><cell cols="2">COV? (%)</cell><cell cols="2">1-NNA? (%)</cell></row><row><cell>Model</cell><cell>CD</cell><cell>EMD</cell><cell>CD</cell><cell>EMD</cell><cell>CD</cell><cell>EMD</cell></row><row><cell>train set</cell><cell cols="6">2.4375 1.4327 52.10 54.90 50.70 50.55</cell></row><row><cell>TreeGAN [6]</cell><cell cols="6">5.4971 3.4242 32.70 25.50 96.80 96.60</cell></row><row><cell cols="7">PointFlow [31] 2.7653 2.0064 47.70 49.20 63.25 66.05</cell></row><row><cell>ShapeGF [45]</cell><cell cols="6">2.8830 1.8508 52.00 50.30 55.65 59.00</cell></row><row><cell>SetVAE [29]</cell><cell cols="6">4.7381 2.9924 40.40 32.50 79.25 95.25</cell></row><row><cell>PDGN [52]</cell><cell cols="6">3.4032 2.3335 42.70 37.30 71.05 86.00</cell></row><row><cell>DPF-Net [33]</cell><cell cols="6">3.1976 2.0731 48.10 52.00 67.10 64.75</cell></row><row><cell>DPM [47]</cell><cell cols="6">2.2471 2.0682 48.10 30.80 62.30 86.50</cell></row><row><cell>PVD [46]</cell><cell cols="6">2.3715 1.4650 48.90 53.10 58.65 57.85</cell></row><row><cell>LION (ours)</cell><cell cols="6">2.4572 1.4472 54.30 54.40 51.85 48.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_35"><head>Table 19 :</head><label>19</label><figDesc>Generation performance metrics on 13 classes using our many-class LION model. Trained on ShapeNet-vol dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_37"><head>Table 20 :</head><label>20</label><figDesc>Generation performance metrics for our LION model that was trained jointly on all 55 ShapeNet classes without class-conditioning.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_38"><head></head><label></label><figDesc>.21, such that future work can compare to LION on this task. .8983 31.<ref type="bibr" target="#b57">82</ref> 50.00 70.45 59.09 Bottle LION (ours) 2.1941 1.4313 39.53 55.81 61.63 50.00</figDesc><table><row><cell></cell><cell></cell><cell cols="2">MMD?</cell><cell cols="2">COV? (%)</cell><cell cols="2">1-NNA? (%)</cell></row><row><cell>Data</cell><cell>Model</cell><cell>CD</cell><cell>EMD</cell><cell>CD</cell><cell>EMD</cell><cell>CD</cell><cell>EMD</cell></row><row><cell>Mug</cell><cell cols="3">LION (ours) 4.2163 2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_39"><head>Table 21 :</head><label>21</label><figDesc>Generation performance metrics for LION trained on ShapeNet's Mug and Bottle classes. Trained on ShapeNet dataset from PointFlow; both the training and testing data are normalized individually into range [?1, 1].</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_40"><head>Table 23 :</head><label>23</label><figDesc>CD? EMD? IOU? DMTet [41] 0.028 0.794 0.8148 LION (ours) 0.027 0.725 0.8177Table 22: We compare the performance of LION to DMTet on the voxel-guided synthesis task for the airplane category. When calculating the reconstruction metrics, we do not perform any diffuse-denoise. Comparison of point cloud auto-encoding performance. Both CD and EMD reconstruction values are multiplied with 1 ? 10 ?2 . signal, unlike LION, which is a highly versatile general 3D generative model. Furthermore, as we demonstrated in the main paper, LION can generate multiple plausible de-voxelized shapes, while DMTet is fully deterministic and can only generate a single reconstruction.</figDesc><table><row><cell cols="6">Dataset Metric PointFlow [31] ShapeGF [45] DPM [47] LION (ours)</cell></row><row><cell>Airplane</cell><cell>CD EMD</cell><cell>0.012 0.511</cell><cell>0.010 0.426</cell><cell>0.035 1.121</cell><cell>0.003 0.012</cell></row><row><cell>Car</cell><cell>CD EMD</cell><cell>0.065 1.271</cell><cell>0.053 0.902</cell><cell>0.083 1.796</cell><cell>0.006 0.009</cell></row><row><cell>Chair</cell><cell>CD EMD</cell><cell>0.101 1.766</cell><cell>0.056 1.213</cell><cell>0.140 2.790</cell><cell>0.007 0.014</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_41"><head>Table 24 :</head><label>24</label><figDesc>Results on denoising experiments. For our LION model, we show two different possible output variations. The right hand side results for our LION model are created by injecting diversity with 200 (first row) or 100 (second and third rows) diffuse-denoise steps in latent space. Voxel-guided synthesis experiments, on different categories. We run diffuse-denoise in latent space to generate diverse plausible clean shapes (first row, left plane: 250 diffuse-denoise steps; first row, right plane: 200 steps; second row, left chair: 200 steps; second row, right chair: 200 steps; third row, left car: 0 steps; third row, right car: 0 steps). Note that even when running no diffuse-denoise, we still obtain slightly different outputs due to different approximate posterior samples from the encoder network.F.9 Synthesis Time and DDIM SamplingOur main results in the paper are all generated using standard 1,000-step DDPM-based ancestral sampling (see Sec. 2). Generating a point cloud sample (with 2,048 points) from LION takes ? 27.12 seconds, where ? 4.04 seconds are used in the shape latent diffusion model and ? 23.05 seconds Comparison of point cloud auto-encoding performance, for models trained on the manyclass dataset. Both CD and EMD reconstruction values are multiplied with 1 ? 10 ?2 .</figDesc><table><row><cell>Normal</cell><cell></cell><cell></cell><cell></cell></row><row><cell>noise:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Uniform</cell><cell></cell><cell></cell><cell></cell></row><row><cell>noise:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Outlier</cell><cell></cell><cell></cell><cell></cell></row><row><cell>noise</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Input</cell><cell>DPM</cell><cell cols="2">PVD LION (ours) LION (ours)</cell></row><row><cell cols="2">Figure 29: input voxel latent points output points</cell><cell>mesh</cell><cell>latent points output points</cell><cell>mesh</cell></row><row><cell cols="2">input voxel latent points output points</cell><cell>mesh</cell><cell>latent points output points</cell><cell>mesh</cell></row><row><cell cols="2">input voxels latent points output points</cell><cell>mesh</cell><cell>latent points output points</cell><cell>mesh</cell></row><row><cell cols="2">Figure 30: Method</cell><cell>CD</cell><cell>EMD</cell></row><row><cell></cell><cell>DPM [47]</cell><cell cols="2">1.477 5.722</cell></row><row><cell></cell><cell cols="3">LION (ours) 0.004 0.009</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_44"><head></head><label></label><figDesc>. The RGB images of the chairs are</figDesc><table><row><cell></cell><cell></cell><cell cols="2">MMD?</cell><cell cols="2">COV? (%)</cell><cell cols="2">1-NNA? (%)</cell></row><row><cell cols="2">DDIM-Steps time(sec)</cell><cell>CD</cell><cell>EMD</cell><cell>CD</cell><cell>EMD</cell><cell>CD</cell><cell>EMD</cell></row><row><cell>5</cell><cell>0.33</cell><cell cols="3">10.117 4.6869 12.1</cell><cell>5.0</cell><cell cols="2">99.00 99.35</cell></row><row><cell>10</cell><cell>0.47</cell><cell cols="3">5.0285 2.7719 23.6</cell><cell>13.1</cell><cell cols="2">93.25 96.50</cell></row><row><cell>25</cell><cell>0.89</cell><cell cols="3">2.9425 1.6577 43.4</cell><cell>39.0</cell><cell cols="2">70.35 73.55</cell></row><row><cell>50</cell><cell>1.64</cell><cell cols="3">2.5965 1.5051 48.7</cell><cell>48.3</cell><cell cols="2">58.20 59.30</cell></row><row><cell>100</cell><cell>3.07</cell><cell cols="3">2.5929 1.4853 50.7</cell><cell>51.8</cell><cell cols="2">53.85 52.45</cell></row><row><cell>150</cell><cell>4.62</cell><cell cols="3">2.4724 1.4570 49.5</cell><cell>52.6</cell><cell cols="2">53.40 51.15</cell></row><row><cell>200</cell><cell>6.02</cell><cell cols="3">2.4321 1.4683 52.3</cell><cell>52.1</cell><cell cols="2">52.00 51.20</cell></row><row><cell>400</cell><cell>11.84</cell><cell cols="3">2.5625 1.4956 50.6</cell><cell>54.4</cell><cell cols="2">53.95 52.85</cell></row><row><cell>600</cell><cell>17.64</cell><cell>2.4353</cell><cell cols="2">1.4471 52.4</cell><cell>51.8</cell><cell cols="2">52.65 50.95</cell></row><row><cell>1,000</cell><cell>27.09</cell><cell cols="3">2.4724 1.4841 51.2</cell><cell>52.1</cell><cell cols="2">52.05 53.20</cell></row><row><cell>DDPM-1,000</cell><cell>27.09</cell><cell cols="3">2.4572 1.4472 54.3</cell><cell>54.4</cell><cell cols="2">51.85 48.95</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/alexzhou907/PVD</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/ThibaultGROUEIX/ChamferDistancePytorch (MIT License) 4 https://github.com/daerduoCarey/PyTorchEMD</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">the 55 classes are airplane, bag, basket, bathtub, bed, bench, birdhouse, bookshelf, bottle, bowl, bus, cabinet, camera, can, cap, car, cellphone, chair, clock, dishwasher, earphone, faucet, file, guitar, helmet, jar, keyboard, knife, lamp, laptop, mailbox, microphone, microwave, monitor, motorcycle, mug, piano, pillow, pistol, pot, printer, remote control, rifle, rocket, skateboard, sofa, speaker, stove, table, telephone, tin can, tower, train, vessel, washer</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://www.turbosquid.com We obtained a custom license from TurboSquid to use this data.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning representations and generative models for 3D point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panos</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Diamanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05795</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Point cloud gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning localized generative models for 3d point clouds via graph convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Valsesia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Fracastoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Magli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR) 2019</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3d volumetric modeling with introspective neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8481" to="8488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sung Woo Park, and Junseok Kwon. 3d point cloud generative adversarial network based on tree structured graph convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong Wook</forename><surname>Shu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3859" to="3868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning implicit fields for generative shape modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Giraffe: Representing scenes as compositional generative neural feature fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Graf: Generative radiance fields for 3d-aware image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards unsupervised learning of generative models for 3d controllable image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Kellnhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Tero Karras, and Gordon Wetzstein. Efficient geometry-aware 3D generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">R</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><forename type="middle">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koki</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxiao</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orazio</forename><surname>Shalini De Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gallo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">arXiv</title>
		<meeting><address><addrLine>Leonidas Guibas, Jonathan Tremblay, Sameh Khamis</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Or-El</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyi</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeong Joon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.11427</idno>
		<title level="m">Stylesdf: High-resolution 3d-consistent image and geometry generation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stylenerf: A style-based 3d aware generator for high-resolution image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.09788</idno>
		<title level="m">CIPS-3D: A 3D-Aware Generator of GANs Based on Conditionally-Independent Pixel Synthesis</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning generative models of textured 3d meshes from real-world images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Lucchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image {gan}s meet differentiable rendering for inverse graphics and interpretable 3d neural rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3d shape generation with grid-based implicit functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Ibing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaak</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leif</forename><forename type="middle">P</forename><surname>Kobbelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">SP-GAN:sphere-guided 3d shape generation and manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke-Hei</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH)</title>
		<meeting>SIGGRAPH)</meeting>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Surfgen: Adversarial 3d shape synthesis with explicit surface discriminators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning gradient fields for shape generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruojin</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guandao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadar</forename><surname>Averbuch-Elor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3d shape generation and completion through point-voxel diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Diffusion probabilistic models for 3d point cloud generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shitong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deformed implicit field: Modeling 3d shapes with learned dense correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Bar-On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagie</forename><surname>Benaim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rana</forename><surname>Hanocka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.03221</idno>
		<title level="m">Text-driven neural stylization for meshes</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Zero-shot text-guided object generation with dream fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Text to mesh without 3d supervision using limit subdivision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasir</forename><surname>Khalid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Belilovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiberiu</forename><surname>Popa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.13333</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Progressive point cloud deconvolution generation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjun</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Cascaded diffusion models for high fidelity image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.15282</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nichol</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Diffusion models beat GANs on image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nichol</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10752</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Score-based generative modeling in latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Glide: Towards photorealistic image generation and editing with text-guided diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10741</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Diffusion autoencoders: Toward a meaningful and decodable representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konpat</forename><surname>Preechakul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nattanat</forename><surname>Chatthee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suttisak</forename><surname>Wizadwongsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Supasorn</forename><surname>Suwajanakorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Score-based generative modeling with critically-damped langevin diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Tackling the generative learning trilemma with denoising diffusion GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Hierarchical text-conditional image generation with clip latents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casey</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06125</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Photorealistic text-to-image diffusion models with deep language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lala</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">S</forename><surname>Sara Mahdavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rapha Gontijo</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Norouzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.11487</idno>
	</analytic>
	<monogr>
		<title level="j">Burcu Karagol Ayan</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Seyed Kamyar Seyed Ghasemipour</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual Conference on Neural Information Processing Systems</title>
		<meeting>the 33rd Annual Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Score-based generative modeling through stochastic differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Shape as points: A differentiable poisson solver</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyou</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;quot;</forename><surname>Chiyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;quot;</forename><surname>Max</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Variational diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Vae with a vampprior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1214" to="1223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Variational autoencoder with implicit optimal priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoharu</forename><surname>Iwata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Yamanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Yagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019-07" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5066" to="5073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Resampled priors for variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics</title>
		<editor>Kamalika Chaudhuri and Masashi Sugiyama</editor>
		<meeting>the Twenty-Second International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2019-04" />
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="16" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">NVAE: A deep hierarchical variational autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">NCP-VAE: Variational autoencoders with noise contrastive priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyoti</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">D2c: Diffusion-denoising models for few-shot conditional generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Distribution matching in variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihaela</forename><surname>Rosca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06847</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Elbo surgery: yet another way to carve up the variational evidence lower bound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew J Johnson</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop in Advances in Approximate Bayesian Inference</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Point-voxel cnn for efficient 3d deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kaiming He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">SDEdit: Guided image synthesis and editing with stochastic differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Wavegrad: Estimating gradients for waveform generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">DiffWave: A Versatile Diffusion Model for Audio Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaji</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Diff-tts: A denoising diffusion model for text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myeonghun</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Jun</forename><surname>Cheon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Byoung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Soo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01409</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Wavegrad 2: Iterative refinement for text-to-speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Najim</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09660</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Grad-tts: A diffusion probabilistic model for text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vovk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Gogoryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tasnima</forename><surname>Sadekova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Kudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Diffgan-tts: High-fidelity and efficient text-to-speech with denoising diffusion gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songxiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11972</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Symbolic music generation with diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curtis</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Society for Music Information Retrieval Conference</title>
		<meeting>the 22nd International Society for Music Information Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Diffusevae: Efficient, controllable and high-fidelity generation from low-dimensional latents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kushagra</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avideep</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.00308</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Density estimation using real NVP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">FFJORD: Free-form continuous dynamics for scalable reversible generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A conditional point diffusionrefinement paradigm for 3d point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">U</forename><surname>Xudong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">AtlasNet: A Papier-M?ch? Approach to Learning 3D Surface Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Aubry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Occupancy networks: Learning 3d reconstruction in function space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4460" to="4470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Convolutional occupancy networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyou</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas Geiger Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Neural splines: Fitting 3d surfaces with infinitely-wide neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Trager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Zorin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zan</forename><surname>Gojcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameh</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Zorin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.13674</idno>
		<title level="m">Sanja Fidler, and Or Litany. Neural fields as learnable kernels for 3d reconstruction</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">ShapeNet: An Information-Rich 3D Model Repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>Stanford University -Princeton University -Toyota Technological Institute at Chicago</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>cs.GR</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Denoising diffusion implicit models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Learning to predict 3d objects with an interpolation-based differentiable renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Modular primitives for high-performance differentiable rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeongho</forename><surname>Seol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Nerf: Representing scenes as neural radiance fields for view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">DIB-R++: Learning to predict lighting and material with a hybrid differentiable renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><surname>Litalien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><forename type="middle">Fuji</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameh</forename><surname>Khamis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>Or Litany, and Sanja Fidler</note>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratul</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Tretschk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Martin-Brualla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lombardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Wetzstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhoefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladislav</forename><surname>Golyanik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.05849</idno>
		<title level="m">Advances in neural rendering</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Texture fields: Learning texture representations in function space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thilo</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Auv-net: Learning aligned uv maps for texture transfer and synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangxue</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawar</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangchang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02411</idno>
		<title level="m">Texturify: Generating textures on 3d shape surfaces</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Learning to efficiently sample from diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03802</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">On fast sampling of diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.00132</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Pich?-Taillefer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.14080</idno>
		<title level="m">Tal Kachman, and Ioannis Mitliagkas. Gotta Go Fast When Generating Data with Score-Based Models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Pseudo numerical methods for diffusion models on manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Progressive distillation for fast sampling of diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Deepfakes and disinformation: Exploring the impact of synthetic political video on deception, uncertainty, and trust in news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Vaccari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chadwick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Media+ Society</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2056305120903408</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Deep learning for deepfakes creation and detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><forename type="middle">Thi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Viet Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuong</forename><forename type="middle">M</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dung</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Duc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeid</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nahavandi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11573</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">The creation and detection of deepfakes: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisroel</forename><surname>Mirsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenke</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Reverse-time diffusion equation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename><surname>Brian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stochastic Processes and their Applications</title>
		<imprint>
			<date type="published" when="1982" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="313" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Time reversal of diffusions. The Annals of Probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Haussmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pardoux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="page" from="1188" to="1205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">A connection between score matching and denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1661" to="1674" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">A view of the em algorithm that justifies incremental, sparse, and other variants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning in graphical models</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="355" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">An introduction to variational autoencoders. Foundations and Trends in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="307" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">A family of embedded runge-kutta formulae</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Dormand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="26" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Poisson surface reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Bolitho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth Eurographics symposium on Geometry processing</title>
		<meeting>the fourth Eurographics symposium on Geometry processing</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Marching cubes: A high resolution 3d surface construction algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">E</forename><surname>Lorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harvey</forename><forename type="middle">E</forename><surname>Cline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;87</title>
		<meeting>the 14th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;87<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="1987" />
			<biblScope unit="page" from="163" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Mitsuba 2: a retargetable forward and inverse renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Merlin</forename><surname>Nimier-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delio</forename><surname>Vicini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tizian</forename><surname>Zeltner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzel</forename><surname>Jakob</surname></persName>
		</author>
		<idno type="DOI">10.1145/3355089.3356498</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International IEEE Workshop on 3D Representation and Recognition</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjoon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02481</idno>
		<title level="m">A large dataset of object scans</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Pix3d: Dataset and methods for single-image 3d shape modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoutong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Point transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16259" to="16268" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
