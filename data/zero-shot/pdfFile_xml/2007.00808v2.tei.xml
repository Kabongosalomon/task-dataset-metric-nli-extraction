<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">APPROXIMATE NEAREST NEIGHBOR NEGATIVE CON- TRASTIVE LEARNING FOR DENSE TEXT RETRIEVAL</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Xiong</surname></persName>
							<email>lexion@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
							<email>chenyan.xiong@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwok-Fung</forename><surname>Tang</surname></persName>
							<email>kwokfung.tang@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bennett</surname></persName>
							<email>paul.n.bennett@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junaid</forename><surname>Ahmed</surname></persName>
							<email>jahmed@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold</forename><forename type="middle">Overwijk</forename><surname>Microsoft</surname></persName>
						</author>
						<title level="a" type="main">APPROXIMATE NEAREST NEIGHBOR NEGATIVE CON- TRASTIVE LEARNING FOR DENSE TEXT RETRIEVAL</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Preprint</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conducting text retrieval in a dense representation space has many intriguing advantages. Yet the end-to-end learned dense retrieval (DR) often underperforms word-based sparse retrieval. In this paper, we first theoretically show the learning bottleneck of dense retrieval is due to the domination of uninformative negatives sampled locally in batch, which yield diminishing gradient norms, large stochastic gradient variances, and slow learning convergence. We then propose Approximate nearest neighbor Negative Contrastive Learning (ANCE), a learning mechanism that selects hard training negatives globally from the entire corpus, using an asynchronously updated ANN index. Our experiments demonstrate the effectiveness of ANCE on web search, question answering, and in a commercial search environment, showing ANCE dot-product retrieval nearly matches the accuracy of BERT-based cascade IR pipeline, while being 100x more efficient. * Lee and Chenyan contributed equally.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Many language systems rely on text retrieval as their first step to find relevant information. For example, search ranking , open domain question answering (OpenQA) <ref type="bibr" target="#b3">(Chen et al., 2017)</ref>, and fact verification <ref type="bibr" target="#b46">(Thorne et al., 2018)</ref> all first retrieve relevant documents for their later stage reranking, machine reading, and reasoning models. All these later-stage models enjoy the advancements of deep learning techniques <ref type="bibr" target="#b44">(Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b48">Wang et al., 2018)</ref>, while, the first stage retrieval still mainly relies on matching discrete bag-of-words, e.g., BM25, which has become the bottleneck of many systems <ref type="bibr" target="#b35">Luan et al., 2020;</ref><ref type="bibr" target="#b53">Zhao et al., 2020)</ref>.</p><p>Dense Retrieval (DR) aims to overcome the sparse retrieval bottleneck by matching texts in a continuous representation space learned via deep neural networks <ref type="bibr" target="#b24">Karpukhin et al., 2020;</ref><ref type="bibr" target="#b35">Luan et al., 2020)</ref>. It has many desired properties: fully learnable representation, easy integration with pretraining, and efficiency support from approximate nearest neighbor (ANN) search <ref type="bibr" target="#b21">(Johnson et al., 2019)</ref>. These make dense retrieval an intriguing potential choice to fundamentally overcome some intrinsic limitations of sparse retrieval, for example, vocabulary mismatch <ref type="bibr" target="#b7">(Croft et al., 2010)</ref>.</p><p>A key challenge in DR is to construct proper negative instances during its representation learning <ref type="bibr" target="#b24">(Karpukhin et al., 2020)</ref>. Unlike in reranking where negatives are naturally the irrelevant documents from previous retrieval stages, in first stage retrieval, DR models have to distinguish relevant documents from all irrelevant ones in the entire corpus. As illustrated in <ref type="figure">Fig. 1</ref>, these global negatives are quite different from negatives retrieved by sparse models.</p><p>Recent research explored various ways to construct negative training instances for dense retrieval <ref type="bibr" target="#b19">(Huang et al., 2020;</ref><ref type="bibr" target="#b24">Karpukhin et al., 2020)</ref>., e.g., using contrastive learning <ref type="bibr" target="#b11">(Faghri et al., 2017;</ref><ref type="bibr" target="#b42">Oord et al., 2018;</ref><ref type="bibr" target="#b18">He et al., 2019;</ref><ref type="bibr" target="#b4">Chen et al., 2020a)</ref> to select hard negatives in current or recent mini-batches. However, as observed in recent research <ref type="bibr" target="#b24">(Karpukhin et al., 2020)</ref>, the in-batch local negatives, though effective in learning word or visual representations, are not significantly better than spare-retrieved negatives in representation learning for dense retrieval. In addition, the accuracy of dense retrieval models often underperform BM25, especially on documents <ref type="bibr" target="#b13">Gao et al., 2020b;</ref><ref type="bibr" target="#b35">Luan et al., 2020)</ref>. <ref type="figure">Figure 1</ref>: <ref type="bibr">T-SNE (Maaten &amp; Hinton, 2008)</ref> representations of query, relevant documents, negative training instances from BM25 (BM25 Neg) or randomly sampled (Rand Neg), and testing negatives (DR Neg) in dense retrieval.</p><p>In this paper, we first theoretically analyze the convergence of dense retrieval training with negative sampling. Using the variance reduction framework <ref type="bibr" target="#b0">(Alain et al., 2015;</ref><ref type="bibr" target="#b25">Katharopoulos &amp; Fleuret, 2018)</ref>, we show that, under conditions commonly met in dense retrieval, local in-batch negatives lead to diminishing gradient norms, resulted in high stochastic gradient variances and slow training convergence -the local negative sampling is the bottleneck of dense retrieval's effectiveness.</p><p>Based on our analysis, we propose Approximate nearest neighbor Negative Contrastive Estimation (ANCE), a new contrastive representation learning mechanism for dense retrieval. Instead of random or in-batch local negatives, ANCE constructs global negatives using the beingoptimized DR model to retrieve from the entire corpus. This fundamentally aligns the distribution of negative samples in training and of irrelevant documents to separate in testing. From the variance reduction point of view, these ANCE negatives lift the upper bound of per instance gradient norm, reduce the variance of the stochastic gradient estimation, and lead to faster learning convergence.</p><p>We implement ANCE using an asynchronously updated ANN index of the corpus representation. Similar to <ref type="bibr" target="#b17">Guu et al. (2020)</ref>, we maintain an Inferencer that parallelly computes the document encodings with a recent checkpoint from the being optimized DR model, and refresh the ANN index used for negative sampling once it finishes, to keep up with the model training. Our experiments demonstrate the advantage of ANCE in three text retrieval scenarios: standard web search <ref type="bibr" target="#b6">(Craswell et al., 2020)</ref>, OpenQA <ref type="bibr" target="#b44">(Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b28">Kwiatkowski et al., 2019)</ref>, and in a commercial search engine's retrieval system. We also empirically validate our theory that the gradient norms on ANCE sampled negatives are much bigger than local negatives and thus improve the convergence of dense retrieval models. Our code and trained models are available at https://aka.ms/ance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>In this section, we discuss the preliminaries of dense retrieval and its representation learning.</p><p>Task Definition: Given a query q and a corpus C, the first stage retrieval is to find a set of documents relevant to the query D + = {d 1 , ..., d i , ..., d n } from C (|D + | |C|), which then serve as input to later more complex models <ref type="bibr" target="#b7">(Croft et al., 2010)</ref>. Instead of using sparse term matches and inverted index, Dense Retrieval calculates the retrieval score f () using similarities in a learned embedding space <ref type="bibr" target="#b35">Luan et al., 2020;</ref><ref type="bibr" target="#b24">Karpukhin et al., 2020)</ref>:</p><formula xml:id="formula_0">f (q, d) = sim(g(q; ?), g(d; ?)),<label>(1)</label></formula><p>where g() is the representation model that encodes the query or document to dense embeddings. The encoder parameter ? provides the main capacity, often fine-tuned from pretrained transformers, e.g., BERT . The similarity function (sim()) is often simply cosine or dot product, to leverage efficient ANN retrieval <ref type="bibr" target="#b21">(Johnson et al., 2019;</ref><ref type="bibr" target="#b15">Guo et al., 2020)</ref>.</p><p>Learning with Negative Sampling: The effectiveness of DR resides in learning a good representation space that maps query and relevant documents together, while separating irrelevant ones. The learning of this representation often follows standard learning to rank <ref type="bibr" target="#b33">(Liu, 2009)</ref>: Given a query q, a set of relevant document D + and irrelevant ones D ? , find the best ? * that:</p><formula xml:id="formula_1">? * = argmin ? q d + ?D + d ? ?D ? l(f (q, d + ), f (q, d ? )).<label>(2)</label></formula><p>The loss l() can be binary cross entropy (BCE), hinge loss, or negative log likelihood (NLL).</p><p>A unique challenge in dense retrieval, targeting first stage retrieval, is that the irrelevant documents to separate are from the entire corpus (D ? = C \ D + ). This often leads to millions of negative instances, which have to be sampled in training:</p><formula xml:id="formula_2">? * = argmin ? q d + ?D + d ? ?D ? l(f (q, d + ), f (q, d ? )).<label>(3)</label></formula><p>A natural choice is to sample negativesD ? from top documents retrieved by BM25. However, they may bias the DR model to merely learn sparse retrieval and do not elevate DR models much beyond BM25 <ref type="bibr" target="#b35">(Luan et al., 2020)</ref>. Another way is to sample negatives in local mini-batches, e.g., as in contrastive learning <ref type="bibr" target="#b42">(Oord et al., 2018;</ref><ref type="bibr" target="#b4">Chen et al., 2020a)</ref>, however, these local negatives do not significantly outperform BM25 negatives <ref type="bibr" target="#b24">(Karpukhin et al., 2020;</ref><ref type="bibr" target="#b35">Luan et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ANALYSES ON THE CONVERGENCE OF DENSE RETRIEVAL TRAINING</head><p>In this section, we provide theoretical analyses on the convergence of representation training in dense retrieval. We first show the connections between learning convergence and gradient norms, then the bounded gradient norms by uninformative negatives, and finally, how in-batch local negatives are ineffective under common conditions in dense retrieval.</p><p>Convergence Rate and Gradient Norms:</p><formula xml:id="formula_3">Let l(d + , d ? ) = l(f (q, d + ), f (q, d ? )</formula><p>be the loss function on the training triple (q, d + , d ? ), P D ? the negative sampling distribution for the given (q, d + ), and p d ? the sampling probability of negative instance d ? , a stochastic gradient decent (SGD) step with importance sampling <ref type="bibr" target="#b0">(Alain et al., 2015)</ref> is:</p><formula xml:id="formula_4">? t+1 = ? t ? ? 1 N p d ? ? ?t l(d + , d ? ),<label>(4)</label></formula><p>with ? t the parameter at t-th step, ? t+1 the one after, and N the total number of negatives. The scaling factor 1 N p d ? is to make sure Eqn. 4 is an unbiased estimator of the full gradient. Then we can characterize the converge rate of this SGD step as the movement to optimal ? * . Following derivations in variance reduction <ref type="bibr" target="#b25">(Katharopoulos &amp; Fleuret, 2018;</ref><ref type="bibr" target="#b22">Johnson &amp; Guestrin, 2018)</ref>, let</p><formula xml:id="formula_5">g d ? = 1 N p d ? ? ?t l(d + , d ? )</formula><p>the weighted gradient, the convergence rate is:</p><formula xml:id="formula_6">E? t = ||? t ? ? * || 2 ? E P D ? (||? t+1 ? ? * || 2 ) (5) = ||? t || 2 ? 2? T t ? * ? E P D ? (||? t ? ?g d ? || 2 ) + 2? * T E P D ? (? t ? ?g d ? ) (6) = ?? 2 E P D ? (||g d ? || 2 ) + 2?? T t E P D ? (g d ? ) ? 2?? * T E P D ? (g d ? ) (7) = 2?E P D ? (g d ? ) T (? t ? ? * ) ? ? 2 E P D ? (||g d ? || 2 ) (8) = 2?E P D ? (g d ? ) T (? t ? ? * ) ? ? 2 E P D ? (g d ? ) T E P D ? (g d ? ) ? ? 2 Tr(V P D ? (g d ? )). (9)</formula><p>This shows we can obtain better convergence rate by sampling from a distribution P D ? that minimizes the variance of the gradient estimator,</p><formula xml:id="formula_7">E P D ? (||g d ? || 2 ), or Tr(V P D ? (g d ? )</formula><p>) as the estimator is unbiased. There exists an optimal distribution that:</p><formula xml:id="formula_8">p * d ? = argmin p d ? Tr(V P D ? (g d ? )) ? ||? ?t l(d + , d ? )|| 2 ,<label>(10)</label></formula><p>which is to sample proportionally to per instance gradient norm. This is a well known result in importance sampling <ref type="bibr" target="#b0">(Alain et al., 2015;</ref><ref type="bibr" target="#b22">Johnson &amp; Guestrin, 2018)</ref>. It can be proved by applying Jensen's inequality on the gradient variance and then verifying that Eqn. 10 achieves the minimum. We do not repeat this proof and refer to <ref type="bibr" target="#b22">Johnson &amp; Guestrin (2018)</ref> for exact derivations.</p><p>Intuitively, an negative instance with larger gradient norm is more likely to reduce the training loss more, while those with diminishing gradients are not informative. Empirically, the correlation of gradient norm and training convergence is also observed in BERT fine-tuning <ref type="bibr" target="#b39">(Mosbach et al., 2020)</ref>.</p><p>Diminishing Gradients of Uninformative Negatives: The oracle distribution in Eqn. 10 is too expensive to compute and the closed form of gradient norms can be complicated in deep neural networks. Nevertheless, for MLP networks, <ref type="bibr" target="#b25">Katharopoulos &amp; Fleuret (2018)</ref> derives an upper bound of the per sample gradient norm:</p><formula xml:id="formula_9">||? ?t l(d + , d ? )|| 2 ? L?||? ? L l(d + , d ? )|| 2 ,<label>(11)</label></formula><formula xml:id="formula_10">q ! ! ! "# " Trainer Inferencer q ! ! ! "$ " Checkpoint k-1 ? Checkpoint k q ! ! ! "$ " q ! ! ! " ? Checkpoint k+1 q ! ! ! "# " ? Inferencing</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index &amp; Search</head><p>Training Positives where L is the number of layers, ? is composed by pre-activation weights and gradients in intermediate layers, and ||? ? L l(d + , d ? )|| 2 is the gradient w.r.t. the last layer. Intuitively, the intermediate layers are more regulated by various normalization techniques; the main moving piece is <ref type="bibr" target="#b25">Katharopoulos &amp; Fleuret, 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ANCE Negatives</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index &amp; Search</head><formula xml:id="formula_11">||? ? L l(d + , d ? )|| 2 (</formula><p>For common learning to rank loss functions, for example, BCE loss and pairwise hinge loss, we can verified that <ref type="bibr" target="#b25">(Katharopoulos &amp; Fleuret, 2018)</ref>:</p><formula xml:id="formula_12">l(d + , d ? ) ? 0 ? ||? ? L l(d + , d ? )|| 2 ? 0 ? ||? ?t l(d + , d ? )|| 2 ? 0.<label>(12)</label></formula><p>Intuitively, negative samples with near zero loss have near zero gradients and contribute little to model convergence. The convergence of dense retrieval model training relies on the informativeness of constructed negatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inefficacy of Local In-Batch Negatives:</head><p>We argue that the in-batch local negatives are unlikely to provide informative samples due to two common properties of text retrieval.</p><p>Let D ? * be the set of informative negatives that are hard to distinguish from D + , and b be the batch size, we have (1) b |C|, the batch size is far smaller than the corpus size;</p><p>(2) |D ? * | |C|, that only a few negatives are informative and the majority of corpus is trivially unrelated.</p><p>Both conditions are easy to verify empirically in dense retrieval benchmarks. The two together make the probability that a random mini-batch includes meaningful negatives p = b|D ? * | |C| 2 close to zero. Selecting negatives from local training batches is unlikely to provide optimal training signals for dense retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">APPROXIMATE NEAREST NEIGHBOR NOISE CONTRASTIVE ESTIMATION</head><p>Our analyses show the importance, if not necessity, to construct negatives globally from the corpus. In this section, we propose Approximate nearest neighbor Negative Contrastive Estimation, (ANCE), which selects negatives from the entire corpus using an asynchronously updated ANN index.</p><p>ANCE samples negatives from the top retrieved documents via the DR model from the ANN index:</p><formula xml:id="formula_13">? * = argmin ? q d + ?D + d ? ?D ? ANCE l(f (q, d + ), f (q, d ? )),<label>(13)</label></formula><p>with D ? ANCE = ANN f (q,d) \ D + and ANN f (q,d) the top retrieved documents by f () from the ANN index. By definition, D ? ANCE are the hardest negatives for the current DR model: D ? ANCE ? D ? * . In theory, these more informative negatives have higher training loss, higher upper bound on the gradient norms, and will improve training convergence.</p><p>ANCE can be used to train any dense retrieval model. For simplicity, we use a simple set up in recent research <ref type="bibr" target="#b35">(Luan et al., 2020)</ref> with BERT Siamese/Dual Encoder (shared between q and d), dot product similarity, and negative log likelihood (NLL) loss.</p><p>Asynchronous Index Refresh: During stochastic training, the DR model f () is updated each minibatch. Maintaining an update-to-date ANN index to select fresh ANCE negatives is challenging, as the index update requires two operations: 1) Inference: refresh the representations of all documents in the corpus with an updated DR model; 2) Index: rebuild the ANN index using updated representations. Although Index is efficient <ref type="bibr" target="#b21">(Johnson et al., 2019)</ref>, Inference is too expensive to compute per batch as it requires a forward pass on the entire corpus which is much bigger than the training batch.</p><p>Thus we implement an asynchronous index refresh similar to <ref type="bibr" target="#b17">Guu et al. (2020)</ref>, and update the ANN index once every m batches, i.e., with checkpoint f k . As illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>, besides the Trainer, we run an Inferencer that takes the latest checkpoint (e.g., f k ) and recomputes the encodings of the entire corpus. In parallel, the Trainer continues its stochastic learning using D ? f k?1 from ANN f k?1 . Once the corpus is re-encoded, Inferencer updates the ANN index (ANN f k ) and feed it to the Trainer.</p><p>In this process, the ANCE negatives (D ? ANCE ) are asynchronously updated to "catch up" with the stochastic training. The gap between the ANN index and the DR model optimization depends on the allocation of computing resources between Trainer and Inferencer. Appendix A.3 shows an 1:1 GPU split is sufficient to minimize the influence of this gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL METHODOLOGIES</head><p>This section describes our experimental setups. More details can be found in Appendix A.1 and A.2.</p><p>Benchmarks: The web search experiments use the TREC 2019 Deep Learning (DL) Track benchmark <ref type="bibr" target="#b6">(Craswell et al., 2020)</ref>, a large scale ad hoc retrieval dataset. We follow the official guideline and evaluate mainly in the retrieval setting, but also results when reranking top 100 BM25 candidates.</p><p>The OpenQA experiments use the Natural Questions (NQ) <ref type="bibr" target="#b28">(Kwiatkowski et al., 2019)</ref> and TriviaQA (TQA) <ref type="bibr" target="#b23">(Joshi et al., 2017)</ref>, following the exact settings from <ref type="bibr" target="#b24">Karpukhin et al. (2020)</ref>. The metrics are Coverage@20/100, which evaluate whether the Top-20/100 retrieved passages include the answer. We also evaluate whether ANCE's better retrieval can propagate to better answer accuracy, by running the state-of-the-art systems' readers on top of ANCE instead of DPR retrieval. The readers are RAG-Token <ref type="bibr" target="#b32">(Lewis et al., 2020b)</ref> on NQ and DPR Reader on TQA, in their suggested settings.</p><p>We also study the effectiveness of ANCE in the first stage retrieval of a commercial search engine's production system. We change the training of a production-quality DR model to ANCE, and evaluate the offline gains in various corpus sizes, encoding dimensions, and exact/approximate search.</p><p>Baselines: In TREC DL, we include best runs in relevant categories and refer to <ref type="bibr" target="#b6">Craswell et al. (2020)</ref> for more baseline scores. We implement recent DR baselines that use the same BERT-Siamese, but vary in negative construction: random sampling in batch (Rand Neg), random sampling from BM25 top 100 (BM25 Neg) <ref type="bibr" target="#b13">Gao et al., 2020b)</ref> and the 1:1 combination of BM25 and Random negatives (BM25 + Rand Neg) <ref type="bibr" target="#b24">(Karpukhin et al., 2020;</ref><ref type="bibr" target="#b35">Luan et al., 2020)</ref>. We also compare with contrastive learning/Noise Contrastive Estimation, which uses hardest negatives in batch (NCE Neg) <ref type="bibr" target="#b16">(Gutmann &amp; Hyv?rinen, 2010;</ref><ref type="bibr" target="#b42">Oord et al., 2018;</ref><ref type="bibr" target="#b4">Chen et al., 2020a)</ref>. In OpenQA, we compare with DPR, BM25, and their combinations <ref type="bibr" target="#b24">(Karpukhin et al., 2020)</ref>.</p><p>Implementation Details: In TREC DL, recent research found MARCO passage training labels cleaner <ref type="bibr" target="#b52">(Yan et al., 2019)</ref> and BM25 negatives can help train dense retrieval <ref type="bibr" target="#b24">(Karpukhin et al., 2020;</ref><ref type="bibr" target="#b35">Luan et al., 2020)</ref>. Thus, we include a "BM25 Warm Up" setting (BM25 ? * ), where the DR models are first trained using MARCO official BM25 Negatives. ANCE is also warmed up by BM25 negatives. All DR models in TREC DL are fine-tuned from RoBERTa base . In OpenQA, we warm up ANCE using the released DPR checkpoints <ref type="bibr" target="#b24">(Karpukhin et al., 2020)</ref>.</p><p>To fit long documents in BERT-Siamese, ANCE uses the two settings from <ref type="bibr" target="#b9">Dai &amp; Callan (2019b)</ref>, FirstP which uses the first 512 tokens of the document, and MaxP, where the document is split to 512-token passages (maximum 4) and the passage level scores are max-pooled. The max-pooling operation is natively supported in ANN. The ANN search uses the Faiss IndexFlatIP Index <ref type="bibr" target="#b21">(Johnson et al., 2019)</ref>. We use 1:1 Trainer:Inference GPU allocation, index refreshing per 10k training batches, batch size 8, and gradient accumulation step 2 on 4 GPUs. For each positive, we uniformly sample one negative from ANN top 200. We measured ANCE efficiency using a single 32GB V100 GPU, on a cloud VM with Intel(R) Xeon(R) Platinum 8168 CPU and 650GB of RAM memory.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION RESULTS</head><p>In this section, we first evaluate the effectiveness and efficiency of ANCE. Then we empirically study the convergence of ANCE training following our theoretical analyses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">EFFECTIVENESS AND EFFICIENCY</head><p>The results on TREC 2019 DL benchmark are in <ref type="table" target="#tab_0">Table 1</ref>. ANCE significantly outperforms all sparse retrieval, including DeepCT, which uses BERT to learn term weights <ref type="bibr" target="#b10">(Dai et al., 2019)</ref>. Among all different negative construction mechanisms, ANCE is the only one that elevates BERT-Siamese to robustly exceed the sparse methods in document retrieval. It also outperforms DPR in passage retrieval in OpenQA <ref type="table" target="#tab_1">(Table 2)</ref>. ANCE's effectiveness is even more observed in real production <ref type="table" target="#tab_2">(Table 3)</ref> with about 15% relative gains all around. Its better retrieval does indeed lead to better answer accuracy with the same readers used in RAG <ref type="bibr" target="#b32">(Lewis et al., 2020b)</ref> and DPR <ref type="table" target="#tab_3">(Table 4)</ref>.</p><p>Among all DR models, ANCE has the smallest gap between its retrieval and reranking accuracy, showing the importance of global negatives in training retrieval models. ANCE retrieval nearly matches the accuracy of the cascade IR with interaction-based BERT Reranker. This overthrows a previously-held belief that modeling term-level interactions is necessary in search <ref type="bibr" target="#b50">(Xiong et al., 2017;</ref><ref type="bibr" target="#b43">Qiao et al., 2019)</ref>. With ANCE, we can learn a representation space that effectively captures the finesse of search relevance. <ref type="table" target="#tab_4">Table 5</ref> measures the efficiency ANCE (FirstP) in TREC DL document retrieval. The online latency is on one query and 100 retrieved documents. DR with standard batching provides a 100x speed up compared to BERT Rerank, a natural benefit from the Siamese network and pre-computable document encoding. In ANCE training, the bulk of computing is to update the encodings of the training corpus using new checkpoints. Assuming the model used to sample negatives and to be learned is the same, this is inevitable but can be mitigated by asynchronous index refresh.  <ref type="bibr" target="#b45">(Roberts et al., 2020)</ref> 34.5 -T5-11B + SSM <ref type="bibr" target="#b45">(Roberts et al., 2020)</ref> 36.6 -REALM <ref type="bibr" target="#b17">(Guu et al., 2020)</ref> 40.4 -DPR <ref type="bibr" target="#b24">(Karpukhin et al., 2020)</ref> 41.5 56.8 DPR + BM25 <ref type="bibr" target="#b24">(Karpukhin et al., 2020)</ref> 39.0 57.0 RAG-Token <ref type="bibr" target="#b32">(Lewis et al., 2020b)</ref> 44.1 55.2 RAG-Sequence <ref type="bibr" target="#b32">(Lewis et al., 2020b)</ref> 44.5 56.1 ANCE + Reader 46.0 57.5 Positive Negative </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">EMPIRICAL ANALYSES ON TRAINING CONVERGENCE</head><p>We first show the long tail distribution of search relevance in dense retrieval. As plotted in <ref type="figure" target="#fig_1">Fig. 3</ref>, there are a few instances per query with significant higher retrieval scores, while the majority form a long tail. In retrieval/ranking, the key challenge is to distinguish the relevant ones among those highest scored ones; the rest is trivially irrelevant. We also empirically measure the probability of local in-batch negatives including informative negatives (D ? * ), by their overlap with top 100 highest scored negatives. This probability, either using NCE Neg or Rand Neg, is zero, the same as our theory assumes. In comparison, the overlap between BM25 Neg with top DR retrieved negatives is 15%, while that of ANCE negatives starts at 63% and converges to 100% by design.</p><p>Then we empirically validate our theory that local negatives lead to lower loss, bounded gradient norm, and thus slow convergence. The training loss and pre-clip gradient norms during DR training are plotted in <ref type="figure" target="#fig_2">Fig. 4</ref>. As expected, the uninformative local negatives are trivial to separate, yielding near-zero training loss, while ANCE global negatives are much harder and maintain a high training loss. The same with our theoretical assumption, the gradient norms of local negatives are indeed restricted closely to zero. In comparison, the gradient norms on ANCE global negatives are bigger by orders of magnitude. This confirms ANCE better approximates the oracle importance sampling distribution p * d ? ? ||? ?t l(d + , d ? )|| 2 and improves learning convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">DISCUSSIONS</head><p>We use BERT-Siamese and NLL loss to be consistent with recent research. We have experimented with cosine similarity and BCE/hinge loss, where we observe even smaller gradient norms on local negatives. But the retrieval accuracy is not much better. We include additional experiments in Appendix. A.2 discusses the surprisingly small overlap (&lt;25%) between dense retrieval results and sparse retrieval results. DR is a fundamentally different approach and more studies are required to understand its behavior. A.3 and A.4 study the asynchronous gaps and hyperparameters. A.5 includes case studies that the irrelevant documents from ANCE are often still "semantically related" and very different from those made by sparse retrieval. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK</head><p>In early research on neural information retrieval (Neu-IR) (Mitra et al., 2018), a common belief was that the interaction models, those that specifically handle term level matches, are more effective though more expensive <ref type="bibr" target="#b14">(Guo et al., 2016;</ref><ref type="bibr" target="#b50">Xiong et al., 2017;</ref>. Many techniques are developed to reduce their cost, for example, distillation <ref type="bibr" target="#b12">(Gao et al., 2020a)</ref> and caching <ref type="bibr" target="#b20">(Humeau et al., 2020;</ref><ref type="bibr" target="#b27">Khattab &amp; Zaharia, 2020;</ref><ref type="bibr" target="#b37">MacAvaney et al., 2020)</ref>. ANCE shows that a properly trained representation-based BERT-Siamese is in fact as effective as the interaction-based BERT ranker. This finding will motivate many new research explorations in Neu-IR.</p><p>Deep learning has been used to improve various components of sparse retrieval, for example, term weighting <ref type="bibr" target="#b9">(Dai &amp; Callan, 2019b)</ref>, query expansion <ref type="bibr" target="#b54">(Zheng et al., 2020)</ref>, and document expansion . Dense Retrieval chooses a different path and conducts retrieval purely in the embedding space via ANN search <ref type="bibr" target="#b24">Karpukhin et al., 2020;</ref><ref type="bibr" target="#b35">Luan et al., 2020)</ref>. This work demonstrates that a simple dense retrieval system can achieve SOTA accuracy, while also behaves dramatically different from classic retrieval. The recent advancement in dense retrieval may raise a new generation of search systems.</p><p>Recent research in contrastive representation learning also shows the benefits of sampling negatives from a larger candidate pool. In computer vision, <ref type="bibr" target="#b18">He et al. (2019)</ref> decouple the negative sampling pool size with training batch size, by maintaining a negative candidate pool of recent batches and updating their representation with momentum. This enlarged negative pool significantly improves unsupervised visual representation learning <ref type="bibr" target="#b5">(Chen et al., 2020b)</ref>. A parallel work  improves DPR by sampling negatives from a memory bank <ref type="bibr" target="#b49">(Wu et al., 2018</ref>) -in which the representations of negative candidates are frozen so more candidates can be stored. Instead of a bigger local pool, ANCE goes all the way along this trajectory and constructs negatives globally from the entire corpus, using an asynchronously updated ANN index.</p><p>Besides being a real world application itself, dense retrieval is also a core component in many other language systems, for example, to retrieval relevant information for grounded language models <ref type="bibr" target="#b26">(Khandelwal et al., 2019;</ref><ref type="bibr" target="#b17">Guu et al., 2020)</ref>, extractive/generative QA <ref type="bibr" target="#b24">(Karpukhin et al., 2020;</ref><ref type="bibr" target="#b32">Lewis et al., 2020b)</ref>, and fact verification , or to find paraphrase pairs for pretraining <ref type="bibr" target="#b31">(Lewis et al., 2020a)</ref>. There dense retrieval models are either frozen or optimized indirectly by signals from their end tasks. ANCE is orthogonal with those lines of research and focuses on the representation learning for dense retrieval. Its better retrieval accuracy can benefit many language systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In this paper, we first provide theoretical analyses on the convergence of representation learning in dense retrieval. We show that under common conditions in text retrieval, the local negatives used in DR training are uninformative, yield low gradient norms, and contribute little to the learning convergence. We then propose ANCE to eliminate this bottleneck by constructing training negatives globally from the entire corpus. Our experiments demonstrate the advantage of ANCE in web search, OpenQA, and the production system of a commercial search engine. Our studies empirically validate our theory that ANCE negatives have much bigger gradient norms, reduce the stochastic gradient variance, and improve training convergence.  <ref type="bibr" target="#b1">(Bajaj et al., 2016)</ref>. The document corpus was post-constructed by back-filling the body texts of the passage's URLs and their labels were inherited from its passages <ref type="bibr" target="#b6">(Craswell et al., 2020)</ref>. The testing sets are labeled by NIST accessors on the top 10 ranked results from past Track participants <ref type="bibr" target="#b6">(Craswell et al., 2020)</ref>.</p><p>TREC DL official metrics include NDCG@10 on test and MRR@10 on MARCO Passage Dev. MARCO Document Dev is noisy and the recall on the DL Track testing is less meaningful due to low label coverage on DR results. There is a two-year gap between the construction of the passage training data and the back-filling of their full document content. Some original documents were no longer available. There is also a decent amount of content changes in those documents during the two-year gap, and many no longer contain the passages. This back-filling perhaps is the reason why many Track participants found the passage training data is more effective than the inherited document labels. Note that the TREC testing labels are not influenced as the annotators were provided the same document contents when judging.</p><p>All the TREC DL runs are trained using these training data. Their inference results on the testing queries of the document and the passage retrieval tasks were evaluated by NIST assessors in the standard TREC-style pooling technique <ref type="bibr" target="#b47">(Voorhees, 2000)</ref>. The pooling depth is set to 10, that is, the top 10 ranked results from all participated runs are evaluated, and these evaluated labels are released as the official TREC DL benchmarks for passage and document retrieval tasks.</p><p>More Details on OpenQA Experiments: All the DPR related experimental settings, baseline systems, and DPR Reader are based on their open source libarary 1 . The RAG-Token reader uses their open-source release in huggingface 2 . The RAG-Seq release in huggingface is not yet stable by the time we did our experiment, thus we choose the RAG-Token in our OpenQA experiment. RAG only releases the NQ models thus we use DPR reader on TriviaQA. We feed top 20 passages from ANCE to RAG-Token on NQ and top 100 passages to DPR's BERT Reader, following the guideline in their open-source codes.</p><p>More Details on Baselines: The most representative sparse retrieval baselines in TREC DL include the standard BM25 ("bm25base" or "bm25base_p"), Best TREC Sparse Retrieval ("bm25tuned_rm3" or "bm25tuned_prf_p") with tuned query expansion <ref type="bibr" target="#b29">(Lavrenko &amp; Croft, 2017)</ref>, and Best DeepCT ("dct_tp_bm25e2", doc only), which uses BERT to estimate the term importance for BM25 <ref type="bibr" target="#b8">(Dai &amp; Callan, 2019a)</ref>. These three runs represent the standard sparse retrieval, best classical sparse retrieval, and the recent progress of using BERT to improve sparse retrieval. We also include the standard cascade retrieval-and-reranking systems BERT Reranker ("bm25exp_marcomb" or "p_exp_rm3_bert"), which is the best run using standard BERT on top of query/doc expansion, from the groups with multiple top MARCO runs .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT-Siamese Configurations:</head><p>We follow the network configurations in <ref type="bibr" target="#b35">Luan et al. (2020)</ref> in all Dense Retrieval methods, which we found provides the most stable results. More specifically, we initialize the BERT-Siamese model with RoBERTa base  and add a 768 ? 768 projection layer on top of the last layer's "[CLS]" token, followed by a layer norm.   As a nature of TREC-style pooling evaluation, only those ranked in the top 10 by the 2019 TREC participating systems were labeled. As a result, documents not in the pool and thus not labeled are all considered irrelevant, even though there may be relevant ones among them. When reusing TREC style relevance labels, it is very important to keep track of the "hole rate" on the evaluated systems, i.e., the fraction of the top K ranked results without TREC labels (not in the pool). A larger hole rate shows that the evaluated methods are very different from those systems that participated in the Track and contributed to the pool, thus the evaluation results are not perfect. Note that the hole rate does not necessarily reflect the accuracy of the system, only the difference of it.</p><p>In TREC 2019 Deep Learning Track, all the participating systems are based on sparse retrieval. Dense retrieval methods often differ considerably from sparse retrievals and in general will retrieve many new documents. This is confirmed in The MS MARCO ranking labels were not constructed based on pooling the sparse retrieval results. They were from Bing <ref type="bibr" target="#b1">(Bajaj et al., 2016)</ref>, which uses many signals beyond term overlap. This makes the recall metric in MS MARCO more robust as it reflects how a single model can recover a complex online system.</p><p>A.3 IMPACT OF ASYNCHRONOUS GAP <ref type="figure" target="#fig_3">Fig. 5</ref> illustrates the behavior of asynchronous learning with different configurations. A large learning rate or a low refreshing rate <ref type="figure" target="#fig_3">(Figure 5</ref>(a) and 5(b)) leads to fluctuations as the async gap of the ANN index may drive the representation learning to undesired local optima. Refreshing as often as every 5k Batches yields a smooth convergence ( <ref type="figure" target="#fig_3">Figure 5(c)</ref>), but requires twice as many GPU allocated to the Inferencer. A 1:1 GPUs allocation of Trainer and Inference with appropriate learning rates is adequate to minimize the impact of async gap. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 HYPERPARAMETER STUDIES</head><p>We show the results of some hyperparameter configurations in <ref type="table" target="#tab_7">Table 7</ref>. The cost of training with BERT makes it difficult to conduct a lot hyperparameter exploration. Often a failed configuration leads to divergence early in training. We barely explore other configurations due to the time-consuming nature of working with pretrained language models. Our DR model architecture is kept consistent with recent parallel work and the learning configurations in <ref type="table" target="#tab_7">Table 7</ref> are about all the explorations we did. Most of the hyperparameter choices are decided solely using the training loss curve and otherwise by the loss in the MARCO Dev set. We found the training loss, validation NDCG, and testing performance align well in our (limited) hyperparameter explorations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 CASE STUDIES</head><p>In this section, we show Win/Loss case studies between ANCE and BM25. Among the 43 TREC 2019 DL Track evaluation queries in the document task, ANCE outperforms BM25 on 29 queries, loses on 13 queries, and ties on the rest 1 query. The winning examples are shown in <ref type="table" target="#tab_9">Table 8</ref> and the losing ones are in <ref type="table" target="#tab_10">Table 9</ref>. Their corresponding ANCE-learned (FirstP) representations are illustrated by t-SNE in <ref type="figure">Fig. 6 and Fig. 7</ref>.</p><p>In general, we found ANCE better captures the semantics in the documents and their relevance to the query. The winning cases show the intrinsic limitations of sparse retrieval. For example, BM25 exact matches the "most popular food" in the query "what is the most popular food in Switzerland" but using the document is about Mexico. The term "Switzerland" only appears in the related question section of the web page.  <ref type="table" target="#tab_9">Table 8</ref>. The losing cases in <ref type="table" target="#tab_10">Table 9</ref> are also quite interesting. Many times we found that it is not that DR fails completely and retrieves documents not related to the query's information needs at all, which was a big concern when we started research in DR. The errors ANCE made include retrieving documents that are related just not exactly relevant to the query, for example, "yoga pose" for "bow in yoga". In other cases, ANCE retrieved wrong documents due to the lack of the domain knowledge: the pretrained language model may not know "active  <ref type="table" target="#tab_10">Table 9</ref>.</p><p>margin" is a geographical terminology, not a financial one (which we did not know ourselves and took some time to figure out when conducting this case study). There are also some cases where the dense retrieved documents make sense to us but were labeled irrelevant.</p><p>The t-SNE plots in <ref type="figure">Fig. 6</ref> and <ref type="figure">Fig. 7</ref> show many interesting patterns of the learned representation space. The ANCE winning cases often correspond to clear separations of different document groups. For losing cases the representation space is more mixed, or there is too few relevant documents which may cause the variances in model performances. There are also many different interesting patterns in the ANCE-learned representation space. We include the t-SNE plots for all 43 TREC DL Track queries in the supplementary material. More future analyses of the learned patterns in the representation space may help provide more insights on dense retrieval.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>ANCE Asynchronous Training. The Trainer learns the representation using negatives from the ANN index. The Inferencer uses a recent checkpoint to update the representation of documents in the corpus and once finished, refreshes the ANN index with most up-to-date encodings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The top DR scores for 10 random TREC DL testing queries. The x-axes are their ranking order. The y-axes are their retrieval scores minus corpus average. All models are warmed up by BM25 Neg. The percentages are the overlaps between the testing and training negatives near convergence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The loss and gradient norms during DR training (after BM25 warm up). The gradient norms are on the bottom (1-4), middle (5-8), and top (9-12) BERT layers. The x-axes are training steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Training loss and testing NDCG of ANCE (FirstP) on documents, with different ANN index refreshing (e.g., per 10k Batch), Trainer:Inferencer GPU allocation, and learning rate (e.g., 1e-5). X-axes is the training steps in thousands.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>t-SNE Plots for Winning Cases in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>t-SNE Plots for Losing Cases in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results in TREC 2019 Deep Learning Track. Results not available are marked as "n.a.", not applicable are marked as "-". Best results in each category are marked bold.</figDesc><table><row><cell></cell><cell cols="2">MARCO Dev</cell><cell cols="2">TREC DL Passage</cell><cell cols="2">TREC DL Document</cell></row><row><cell></cell><cell cols="2">Passage Retrieval</cell><cell cols="2">NDCG@10</cell><cell cols="2">NDCG@10</cell></row><row><cell></cell><cell cols="5">MRR@10 Recall@1k Rerank Retrieval Rerank</cell><cell>Retrieval</cell></row><row><cell>Sparse &amp; Cascade IR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BM25</cell><cell>0.240</cell><cell>0.814</cell><cell>-</cell><cell>0.506</cell><cell>-</cell><cell>0.519</cell></row><row><cell>Best DeepCT</cell><cell>0.243</cell><cell>n.a.</cell><cell>-</cell><cell>n.a.</cell><cell>-</cell><cell>0.554</cell></row><row><cell>Best TREC Trad Retrieval</cell><cell>0.240</cell><cell>n.a.</cell><cell>-</cell><cell>0.554</cell><cell>-</cell><cell>0.549</cell></row><row><cell>BERT Reranker</cell><cell>-</cell><cell>-</cell><cell>0.742</cell><cell>-</cell><cell>0.646</cell><cell>-</cell></row><row><cell>Dense Retrieval</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Rand Neg</cell><cell>0.261</cell><cell>0.949</cell><cell>0.605</cell><cell>0.552</cell><cell>0.615</cell><cell>0.543</cell></row><row><cell>NCE Neg</cell><cell>0.256</cell><cell>0.943</cell><cell>0.602</cell><cell>0.539</cell><cell>0.618</cell><cell>0.542</cell></row><row><cell>BM25 Neg</cell><cell>0.299</cell><cell>0.928</cell><cell>0.664</cell><cell>0.591</cell><cell>0.626</cell><cell>0.529</cell></row><row><cell>DPR (BM25 + Rand Neg)</cell><cell>0.311</cell><cell>0.952</cell><cell>0.653</cell><cell>0.600</cell><cell>0.629</cell><cell>0.557</cell></row><row><cell>BM25 ? Rand</cell><cell>0.280</cell><cell>0.948</cell><cell>0.609</cell><cell>0.576</cell><cell>0.637</cell><cell>0.566</cell></row><row><cell>BM25 ? NCE Neg</cell><cell>0.279</cell><cell>0.942</cell><cell>0.608</cell><cell>0.571</cell><cell>0.638</cell><cell>0.564</cell></row><row><cell>BM25 ? BM25 + Rand</cell><cell>0.306</cell><cell>0.939</cell><cell>0.648</cell><cell>0.591</cell><cell>0.626</cell><cell>0.540</cell></row><row><cell>ANCE (FirstP)</cell><cell>0.330</cell><cell>0.959</cell><cell>0.677</cell><cell>0.648</cell><cell>0.641</cell><cell>0.615</cell></row><row><cell>ANCE (MaxP)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.671</cell><cell>0.628</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Single Task</cell><cell cols="2">Multi Task</cell></row><row><cell></cell><cell>NQ</cell><cell>TQA</cell><cell>NQ</cell><cell>TQA</cell></row><row><cell>Retriever</cell><cell cols="4">Top-20/100 Top-20/100 Top-20/100 Top-20/100</cell></row><row><cell>BM25</cell><cell>59.1/73.7</cell><cell>66.9/76.7</cell><cell>-/-</cell><cell>-/-</cell></row><row><cell>DPR</cell><cell>78.4/85.4</cell><cell>79.4/85.0</cell><cell>79.4/86.0</cell><cell>78.8/84.7</cell></row><row><cell>BM25+DPR</cell><cell>76.6/83.8</cell><cell>79.8/84.5</cell><cell>78.0/83.9</cell><cell>79.9/84.4</cell></row><row><cell>ANCE</cell><cell>81.9/87.5</cell><cell>80.3/85.3</cell><cell>82.1/87.9</cell><cell>80.3/85.2</cell></row></table><note>Retrieval results (Answer Coverage at Top-20/100) on Natural Questions (NQ) and Trivial QA (TQA) in the setting from Karpukhin et al. (2020).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell cols="3">Corpus Size Dim Search</cell><cell>Gain</cell></row><row><cell>250 Million</cell><cell>768</cell><cell cols="2">KNN +18.4%</cell></row><row><cell>8 Billion</cell><cell>64</cell><cell cols="2">KNN +14.2%</cell></row><row><cell>8 Billion</cell><cell>64</cell><cell cols="2">ANN +15.5%</cell></row></table><note>Relative gains in the first stage retrieval of a commercial search engine. The gains are from changing the training of a produc- tion DR model to ANCE.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>OpenQA Test Scores in Single Task Setting. ANCE+Reader switches the retrieve of a system from DPR to ANCE and keeps the same reading model, which is RAG-Token on Natural Questions (NQ) and DPR Reader on Trivia QA (TQA).</figDesc><table><row><cell>Model</cell><cell>NQ TQA</cell></row><row><cell>T5-11B</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Efficiency of ANCE Search and Training.</figDesc><table><row><cell>Operation</cell><cell cols="2">Offline Online</cell></row><row><cell>BM25 Index Build</cell><cell>3h</cell><cell>-</cell></row><row><cell>BM25 Retrieval</cell><cell>-</cell><cell>37ms</cell></row><row><cell>BERT Rerank</cell><cell>-</cell><cell>1.15s</cell></row><row><cell>Sparse IR Total (BM25 + BERT)</cell><cell>-</cell><cell>1.42s</cell></row><row><cell>ANCE Inference</cell><cell></cell><cell></cell></row><row><cell>Encoding of Corpus/Per doc</cell><cell>10h/4.5ms</cell><cell>-</cell></row><row><cell>Query Encoding</cell><cell>-</cell><cell>2.6ms</cell></row><row><cell>ANN Retrieval (batched q)</cell><cell>-</cell><cell>9ms</cell></row><row><cell>Dense Retrieval Total</cell><cell cols="2">-11.6ms</cell></row><row><cell>ANCE Training</cell><cell></cell><cell></cell></row><row><cell>Encoding of Corpus/Per doc</cell><cell>10h/4.5ms</cell><cell>-</cell></row><row><cell>ANN Index Build</cell><cell>10s</cell><cell>-</cell></row><row><cell>Neg Construction Per Batch</cell><cell>72ms</cell><cell>-</cell></row><row><cell>Back Propagation Per Batch</cell><cell>19ms</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Coverage of TREC 2019 DL Track labels on Dense Retrieval methods. Overlap with BM25 is calculated on top 100 retrieved documents. There are two tasks in the TREC DL 2019 Track: document retrieval and passage retrieval. The training and development sets are from MS MARCO, which includes passage level relevance labels for one million Bing queries</figDesc><table><row><cell></cell><cell></cell><cell>TREC DL Passage</cell><cell></cell><cell></cell><cell>TREC DL Document</cell><cell></cell></row><row><cell>Method</cell><cell cols="6">Recall@1K Hole@10 Overlap w. BM25 Recall@100 Hole@10 Overlap w. BM25</cell></row><row><cell>BM25</cell><cell>0.685</cell><cell>5.9%</cell><cell>100%</cell><cell>0.387</cell><cell>0.2%</cell><cell>100%</cell></row><row><cell>BM25 Neg</cell><cell>0.569</cell><cell>25.8%</cell><cell>11.9%</cell><cell>0.217</cell><cell>28.1%</cell><cell>17.9%</cell></row><row><cell>BM25 + Rand Neg</cell><cell>0.662</cell><cell>20.2%</cell><cell>16.4%</cell><cell>0.240</cell><cell>21.4%</cell><cell>21.0%</cell></row><row><cell>ANCE (FirstP)</cell><cell>0.661</cell><cell>14.8%</cell><cell>17.4%</cell><cell>0.266</cell><cell>13.3%</cell><cell>24.4%</cell></row><row><cell>ANCE (MaxP)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.286</cell><cell>11.9%</cell><cell>24.9%</cell></row><row><cell>A APPENDIX</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">A.1 MORE EXPERIMENTAL DETAILS</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">More Details on TREC DL Benchmarks:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Implementation Details:The training often takes about 1-2 hours per ANCE epoch, which is whenever new ANCE negative is ready, it immediately replaces existing negatives in training, without waiting. It converges in about 10 epochs, similar to other DR baselines. The optimization uses LAMB optimizer, learning rate 5e-6 for document and 1e-6 for passage retrieval, and linear warm-up and decay after 5000 steps. More detailed hyperparameter settings can be found in our code release.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Results of several different hyperparameter configurations. "Top K Neg" lists the top k ANN retrieved candidates from which we sampled the ANCE negatives from.</figDesc><table><row><cell></cell><cell></cell><cell>Hyperparameter</cell><cell></cell><cell cols="2">MARCO Dev Passage TREC DL Document</cell></row><row><cell></cell><cell cols="3">Learning rate Top K Neg Refresh (step)</cell><cell>Retrieval MRR@10</cell><cell>Retrieval NDCG@10</cell></row><row><cell>Passage ANCE</cell><cell>1e-6</cell><cell>200</cell><cell>10k</cell><cell>0.33</cell><cell>-</cell></row><row><cell></cell><cell>1e-6</cell><cell>500</cell><cell>10k</cell><cell>0.31</cell><cell>-</cell></row><row><cell></cell><cell>2e-6</cell><cell>200</cell><cell>10k</cell><cell>0.29</cell><cell>-</cell></row><row><cell></cell><cell>2e-7</cell><cell>500</cell><cell>20k</cell><cell>0.303</cell><cell>-</cell></row><row><cell></cell><cell>2e-7</cell><cell>1000</cell><cell>20k</cell><cell>0.302</cell><cell>-</cell></row><row><cell>Document ANCE</cell><cell>1e-5</cell><cell>100</cell><cell>10k</cell><cell>-</cell><cell>0.58</cell></row><row><cell></cell><cell>1e-6</cell><cell>100</cell><cell>20k</cell><cell>-</cell><cell>0.59</cell></row><row><cell></cell><cell>1e-6</cell><cell>100</cell><cell>5k</cell><cell>-</cell><cell>0.60</cell></row><row><cell></cell><cell>5e-6</cell><cell>200</cell><cell>10k</cell><cell>-</cell><cell>0.614</cell></row><row><cell></cell><cell>1e-6</cell><cell>200</cell><cell>10k</cell><cell>-</cell><cell>0.61</cell></row><row><cell cols="5">A.2 OVERLAP WITH SPARSE RETRIEVAL IN TREC 2019 DL TRACK</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>All DR methods have very low overlap with the official BM25 in their top 100 retrieved documents. At most, only 25% of documents retrieved by DR are also retrieved by BM25. This makes the hole rate quite high and the recall metric not very informative. It also suggests that DR methods might benefit more in this year's TREC 2020 Deep Learning Track if participants are contributing DR based systems.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Queries in the TREC 2019 DL Track Document Ranking Tasks where ANCE performs better than BM25. Snippets are manually extracted. The documents in the first disagreed ranking position are shown, where on all examples ANCE won. The NDCG@10 of ANCE and BM25 in the corresponding query is listed.</figDesc><table><row><cell></cell><cell>ANCE</cell><cell>BM25</cell></row><row><cell>Query:</cell><cell cols="2">qid (104861): Cost of interior concrete flooring</cell></row><row><cell>Title:</cell><cell cols="2">Concrete network: Concrete Floor Cost Pinterest: Types of Flooring</cell></row><row><cell>DocNo:</cell><cell>D293855</cell><cell>D2692315</cell></row><row><cell>Snippet:</cell><cell>For a concrete floor with a basic finish,</cell><cell>Know About Hardwood Flooring And</cell></row><row><cell></cell><cell>you can expect to pay $2 to $12 per</cell><cell>Its Types White Oak Floors Oak Floor-</cell></row><row><cell></cell><cell>square foot. . .</cell><cell>ing Laminate Flooring In Bathroom . . .</cell></row><row><cell cols="2">Ranking Position: 1</cell><cell>1</cell></row><row><cell>TREC Label:</cell><cell>3 (Very Relevant)</cell><cell>0 (Irrelevant)</cell></row><row><cell>NDCG@10:</cell><cell>0.86</cell><cell>0.15</cell></row><row><cell>Query:</cell><cell cols="2">qid (833860): What is the most popular food in Switzerland</cell></row><row><cell>Title:</cell><cell>Wikipedia: Swiss cuisine</cell><cell>Answers.com: Most popular traditional</cell></row><row><cell></cell><cell></cell><cell>food dishes of Mexico</cell></row><row><cell>DocNo:</cell><cell>D1927155</cell><cell>D3192888</cell></row><row><cell>Snippet:</cell><cell>Swiss cuisine bears witness to many re-</cell><cell>One of the most popular traditional Mex-</cell></row><row><cell></cell><cell>gional influences, . . . Switzerland was</cell><cell>ican deserts is a spongy cake . . . (in</cell></row><row><cell></cell><cell>historically a country of farmers, so tra-</cell><cell>the related questions section) What is</cell></row><row><cell></cell><cell>ditional Swiss dishes tend not to be. . .</cell><cell>the most popular food dish in Switzer-</cell></row><row><cell></cell><cell></cell><cell>land?. . .</cell></row><row><cell cols="2">Ranking Position: 1</cell><cell>1</cell></row><row><cell>TREC Label:</cell><cell>3 (Very Relevant)</cell><cell>0 (Irrelevant)</cell></row><row><cell>NDCG@10:</cell><cell>0.90</cell><cell>0.14</cell></row><row><cell>Query:</cell><cell>qid (1106007): Define visceral</cell><cell></cell></row><row><cell>Title:</cell><cell>Vocabulary.com: Visceral</cell><cell>Quizlet.com: A&amp;P EX3 autonomic 9-10</cell></row><row><cell>DocNo:</cell><cell>D542828</cell><cell>D830758</cell></row><row><cell>Snippet:</cell><cell>When something's visceral, you feel it</cell><cell>Acetylcholine A neurotransmitter liber-</cell></row><row><cell></cell><cell>in your guts. A visceral feeling is in-</cell><cell>ated by many peripheral nervous system</cell></row><row><cell></cell><cell>tuitive -there might not be a rational</cell><cell>neurons and some central nervous sys-</cell></row><row><cell></cell><cell>explanation, but you feel that you know</cell><cell>tem neurons. . .</cell></row><row><cell></cell><cell>what's best. . .</cell><cell></cell></row><row><cell cols="2">Ranking Position: 1</cell><cell>1</cell></row><row><cell>TREC Label:</cell><cell>3 (Very Relevant)</cell><cell>0 (Irrelevant)</cell></row><row><cell>NDCG@10:</cell><cell>0.80</cell><cell>0.14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Queries in the TREC 2019 DL Track Document Ranking Tasks where ANCE performs worse than BM25. Snippets are manually extracted. The documents in the first position where BM25 wins are shown. The NDCG@10 of ANCE and BM25 in the corresponding query is listed. Typos in the query are from the real web search queries in TREC. function) is a function between ordered sets that preserves or reverses the given order... For example, if y=g(x) is strictly monotonic on the range [a,b] . . . I'm going to write a series of articles about the things SQL needs to work faster and more efficienly. . . 've been doing yoga for a few weeks now and already notice that my flexiablity has increased drastically. . . . That depends on the posture itself . . . Bow Pose is an intermediate yoga backbend that deeply opens the chest and the front of the body. . . Hold for up to 30 seconds . . .</figDesc><table><row><cell></cell><cell>ANCE</cell><cell>BM25</cell></row><row><cell>Query:</cell><cell cols="2">qid (182539): Example of monotonic function</cell></row><row><cell>Title:</cell><cell>Wikipedia: Monotonic function</cell><cell>Explain Extended: Things SQL needs:</cell></row><row><cell></cell><cell></cell><cell>sargability of monotonic functions</cell></row><row><cell>DocNo:</cell><cell>D510209</cell><cell>D175960</cell></row><row><cell>Snippet:</cell><cell>In mathematics, a monotonic function</cell><cell></cell></row><row><cell cols="2">(or monotone Ranking Position: 1</cell><cell>1</cell></row><row><cell>TREC Label:</cell><cell>0 (Irrelevant)</cell><cell>2 (Relevant)</cell></row><row><cell>NDCG@10:</cell><cell>0.25</cell><cell>0.61</cell></row><row><cell>Query:</cell><cell>qid (1117099): What is a active margin</cell><cell></cell></row><row><cell>Title:</cell><cell>Wikipedia: Margin (finance)</cell><cell>Yahoo Answer: What is the difference</cell></row><row><cell></cell><cell></cell><cell>between passive and active continental</cell></row><row><cell></cell><cell></cell><cell>margins</cell></row><row><cell>DocNo:</cell><cell>D166625</cell><cell>D2907204</cell></row><row><cell>Snippet:</cell><cell>In finance, margin is collateral that the</cell><cell>An active continental margin is found on</cell></row><row><cell></cell><cell>holder of a financial instrument . . .</cell><cell>the leading edge of the continent where</cell></row><row><cell></cell><cell></cell><cell>. . .</cell></row><row><cell cols="2">Ranking Position: 2</cell><cell>2</cell></row><row><cell>TREC Label:</cell><cell>0 (Irrelevant)</cell><cell>3 (Very Relevant)</cell></row><row><cell>NDCG@10:</cell><cell>0.44</cell><cell>0.74</cell></row><row><cell>Query:</cell><cell cols="2">qid (1132213): How long to hold bow in yoga</cell></row><row><cell>Title:</cell><cell>Yahoo Answer: How long should you</cell><cell>yogaoutlet.com: How to do bow pose in</cell></row><row><cell></cell><cell>hold a yoga pose for</cell><cell>yoga</cell></row><row><cell>DocNo:</cell><cell>D3043610</cell><cell>D3378723</cell></row><row><cell cols="2">Snippet: so iRanking Position: 3</cell><cell>3</cell></row><row><cell>TREC Label:</cell><cell>0 (Irrelevant)</cell><cell>3 (Very Relevant)</cell></row><row><cell>NDCG@10:</cell><cell>0.66</cell><cell>0.74</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/facebookresearch/DPR. 2 https://huggingface.co/transformers/master/model_doc/rag.html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Variance reduction in sgd by distributed importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinnadhurai</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06481</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Ms marco: A human generated machine reading comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mcnamara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09268</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Pre-training tasks for embedding-based large-scale retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin-Wen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03932</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reading wikipedia to answer open-oomain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1870" to="1879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Overview of the trec 2019 deep learning track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text REtrieval Conference (TREC). TREC</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Search engines: information retrieval in practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Strohman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Addison-Wesley Reading</publisher>
			<biblScope unit="volume">520</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Context-aware sentence/passage term importance estimation for first stage retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10687</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deeper text understanding for ir with contextual neural language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="985" to="988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transformer-XL: attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Vse++: Improving visual-semantic embeddings with hard negatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05612</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Understanding bert rankers under distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 ACM SIGIR on International Conference on Theory of Information Retrieval</title>
		<meeting>the 2020 ACM SIGIR on International Conference on Theory of Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="149" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Complementing lexical retrieval with semantic residual embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13969</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A deep relevance matching model for ad-hoc retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 25th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Accelerating large-scale inference with anisotropic vector quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Lindgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Simcha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Chern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10396</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: a new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyv?rinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 13th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08909</idno>
		<title level="m">Realm: retrieval-augmented language model pre-training</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Embedding-based retrieval in facebook search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jui-Ting</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuying</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pronin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janani</forename><surname>Padmanabhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Ottaviano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2553" to="2561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Poly-encoders: architectures and pre-training strategies for fast and accurate multi-sentence scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Humeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Billion-scale similarity search with gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Training deep models faster with robust, approximate importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tyler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7265" to="7275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Triviaqa: a large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1601" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04906</idno>
		<title level="m">Dense passage retrieval for open-domain question answering</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Not all samples are created equal: Deep learning with importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Fleuret</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00942</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Generalization through memorization: Nearest neighbor language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00172</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Colbert: Efficient and effective passage search via contextualized late interaction over bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12832</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Natural questions: a benchmark for question answering research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="453" to="466" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Relevance-based language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computing Machinery</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="260" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Latent retrieval for weakly supervised open domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6086" to="6096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gargi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15020</idno>
		<title level="m">Pre-training via paraphrasing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandara</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>K?ttler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.11401</idno>
		<title level="m">Tim Rockt?schel, et al. Retrieval-augmented generation for knowledgeintensive nlp tasks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to rank for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and trends in information retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="225" to="331" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">a robustly optimized BERT pretraining approach</title>
		<meeting><address><addrLine>RoBERTa</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00181</idno>
		<title level="m">Sparse, dense, and attentional representations for text retrieval</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Macavaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaele</forename><surname>Nardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Perego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazli</forename><surname>Tonellotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ophir</forename><surname>Goharian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frieder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14255</idno>
		<title level="m">Efficient document re-ranking for transformers by precomputing term representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">An introduction to neural information retrieval. Foundations and Trends? in Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Bhaskar Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Craswell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Mosbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksym</forename><surname>Andriushchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Klakow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04884</idno>
		<title level="m">On the stability of fine-tuning bert: Misconceptions, explanations, and strong baselines</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04085</idno>
		<title level="m">Passage Re-ranking with BERT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Document expansion by query prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08375</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Understanding the behaviors of bert in ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07531</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">How much knowledge can you pack into the parameters of a language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08910</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The fact extraction and verification (FEVER) shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oana</forename><surname>Cocarascu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpit</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Fact Extraction and VERification (FEVER)</title>
		<meeting>the 1st Workshop on Fact Extraction and VERification (FEVER)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Variations in relevance judgments and the measurement of retrieval effectiveness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ellen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="697" to="716" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<title level="m">Glue: a multi-task benchmark and analysis platform for natural language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Unsupervised feature learning via non-parametric instance-level discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.01978</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">End-to-end neural ad-hoc ranking with kernel pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Answering complex open-domain questions with multi-hop dense retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorraine</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srini</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barlas</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oguz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.12756</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Idst at trec 2019 deep learning track: Deep cascade ranking with generation-based document expansion and pre-trained language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangnan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text REtrieval Conference. TREC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Transformer-xh: multi-evidence reasoning with extra hop attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corby</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Bert-qe: Contextualized query expansion for document re-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.07258</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
