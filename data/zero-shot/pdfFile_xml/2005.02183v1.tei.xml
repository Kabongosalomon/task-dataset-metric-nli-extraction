<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Comparing SNNs and RNNs on Neuromorphic Vision Datasets: Similarities and Differences</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Precision Instrument</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>93106</postCode>
									<settlement>Santa Barbara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Precision Instrument</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Center for Brain Inspired Computing Research</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Deng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>93106</postCode>
									<settlement>Santa Barbara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Precision Instrument</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Center for Brain Inspired Computing Research</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Precision Instrument</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Tian</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">Lab of Cognitive Neuroscience</orgName>
								<orgName type="institution" key="instit1">THBI</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Precision Instrument</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Precision Instrument</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>93106</postCode>
									<settlement>Santa Barbara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Comparing SNNs and RNNs on Neuromorphic Vision Datasets: Similarities and Differences</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Spiking Neural Networks</term>
					<term>Recurrent Neural Net- works</term>
					<term>Long Short-Term Memory</term>
					<term>Neuromorphic Dataset</term>
					<term>Spa- tiotemporal Dynamics</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neuromorphic data, recording frameless spike events, have attracted considerable attention for the spatiotemporal information components and the event-driven processing fashion. Spiking neural networks (SNNs) represent a family of event-driven models with spatiotemporal dynamics for neuromorphic computing, which are widely benchmarked on neuromorphic data. Interestingly, researchers in the machine learning community can argue that recurrent (artificial) neural networks (RNNs) also have the capability to extract spatiotemporal features although they are not event-driven. Thus, the question of "what will happen if we benchmark these two kinds of models together on neuromorphic data" comes out but remains unclear.</p><p>In this work, we make a systematic study to compare SNNs and RNNs on neuromorphic data, taking the vision datasets as a case study. First, we identify the similarities and differences between SNNs and RNNs (including the vanilla RNNs and LSTM) from the modeling and learning perspectives. To improve comparability and fairness, we unify the supervised learning algorithm based on backpropagation through time (BPTT), the loss function exploiting the outputs at all timesteps, the network structure with stacked fully-connected or convolutional layers, and the hyperparameters during training. Especially, given the mainstream loss function used in RNNs, we modify it inspired by the rate coding scheme to approach that of SNNs. Furthermore, we tune the temporal resolution of datasets to test model robustness and generalization. At last, a series of contrast experiments are conducted on two types of neuromorphic datasets: DVS-converted (N-MNIST) and DVS-captured (DVS Gesture). Extensive insights regarding recognition accuracy, feature extraction, temporal resolution and contrast, learning generalization, computational complexity and parameter volume are provided, which are beneficial for the model selection on different workloads and even for the invention of novel neural models in the future.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Neuromorphic vision datasets <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref> sense the dynamic change of pixel intensity and record the resulting spike events using dynamic vision sensors (DVS) <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b6">[7]</ref>. Compared to conventional frame-based vision datasets, the frameless neuromorphic vision datasets have rich spatiotemporal components by interacting the spatial and temporal information and follow the event-driven processing fashion triggered by binary spikes. Owing to these unique features, neuromorphic data have attracted considerable attention in many applications such as visual recognition <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b10">[11]</ref> [12]- <ref type="bibr" target="#b13">[14]</ref>, motion segmentation <ref type="bibr" target="#b14">[15]</ref>, tracking control <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref>, robotics <ref type="bibr" target="#b18">[19]</ref>, etc. Currently, there are two types of neuromorphic vision datasets: one is converted from static datasets by scanning each image in front of DVS cameras, e.g. N-MNIST <ref type="bibr" target="#b0">[1]</ref> and CIFAR10-DVS <ref type="bibr" target="#b2">[3]</ref>; the other is directly captured by DVS cameras from moving objects, e.g. DVS Gesture <ref type="bibr" target="#b13">[14]</ref>.</p><p>Spiking neural networks (SNNs) <ref type="bibr" target="#b19">[20]</ref>, inspired by brain circuits, represent a family of models for neuromorphic computing. Each neuron in an SNN model updates the membrane potential based on its memorized state and current inputs, and fires a spike when the membrane potential crosses a threshold. The spiking neurons communicate with each other using binary spike events rather than continuous activations in artificial neural networks (ANNs), and an SNN model carries both spatial and temporal information. The rich spatiotemporal dynamics and event-driven paradigm of SNNs hold great potential in efficiently handling complex tasks such as spike pattern recognition <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b20">[21]</ref>, optical flow estimation <ref type="bibr" target="#b21">[22]</ref>, and simultaneous localization and map (SLAM) building <ref type="bibr" target="#b22">[23]</ref>, which motivates their wide deployment on low-power neuromorphic devices <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b25">[26]</ref>. Since the behaviors of SNNs naturally match the characteristics of neuromorphic data, a considerable amount of literature benchmark the performance of SNNs on neuromorphic datasets <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b28">[29]</ref>.</p><p>Originally, neuromorphic computing and machine learning are two domains developing in parallel and are usually independent of each other. It seems that this situation is changing as more interdisciplinary researches emerge <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. In this context, researchers in the machine learning community can argue that SNNs are not unique for the processing of neuromorphic data. The reason is that recurrent (artificial) neural networks (RNNs) can also memorize previous states and behave spatiotemporal dynamics, even though they are not event-driven. By treating the spike events as normal binary values in {0, 1}, RNNs are able to process neuromorphic datasets too. In essence, RNNs have been widely applied in many tasks with timing sequences such as language modeling <ref type="bibr" target="#b30">[31]</ref>, speech recognition <ref type="bibr" target="#b31">[32]</ref>, and machine translation <ref type="bibr" target="#b32">[33]</ref>; whereas, there are rare studies that evaluate the performance of RNNs on neuromorphic data, thus the mentioned debate still remains open.</p><p>In this work, we try to answer what will happen when benchmarking SNNs and RNNs together on neuromorphic data, taking the vision datasets as a case study. First, we identify the similarities and differences between SNNs and RNNs from the modeling and learning perspectives. For comparability and fairness, we unify several things: i) supervised learning algorithm based on backpropagation through time (BPTT); ii) loss function inspired by the SNN-oriented rate coding scheme; iii) network structure based on stacked fullyconnected (FC) or convolutional (Conv) layers; iv) hyperparameters during training. Moreover, we tune the temporal resolution of neuromorphic vision datasets to test the model robustness and generalization. At last, we conduct a series of contrast experiments on typical neuromorphic vision datasets and provide extensive insights. Our work holds potential in guiding the model selection on different workloads and stimulating the invention of novel neural models. For clarity, we summarize our contributions as follows:</p><p>? We present the first work that systematically compares SNNs and RNNs on neuromorphic datasets. ? We identify the similarities and differences between SNNs and RNNs, and unify the learning algorithm, loss function, network structure, and training hyperparameters to ensure the comparability and fairness. Especially, we modify the mainstream loss function of RNNs to approach that of SNNs and tune the temporal resolution of neuromorphic vision datasets to test model robustness and generalization. ? On two kinds of typical neuromorphic vision datasets: DVS-converted (N-MNIST) and DVS-captured (DVS Gesture), we conduct a series of contrast experiments that yield extensive insights regarding recognition accuracy, feature extraction, temporal resolution and contrast, learning generalization, computational complexity and parameter volume (detailed in Section IV and summarized in Section V), which are beneficial for future model selection and construction. The rest of this paper is organized as follows: Section II introduces some preliminaries of neuromorphic vision datasets, SNNs, and RNNs; Section III details our methodology to make SNNs and RNNs comparable and ensure the fairness; Section IV shows the experimental results and provides our insights; Finally, Section V concludes and discusses the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PRELIMINARIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Neuromorphic Vision Datasets</head><p>A neuromorphic vision dataset consists of many spike events, which are triggered by the intensity change (increase or decrease) of each pixel in the sensing field of the DVS camera <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b17">[18]</ref>. A DVS camera records the spike events in two channels according to the different change directions, e.g. the On channel for intensity increase and the Off channel for intensity decrease. The whole spike train in a neuromorphic vision dataset can be represented as an H ? W ? 2 ? T 0 sized spike pattern, where H, W are the height and width of the sensing field, respectively, T 0 stands for the length of recording time, and "2" indicates the two channels. As mentioned in Introduction, currently there are two types of neuromorphic vision datasets: DVS-converted and DVS-captured, which are detailed as below.</p><p>DVS-Converted Dataset. Generally, DVS-converted datasets are converted from frame-based static image datasets. The spike events in a DVS-converted dataset are acquired by scanning each image with repeated closed-loop smooth (RCLS) movement in front of a DVS camera <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b33">[34]</ref>, where the movement incurs pixel intensity changes. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates a DVS-converted dataset named N-MNIST <ref type="bibr" target="#b0">[1]</ref>. The original MNIST dataset includes 60000 28 ? 28 static images of handwritten grayscale digits for training and extra 10000 for testing; accordingly, the DVS camera converts each image in MNIST into a 34 ? 34 ? 2 ? T 0 spike pattern in N-MNIST. The larger sensing field is caused by the RCLS movement. Compared to the original frame-based static image dataset, the converted frameless dataset contains additional temporal information while retaining the similar spatial information. Nevertheless, the extra temporal information cannot become dominant due to the static information source, and some works even point out that the DVS-converted datasets might be not good enough to benchmark SNNs <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b34">[35]</ref>.</p><p>Te m po ra l ax is DVS-Captured Dataset. In contrast, DVS-captured datasets generate spike events via natural motion rather than the simulated movement used in the generation of DVS-converted datasets. <ref type="figure" target="#fig_2">Figure 2</ref> depicts a DVS-captured dataset named DVS Gesture <ref type="bibr" target="#b13">[14]</ref>. There are 11 hand and arm gestures performed by one subject in each trail, and there are total 122 trails in the dataset. Three lighting conditions including natural light, fluorescent light, and LED light are selected to control the effects of shadow and flicker on the DVS camera, providing a bias improvement for the data distribution. Different from the DVS-converted datasets, both temporal and spatial information in DVS-captured datasets are featured as essential components.</p><formula xml:id="formula_0">(a) (b) (c) (d)</formula><p>Te m po ra l ax is  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Spiking Neural Networks</head><p>There are several different spiking neuron models such as leaky integrate and fire (LIF) <ref type="bibr" target="#b35">[36]</ref>, Izhikevich <ref type="bibr" target="#b36">[37]</ref>, and Hodgkin-Huxley <ref type="bibr" target="#b37">[38]</ref>, among which LIF is the most widely used in practice due to the lower complexity <ref type="bibr" target="#b38">[39]</ref>. In this work, we adopt the mainstream solution, i.e. taking LIF for neuron simulation. By connecting many spiking neurons through synapses, we can construct an SNN model. In this paper, for simplicity, we just consider feedforward SNNs with stacked FC or Conv layers.</p><p>There are two state variables in a LIF neuron: membrane potential (u) and output activity (o). u is a continuous value while o can only be a binary value, i.e. firing a spike or not. The behaviors of an SNN layer can be described as</p><formula xml:id="formula_1">? ? ? ? ? ? du u u n (t) dt = ?u u u n (t) + W W W n o o o n?1 (t) o n i (t) = 1 &amp; u n i (t) = u 0 , if u n i (t) ? u th o n i (t) = 0, if u n i (t) &lt; u th<label>(1)</label></formula><p>where t denotes time, n and i are indices of layer and neuron, respectively, ? is a time constant, and W W W is the synaptic weight matrix between two adjacent layers. The neuron fires a spike and resets u = u 0 only when u exceeds a firing threshold (u th ), otherwise, the membrane potential would just leak. Notice that o o o 0 (t) denotes the network input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Recurrent Neural Networks</head><p>In this work, RNNs mainly mean recurrent ANNs rather than SNNs. We select two kinds of RNN models in this work: one is the vanilla RNN and the other is the modern RNN named long short-term memory (LSTM).</p><p>Vanilla RNN. RNNs introduce temporal dynamics via recurrent connections. There is only one continuous state variable in a vanilla RNN neuron, called hidden state (h). The behaviors of a vanilla RNN layer follow</p><formula xml:id="formula_2">h h h t,n = ?(W W W n 1 h h h t,n?1 + W W W n 2 h h h t?1,n + b b b n )<label>(2)</label></formula><p>where t and n denote the indices of timestep and layer, respectively, W W W 1 is the weight matrix between two adjacent layers, W W W 2 is the intra-layer recurrent weight matrix, and b b b is a bias vector. ?(?) is an activation function, which can be the tanh(?) function in general for vanilla RNNs. Similar to the o o o 0 (t) for SNNs, h h h t,0 also denotes the network input of RNNs, i.e. x x x t . Long Short-Term Memory (LSTM). LSTM is designed to improve the long-term temporal dependence over vanilla RNNs by introducing complex gates to alleviate gradient vanishing <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>. An LSTM layer can be described as</p><formula xml:id="formula_3">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? f f f t,n = ? f (W W W n f 1 h h h t,n?1 + W W W n f 2 h h h t?1,n + b b b n f ) i i i t,n = ? i (W W W n i1 h h h t,n?1 + W W W n i2 h h h t?1,n + b b b n i ) o o o t,n = ? o (W W W n o1 h h h t,n?1 + W W W n o2 h h h t?1,n + b b b n o ) g g g t,n = ? g (W W W n g1 h h h t,n?1 + W W W n g2 h h h t?1,n + b b b n g ) c c c t,n = c c c t?1,n ? f f f t,n + g g g t,n ? i i i t,n h h h t,n = ? c (c c c t,n ) ? o o o t,n<label>(3)</label></formula><p>where t and n denote the indices of timestep and layer, respectively, f f f , i i i, o o o are the states of forget, input, and output gates, respectively, and g g g is the input activation. Each gate has its own weight matrices and bias vector. c c c and h h h are cellular and hidden states, respectively. ?(?) and ?(?) are sigmoid and tanh functions, respectively, and ? is the Hadamard product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>To avoid ambiguity, we would like to emphasize again that our "SNNs vs. RNNs" in this work is defined as "feedforward SNNs vs. recurrent ANNs". For simplicity, we only select two representatives from the RNN family, i.e. vanilla RNNs and LSTM. In this section, we first rethink the similarities and differences between SNNs and RNNs from the modeling and learning perspectives, and discuss how to ensure the comparability and fairness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Rethinking the Similarities</head><p>Before analysis, we first convert Equation (1) to its iterative version to make it compatible with the format in Equation (2)-(3). This can be achieved by solving the first-order differential equation in Equation <ref type="formula" target="#formula_1">(1)</ref>, which yields</p><formula xml:id="formula_4">u u u t,n = e ? dt ? u u u t?1,n ? (1 ? o o o t?1,n ) + W W W n o o o t,n?1 o o o t,n = f (u u u t,n ? u th )<label>(4)</label></formula><p>where e ? dt ? reflects the leakage effect of the membrane potential, and f (?) is a step function that satisfies  <ref type="formula" target="#formula_4">4)</ref>, it can be seen that the models of SNNs and RNNs are quite similar, involving both temporal and spatial dimensions. <ref type="figure" target="#fig_3">Figure 3</ref> further visualizes the information propagation paths of SNNs, vanilla RNNs, and LSTM in both forward and backward passes. Here we denote the hidden state before ? in vanilla RNNs ash and denote the gate states before ?, ? in LSTM asf ,?,?,g.</p><formula xml:id="formula_5">f (x) = 1 t t-1 n-1 n ? w # $ Layer Timestep % h t,n-1 h t,n-1 ? % h t-1,n-1 h t-1,n-1 w ' $(# w # $ ? % h t,n h t,n ? % h t-1,n h t-1,n w ' $ t t-1 n-1 n ?? w # $ Layer Timestep ? % h t,n-1 ?h t,n-1 ?? ? % h t-1,n-1 ?h t-1,n-1 w ' $(# w # $ ?? ? % h t,n ?h t,n ?? ? % h t?1,n ?h t-1,n w ' $ t t-1 n-1 n f Layer Timestep u t,n-1 o t,n-1 f u t-1,n-1 o t-1,n-1 W $ f u t,n o t,n f u t-1,n o t-1,n W $ t t-1 n-1 n f' Layer Timestep ?u t,n-1 ?o t,n-1 f' ?u t-1,n-1 ?o t-1,n-1 W $ f' ?u t,n ?o t,n f' ?u t-1,n ?o t-1,n W $ w # $ t t-1 n-1 n Layer Timestep h t,n-1 h t-1,n-1 h t,n h t-1,n w # $ w ' $ w ' $(# w # $ t t-1 n-1 n Layer Timestep ?h t,n-1 ?h t-1,n-1 ?h t,n ?h t-1,n w # $ w ' $ w ' $(# (a) (b) (c)</formula><p>Spatiotemporal Dynamics in the Forward Pass. First, the forward propagation paths of SNNs and vanilla RNNs are similar if u and o of SNNs are regarded ash and h of vanilla RNNs, respectively. Second, for LSTM, there are more intermediate states inside a neuron, includingf ,?,?,g and the cellular state. Although the neuron becomes complicated, the overall spatiotemporal path is still similar if we just pay attention to the propagation of the hidden state h. Interestingly, the internal membrane potential of each spiking neuron can directly affect the neuronal state at the next timestep, which differs them from vanilla RNNs but acts similarly as the forget gate of LSTM.</p><p>Spatiotemporal Backpropagation. For SNNs, the learning algorithms significantly vary in literature, for example, including unsupervised learning <ref type="bibr" target="#b41">[42]</ref>, ANN-to-SNN conversion <ref type="bibr" target="#b42">[43]</ref>, and supervised learning <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b43">[44]</ref>. Since RNNs are usually trained by the gradient-descent-based supervised algorithm in the machine learning domain, we select a recent BPTT-inspired spatiotemporal backpropagation algorithm <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b20">[21]</ref> for SNNs to make our comparison fair. Also from <ref type="figure" target="#fig_3">Figure 3</ref>, it can be seen that the gradient propagation paths of SNNs, vanilla RNNs, and LSTM also follow the similar spatiotemporal fashion. Moreover, we detail the backpropagation formula of each model for better understanding. Notice that the variable ? denotes the gradient, for example, ?o = ?L ?o where L is the loss function of the network. For SNNs, we have</p><formula xml:id="formula_6">?o o o t,n = (W W W n+1 ) T ?u u u t,n+1 ? e ? dt ? ?u u u t+1,n ? u u u t,n ?u u u t,n = ?o o o t,n ? f + e ? dt ? ?u u u t+1,n ? (1 ? o o o t,n )<label>(5)</label></formula><p>where the firing function is non-differentiable. To this end, a Dirac-like function is introduced to approximate its derivative <ref type="bibr" target="#b11">[12]</ref>. Specifically, f (?) can be calculated by</p><formula xml:id="formula_7">f (u ? u th ) ? 1 a , |u ? u th | ? a 2 0, otherwise<label>(6)</label></formula><p>where a is a hyper-parameter to control the gradient width when passing the firing function during backpropagation. For vanilla RNNs, we have a similar format as follows</p><formula xml:id="formula_8">?h h h t,n = (W W W n+1 1 ) T (?h h h t,n+1 ? ? ) + (W W W n 2 ) T (?h h h t+1,n ? ? ). (7)</formula><p>For LSTM, the situation becomes complicated due to the interaction between gates. Specifically, we can similarly have</p><formula xml:id="formula_9">?h h h t,n = ?h h h t,n+1 ?h h h t,n ?h h h t,n+1 + ?h h h t+1,n ?h h h t,n ?h h h t+1,n<label>(8)</label></formula><p>where the two items on the right side represent the spatial gradient backpropagation and the temporal gradient backpropagation, respectively. Moreover, we can yield</p><formula xml:id="formula_10">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?h h h t,n+1 ?h h h t,n = (W W W n+1 o1 ) T diag(? c (c c c t,n+1 ) ? ? o ) + (W W W n+1 f 1 ) T diag(o o o t,n+1 ? ? c ? c c c t?1,n+1 ? ? f ) + (W W W n+1 i1 ) T diag(o o o t,n+1 ? ? c ? g g g t,n+1 ? ? i ) + (W W W n+1 g1 ) T diag(o o o t,n+1 ? ? c ? i i i t,n+1 ? ? g ) ?h h h t+1,n ?h h h t,n = (W W W n o2 ) T diag(? c (c c c t+1,n ) ? ? o ) + (W W W n f 2 ) T diag(o o o t+1,n ? ? c ? c c c t,n ? ? f ) + (W W W n i2 ) T diag(o o o t+1,n ? ? c ? g g g t+1,n ? ? i ) + (W W W n g2 ) T diag(o o o t+1,n ? ? c ? i i i t+1,n ? ? g )<label>(9)</label></formula><p>where diag(?) converts a vector into a diagonal matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Rethinking the Differences</head><p>Although SNNs, vanilla RNNs, and LSTM are similar in terms of information propagation paths, they are still quite different. In this subsection, we give our rethinking on their differences.</p><p>Connection Pattern. From Equation <ref type="formula" target="#formula_2">(2)-(4)</ref>, it can be seen that the connection pattern of these models are different. First, for the neurons in the same layer, SNNs only have self-recurrence within each neuron, while RNNs have crossrecurrence among neurons. Specifically, the self-recurrence means that there are only intra-neuron recurrent connections; by contrast, the cross-recurrence allows inter-neuron recurrent connections within each layer. Second, the recurrent weights of SNNs are determined by the leakage factor of the membrane potential, which is restricted at ?e ? dt ? ; while in RNNs, the recurrent weights are trainable parameters. To make them clear, we use <ref type="figure" target="#fig_4">Figure 4</ref> to visualize the connection pattern of SNNs and RNNs and use <ref type="figure" target="#fig_5">Figure 5</ref> to show the distribution of recurrent weights collected from practical models, which reflect the theoretical analysis. Neuron Model. Besides the analysis of the connection pattern, we discuss the modeling details inside each neuron unit. As depicted in <ref type="figure">Figure 6</ref>, apparently, there are no gates in vanilla RNNs, unlike the complex gates in LSTM. For SNNs, as aforementioned, the extra membrane potential path is similar to the forget gate of LSTM; however, the reset mechanism bounds the membrane potential, unlike the unbounded cellular state in LSTM. In addition, as <ref type="figure">Figure 7</ref> shows, the activation function of SNNs is a firing function, which is essentially a step function with binary outputs; while the activation functions in vanilla RNNs and LSTM are continuous functions such as tanh and sigmoid. Loss Function. Under the framework of gradient descent based supervised learning, a loss function is critical for the overall optimization. The loss function formats for SNNs and RNNs are different. Specifically, for SNNs, the spike rate coding scheme is usually combined with the mean square error (MSE) to form a loss function, which can be abstracted as</p><formula xml:id="formula_11">h t-1,n h t,n-1 h t,n W # $ W % $ u t-1,n o t,n W $ c t-1,n c t,n h t,n RNN SNN LSTM Element-wise Operation Activation Function 1 - Vector Transfer Concatenate Copy (a) (b) (c) + h t,n-1 h t-1,n o t,n-1 o t-1,n u t,n ) *+ ,</formula><formula xml:id="formula_12">L = ||Y Y Y label ? 1 T t t=1 o o o t,N || 2 2 (10) where Y Y Y label is the label, o o o t,</formula><p>N is the output of the last layer, and T is the number of simulation timesteps during training. This loss function takes the output spikes fired at all timesteps into account, and thus the neuron fires the most determines the recognition result. Different from Equation <ref type="formula" target="#formula_1">(10)</ref>, the mainstream loss function for RNNs usually obeys  </p><formula xml:id="formula_13">L = ||Y Y Y label ? W W W y h h h T,N || 2 2<label>(11)</label></formula><formula xml:id="formula_14">SNNs ! Self-Neuron ?e ? dt ? Forget Gate Binary: f ire (f ) L = ||Y Y Y label ? 1 T t t=1 o o o t,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison Methodology</head><p>Based on the above analysis, we summarize the similarities and differences among SNNs, vanilla RNNs, and LSTM in <ref type="table" target="#tab_1">Table I</ref>. Owing to the similar spatiotemporal dynamics, it is possible to benchmark all these models on neuromorphic datasets. Moreover, facing the differences, we appropriately unify the following aspects to ensure comparability and fairness in our evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Dataset Selection and Temporal Resolution Tuning:</head><p>We benchmark all models on two neuromorphic vision datasets: one is a DVS-converted dataset named N-MNIST and the other is a DVS-captured dataset named DVS Gesture, which are already introduced in Section II-A. The detailed information of the two selected datasets is provided in <ref type="table" target="#tab_1">Table  II</ref>. For SNNs, the processing of neuromorphic data is natural due to the same spatiotemporal components and event-driven fashion; while for RNNs, the spike data are just treated as binary values, i.e. {0, 1}. Usually, the original recording time length of each spike pattern is very long, e.g. 10 5 . The underlying reason is due to the fine-grained temporal resolution of DVS cameras, originally at ?s level. However, the simulation timestep number of neural networks cannot be too large, otherwise, the time and memory costs during training are unaffordable. To this end, we consider the flexibility in tuning the temporal resolution. Specifically, every multiple successive slices of spike events in the original dataset within each temporal resolution unit are collapsed along the temporal dimension into one slice.</p><p>Here the temporal collapse means there will be a spike at the resulting pixel if there exist spikes at the same location in any original slices within the collapse window. We describe the collapse process as</p><formula xml:id="formula_15">S S S t = sign( t S S S t ), s.t. t ? [? dt ?t, ? dt ?(t+1)?1] (12)</formula><p>where S S S denotes the original slice sequence, t is the original recording timestep index, S S S denotes the new slice sequence after collapse, t is the new recording timestep index, and ? dt is the temporal resolution factor. sign is defined as:</p><formula xml:id="formula_16">sign(x) = 1, if x &gt; 0; sign(x) = 0, if x = 0; sign(x) &lt; 0, if x &lt; 0. After collapse, the original slice sequence {S S S t , t ? [0, T 0 ? 1]} becomes a new slice sequence {S S S t , t ? [0, T 0 /? dt ? 1]}.</formula><p>Apparently, the actual temporal resolution (dt) satisfies</p><formula xml:id="formula_17">dt = ? dt dt 0<label>(13)</label></formula><p>where dt 0 is the original temporal resolution. <ref type="figure" target="#fig_7">Figure 8</ref> illustrates an example of temporal collapse with ? dt = 3.  A large temporal resolution will increase the spike rate of new slices, as demonstrated in <ref type="figure" target="#fig_8">Figure 9</ref>. In addition, at each simulation timestep in Equation <ref type="formula" target="#formula_2">(2)</ref>-(4), the neural network processes one slice after the temporal collapse. Therefore, if the simulation timestep number T remains fixed, a larger temporal resolution could extend the actual simulation time, which is able to capture more temporal dependence in the neuromorphic dataset. By tuning the temporal resolution, we create opportunities to extract more insights from the change of model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Learning Algorithm and Loss Function Design:</head><p>Since RNNs are normally trained by supervised BPTT algorithm, to make the comparison fair, we select a recent BPTT-inspired learning algorithm with spatiotemporal gradient propagation for SNNs <ref type="bibr" target="#b11">[12]</ref>. Regarding the loss function required by gradient descent, the one in Equation <ref type="formula" target="#formula_1">(10)</ref> based on the rate coding scheme and MSE is widely used for SNNs. Under this loss function, there is gradient feedback at every timestep, which can alleviate the gradient vanishing problem to some extent.</p><p>In contrast, the existing loss functions for RNNs are flexible, including the mainstream one shown in Equation <ref type="formula" target="#formula_1">(11)</ref> that considers only the output at the last timestep and others that consider the outputs at all timesteps <ref type="bibr" target="#b44">[45]</ref>- <ref type="bibr" target="#b46">[47]</ref> such as</p><formula xml:id="formula_18">L = 1 T t ||Y Y Y label t ? W W W y h h h t,N || 2 2 .<label>(14)</label></formula><p>However, even if using the above loss function that considers the outputs at all timesteps, it is still slightly different from the one in Equation <ref type="formula" target="#formula_1">(10)</ref> for SNNs. To make the comparison fair, we provide two kinds of loss function configuration for RNNs. One is the mainstream loss function as in Equation <ref type="formula" target="#formula_1">(11)</ref>; the other is a modified version of Equation <ref type="formula" target="#formula_1">(14)</ref>, i.e.,</p><formula xml:id="formula_19">L = ||Y Y Y label ? 1 T t (W W W y h h h t,N )|| 2 2 .<label>(15)</label></formula><p>For clarity, we term the above format in Equation <ref type="formula" target="#formula_1">(15)</ref> for RNNs as the rate-coding-inspired loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Network Structure and Hyper-parameter Setting:</head><p>The FC layer based structure is widely used in SNNs and RNNs, which is termed as multilayered perceptron (MLP) based structure in this work. Whereas, the learning performance of MLP-based structures is usually poor, especially for visual tasks. To this end, the Conv layer based structure is introduced into SNNs to improve the learning performance <ref type="bibr" target="#b20">[21]</ref>, which is termed as convolutional neural network (CNN) based structure in this work. Facing this situation, besides the basic MLP structure, we also implement the CNN structure for RNNs, including both vanilla RNNs and LSTM. In this way, the comparison between different models is restricted on the same network structure, which is more fair. <ref type="table" target="#tab_1">Table  III</ref> provides the network structure configuration on different datasets. Since N-MNIST is a simpler task, we only use the MLP structure; while for DVS Gesture, we adopt both MLP and CNN structures.  Note: nC3-Conv layer with n output feature maps and 3 ? 3 weight kernel size, MP4-max pooling with 4 ? 4 pooling kernel size, AP2-average pooling with 2 ? 2 pooling kernel size.</p><p>Besides the network structure, the training process needs some hyper-parameters such as number of epochs, number of timesteps, batch size, learning rate, etc. To ensure fairness, we unify the training hyper-parameters of different models. Specifically, as listed in <ref type="table" target="#tab_1">Table IV</ref>, except for the unique hyperparameters for SNNs, others are shared by all models. In summary, with the above rethinking on the similarities and differences as well as the proposed solutions, we successfully unify several aspects involving testing datasets, temporal resolution, learning algorithm, loss function, network structure, hyper-parameter, etc., which are listed in <ref type="table" target="#tab_6">Table V</ref>. This unification ensures the comparability between SNNs and RNNs, and further makes the comparison fair, which lays the foundation of this work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS</head><p>With the unification mentioned in <ref type="table" target="#tab_6">Table V</ref>, we conduct a series of contrast experiments and extract some insights in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setup</head><p>All models are implemented in the open-source framework, Pytorch <ref type="bibr" target="#b47">[48]</ref>. The configurations of network structures and training hyper-parameters are already provided in <ref type="table" target="#tab_1">Table III and  Table IV</ref>, respectively. The temporal resolution has six levels on N-MNIST (dt = {1ms, 2ms, 3ms, 5ms, 10ms, 20ms}) and other six levels on DVS Gesture (dt = {1ms, 5ms, 10ms, 15ms, 20ms, 25ms}). With a given number of simulation timesteps (i.e. T ), it means only the first T slices of each spike pattern will be used during training and testing. Usually, we fix the T value; while in Section IV-C, we fix the simulation temporal length (i.e. T ? dt) rather than T . Unless otherwise specified, the leakage factor (i.e. e ? dt ? ) is fixed at 0.3. In addition, the Adam (adaptive moment estimation) optimizer <ref type="bibr" target="#b48">[49]</ref> with the default parameter setting (? = 1e ?4 , ? 1 = 0.9, ? 2 = 0.999, = 1e ?8 ) is used for the adjustment of network parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Overall Accuracy Comparison</head><p>Tables VI-VIII list the accuracy results of a series of contrast experiments on both N-MNIST and DVS Gesture datasets. On N-MNIST, SNNs achieve the best accuracy among the common models. Interestingly, when we apply the rate-codinginspired loss function (Equation <ref type="formula" target="#formula_1">(15)</ref>), RNNs can achieve    comparable or even better accuracy than SNNs. A similar trend is also found in DVS Gesture. However, it seems that the vanilla RNNs cannot outperform SNNs on DVS Gesture, especially in the MLP-based cases, even if the rate-codinginspired loss function is used. The underlying reason might be due to the gradient problem. As well known, compared to vanilla RNNS, LSTM can alleviate the gradient vanishing issue via the complex gate structure, thus achieving much longer temporal dependence <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>. For SNNs, the membrane potential can directly impact the neuronal state at the next timestep, leading to one more information propagation path over vanilla RNNs in both forward and backward passes (see <ref type="figure" target="#fig_3">Figure 3</ref>). This extra path acts similarly as the LSTM's forget gate (i.e. the most important gate of LSTM <ref type="bibr" target="#b49">[50]</ref>), thus it can also memorize longer-term dependence than vanilla RNNs and improve accuracy. <ref type="figure" target="#fig_0">Figure 10</ref> further presents the training curves of these models on N-MNIST. It could be observed that the common RNNs converge poorly on neuromorphic datasets while the RNNs with the rate-coding-inspired loss function can shift the training curves upward, which demonstrates of the effectiveness of the rate-coding-inspired loss function. Moreover, we find that SNNs and LSTM converge faster than vanilla RNNs. All these observations are consistent with the results in <ref type="table" target="#tab_1">Tables  VI-VIII.</ref> Besides the above analysis, we further visualize the feature maps of Conv layers on DVS Gesture to see what happens, as shown in <ref type="figure" target="#fig_0">Figure 11</ref>. For simplicity, here we only visualize the case of dt = 15ms; the RNN models are improved by the Timestep Layer <ref type="figure" target="#fig_0">Figure 11</ref>: Visualization of feature maps across timesteps in different CNN-based models on DVS Gesture. We set dt = 15ms and take the left arm clockwise gesture for example.</p><p>rate-coding-inspired loss function. Among the three models, the vanilla RNN has the most clear feature maps, close to the input slices at the corresponding timesteps. However, the feature maps of SNN and LSTM models obviously include an integration of the current timestep and traces of previous timesteps, which owes to the extra membrane potential path of SNNs and the complex gate structure of LSTM. In the feature maps of SNNs and LSTM, the entire area passed by the dynamic gesture is lighted up, making them look like comets. This feature integration strengthens the temporal dependence, which further changes the later layers from learning temporal features to learning spatial features to some extent. On DVS-captured datasets like DVS Gesture, the different input slices across timesteps jointly constitute the final pattern to recognize; while on DVS-converted datasets like N-MNIST, the slices at different timesteps are close. This, along with the longer-term memory of SNNs and LSTM, can explain that the accuracy gap between vanilla RNNs and SNNs/LSTM is larger on DVS Gesture than that on N-MNIST. Furthermore, we do an extra experiment to investigate the influence of the membrane potential leakage and reset mecha- nisms for SNNs. Here we test on N-MNIST with dt = 7.5ms.</p><p>As presented in <ref type="table" target="#tab_1">Table IX</ref>, the removal of these mechanisms will degrade the accuracy. In fact, both leakage and reset can reduce the membrane potential, thus lowering the spike rate to some extent, which is helpful to improve the neuronal selectivity. Interestingly, we find the joint impact of the two mechanisms is larger than the impact of any one of them.  <ref type="bibr" target="#b43">[44]</ref> Input-10000FC-10 98.66% DART <ref type="bibr" target="#b9">[10]</ref> DART Feature Descriptor 97.95% SLAYER <ref type="bibr" target="#b26">[27]</ref> Input-500FC-500FC-10 98.89% SLAYER <ref type="bibr" target="#b26">[27]</ref> Input-12C5-AP2-64C5-AP2-10 99.20% STBP <ref type="bibr" target="#b11">[12]</ref> Input-800FC-10 98.78% Wu et al. <ref type="bibr" target="#b20">[21]</ref> SNN <ref type="figure" target="#fig_7">(CNN-based 8</ref>  At last, we provide the accuracy results of several prior works that applied SNNs on the two neuromorphic datasets. Note that we do not provide results involving RNNs since rare work tested them on neuromorphic datasets. As depicted in <ref type="table" target="#tab_11">Table X</ref>, our SNN models can achieve acceptable results, although not the best. Since our focus is the comparison between SNNs and RNNs rather than beating prior work, we do not adopt large models and complex optimization strategies used in prior work to improve accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Temporal Resolution Analysis</head><p>Also from Tables VI-VIII, we find that as the temporal resolution grows larger, the accuracy will be improved. The reasons are two-fold: on the one hand, the spike events become dense when dt lies in large values (see <ref type="figure" target="#fig_8">Figure 9</ref>), which usually forms more effectual features in each slice; on the other hand, with the same number of simulation timesteps (i.e. T ) during training, a larger temporal resolution can include more slices in the original dataset, which provides more information of the moving object. Furthermore, we find that SNNs achieve significant accuracy superiority on DVS-captured datasets like DVS Gesture when the temporal resolution is small (e.g. dt ? 10ms). This indicates that, unlike the continuous RNNs, the event-driven SNNs are more suitable to extract sparse features, which is also pointed out in <ref type="bibr" target="#b28">[29]</ref>. On DVS-converted datasets like N-MNIST, the sparsity gap of spike events under different temporal resolution is usually smaller than that on DVS-captured datasets, thus the accuracy superiority of SNNs is degraded. In essence, the influence of temporal resolution increase is not always positive. As illustrated in <ref type="figure" target="#fig_7">Figure 8</ref>, the temporal collapse as dt grows also loses some spikes, leading to temporal precision loss. To investigate the negative effect, we conduct experiments on N-MNIST with large dt values. To eliminate the impact of different simulation temporal length (i.e. T ?dt) when dt varies, we adapt the number of simulation timesteps to ensure the same simulation temporal length, i.e. fixing T ? dt = 40ms here. The results are given in <ref type="table" target="#tab_1">Table XI.</ref> As dt excessively increases, the accuracy degrades due to the temporal precision loss. Next, we do a simple experiment to test the generalization ability of models under different temporal resolutions. We train an SNN model, a vanilla RNN model (rate-codinginspired), and an LSTM model (rate-coding-inspired) on N-MNIST under dt = 3ms, and then test the trained models under dt = {3ms, 2ms, 1ms}. Also, we keep the simulation temporal length identical as above, fixing T ? dt = 45ms here. Unless otherwise specified, the leakage factor equals 0.3. <ref type="table" target="#tab_1">Table XII</ref> reports the accuracy results, and the training curves can be found in <ref type="figure" target="#fig_0">Figure 12</ref>. We have two observations: (1) the testing under dt = 2ms and dt = 1ms loses accuracy, and the degradation increases significantly as dt becomes much smaller such as dt = 1ms; (2) the SNN model presents better generalization ability. Specifically, when testing under dt = 1ms, the SNN model only loses 2.18% accuracy, while the vanilla RNN model and the LSTM model lose 19.82% and 20.87% accuracy, respectively, which are much higher than the loss of the SNN model.</p><p>We explain the above robustness of SNNs as follows. First, as mentioned earlier, SNNs are naturally suited for processing sparse features under smaller temporal resolution owing to the event-driven paradigm. Second, different from the trainable cross-neuron recurrent weights in RNNs, SNNs use self-neuron recurrence with restricted weights (i.e. the leakage factor ?e ? dt ? ). This recurrence restriction stabilizes the SNN model thus leading to improved generalization. To evidence the latter prediction, we additionally test the performance of the SNN model with trainable cross-neuron recurrent weights and present the results in <ref type="table" target="#tab_1">Table XII</ref>. As expected, the generalization ability dramatically degrades, like RNNs. This might be caused by the increased number of parameters and more complex dynamics after introducing the trainable crossneuron recurrence. Additionally, we try to identify whether the leakage factor would affect the generalization ability of SNNs. In all previous experiments, the leakage factor is fixed at 0.3; by contrast, we further test an SNN model with adaptive leakage factors by fixing only ? but varying dt. Also from <ref type="table" target="#tab_1">Table XII</ref>, it can be seen that the adaptive leakage factor just slightly improves the robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Loss Function &amp; Temporal Contrast</head><p>In Section IV-B, we have observed that the rate-codinginspired loss function can boost more accuracy on DVScaptured datasets. In this subsection, we do a deeper analysis on this phenomenon. We define the temporal contrast of a neuromorphic vision dataset as the cross-entropy between slices at different timesteps. Specifically, we denote S S S[t, t + k] as the slices between the t-th timestep and the (t + k)-th timestep. Thus, there exists a cross-entropy value between S S S[t x , t x + k] and S S S[t y , t y + k] where t x and t y can be any two given timesteps. Here we define the cross-entropy value as</p><formula xml:id="formula_20">CE(t x , t y ) = ? 1 N i (S i [t x , t x + k]log (S i [t y , t y + k]) + (1 ? S i [t x , t x + k])log (1 ? S i [t y , t y + k]))<label>(16)</label></formula><p>where i and N are the index and number of elements in S S S, respectively. Note that log is the log function with numerical optimization: log (x) = log(g (x)). We set</p><formula xml:id="formula_21">? ? ? ? ? g (x) = , if x ? [0, ) g (x) = x, if x ? [ , 1 ? ] g (x) = 1 ? , if x ? (1 ? , 1]<label>(17)</label></formula><p>where = 1e ?16 is a small constant. The reason we do the above optimization is because the elements in S S S can only be binary values within {0, 1}, which might cause zero or negative infinity results when passing through the log function. Then, we visualize the cross-entropy matrices of the two neuromorphic vision datasets we use: the DVS-converted N-MNIST and the DVS-captured DVS Gesture, as presented in <ref type="figure" target="#fig_0">Figure 13</ref>.  Apparently, it can be seen that the temporal contrast of DVS-captured datasets is much larger than that of DVSconverted datasets. This indicates that there are more temporal components in DVS-captured datasets, while the slices at different timesteps in DVS-converted datasets are close. Furthermore, we provide the statistic data, including mean and variance, of the cross-entropy matrices derived from the above two datasets. The calculation rules of mean and variance follow the normal definitions in statistics. As shown in <ref type="table" target="#tab_1">Table  XIII</ref>, it also demonstrates that the data variance of DVScaptured datasets is much larger than that of DVS-converted datasets, which is consistent with the conclusion from <ref type="figure" target="#fig_0">Figure  13</ref>. By taking the outputs at different timesteps into account, the rate-coding-inspired loss function in Equation <ref type="formula" target="#formula_1">(15)</ref> is able to provide error feedback at all timesteps thus optimizing the final recognition performance. The above quantitative analysis can well explain the underlying reason that the rate-codinginspired loss function can gain more accuracy boost on DVS Gesture than the gain on N-MNIST. We should note that, when the temporal contrast is too large, the effectiveness of the rate-coding-inspired loss function might be degraded due to the divergent gradient directions at different timesteps, which needs more practice in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Number of Parameters and Operations</head><p>Besides the accuracy analysis, we further consider the memory and compute costs during model running. For the computational complexity, we take one layer with M neurons as an example. In the forward pass, we count the operations when it propagates the activities to the next timestep and the next layer with N neurons; while in the backward pass, we count the operations when it receives the gradients from the next timestep and the next layer. Notice that we only count the matrix operations because they occupy much more complexity than the vector and scalar ones. The comparison is presented in <ref type="table" target="#tab_1">Table XIV</ref>, which is mainly derived from Equation (2)- <ref type="bibr" target="#b8">(9)</ref>. Apparently, the SNN model consumes fewer operations owing to the self-neuron recurrence and avoids costly multiplications in the forward pass owing to the spike format. Furthermore, the event-driven computation in the forward pass can further reduce the required operations that are proportional to the normalized spike rate.  On the other hand, the memory overhead is mainly determined by the number of parameters, especially when performing inference with only the forward pass on edge devices. <ref type="figure" target="#fig_0">Figure 14</ref> compares the number of model parameters of SNNs, vanilla RNNs, and LSTM. Here we take the models we used on the DVS Gesture dataset for illustration. We find that the parameter amount of SNNs is much smaller than those of RNNs. Overall, SNNs only occupy about 80% and 20% parameters compared with the vanilla RNNs and LSTM, respectively.  Interestingly, despite the fewer operations and parameters of SNNs, the extra membrane potential path helps them achieve comparable (under large temporal resolution) or even better (under small temporal resolution) recognition accuracy than LSTM with complex gates; in the meantime, the self-neuron recurrence and the restricted recurrent weights make them more lightweight and robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Discussion on Audio Data</head><p>In the previous content, we focus on vision datasets. Actually, another important branch of data sources with spatiotemporal components is the audio data, which has also been used in neuromorphic computing <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>. To gently extend the scope of this work, we provide an extra experiment on an audio dataset in this subsection. We select the Spoken-Digits <ref type="bibr" target="#b53">[54]</ref> for testing. The network structure is "Input-512FC-10". The hyper-parameter setting is the same as that on N-MNIST except for the number of simulation timesteps, i.e. T . We set T = 75 during training, while varying it during testing to explore the generalization. The results are listed in <ref type="table" target="#tab_6">Table XV</ref>. It can be seen that the vanilla RNNs perform the worst while SNNs are the best. Furthermore, SNNs show better generalization ability on this dataset, which is consistent with the observation in Section IV-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this work, we conduct a systematic investigation to compare SNNs and RNNs on neuromorphic vision datasets and then compare their performance and complexity. To make SNNs and RNNs comparable and improve fairness, we first identify several similarities and differences between them from the modeling and learning perspectives, and then unify the dataset selection, temporal resolution, learning algorithm, loss function, network structure, and training hyper-parameters. Especially, inspired by the rate coding scheme of SNNs, we modify the mainstream loss function of RNNs to approach that of SNNs; to test model robustness and generalization, we propose to tune the temporal resolution of neuromorphic vision datasets. Based on a series of contrast experiments on N-MNIST (a DVS-converted dataset) and DVS Gesture (a DVS-captured dataset), we achieve extensive insights in terms of recognition accuracy, feature extraction, temporal resolution and contrast, learning generalization, computational complexity and parameter volume. For better readability, we summarize our interesting findings as below:</p><p>? SNNs are usually able to achieve better accuracy than common RNNs. Whereas, the rate-coding-inspired loss function can boost the accuracy of RNNs especially LSTM to be comparable or even slightly better than that of SNNs. ? The event-driven paradigm of SNNs makes them more suitable to process sparse features. Therefore, in the cases of small temporal resolution with sparse spike events, SNNs hold obvious accuracy superiority. ? On one hand, LSTM can memorize long-term dependence via the complex gates, while the extra membrane potential path of SNNs also brings longer-term memory than vanilla RNNs; on the other hand, the temporal contrast of slices in DVS-captured datasets is much larger than that in DVS-converted datasets, thus the processing of DVS-captured datasets depends more on the long-term memorization ability. These two sides can explain the reason that SNNs and LSTM significantly outperform vanilla RNNs on DVS Gesture, while this gap is small on N-MNIST. ? The self-neuron recurrence pattern and restricted recurrent weights of SNNs greatly reduce the number of parameters and operations, which improves both the running efficiency and the model generalization. We believe that the above conclusions can benefit the neural network selection and design on different workloads in the future. We simply discuss several examples. On DVS-converted datasets, the accuracy gap between different models is small so that any model selection is acceptable. On DVS-captured datasets, we do not recommend vanilla RNNs due to the low accuracy. When the temporal resolution is large, we recommend LSTM with the rate-coding-inspired loss function; while when the temporal resolution is small, we recommend SNNs. If we need a compact model size, we always recommend SNNs that have significantly fewer parameters and operations. Moreover, it might be possible to improve models by borrowing ideas from each other. For instance, vanilla RNNs can be further enhanced by introducing more information propagation paths like the membrane potential path in SNNs; LSTM can be made more compact and robust by introducing the recurrence restriction; SNNs can be improved by introducing more gates like LSTM. It is even possible to build a hybrid neural network model by combining multiple kinds of neurons, thus taking the advantages of different models and alleviating their respective defects. In addition, we mainly focus on vision datasets and just provide very limited exploration on audio data in this work. More extensive experiments in a wide spectrum of tasks are highly expected.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example of DVS-converted dataset named N-MNIST: (a) original static images in the MNIST dataset; (b) the repeated closed-loop smooth (RCLS) movement of static images; (c) the resulting spike pattern recorded by DVS; (d) slices of spike events at different timesteps. The blue and red colors denote the On and Off channels, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Left hand wave Air guitar Right arm clockwise</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>An example of DVS-captured dataset named DVS Gesture: (a) spike pattern examples, where all spike events within a time window t w are compressed into a single static image for visualization; (b) the spike pattern recorded by DVS; (c) slices of spike events at different timesteps, where the right hand lies at a different location in each slice thus forming a hand wave along the time dimension. The blue and red colors denote the On and Off channels, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Information propagation paths of (a) SNNs, (b) vanilla RNNs, and (c) LSTM in both forward and backward passes. For clarity, we do not show the dataflow inside LSTM neurons. when x ? 0, otherwise f (x) = 0. This iterative LIF model incorporates all behaviors of a spiking neuron, including integration, fire, and reset. Now, from Equation (2)-(</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Connection pattern of (a) SNNs and (b) RNNs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Distribution of recurrent weights in (a) SNNs and (b) RNNs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Neuron unit of (a) SNNs, (b) vanilla RNNs, and (c) LSTM. Activation function of (a) firing in SNNs, and (b) tanh tanh tanh and (c) sigmoid sigmoid sigmoid in RNNs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Illustration of temporal collapse for tunable temporal resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Slices of N-MNIST dataset under different temporal resolution, taking the digit "3" for example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Training curves of different models under different dt settings on N-MNIST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Generalization ability test under the same simulation temporal length but different temporal resolution. Here we train the MLP-based SNNs and enhanced RNNs on N-MNIST under dt = 3ms and test them under dt = {3ms, 2ms, 1ms}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 :</head><label>13</label><figDesc>Visualization of the cross-entropy matrices calculated from N-MNIST and DVS Gesture. The colorful block at the coordinate (t x , t y ) represents the cross-entropy value between S S S[t x , t x + k] and S S S[t y , t y + k]. Here we set k = 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>N ) ADDs O(M N + M 2 ) MACs O[4(M N + M 2 )] MACs Backward O(M N ) MACs O(M N + M 2 ) MACs O(8M N ) MULs + O(M N + M 2 ) MACs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 14 :</head><label>14</label><figDesc>Number of model parameters on DVS Gesture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Email addresses: hewh16@mails.tsinghua.edu.cn (W. He), wu-yj16@mails.tsinghua.edu.cn (Y. Wu), leideng@ucsb.edu (L. Deng), liguoqi@mail.tsinghua.edu.cn (G. Li), haoyu-wa16@mails.tsinghua.edu.cn (H. Wang), tianyang16@mails.tsinghua.edu.cn (Y. Tian), dingw17@mails.tsinghua.edu.cn (W. Ding), wwh@mail.tsinghua.edu.cn (W. Wang), yuanxie@ucsb.edu (Y. Xie).</figDesc><table /><note>* Corresponding author: Lei Deng.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table I :</head><label>I</label><figDesc>Summary of similarities and differences among SNNs, vanilla RNNs, and LSTM.</figDesc><table><row><cell>Model</cell><cell>Spatiotemporal</cell><cell>Recurrence</cell><cell>Recurrent</cell><cell>Gate Structure</cell><cell>Activation Function</cell><cell>Loss Function</cell></row><row><cell></cell><cell>Path</cell><cell>Pattern</cell><cell>Weights</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>T,N is the hidden state of the last layer at the last timestep and W W W y is a trainable weight matrix.</figDesc><table><row><cell>Vanilla RNNs LSTM</cell><cell>! !</cell><cell>Cross-Neuron Cross-Neuron</cell><cell>Trainable Trainable</cell><cell>% Multiple Gates</cell><cell>Continuous: tanh (?) Continuous: sigmoid (?) &amp; tanh (?)</cell><cell>Flexible Flexible</cell><cell>N || 2 2</cell></row><row><cell>where h h h</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table II :</head><label>II</label><figDesc>Details of the selected neuromorphic datasets.</figDesc><table><row><cell>Dataset</cell><cell>N-MNIST</cell><cell>DVS Gesture</cell></row><row><cell>Description</cell><cell cols="2">Handwritten Digits Human Gestures</cell></row><row><cell>Slice Size</cell><cell>34?34</cell><cell>128?128</cell></row><row><cell>dt 0</cell><cell>1?s</cell><cell>1?s</cell></row><row><cell>#Training Slices</cell><cell>60000</cell><cell>1176</cell></row><row><cell>#Testing Slices</cell><cell>10000</cell><cell>288</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table III :</head><label>III</label><figDesc>Network structure configuration.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table IV :</head><label>IV</label><figDesc>Hyper-parameter setting.</figDesc><table><row><cell>Model</cell><cell>Hyper-parameter</cell><cell>Description</cell><cell>N-MNIST</cell><cell cols="2">DVS Gesture</cell></row><row><cell></cell><cell>Max Epoch</cell><cell>-</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>Shared</cell><cell>Batch Size T</cell><cell>-Timestep Number</cell><cell>50 15</cell><cell>36 60</cell><cell>36 60</cell></row><row><cell></cell><cell>lr</cell><cell>Learning Rate</cell><cell>1e ?4</cell><cell>1e ?4</cell><cell>1e ?4</cell></row><row><cell></cell><cell>u th</cell><cell>Firing Threshold</cell><cell>0.3</cell><cell>0.3</cell><cell>0.3</cell></row><row><cell>SNNs</cell><cell>e ? dt ?</cell><cell>Leakage Factor</cell><cell>0.3</cell><cell>0.3</cell><cell>0.3</cell></row><row><cell></cell><cell>a</cell><cell>Gradient Width</cell><cell>0.25</cell><cell>0.25</cell><cell>0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table V :</head><label>V</label><figDesc>Unification for comparison.</figDesc><table><row><cell>Neuromorphic Vision Dataset</cell><cell>N-MNIST &amp; DVS Gesture</cell></row><row><cell>Temporal Resolution</cell><cell>Tunable (dt)</cell></row><row><cell>Learning Algorithm</cell><cell>BPTT-inspired (SNNs); BPTT (RNNs)</cell></row><row><cell>Loss Function</cell><cell>Rate Coding (SNNs); Mainstream or Rate-Coding-Inspired (RNNs)</cell></row><row><cell>Network Structure</cell><cell>MLP &amp; CNN</cell></row><row><cell>Hyper-parameter</cell><cell>SNN-Specialized &amp; Shared</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table VI :</head><label>VI</label><figDesc>Accuracy of MLP-based models on N-MNIST.</figDesc><table><row><cell>dt</cell><cell>SNNs</cell><cell>Vanilla RNNs</cell><cell>LSTM</cell><cell cols="2">Vanilla RNNs (Rate-Coding-inspired) (Rate-Coding-inspired) LSTM</cell></row><row><cell cols="2">1ms 84.96%</cell><cell>66.74%</cell><cell>65.33%</cell><cell>87.62%</cell><cell>82.25%</cell></row><row><cell cols="2">2ms 95.94%</cell><cell>90.54%</cell><cell>93.18%</cell><cell>95.97%</cell><cell>97.08%</cell></row><row><cell cols="2">3ms 98.19%</cell><cell>95.41%</cell><cell>96.70%</cell><cell>98.15%</cell><cell>98.69%</cell></row><row><cell cols="2">5ms 98.28%</cell><cell>92.37%</cell><cell>94.24%</cell><cell>98.58%</cell><cell>98.68%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table VII :</head><label>VII</label><figDesc>Accuracy of MLP-based models on DVS Gesture.</figDesc><table><row><cell>dt</cell><cell>SNNs</cell><cell>Vanilla RNNs</cell><cell>LSTM</cell><cell>Vanilla RNNs (Rate-Coding-inspired)</cell><cell>LSTM (Rate-Coding-inspired)</cell></row><row><cell>1ms</cell><cell>54.51%</cell><cell>16.32%</cell><cell>19.79%</cell><cell>19.44%</cell><cell>50.35%</cell></row><row><cell>5ms</cell><cell>76.04%</cell><cell>27.78%</cell><cell>45.49%</cell><cell>30.90%</cell><cell>74.31%</cell></row><row><cell cols="2">10ms 82.63%</cell><cell>30.56%</cell><cell>37.15%</cell><cell>36.11%</cell><cell>84.03%</cell></row><row><cell cols="2">15ms 85.07%</cell><cell>33.68%</cell><cell>46.88%</cell><cell>42.01%</cell><cell>85.42%</cell></row><row><cell cols="2">20ms 86.81%</cell><cell>38.54%</cell><cell>42.71%</cell><cell>52.78%</cell><cell>88.19%</cell></row><row><cell cols="2">25ms 87.50%</cell><cell>19.44%</cell><cell>44.79%</cell><cell>50.35%</cell><cell>86.81%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table VIII :</head><label>VIII</label><figDesc>Accuracy of CNN-based models on DVS Gesture.</figDesc><table><row><cell></cell><cell>dt</cell><cell>SNNs</cell><cell>Vanilla RNNs</cell><cell>LSTM</cell><cell cols="2">Vanilla RNNs (Rate-Coding-inspired)</cell><cell>LSTM (Rate-Coding-inspired)</cell></row><row><cell></cell><cell>1ms</cell><cell>71.53%</cell><cell>48.26%</cell><cell>64.58%</cell><cell>56.25%</cell><cell>65.63%</cell></row><row><cell></cell><cell>5ms</cell><cell>87.15%</cell><cell>51.74%</cell><cell>79.86%</cell><cell>75.35%</cell><cell>85.76%</cell></row><row><cell></cell><cell cols="2">10ms 91.67%</cell><cell>56.94%</cell><cell>84.72%</cell><cell>82.99%</cell><cell>86.81%</cell></row><row><cell></cell><cell cols="2">15ms 93.05%</cell><cell>58.68%</cell><cell>91.67%</cell><cell>84.02%</cell><cell>93.40%</cell></row><row><cell></cell><cell cols="2">20ms 92.71%</cell><cell>65.97%</cell><cell>89.24%</cell><cell>90.27%</cell><cell>92.70%</cell></row><row><cell></cell><cell cols="2">25ms 93.40%</cell><cell>70.49%</cell><cell>92.36%</cell><cell>92.01%</cell><cell>93.75%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SNNs</cell></row><row><cell>Acc (%)</cell><cell cols="2">dt = 1ms</cell><cell>dt = 2ms</cell><cell></cell><cell>dt = 2ms dt = 3ms</cell><cell>dt = 5ms</cell><cell>(Rate-Coding-inspired) Vanilla RNNs Vanilla RNNs LSTM</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>LSTM</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(Rate-Coding-inspired)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table IX :</head><label>IX</label><figDesc>Influence of membrane potential leakage and reset.</figDesc><table><row><cell>Leakage Reset ! ! 98.05% Accuracy ! % 98.13% % ! 97.66% % % 96.27%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table X :</head><label>X</label><figDesc>Accuracy results of prior work on SNNs.</figDesc><table><row><cell>Dataset</cell><cell>Work</cell><cell>Network Structure</cell><cell>Accuracy</cell></row><row><cell></cell><cell>SKIM [51]</cell><cell>Input-800FC-10</cell><cell>92.87%</cell></row><row><cell></cell><cell>Lee et al.</cell><cell></cell><cell></cell></row><row><cell>N-MNIST</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table XI :</head><label>XI</label><figDesc>Influence of temporal resolution under the same simulation temporal length.</figDesc><table><row><cell>dt</cell><cell>SNNs</cell><cell>Vanilla RNNs (Rate-Coding-inspired)</cell><cell>LSTM (Rate-Coding-inspired)</cell></row><row><cell>5ms</cell><cell>97.48%</cell><cell>97.97%</cell><cell>98.48%</cell></row><row><cell cols="2">10ms 96.99%</cell><cell>98.13%</cell><cell>98.45%</cell></row><row><cell cols="2">20ms 96.59%</cell><cell>97.47%</cell><cell>97.83%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table XII :</head><label>XII</label><figDesc>Accuracy results of generalization test under the same simulation temporal length but different temporal resolution.</figDesc><table><row><cell>dt during Testing</cell><cell>3ms</cell><cell>2ms</cell><cell>1ms</cell></row><row><cell>SNN</cell><cell cols="3">98.19% 97.31% 96.01%</cell></row><row><cell>SNN (Adaptive Leakage)</cell><cell cols="3">98.19% 97.83% 97.04%</cell></row><row><cell>SNN (Cross-Recurrence)</cell><cell cols="3">98.32% 97.62% 73.51%</cell></row><row><cell cols="4">Vanilla RNN (Rate-Coding-inspired) 98.15% 97.09% 78.33%</cell></row><row><cell>LSTM (Rate-Coding-inspired)</cell><cell cols="3">98.69% 97.98% 77.82%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table XIII :</head><label>XIII</label><figDesc>Mean and variance of the cross-entropy matrices.</figDesc><table><row><cell></cell><cell cols="2">N-MNIST DVS Gesture</cell></row><row><cell>Mean</cell><cell>0.2304</cell><cell>2.6912</cell></row><row><cell>Variance</cell><cell>0.0241</cell><cell>1.2701</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table XIV :</head><label>XIV</label><figDesc>Computational complexity comparison. ADDsadditions, MULs -multiplications, MACs -multiplications and additions, ? -normalized spike rate.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table XV :</head><label>XV</label><figDesc>Accuracy results on audio data.</figDesc><table><row><cell>T during Testing</cell><cell>SNNs</cell><cell cols="2">Vanilla RNNs (Rate-Coding-inspired) (Rate-Coding-inspired) LSTM</cell></row><row><cell>75</cell><cell>98.14%</cell><cell>96.81%</cell><cell>97.59%</cell></row><row><cell>25</cell><cell>89.23%</cell><cell>72.86%</cell><cell>87.55%</cell></row><row><cell>15</cell><cell>77.82%</cell><cell>46.14%</cell><cell>68.27%</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The work was partially supported by National Science Foundation (Grant No. 1725447), Beijing Academy of Artificial Intelligence (BAAI), Tsinghua University Initiative Scientific Research Program, and a grant from the Institute for Guo Qiang, Tsinghua University.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Converting static image datasets to spiking neuromorphic datasets using saccades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Orchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jayawant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thakor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">437</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A dataset for visual navigation with neuromorphic methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Barranco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fermuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">49</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cifar10-dvs: An event-stream dataset for object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">309</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Frame-free dynamic digital vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Intl. Symp. on Secure-Life Electronics, Advanced Electronics for Quality Life and Society</title>
		<meeting>Intl. Symp. on Secure-Life Electronics, Advanced Electronics for Quality Life and Society</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A 128?128 120 db 15?s latency asynchronous temporal contrast vision sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lichtsteiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE journal of solid-state circuits</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="566" to="576" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A 128?128 1.5% contrast sensitivity 0.9% fpn 3 ?s latency 4 mw asynchronous framefree dynamic vision sensor using transimpedance preamplifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serrano-Gotarredona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Linares-Barranco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="827" to="838" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A dynamic vision sensor with 1% temporal contrast sensitivity and in-pixel asynchronous delta modulator for event encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2149" to="2160" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hfirst: a temporal approach to object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Orchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Etienne-Cummings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thakor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benosman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2028" to="2040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Feedforward categorization on aer motion events using cortex-like features in a spiking neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Linares-Barranco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1963" to="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dart: distribution aware retinal transform for eventbased cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Orchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Le Thi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An event-driven categorization model for aer image sensors using multispike encoding and learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Orchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatio-temporal backpropagation for training high-performance spiking neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Synaptic plasticity dynamics for deep continuous local learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mostafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Neftci</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.10766</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A low power, fully event-based gesture recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Melano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mckinstry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Di</forename><surname>Nolfo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andreopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Garreau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mendoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7243" to="7252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A saccade based framework for real-time motion segmentation using event based vision sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Principe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Thakor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Kukreja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">83</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Toward real-time particle tracking using an event-based dynamic vision sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Drazen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lichtsteiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>H?fliger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbr?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Experiments in Fluids</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1465</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An embedded aer dynamic vision sensor for low-latency pole balancing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Conradt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="780" to="785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dvs benchmark datasets for object tracking, action recognition, and object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">405</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robotic goalie with 3 ms reaction time at 4% cpu load using event-based dynamic vision sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">223</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Networks of spiking neurons: the third generation of neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1659" to="1671" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Direct training for spiking neural networks: Faster, larger, better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1311" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spiking optical flow for event-based sensors using ibm&apos;s truenorth neurosynaptic system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Haessig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benosman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Orchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on biomedical circuits and systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="860" to="870" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ultimate slam? combining events, images, and imu for robust visual slam in hdr and high-speed scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rebecq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Horstschaefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="994" to="1001" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A million spiking-neuron integrated circuit with a scalable communication network and interface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Merolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Alvarez-Icaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sawada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Akopyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Imam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">345</biblScope>
			<biblScope unit="issue">6197</biblScope>
			<biblScope unit="page" from="668" to="673" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Loihi: A neuromorphic manycore processor with on-chip learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chinya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Choday</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dimou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Imam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="82" to="99" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards artificial general intelligence with hybrid tianjic chip architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">572</biblScope>
			<biblScope unit="issue">7767</biblScope>
			<biblScope unit="page" from="106" to="111" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Slayer: Spike layer error reassignment in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Orchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hybrid macro/micro level backpropagation for training deep spiking neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7005" to="7015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rethinking the performance comparison between snns and anns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="294" to="307" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tianjic: A unified and scalable chip bridging spikebased and continuous neural computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Statistical language models based on neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-04-02" />
			<biblScope unit="volume">80</biblScope>
		</imprint>
	</monogr>
	<note>Presentation at Google, Mountain View</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="167" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Poker-dvs and mnistdvs. their history, how they were made, and other details</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serrano-Gotarredona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Linares-Barranco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">481</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Is neuromorphic mnist neuromorphic? analyzing the discriminative power of neuromorphic datasets in the time domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01013</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Lapicque&apos;s introduction of the integrate-and-fire model neuron</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Abbott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain research bulletin</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="303" to="304" />
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Simple model of spiking neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Izhikevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1569" to="1572" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A quantitative description of membrane current and its application to conduction and excitation in nerve</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Hodgkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Huxley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of physiology</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="500" to="544" />
			<date type="published" when="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Which model to use for cortical spiking neurons?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Izhikevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1063" to="1070" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning to forget: Continual prediction with lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cummins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised learning of digit recognition using spike-timing-dependent plasticity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">U</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in computational neuroscience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">99</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">U</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Binas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pfeiffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>ieee</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Training deep spiking neural networks using backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pfeiffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">508</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Backpropagation through time: what it does and how to do it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">A guide to recurrent neural networks and backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Boden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>the Dallas project</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Backpropagation algorithms and reservoir computing in recurrent neural networks for the forecasting of complex spatiotemporal dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vlachas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sapsis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Girvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koumoutsakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2342" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Skimming digits: neuromorphic classification of spikeencoded images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Orchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tapson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Benosman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Schaik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">184</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A spiking neural network framework for robust sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">836</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep spiking neural networks for large vocabulary automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Y?lmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroscience</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">199</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Graff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
