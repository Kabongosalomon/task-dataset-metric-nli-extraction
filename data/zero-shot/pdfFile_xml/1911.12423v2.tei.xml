<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AdaShare: Learning What To Share For Efficient Deep Multi-Task Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximeng</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Boston University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
							<email>rpanda@</email>
							<affiliation key="aff1">
								<orgName type="laboratory">MIT-IBM Watson AI Lab</orgName>
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
							<email>rsferis@us.ibm.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">MIT-IBM Watson AI Lab</orgName>
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
							<email>saenko@bu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Boston University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">MIT-IBM Watson AI Lab</orgName>
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AdaShare: Learning What To Share For Efficient Deep Multi-Task Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>page: https://cs-people.bu.edu/sunxm/AdaShare/project.html.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-task learning is an open and challenging problem in computer vision. The typical way of conducting multi-task learning with deep neural networks is either through handcrafted schemes that share all initial layers and branch out at an adhoc point, or through separate task-specific networks with an additional feature sharing/fusion mechanism. Unlike existing methods, we propose an adaptive sharing approach, called AdaShare, that decides what to share across which tasks to achieve the best recognition accuracy, while taking resource efficiency into account. Specifically, our main idea is to learn the sharing pattern through a task-specific policy that selectively chooses which layers to execute for a given task in the multi-task network. We efficiently optimize the task-specific policy jointly with the network weights, using standard back-propagation. Experiments on several challenging and diverse benchmark datasets with a variable number of tasks well demonstrate the efficacy of our approach over state-of-the-art methods. Project</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-task learning (MTL) focuses on simultaneously solving multiple related tasks and has attracted much attention in recent years. Compared with single-task learning, it can significantly reduce the training and inference time, while improving generalization performance and prediction accuracy by learning a shared representation across related tasks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b55">56]</ref>. However, a fundamental challenge of MTL is deciding what parameters to share across which tasks for efficient learning of multiple tasks. Most of the prior works rely on hand-designed architectures, usually composed of shared initial layers, after which all tasks branch out simultaneously at an adhoc point in the network (hard-parameter sharing) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b11">12]</ref>. However, there is a large number of possible options for tweaking such architectures, in fact, too large to tune an optimal configuration manually, especially for deep neural networks with hundreds or thousands of layers. It is even more difficult when the number of tasks grows and an improper sharing scheme across unrelated tasks may cause "negative transfer", a severe problem in multi-task learning <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b26">27]</ref>. Furthermore, it has been empirically observed that different sharing patterns tend to work best for different task combinations <ref type="bibr" target="#b38">[39]</ref>.</p><p>More recently, we see a shift of paradigm in deep multi-task learning, where a set of task-specific networks are used in combination with feature sharing/fusion for more flexible multi-task learning (soft-parameter sharing) <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b48">49]</ref>. While this line of work has obtained reasonable accuracy on commonly used benchmark datasets, it is not computationally or memory efficient, as the size of the model grows proportionally with respect to the number of tasks.</p><p>In this paper, we argue that an optimal MTL algorithm should not only achieve high accuracy on all tasks, but also restrict the number of new network parameters as much as possible as the number of tasks grows. This is extremely important for many resource-limited applications such as autonomous vehicles and mobile platforms that would benefit from multi-task learning. Motivated by this, we  <ref type="figure">Figure 1</ref>: A conceptual overview of our approach. Consider a deep multi-task learning scenario with two tasks such as Semantic Segmentation (Seg) and Surface Normal Prediction (SN). Traditional hard-parameter sharing uses the same initial layers and splits the network into task-specific branches at an adhoc point (designed manually). On the other hand, Soft-parameter sharing shares features via a set of task-specific networks, which does not scale well as the number of tasks increases. In contrast, we propose AdaShare, a novel efficient sharing scheme that learns separate execution paths for different tasks through a task-specific policy applied to a single multi-task network. Here, we show an example task-specific policy learned using AdaShare for the two tasks.</p><p>wish to obtain the best utilization of a single network by exploring efficient knowledge sharing across multiple tasks. Specifically, we ask the following question: Can we determine which layers in the network should be shared across which tasks and which layers should be task-specific to achieve the best accuracy/memory footprint trade-off for scalable and efficient multi-task learning?</p><p>To this end, we propose AdaShare, a novel and differentiable approach for efficient multi-task learning that learns the feature sharing pattern to achieve the best recognition accuracy, while restricting the memory footprint as much as possible. Our main idea is to learn the sharing pattern through a task-specific policy that selectively chooses which layers to execute for a given task in the multi-task network. In other words, we aim to obtain a single network for multi-task learning that supports separate execution paths for different tasks, as illustrated in <ref type="figure">Figure 1</ref>. As decisions to form these task-specific execution paths are discrete and non-differentiable, we rely on Gumbel Softmax Sampling <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b34">35]</ref> to learn them jointly with the network parameters through standard back-propagation, without using reinforcement learning (RL) <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b61">62]</ref> or any additional policy network <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17]</ref>. We design the loss to achieve both competitive performance and resource efficiency required for multi-task learning. Additionally, we also present a simple yet effective training strategy inspired by the idea of curriculum learning <ref type="bibr" target="#b3">[4]</ref>, to facilitate the joint optimization of task-specific policies and network weights. Our results show that AdaShare outperforms state-of-the-art approaches, whilst being more parameter efficient and therefore scaling more elegantly with the number of tasks.</p><p>The main contributions of our work are as follows:</p><p>? We propose a novel and differentiable approach for adaptively determining the feature sharing pattern across multiple tasks (what layers to share across which tasks) in deep multi-task learning. ? We learn the feature sharing pattern jointly with the network weights using standard backpropagation through Gumbel Softmax Sampling, making it highly efficient. We also introduce two new loss terms for learning a compact multi-task network with effective knowledge sharing across tasks and a curriculum learning strategy to benefit the optimization. ? We conduct extensive experiments on several MTL benchmarks (NYU v2 <ref type="bibr" target="#b39">[40]</ref>, CityScapes <ref type="bibr" target="#b10">[11]</ref>, Tiny-Taskonomy <ref type="bibr" target="#b67">[68]</ref>, DomainNet <ref type="bibr" target="#b41">[42]</ref>, and text classification datasets <ref type="bibr" target="#b7">[8]</ref>) with variable number of tasks to demonstrate the superiority of our proposed approach over state-of-the-art methods.</p><p>parameter sharing of hidden layers <ref type="bibr" target="#b46">[47]</ref>. Hard-parameter sharing usually relies on hand-designed architectures composed of hidden layers that are shared across all tasks and specialized branches that learn task-specific features <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b11">12]</ref>. Only a few methods have attempted to learn multibranch network architectures, using greedy optimization based on task affinity measures <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b56">57]</ref> or convolutional filter grouping <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b53">54]</ref>. In contrast, our approach allows learning of much more flexible architectures beyond tree-like structures, which have proven effective in multi-task learning <ref type="bibr" target="#b37">[38]</ref>, and relies on a more efficient end-to-end learning method instead of greedy search based on task affinity measures. In parallel, soft-parameter sharing approaches, such as Cross-stitch <ref type="bibr" target="#b38">[39]</ref>, Sluice <ref type="bibr" target="#b47">[48]</ref> and NDDR <ref type="bibr" target="#b15">[16]</ref>, consist of a network column for each task, and define a mechanism for feature sharing between columns. In contrast, our approach achieves superior accuracy while requiring a significantly smaller number of parameters. Attention-based methods, e.g. MTAN <ref type="bibr" target="#b32">[33]</ref> and Attentive Single-Tasking <ref type="bibr" target="#b36">[37]</ref>, introduce a task-specific attention branch per task paired with the shared backbone. Instead of introducing additional attention mechanism, our method adopts adaptive computation that not only encourages positive sharing among tasks via shared blocks but also minimizes negative interference by using task-specific blocks when necessary. More recently, Deep Elastic Network (DEN) <ref type="bibr" target="#b0">[1]</ref> specify each network filter to be used or not for each task via learning an additional policy network using complex RL policy gradients <ref type="bibr" target="#b0">[1]</ref>. Alternately, we propose a simpler yet effective method which learns to determine the execution of each network layer for each task via direct gradient descent without any additional network. We include a comprehensive comparison with Deep Elastic Network <ref type="bibr" target="#b0">[1]</ref> later in our experiments.</p><p>Neural Architecture Search. Neural Architecture Search (NAS), which aims to automate the design of the network architecture <ref type="bibr" target="#b14">[15]</ref>, has been studied using different strategies, including reinforcement learning <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b70">71]</ref>, evolutionary computation <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b43">44]</ref>, and gradient-based optimization <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b64">65]</ref>. Inspired by NAS, in this work we directly learn the sharing pattern in a single network for scalable and efficient multi-task learning. Some recent works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31]</ref>, in NLP and character recognition, also try to learn the multi-task sharing via RL or evolutionary computation. RL policy gradients are often complex, unwieldy to train and require techniques to reduce variance during training as well as carefully selected reward functions. By contrast, AdaShare utilizes a gradient based optimization, which is extremely fast and more computationally efficient than <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>Adaptive Computation. Many adaptive computation methods have been recently proposed to dynamically route information in neural networks with the goal of improving computational efficiency <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b0">1]</ref>. BlockDrop <ref type="bibr" target="#b61">[62]</ref> effectively reduces the inference time by learning to dynamically select which layers to execute per sample during inference, exploiting the fact that ResNets behave like ensembles of relatively shallow networks <ref type="bibr" target="#b58">[59]</ref>. Routing networks <ref type="bibr" target="#b45">[46]</ref> has also been proposed for adaptive selection of non-linear functions using a recursive policy network trained by reinforcement learning (RL). In transfer learning, SpotTune <ref type="bibr" target="#b16">[17]</ref> learns to adaptively route information through finetuned or pre-trained layers. While our approach is inspired by these methods, in this paper we focus on adaptively deciding what layers to share in multi-task learning using an efficient approach that jointly optimizes the network weights and policy distribution parameters, without using RL algorithms <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b0">1]</ref> or any additional policy network as in <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b0">1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>Given a set of K tasks T = {T 1 , T 2 , ? ? ? , T K } defined over a dataset, our goal is to seek an adaptive feature sharing mechanism that decides what network layers should be shared across which tasks and what layers should be task-specific in order to improve the accuracy, while taking the resource efficiency into account for scalable multi-task learning.</p><p>Approach Overview. <ref type="figure">Figure 2</ref> illustrates an overview of our proposed approach. Generally, we seek a binary random variable u l,k (a.k.a policy) for each layer l and task T k that determines whether the l-th layer in a deep neural network is selected to execute or skipped when solving T k to obtain the optimal sharing pattern, yielding the best overall performance over the task set T .</p><p>Shortcut connections are widely used in recent network architectures (e.g. ResNet <ref type="bibr" target="#b17">[18]</ref>, ResNeXt <ref type="bibr" target="#b63">[64]</ref>, and DenseNet <ref type="bibr" target="#b20">[21]</ref>) and achieve strong performance in many recognition tasks. These connections make these architectures resilient to removal of layers <ref type="bibr" target="#b58">[59]</ref>, which benefits our method. In this paper, we consider using ResNets <ref type="bibr" target="#b17">[18]</ref> with L residual blocks. In particular, a residual block is said to be shared across two tasks if it is being used by both of them, or task-specific if it is being used by Task-Specific Policy <ref type="figure">Figure 2</ref>: Illustration of our proposed approach. AdaShare learns the layer sharing pattern among multiple tasks through predicting a select-or-skip policy decision sampled from the learned task-specific policy distribution (logits). These select-or-skip vectors define which blocks should be executed in different tasks. A block is said to be shared across two tasks if it is being used by both of them or task-specific if it is being used by only one task for predicting the output. During training, both policy logits and network parameters are jointly learned using standard back-propagation through Gumbel-Softmax Sampling. We use task-specific losses and policy regularizations (to encourage sparsity and sharing) in training. Best viewed in color.</p><p>only one task for predicting the output. In this way, the select-or-skip policy of all blocks and tasks (U = {u l,k } l?L,k?K ) determines the adaptive feature sharing mechanism over the given task set T .</p><p>As the number of potential configurations for U is 2 L?K which grows exponentially with the number of blocks and tasks, it becomes intractable to manually find such a U to get the optimal feature sharing pattern in multi-task learning. Instead of handcrafting this policy, we adopt Gumbel-Softmax Sampling <ref type="bibr" target="#b24">[25]</ref> to optimize U jointly with the network parameters W through standard back-propagation. Moreover, we introduce two policy regularizations to achieve effective knowledge sharing in a compact multi-task network, as well as a curriculum learning strategy to stabilize the optimization in the early stages. After the training finishes, we sample the binary decision u l,k for each block l from u l,k to decide what blocks to select or skip in the task T k . Specifically, with the help of the select-or-skip decisions, we form a novel and non-trivial network architecture for MTL parameter-sharing, and share knowledge at different levels across all tasks in a flexible and efficient way. At test time, when a novel input is presented to the multi-task network, the optimal policy is followed, selectively choosing what blocks to compute for each task. Our proposed approach not only encourages positive sharing among tasks via shared blocks but also minimizes negative interference by using task-specific blocks when necessary.</p><p>Learning a Task-Specific Policy. In AdaShare, we learn the select-or-skip policy U and network weights W jointly through standard back-propagation from our designed loss functions. However, each select-or-skip policy u l,k is discrete and non-differentiable and this makes direct optimization difficult. Therefore, we adopt Gumbel-Softmax Sampling <ref type="bibr" target="#b24">[25]</ref> to resolve this non-differentiability and enable direct optimization of the discrete policy u l,k using back-propagation.</p><p>Gumbel-Softmax Sampling. The Gumbel-Softmax trick <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b35">36]</ref> is a simple and effective way to substitutes the original non-differentiable sample from a discrete distribution with a differentiable sample from a corresponding Gumbel-Softmax distribution. We let ? l,k = [1 ? ? l,k , ? l,k ] be the distribution vector of the binary random variable u l,k that we want to optimize, where the logit ? l,k represents the probability that the l-th block is selected to execute in the task T k .</p><p>In Gumbel-Softmax Sampling, instead of directly sampling a select-or-skip decision u l,k for the l-th block in the task T k from its distribution ? l,k , we generate it as,</p><formula xml:id="formula_0">u l,k = arg max j?{0,1} log ? l,k (j) + G l,k (j) ,<label>(1)</label></formula><p>where G l,k = ? log(? log U l,k ) is a standard Gumbel distribution with U l,k sampled from a uniform i.i.d. distribution Unif(0, 1). To remove the non-differentiable argmax operation in Eq. 1, the Gumbel Softmax trick relaxes one-hot(u l,k ) ? {0, 1} 2 (the one-hot encoding of u l,k ) to v l,k ? R 2 (the soft select-or-skip decision for the l-th block in T k ) with the reparameterization trick <ref type="bibr" target="#b24">[25]</ref>:</p><formula xml:id="formula_1">v l,k (j) = exp (log ? l,k (j) + G l,k (j))/? i?{0,1} exp (log ? l,k (i) + G l,k (i))/? ,<label>(2)</label></formula><p>where j ? {0, 1} and ? is the temperature of the softmax. Clearly, when ? &gt; 0, the Gumbel-Softmax distribution p ? (v l,k ) is smooth so ? l,k (or ? l,k ) can be directly optimized by gradient descent, and when ? approaches 0, the soft decision v l,k becomes the same as one-hot(u l,k ) and the corresponding Gumbel-Softmax distribution p ? (v l,k ) becomes identical to the discrete distribution ? l,k .</p><p>Following <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b60">61]</ref>, we optimize the discrete policy u l,k , ? l ? L, k ? K at once. During the training, we use the soft task-specific decision v l,k given by Eq. 2 in both forward and backward passes <ref type="bibr" target="#b60">[61]</ref>. Also, we set ? = 5 as the initial value and gradually anneal it down to 0 during the training, as in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b60">61]</ref>. After the learning of the policy distribution, we obtain the discrete task-specific decision U by sampling from the learned policy distribution p(U).</p><p>Loss Functions. Task-specific losses only optimizes for accuracy without taking efficiency into account. However, we prefer to form a compact sub-model for each single task, in which blocks are omitted as much as possible without deteriorating the prediction accuracy. To this end, we propose a sparsity regularization L sparsity to enhance the model's compactness by minimizing the log-likelihood of the probability of a block being executed as</p><formula xml:id="formula_2">Lsparsity = l?L,k?K log ? l,k .<label>(3)</label></formula><p>Furthermore, we introduce a loss L sharing that encourages residual block sharing across tasks to avoid the whole network being split up by tasks with little knowledge shared among them. Encouraging sharing reduces the redundancy of knowledge separately kept in task-specific blocks of related tasks and results in an more efficient sharing scheme that better utilizes residual blocks. Specifically, we minimize the weighted sum of L 1 distances between the policy logits of different tasks with an emphasis on encouraging the sharing of bottom blocks which contain low-level knowledge. More formally, we define L sharing as</p><formula xml:id="formula_3">L sharing = k 1 ,k 2 ?K l?L L ? l L |? l,k 1 ? ? l,k 2 |.<label>(4)</label></formula><p>Finally, the overall loss L is defined as</p><formula xml:id="formula_4">L total = k ? k L k + ?spLsparsity + ? sh L sharing ,<label>(5)</label></formula><p>where L k represent the task-specific losses with task weightings ? k . ? sp and ? sh are the balance parameters for L sparsity and L sharing respectively. The additional losses push the policy learning to automatically induce resource efficiency while preserving the recognition accuracy of different tasks.</p><p>Training Strategy. Following <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b64">65]</ref>, we optimize over the network weights and policy distribution parameters alternately on separate training splits. To encourage the better convergence, we "warm up" the network weights by sharing all blocks across tasks (i.e., hard-parameter sharing) for a few epochs to provide a good starting point for the policy learning. Furthermore, instead of optimizing over the whole decision space in the early training stage, we develop a simple yet effective strategy to gradually enlarge the decision space and form a set of learning tasks from easy to hard, inspired by curriculum learning <ref type="bibr" target="#b3">[4]</ref>. Specifically, for the l-th (l &lt; L) epoch, we only learn the policy distribution of last l blocks. We then gradually learn the distribution parameters of additional blocks as l increases and learn the joint distribution for all blocks after L epochs. After the policy distribution parameters get fully trained, we sample a select-or-skip decision, i.e., feature sharing pattern, from the best policy to form a new network and optimize using the full training set.</p><p>Parameter Complexity. Note that unlike <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17]</ref>, we optimize over the logits A = ? l,k l?L,k?K for the overall select-or-skip policy U directly instead of learning a policy network from the semantic task embedding or an image input. As a result, besides the original network, we only occupy L additional parameters for any new task, which results in a negligible parameter count increase over the total number of network parameters. Our model has also a significantly lower number of parameters (about 50% lower while learning two tasks) compared to the recent deep multi-task learning methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b32">33]</ref>. Therefore, in terms of memory, our model scales very well with more tasks learned together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we conduct extensive experiments to show that our model outperforms many strong baselines and dramatically reduces the number of parameters and computation for efficient multi-task learning <ref type="table" target="#tab_2">(Tables 1-4</ref>). Interestingly, we discover that unlike hard-parameter sharing models, our learned policy often prefers to have task-specific blocks in ResNet's conv3_x layers rather than the last few layers <ref type="figure">(Figure 3: (a)</ref>). Moreover, we also show that reasonable task correlation can be obtained from our learned task-specific policy logits ( <ref type="figure">Figure 3</ref>: (b), <ref type="figure" target="#fig_1">Figure 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets and Tasks.</head><p>We evaluate the performance of our approach using several standard datasets, namely NYU v2 <ref type="bibr" target="#b39">[40]</ref> (used for joint Semantic Segmentation and Surface Normal Prediction as in <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b15">16]</ref>, as well as these two tasks together with Depth Prediction as in <ref type="bibr" target="#b32">[33]</ref>), CityScapes <ref type="bibr" target="#b10">[11]</ref>, considering joint Semantic Segmentation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b18">19]</ref> and Depth Prediction as in <ref type="bibr" target="#b32">[33]</ref>, and Tiny-Taskonomy <ref type="bibr" target="#b67">[68]</ref>, with 5 sampled representative tasks (Semantic Segmentation, Surface Normal Prediction, Depth Prediction, Keypoint Detection and Edge Detection) as in <ref type="bibr" target="#b51">[52]</ref>. We also test AdaShare via performing the same task in different data domains such as image classification on 6 domains in DomainNet <ref type="bibr" target="#b41">[42]</ref> and text classification on 10 publicly available datasets from <ref type="bibr" target="#b7">[8]</ref>. More details on the datasets and tasks are included in the supplementary material.</p><p>Baselines. We compare our approach with following baselines. First, we consider a Single-Task baseline, where we train each task separately using a task-specific backbone and a task-specific head for each task. Second, we use a popular Multi-Task baseline, in which all tasks share the backbone network but have separate task-specific heads at the end. Finally, we compare our method with state-of-the-art multi-task learning approaches, including Cross-Stitch Networks <ref type="bibr">[</ref> , which uses an additional network to learn channel-wise policy for each task with RL. We use the same backbone and task-specific heads for all methods (including our proposed approach) for a fair comparison.</p><p>Evaluation Metrics. In both NYU v2 and CityScapes, Semantic Segmentation is evaluated via mean Intersection over Union (mIoU) and Pixel Accuracy (Pixel Acc). For Surface Normal Prediction, we use mean and median angle distances between the prediction and ground truth of all pixels (the lower the better). We also compute the percentage of pixels whose prediction is within the angles of 11.25 ? , 22.5 ? and 30 ? to the ground truth <ref type="bibr" target="#b12">[13]</ref> (the higher the better). For Depth Prediction, we compute absolute and relative errors as the evaluation metrics (the lower the better) and measure the relative difference between the prediction and ground truth via the percentage of ? = max{ y pred ygt , ygt y pred } within threshold 1.25, 1.25 2 and 1.25 <ref type="bibr">3 [14]</ref> (the higher the better). In Tiny-Taskonomy, we compute the task-specific loss on test images as the performance measurement for a given task, as in <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b51">52]</ref>. For image classification and text recognition, we report classification accuracy for each domain/dataset. Instead of reporting the absolute task performance with multiple metrics for each task T i , we follow <ref type="bibr" target="#b36">[37]</ref> and report a single relative performance ? Ti with respect to the Single-Task baseline to clearly show the positive/negative transfer in different baselines:</p><formula xml:id="formula_5">? Ti = 1 |M | |M | j=0 (?1) lj (M Ti,j ? M ST L,j )/M ST L,j * 100%,<label>(6)</label></formula><p>where l j = 1 if a lower value represents better for the metric M j and 0 otherwise. Finally, we average over all tasks to get overall performance ? T = 1    Experimental Settings. We use Deeplab-ResNet <ref type="bibr" target="#b8">[9]</ref> with atrous convolution, a popular architecture for pixel-wise prediction tasks, as our backbone and the ASPP [9] architecture as task-specific heads. We adopt ResNet-34 (16 blocks) for most scenarios, and use ResNet-18 (8 blocks) for the simple 2-task scenario on the NYU v2 Dataset. For DomainNet, we use the original ResNet-34 as backbone and adopt VD-CNN <ref type="bibr" target="#b9">[10]</ref> for text classification. Following <ref type="bibr" target="#b60">[61]</ref>, we use Adam <ref type="bibr" target="#b27">[28]</ref> to update the policy distribution parameters and SGD to update the network parameters. At the end of the policy training, we sample select-or-skip decisions from the policy distribution to be trained from scratch. Specifically, we sample 8 different network architectures from the learned policy and report the best re-train performance as our result. We use cross-entropy loss for Semantic Segmentation as well as classification tasks, and the inverse of cosine similarity between the normalized prediction and ground truth for Surface Normal Prediction. L1 loss is used for all other tasks. Pre-training depends on tasks and we observe that it improves the overall performance of AdaShare by 11.3% in NYUv2 3-Task learning. However, to get rid of the unfairness brought by different pretrained model, we start from scratch for a fair comparison among different methods in all our experiments.</p><formula xml:id="formula_6">|T | K i=1 ? Ti .</formula><formula xml:id="formula_7">Models # Params ? ? T1 ? ? T2 ? ? T3 ? ? T4 ? ? T5 ? ? T ? Multi-Task -80.0 -3.</formula><p>Quantitative Results. We report all metrics and the relative performance of two tasks in NYU-v2 2-Task Learning (see <ref type="table" target="#tab_2">Table 1</ref>) and report all metrics of Single-Task Baseline and the relative performance of other methods due to the limited space in other cases (see <ref type="table" target="#tab_4">Table 2</ref> <ref type="bibr">-4)</ref>. We recommend readers to refer to supplementary material for the full comparison of all metrics.</p><p>In NYU v2 2-Task Learning, AdaShare outperforms all the baselines on 4 metrics out of 7 and achieves the second best on 1 metric (see <ref type="table" target="#tab_2">Table 1</ref>). Compared to Single-task, Cross-Stitch, Sluice, and NDDR-CNN, which use separate backbones for each task, our approach obtains superior task performance with less than half of the number of parameters. Moreover, AdaShare also outperforms the vanilla Multi-Task baseline and DEN <ref type="bibr" target="#b0">[1]</ref>, the most competitive approaches in terms of number of parameters, showing that it is able to pick an optimal combination of shared and task-specific knowledge with the same number of network parameters without using any additional policy network.</p><p>Similarly, for other learning scenarios (  The darkness of a block represents the probability of that block selected for the given task. We also provide the select-and-skip decision U from our AdaShare. In (b), we provide the task correlation, i.e. the cosine similarity between task-specific dataset. Two 3D tasks (Surface Normal Prediction and Depth Prediction) are more correlated and so as two 2D tasks (Keypoint Detection and Edge Detection).</p><p>order to improve the performance of Semantic Segmentation. In contrast, our approach is still able to improve the segmentation performance instead of suffering from the negative interference by the other two tasks. The same reduction in negative transfer is also observed in Surface Normal Prediction in Tiny-Taskonomy 5-Task Learning. However, our proposed approach AdaShare still performs the best using less than 1/5 parameters of most of the baselines <ref type="table" target="#tab_6">(Table 4)</ref>.</p><p>Moreover, our proposed AdaShare also achieves better overall performance across the same task on different domains. For image classification on DomainNet <ref type="bibr" target="#b41">[42]</ref>, AdaShare improves average accuracy over Multi-Task baseline on 6 different visual domains by 4.6% (62.2% vs. 57.6%), with the maximum 16% improvement in quickdraw domain. For text classification task, AdaShare outperforms the Multi-Task baseline by 7.2% (76.1% vs. 68.9%) in average over 10 different NLP datasets <ref type="bibr" target="#b7">[8]</ref> and maximally improves 27.8% in sogou_news dataset. Policy Visualization and Task Correlation. In <ref type="figure">Figure 3</ref>: (a), we visualize our learned policy distributions (via logits) and the feature sharing policy in Tiny-Taskonomy 5-Task Learning (more visualizations are included in supplementary material). We also adopt the cosine similarity between task-specific policy logits as an effective representation of task correlations <ref type="figure">(Figure 3</ref>: (b), <ref type="figure" target="#fig_1">Figure 4</ref>). We have the following key observations. (a) The execution probability of each block for task k shows that not all blocks contribute to the task equally and it allows AdaShare to mediate among tasks and decide task-specific blocks adaptive to the given task set. (b) Our learned policy prefers to have more blocks shared only among a sub-group of tasks in ResNet's conv3_x layers, where middle/high-level features, which are more task specific, are starting to get captured. By having blocks shared by a sub-group of tasks, AdaShare encourages the positive transfer and relieves the effect of negative transfer, resulting in better overall performance. (c) We clearly observe that Surface Normal Prediction and Depth Prediction, two different 3D tasks, are more correlated, and that Keypoint prediction and Edge detection, two different 2D tasks are more correlated (see <ref type="figure">Figure 3</ref>: (b)). Similarly, <ref type="figure" target="#fig_1">Figure 4</ref> shows that the domain real is closer to painting than quickdraw in DomainNet. Both results follow the intuition that similar tasks should have similar execution distribution to share knowledge. Note that the cosine similarity purely measures the correlation between the normalized execution probabilities of different tasks, which is not influenced by the different optimization uncertainty of different tasks. Ablation Studies. We present four groups of ablation studies in NYU-v2 3-Task learning to test our learned policy, the effectiveness of different training losses and optimization method ( <ref type="table" target="#tab_11">Table 5)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Re al Pa in tin g Cl ip ar t Qu ick dra w In fo gr ap h</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computation Cost (FLOPs</head><p>Comparison with Stochastic Depth. Stochastic Depth <ref type="bibr" target="#b21">[22]</ref> randomly drops blocks as a regularization during the training and uses the full model in the inference. We compare AdaShare with Stochastic Depth in our multi-task setting and observe that AdaShare gains more improvement (overall 5.8% improvement in <ref type="table" target="#tab_11">Table 5</ref>), which distinguishes AdaShare from a regularization technique.</p><p>Comparison with Random Policy. We perform two different experiments such as 'Random #1' experiment, where we keep the same number of skipped blocks in total for all tasks and randomize their locations and 'Random #2, where we further force the same number of skipped blocks per task as AdaShare. We report the best performance among eight samples in each experiment. In <ref type="table" target="#tab_11">Table 5</ref>, both random experiments improve the performance of Multi-Task baseline by incorporating shared and task-specific blocks in the model. Also, Random #2 works better than Random #1, which reveals that the number of blocks assigned to each task actually matters and our method makes a good prediction of it. Our model still outperforms Random #2, demonstrating that AdaShare correctly predicts the location of those skipped blocks, which forms the final sharing pattern in our approach. Ablation on Training Losses and Strategies.</p><p>We perform experiments to show the effectiveness of curriculum learning, sparsity regularization L sparsity and the sharing loss L sharing in our model. With all the components working, our approach works the best in all three tasks (see <ref type="table" target="#tab_11">Table 5</ref>), indicating that three components benefit the policy learning.</p><p>Comparison with Instance-specific Policy. We employ the same policy network <ref type="bibr" target="#b0">[1]</ref> to compute the select-and-skip decision per test image for each task <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b61">62]</ref>. AdaShare, with task-specific policy, outperforms the instancespecific policy (in <ref type="table" target="#tab_11">Table 5</ref>), as the discrepancy among tasks dominates over the discrepancy among samples in multi-task learning. Instance-specific methods often introduce extra optimization difficulty and result in worse convergence. Comparison with AdaShare-RL. We replace Gumbel-Softmax Sampling with REINFORCE to optimize the select-or-skip policy while other parts are unchanged. <ref type="table" target="#tab_11">Table 5</ref> shows AdaShare is better than AdaShare-RL in each task and overall performance, in line with the comparison in <ref type="bibr" target="#b62">[63]</ref>.</p><p>Extension to other Architectures. We implement AdaShare using Wide ResNets (WRN) <ref type="bibr" target="#b66">[67]</ref> and MobileNet-v2 <ref type="bibr" target="#b49">[50]</ref> in addition to ResNets. AdaShare outperforms the Multi-Task baseline by 5.8% and 3.2% using WRN and MobileNet respectively in NYU-v2 2-Task <ref type="table" target="#tab_12">(Table 6</ref>). We also observe a similar trend on CityScapes 2-Task learning. This shows effectiveness of our proposed approach across different network architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present a novel approach for adaptively determining the feature sharing strategy across multiple tasks in deep multi-task learning. We learn the feature sharing policy and network weights jointly using standard back-propagation without adding any significant number of parameters. We also introduce two resource-aware regularizations for learning a compact multi-task network with much fewer parameters while achieving the best overall performance across multiple tasks. We show the effectiveness of our proposed approach on five standard datasets, outperforming several competing methods. Moving forward, we would like to explore AdaShare using a much higher task-to-layer ratio, which may require increase in network capacity to superimpose all the tasks into a single multi-task network. Moreover, we will extend AdaShare for finding a fine-grained channel sharing pattern instead of layer-wise policy across tasks, for more efficient deep multi-task learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>Our research improves the capacity of deep neural networks to solve many tasks at once in a more efficient manner. It enables the use of smaller networks to support more tasks, while performing knowledge transfer between related tasks to improve their accuracy. For example, we showed that our proposed approach can solve five computer vision tasks (semantic segmentation, surface normal prediction, depth prediction, keypoint detection and edge estimation) with 80% fewer parameters while achieving the same performance as the standard approach.</p><p>Our approach can thus have a positive impact on applications that require multiple tasks such as computer vision for robotics. Potential applications could be in assistive robots, autonomous navigation, robotic picking and packaging, rescue and emergency robotics and AR/VR systems. Our research can reduce the memory and power consumption of such systems and enable them to be deployed for longer periods of time and become smaller and more agile. The lessened power consumption could have a high impact on the environment as AI systems become more prevalent.</p><p>Negative impacts of our research are difficult to predict, however, it shares many of the pitfalls associated with deep learning models. These include susceptibility to adversarial attacks and data poisoning, dataset bias, and lack of interpretablity. Other risks associated with deployment of computer vision systems include privacy violations when images are captured without consent, or used to track individuals for profit, or increased automation resulting in job losses. While we believe that these issues should be mitigated, they are beyond the scope of this paper. Furthermore, we should be cautious of the result of failure of the system which could impact the performance/user experience of the high-level AI systems relied on our research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Full Details on the Datasets and Tasks</head><p>CityScapes. The CityScapes dataset <ref type="bibr" target="#b10">[11]</ref> consists of high resolution street-view images. We use this dataset for two tasks: semantic segmentation and depth estimation, as in <ref type="bibr" target="#b32">[33]</ref>. We adopt 19-class annotation for semantic segmentation and use the official train/test splits for experiments. During the training, all the input images are resized to 321 x 321 by random flipping, cropping and rescaling and we test on the full resolution 480 x 640.</p><p>NYU v2. The NYUv2 dataset <ref type="bibr" target="#b39">[40]</ref> is consisted with RGB-D indoor scene images. We use this dataset in two different scenarios. First, we consider semantic segmentation and surface normal prediction together <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b15">16]</ref> and then, use depth prediction along with semantic segmentation and surface normal prediction for experimenting on a 3-task scenario, as in <ref type="bibr" target="#b32">[33]</ref>. We use 40-class annotation for semantic segmentation and the official train/val splits which include 795 images for training and 654 images for validation. We use the publicly available surface normals provided by <ref type="bibr" target="#b15">[16]</ref> in our experiments. During the training, we resize the input images to 224 x 224 and test on the full resolution 256 x 512.</p><p>Tiny-Taskonomy. Taskonomy <ref type="bibr" target="#b67">[68]</ref> is large-scale dataset consisting of 4.5 million images from over 500 buildings with annotations available for 26 tasks. Considering the huge size of full Taskonomy dataset (?12TB in size), we use its officially released tiny train/val/test splits instead of the full dataset. Tiny Taskonomy consists of 381,840 indoor images from 35 buildings with annotations available for 26 tasks. Following <ref type="bibr" target="#b51">[52]</ref>, we sampled 5 representative tasks out of 26 tasks for our experiments, namely Semantic Segmentation, Surface Normal Prediction, Depth Prediction, Keypoint Detection and Edge Detection. We use the official train/test splits that include images from 25 buildings for training and images from 5 buildings for testing. This dataset is more challenging as the model has to learn semantic, 3D and 2D structures at the same time for solving these tasks.</p><p>DomainNet. DomainNet <ref type="bibr" target="#b41">[42]</ref> is a recent benchmark for multi-source domain adaptation in object recognition. It is one of the large-scale domain adaptation benchmark with 0.6m images across six domains (clipart, infograph, painting, quickdraw, real, sketch) and 345 categories. We consider each domain as a task and use the official train/test splits in our experiments.</p><p>Text Classification. We perform text classification on a group of 10 publicly available datasets from <ref type="bibr" target="#b7">[8]</ref>, namely ag_news, amazon_review_full, amazon_review_polarity, dbpedia, sogou_news, yahoo_answers, yelp_review_full, yelp_review_polarity, SST-1 and SST-2. These datasets include both multi-class and binary classification tasks. We consider classification within a dataset as task and use the official train/test splits provided by the datasets in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation Details</head><p>Our training is separated into two phases: the Policy Learning Phase and the Re-training Phase. For NYU v2 <ref type="bibr" target="#b12">[13]</ref> and CityScapes <ref type="bibr" target="#b10">[11]</ref>, we update the network 20,000 iterations for both the Policy Learning and Re-training Phases. For Tiny-Taskonomy <ref type="bibr" target="#b67">[68]</ref>, the network is trained for 100,000 iterations in the Policy Learning Phase and 30,000 in the Re-training Phase. In the Policy Learning Phase, we warm up the network by 20% of total iterations. We train all baselines with the same number of iterations with it in the Re-training Phase to form a fair comparison. In both phases, we use the early stop to get the best performance during the training. In <ref type="table" target="#tab_13">Table 7</ref>, we provide the learning rate and loss weightings per dataset. We use the same parameter set for our model and baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Implementation of Baselines</head><p>We implement and adapt Cross-Stitch <ref type="bibr" target="#b38">[39]</ref>, Sluice <ref type="bibr" target="#b47">[48]</ref>, NDDR-CNN <ref type="bibr" target="#b15">[16]</ref>, MTAN <ref type="bibr" target="#b32">[33]</ref>, and DEN <ref type="bibr" target="#b0">[1]</ref> to the ResNet architecture following the details in paper and their released code. For Cross-Stitch and Sluice, we insert the linear feature fusion layers after each residual block. For Sluice, we use the orthogonality constraint between two subspaces of the layer-wise feature space <ref type="bibr" target="#b47">[48]</ref>. We add each NDDR-layer for feature fusion after each group of blocks, e.g. conv1_x, conv2_x, as mentioned in <ref type="bibr" target="#b15">[16]</ref>. For MTAN, we adapt the attention module which was designed for VGG-16 encoder networks to every residual block in ResNet. In each attention module, we keep the same convolution layers and change input/output channels and spatial dimensions to match the ResNet's architecture <ref type="bibr" target="#b0">1</ref> . Please refer to <ref type="bibr" target="#b32">[33]</ref> for more details. Moreover, instead of 7-class segmentation in <ref type="bibr" target="#b32">[33]</ref>, we report the standard 19-class segmentation in our work. We also experiment with 7-class segmentation, AdaShare achieves average 4% improvement on 5 metrics using 58.5% of parameters fewer than MTAN. For DEN <ref type="bibr" target="#b0">[1]</ref>, we consult their public code for implementation details and use the same backbone and task-specific heads with AdaShare for a fair comparison. We empirically set ? = 1 in DEN to get better performance (compared to ? = 0.1). For Stochastic Depth <ref type="bibr" target="#b21">[22]</ref>, we randomly drop blocks for each task (with a linear decay rule pL = 0.5 in our implementation) during the training and use all blocks for each task in test.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Full Comparison of All Metrics</head><p>In this section, we provide the full comparison of all metrics in CityScapes 2-Task Learning, NYU-v2 3-Task Learning and Tiny-Taskonomy 5-Task Learning (see <ref type="table" target="#tab_14">Table 8</ref>-10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E FLOPs and Inference Time</head><p>We report FLOPs of different multi-task learning baselines and their inference time for all tasks of a single image. <ref type="table" target="#tab_2">Table 11</ref> shows that AdaShare reduces FLOPs and inference time in most cases by skipping blocks in some tasks while not adopting any auxiliary networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Policy Visualizations</head><p>We visualize the policy and sharing patterns learned by AdaShare in NYU-v2 2-Task Learning, CityScapes 2-Task Learning and NYU-v2 3-Task Learning (see <ref type="figure" target="#fig_2">Figure 5</ref>). The observations of policy visualization in the main paper still hold in these scenarios.</p><p>We experiment on five tasks (Semantic Segmentation, Surface Normal Prediction, Depth Prediction, Keypoint Prediction and Edge Prediction) for Tiny-Taskonomy dataset. In the main paper (see Section 4.1), we visualize the policy decision for five tasks. In this section, we further investigate the sharing patterns of subset of tasks (see <ref type="figure" target="#fig_3">Figure 6</ref>), e.g., Semantic Segmentation and Surface Normal Prediction ( <ref type="figure" target="#fig_3">Figure 6.(a)</ref>       <ref type="figure" target="#fig_3">Figure 6.(c)</ref>). These subset of tasks are same as the tasks considered in NYU v2 2-Task Learning, CityScapes 2-Task Learning and NYU v2 3-Task Learning respectively. In each subset of tasks, we both have shared blocks and task-specific (or not shared by all tasks) blocks. The sharing patterns help the model to share the knowledge between tasks when necessary and own the individual knowledge for a single task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Class-wise Segmentation Performance</head><p>The performance of Semantic Segmentation can be easily affected by both Surface Normal Prediction and Depth Prediction tasks on NYU v2 dataset, but our method mitigates this negative interference and further improves the performance. In this section, we closely investigate the performance (Pixel Accuracy) per class and their relationship with the number of labeled pixels. From <ref type="figure" target="#fig_4">Figure 7</ref>, we find that we improve the performance of most classes including those with less labeled data compared to MTAN <ref type="bibr" target="#b32">[33]</ref> (the most competitive MTL baseline in semantic segmentation performance).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Qualitative Visualization</head><p>In this section, we visualize the results of Multi-Task, MTAN (the best baseline), DEN (ICCV 2019) and AdaShare in NYU v2 3-task learning. From the comparison (see <ref type="figure" target="#fig_5">Figure 8</ref>), we observe that AdaShare predicts the class label more accurately in Semantic Segmentation; predicts the normal vector closer to the ground truth in Surface Normal Prediction; gives clearer contour of object in Semantic Segmentation, Surface Normal Prediction and Depth Prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Ablation Studies on CityScapes</head><p>We present four groups of ablation studies in CityScapes 2-Task Learning to test our learned policy, the effectiveness of different training losses and optimization method <ref type="table" target="#tab_2">(Table 12)</ref>. Similar to ablation studies on NYU-v2 3 task learning (in the main paper), our proposed AdaShare outperforms its variants in most of individual metrics and overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Full Comparison of Ablation Studies on NYU-v2 3-Task</head><p>In addition to the relative performance of ablation studies in NYU-v2 3-Task Learning, we provide the full comparison of all metrics (see <ref type="table" target="#tab_2">Table 13</ref>).    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MTAN</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :Taskonomy 5 -</head><label>35</label><figDesc>Policy Visualization and Task Correlation. (a) We visualize the learned policy logits A in Tiny-Task learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Task Correlation inDomainNet. Similar tasks are more correlated, such as real is closer to painting than quickdraw.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Policy Visualization. We visualize the learned policy logits A in NYU-v2 2-Task Learning, CityScapes 2-Task Learning and NYU-v2 3-Task Learning. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Policy Visualization of subset of tasks on Tiny-Taskonomy. We visualize the sharing patterns of subset of tasks: {Semantic Segmentation (Seg), Surface Normal Prediction (SN)}, {Seg, Depth Prediction (Depth)} and {Seg, SN and Depth}. For example, in (a), Seg and SN share 14 out of 16 blocks in total and Seg owns 2 task-specific blocks. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>for semantic segmentation and Number of pixel labels per class (AdaShare v.s. MTAN) Change in Pixel Accuracy for Semantic Segmentation classes of AdaShare over MTAN (blue bars). The class is ordered by the number of pixel labels (the black line). Compare to MTAN, we improve the performance of most classes including those with less labeled data. and Depth Prediction (Figure 6.(b)) and Semantic Segmentation, Surface Normal Prediction and Depth Prediction (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Qualitative Visualization of Multi-Task, MTAN, DEN and AdaShare Performance in NYU v2 3-task Learning. The red boxes represent the regions of interest. Our proposed method, AdaShare gives more accurate prediction and clearer contour in Semantic Segmentation (Seg), Surface Normal Prediction (SN) and Depth Prediction (Depth). Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Input Outputs MTL Architectures Traditional Hard-parameter Sharing Seg SN Seg SN Soft-parameter Sharing Seg SN</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>Task1-</cell></row><row><cell></cell><cell>Specific</cell></row><row><cell></cell><cell>Task2-</cell></row><row><cell></cell><cell>Specific</cell></row><row><cell></cell><cell>Shared</cell></row><row><cell>AdaShare</cell><cell>Skipped</cell></row><row><cell>(Ours)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Cross Entropy Loss Cosine Similarity Loss Sharing Loss Sparsity Loss Sparsity Loss</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>T1</cell><cell>Semantic Segmentation</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Gumbel-Softmax</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Sampling</cell></row><row><cell></cell><cell>Compact View</cell><cell></cell><cell>T2</cell><cell></cell><cell>Forward</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Back-prop</cell></row><row><cell>Task 1 Logits</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Supervision</cell></row><row><cell>Task 1 Policy</cell><cell>Task 1</cell><cell></cell><cell>T1</cell><cell></cell><cell>Task1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Task2</cell></row><row><cell>Task 2 Policy</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Task 2</cell><cell></cell><cell>T2</cell><cell></cell><cell>Shared</cell></row><row><cell>Task 2 Logits</cell><cell>Expanded View</cell><cell>Backbone</cell><cell>Task-Specific Heads</cell><cell>Surface Normal</cell><cell>Skipped</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>NYU v2 2-Task Learning. AdaShare achieves the best performance (bold) on 4 out of 7 metrics and second best (underlined) on 1 metric across Semantic Segmentation and Surface Normal Prediction using less than 1/2 parameters of most baselines. T1: Semantic Segmentation; T2: Surface Normal Prediction. T1 ? Mean Median 11.25 ? 22.5 ? 30 ? ? T2 ? ? T ?</figDesc><table><row><cell cols="5">Model ? Single-Task T 1 : Semantic Seg. mIoU ? Pixel Acc ? # Params (%) ? 0.0 27.8 58.5</cell><cell>0.0</cell><cell cols="4">T 2 : Surface Normal Prediction Error ? ??, within ? 17.3 14.4 37.2 73.7 85.1</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell>Multi-Task</cell><cell>-50.0</cell><cell>22.6</cell><cell>55.0</cell><cell cols="2">-12.3</cell><cell>16.9</cell><cell>13.7</cell><cell>41.0</cell><cell>73.1 84.3</cell><cell>+ 3.1</cell><cell>-4.6</cell></row><row><cell>Cross-Stitch</cell><cell>0.0</cell><cell>25.3</cell><cell>57.4</cell><cell cols="2">-5.4</cell><cell>16.6</cell><cell>13.2</cell><cell>43.7</cell><cell>72.4 83.8</cell><cell>+ 5.3</cell><cell>-0.1</cell></row><row><cell>Sluice</cell><cell>0.0</cell><cell>26.6</cell><cell>59.1</cell><cell cols="2">-1.6</cell><cell>16.6</cell><cell>13.0</cell><cell>44.1</cell><cell>73.0 83.9</cell><cell>+ 6.0</cell><cell>+ 2.2</cell></row><row><cell>NDDR-CNN</cell><cell>+ 6.5</cell><cell>28.2</cell><cell>60.1</cell><cell cols="2">+ 2.1</cell><cell>16.8</cell><cell>13.5</cell><cell>42.8</cell><cell>72.1 83.7</cell><cell>+ 4.1</cell><cell>+ 3.1</cell></row><row><cell>MTAN</cell><cell>+ 23.5</cell><cell>29.5</cell><cell>60.8</cell><cell cols="2">+ 5.0</cell><cell>16.5</cell><cell>13.2</cell><cell>44.1</cell><cell>72.8 83.7</cell><cell>+ 5.7</cell><cell>+ 5.4</cell></row><row><cell>DEN</cell><cell>-39.0</cell><cell>26.3</cell><cell>58.8</cell><cell cols="2">-2.4</cell><cell>17.0</cell><cell>14.3</cell><cell>39.5</cell><cell>72.2 84.7</cell><cell>-1.2</cell><cell>-0.6</cell></row><row><cell>AdaShare</cell><cell>-50.0</cell><cell>29.6</cell><cell>61.3</cell><cell cols="2">+ 5.6</cell><cell>16.6</cell><cell>12.9</cell><cell>45.0</cell><cell>72.1 83.2</cell><cell>+ 6.2</cell><cell>+ 5.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>CityScapes 2-Task Learning. T1: Semantic Segmentation, T2: Depth Prediction Models # Params ? ? T1 ? ? T2 ? ? T ? Rel.: 0.33, ? &lt; 1.25, 1.25 2 , 1.25 3 : 70.3, 86.3, 93.3.</figDesc><table><row><cell>Multi-Task</cell><cell>-50.0</cell><cell>-3.7</cell><cell>-0.5</cell><cell>-2.1</cell></row><row><cell>Cross-Stitch</cell><cell>0</cell><cell>-0.1</cell><cell>+5.8</cell><cell>+2.8</cell></row><row><cell>Sluice</cell><cell>0</cell><cell>-0.8</cell><cell>+4.0</cell><cell>+1.6</cell></row><row><cell>NDDR-CNN</cell><cell>+3.5</cell><cell>+1.3</cell><cell>+3.3</cell><cell>+2.3</cell></row><row><cell>MTAN</cell><cell>-20.5</cell><cell>+0.5</cell><cell>+4.8</cell><cell>+2.7</cell></row><row><cell>DEN</cell><cell>-44.0</cell><cell>-3.1</cell><cell>-1.6</cell><cell>-2.4</cell></row><row><cell>AdaShare</cell><cell>-50.0</cell><cell>+1.8</cell><cell>+3.8</cell><cell>+2.8</cell></row><row><cell cols="5">Single-Task. Seg -mIoU: 40.2, PAcc: 74,7; Depth -Abs.:</cell></row><row><cell>0.017,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>NYU v2 3-Task Learning. T1: Semantic Segmentation, T2: Surface Normal Pred., T3: Depth Pred. T1 ? ? T2 ? ? T3 ? ? T ? Single-Task. Seg -mIoU: 27.5, PAcc: 58.9; SN Mean: 17.5, Median: 15.2, ?? &lt; 11.25 ? , 22.5 ? , 30 ? : 34.9, 73.3, 85.7; Depth -Abs.: 0.62, Rel.: 0.25, ? &lt; 1.25, 1.25 2 , 1.25 3 : 57.9, 85.8, 95.</figDesc><table><row><cell>Multi-Task</cell><cell>-66.7</cell><cell>-7.6</cell><cell>+7.5</cell><cell>+5.2</cell><cell>+1.7</cell></row><row><cell>Cross-Stitch</cell><cell>0.0</cell><cell>-4.9</cell><cell>+4.2</cell><cell>+ 4.7</cell><cell>+ 1.3</cell></row><row><cell>Sluice</cell><cell>0.0</cell><cell>-8.4</cell><cell>+2.9</cell><cell>4.1</cell><cell>-0.5</cell></row><row><cell>NDDR-CNN</cell><cell>+5.0</cell><cell>-15.0</cell><cell>+2.9</cell><cell>-3.5</cell><cell>-5.2</cell></row><row><cell>MTAN</cell><cell>+3.7</cell><cell>-4.2</cell><cell>+8.7</cell><cell>+ 3.8</cell><cell>+2.7</cell></row><row><cell>DEN</cell><cell>-62.7</cell><cell>-9.9</cell><cell>+1.7</cell><cell>-35.2</cell><cell>-14.5</cell></row><row><cell>AdaShare</cell><cell>-66.7</cell><cell>+8.8</cell><cell>+7.9</cell><cell>+10.1</cell><cell>+8.9</cell></row></table><note>Models # Params ? ?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Tiny</figDesc><table /><note>-Taskonomy 5-Task Learning. T1: Semantic Segmentation, T2: Surface Normal Prediction, T3: Depth Prediction, T4: Keypoint Estimation, T5: Edge Estimation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 1</head><label>1</label><figDesc></figDesc><table /><note>-4 show the task performance in four different learning scenarios, namely NYU-v2 2-Task Learning, CityScapes 2-Task Learning, NYU-v2 3-Task Learning and Tiny- Taskonomy 5-Task Learning.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>0.65</cell><cell cols="2">0.70</cell><cell></cell><cell>0.75</cell><cell></cell><cell>0.80</cell><cell></cell><cell>0.85</cell><cell></cell><cell>0.95</cell><cell></cell><cell>0.90</cell><cell>Tiny-Taskonomy</cell></row><row><cell></cell><cell>Seg</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Seg</cell></row><row><cell></cell><cell>SN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Tasks</cell><cell>Keypoint Depth</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SN</cell></row><row><cell></cell><cell>Edge</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Depth</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell><cell>14</cell><cell>15</cell><cell>16</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Blocks</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Keypoint</cell></row><row><cell cols="2">Network</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Edge</cell></row><row><cell cols="2">Shared by All (9/16)</cell><cell cols="3">Forwarding: Seg + SN + Depth + Edge (1/16)</cell><cell>Seg</cell><cell cols="3">SN Seg + SN + Keypoint + Edge (2/16)</cell><cell cols="3">Depth Seg + Depth + Edge + Keypoint (1/16)</cell><cell cols="4">Keypoint Seg + SN + Depth + Edge Keypoint (2/16)</cell><cell>Seg + Keypoint + Depth (1/16)</cell><cell>S e g</cell><cell>S N D e p t h K e y p o i n t</cell><cell>E d g e</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">(a) Policy Visualization</cell><cell></cell><cell></cell><cell></cell><cell>(b) Task Correlation</cell></row></table><note>-4), AdaShare significantly outperforms all the baselines on overall relative performance while saving at least 50%, up to 80%, of parameters compared to most of the baselines. AdaShare also outperforms the Multi-Task baseline and DEN with similar parameter usage. Specifically, for Semantic Segmentation in NYU-v2 3-Task Learning, we observe that the performance of all the baselines are worse than the Single-Task baseline, showing that knowledge from Surface Normal Prediction and Depth Prediction should be carefully selected in Seg + SN + Depth + Keypoint + Edge</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>Ablation</figDesc><table><row><cell>Stochastic Depth</cell><cell>-2.4</cell><cell>+7.5</cell><cell>+4.0</cell><cell>+3.1</cell></row><row><cell>Random # 1</cell><cell>-2.3</cell><cell>+5.4</cell><cell>-0.8</cell><cell>+ 1.3</cell></row><row><cell>Random # 2</cell><cell>+ 3.3</cell><cell>+8.4</cell><cell>+8.1</cell><cell>+ 6.6</cell></row><row><cell>w/o curriculum</cell><cell>+2.1</cell><cell>+7.4</cell><cell>+7.2</cell><cell>+ 5.6</cell></row><row><cell>w/o L sparsity</cell><cell>-4.2</cell><cell>+ 4.8</cell><cell>+1.6</cell><cell>+0.7</cell></row><row><cell>w/o L sharing</cell><cell>-0.9</cell><cell>+9.0</cell><cell>+8.5</cell><cell>+5.6</cell></row><row><cell>AdaShare-Instance</cell><cell>-3.7</cell><cell>+ 5.3</cell><cell>-22.3</cell><cell>-6.9</cell></row><row><cell>AdaShare-RL</cell><cell>-2.8</cell><cell>0.0</cell><cell>-8.2</cell><cell>-3.7</cell></row><row><cell>AdaShare</cell><cell>+8.8</cell><cell>+7.9</cell><cell>+10.1</cell><cell>+8.9</cell></row></table><note>Study on NYU v2 3-Task Learn- ing. T1: Semantic Segmentation, T2: Surface Normal Prediction, T3: Depth Prediction. Models ? T1 ? ? T2 ? ? T2 ? ? T ?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Different Network Archi-</figDesc><table><row><cell cols="4">tectures on NYU v2 2-Task Learn-</cell></row><row><cell cols="4">ing. T1: Semantic Segmentation, T2:</cell></row><row><cell cols="3">Surface Normal Prediction.</cell><cell></cell></row><row><cell>Models</cell><cell cols="3">? T1 ? ? T2 ? ? T ?</cell></row><row><cell></cell><cell>WRN</cell><cell></cell><cell></cell></row><row><cell cols="2">Multi-Task -0.35</cell><cell>9.63</cell><cell>4.64</cell></row><row><cell>AdaShare</cell><cell>9.36</cell><cell cols="2">11.53 10.44</cell></row><row><cell></cell><cell cols="2">MobileNet-v2</cell><cell></cell></row><row><cell>Multi-Task</cell><cell>0.18</cell><cell>8.02</cell><cell>4.10</cell></row><row><cell>AdaShare</cell><cell>4.16</cell><cell>10.61</cell><cell>7.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Hyper-parameters for NYU v2 2-task learning, CityScapes 2-task learning, NYU v2 3-task learning and Tiny-Taskonomy 5-task learning. We provide the learning rates (weight lr and policy lr) including ?seg, ?sn, ? depth , ? kp and ? edge as the task weightings for Semantic Segmentation, Surface Normal Prediction, Depth Prediction, Keypoint Prediction and Edge Detection respectively. ?sp and ? sh are the weights for sparsity regularization (Lsparsity) and sharing encouragement (L sharing ) respectively in policy learning.Datasetweight lr policy lr ? seg ? sn ? depth ? kp ? edge</figDesc><table><row><cell>? sp</cell><cell>? sh</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>CityScapes 2-Task Learning. Our proposed AdaShare achieves the best performance (bold) on five</figDesc><table><row><cell cols="6">out of seven metrics and second best (underlined) on one metric across Semantic Segmentation and Depth</cell></row><row><cell cols="4">Prediction using less than 1/2 parameters of most baselines.</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell># Params ?</cell><cell cols="2">Semantic Seg. Pixel mIoU ? Acc ?</cell><cell cols="2">Depth Prediction ?, within ? Rel 1.25 1.25 2 1.25 3 Error? Abs</cell></row><row><cell>Single-Task</cell><cell>2</cell><cell>40.2</cell><cell>74.7</cell><cell>0.017 0.33 70.3 86.3</cell><cell>93.3</cell></row><row><cell>Multi-Task</cell><cell>1</cell><cell>37.7</cell><cell>73.8</cell><cell>0.018 0.34 72.4 88.3</cell><cell>94.2</cell></row><row><cell>Cross-Stitch</cell><cell>2</cell><cell>40.3</cell><cell>74.3</cell><cell>0.015 0.30 74.2 89.3</cell><cell>94.9</cell></row><row><cell>Sluice</cell><cell>2</cell><cell>39.8</cell><cell>74.2</cell><cell>0.016 0.31 73.0 88.8</cell><cell>94.6</cell></row><row><cell>NDDR-CNN</cell><cell>2.07</cell><cell>41.5</cell><cell>74.2</cell><cell>0.017 0.31 74.0 89.3</cell><cell>94.8</cell></row><row><cell>MTAN</cell><cell>2.41</cell><cell>40.8</cell><cell>74.3</cell><cell>0.015 0.32 75.1 89.3</cell><cell>94.6</cell></row><row><cell>DEN</cell><cell>1.12</cell><cell>38.0</cell><cell>74.2</cell><cell>0.017 0.37 72.3 87.1</cell><cell>93.4</cell></row><row><cell>AdaShare</cell><cell>1</cell><cell>41.5</cell><cell>74.9</cell><cell>0.016 0.33 75.5 89.8</cell><cell>94.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 :</head><label>9</label><figDesc>NYU v2 3-Task Learning. Our proposed method AdaShare achieves the best performance (bold) on ten out of twelve metrics across Semantic Segmentation, Surface Normal Prediction and Depth Prediction using less than 1/3 parameters of most of the baselines. Mean Median 11.25 ? 22.5 ? 30 ? Abs Rel 1.25 1.25 2 1.25 3</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Semantic Seg.</cell><cell></cell><cell cols="3">Surface Normal Prediction</cell><cell></cell><cell>Depth Prediction</cell></row><row><cell>Model</cell><cell># Params ?</cell><cell cols="2">mIoU ? Pixel Acc ?</cell><cell cols="2">Error ?</cell><cell></cell><cell>?, within ?</cell><cell>Error ?</cell><cell>?, within ?</cell></row><row><cell>Single-Task</cell><cell>3</cell><cell>27.5</cell><cell>58.9</cell><cell>17.5</cell><cell>15.2</cell><cell>34.9</cell><cell cols="3">73.3 85.7 0.62 0.25 57.9 85.8</cell><cell>95.7</cell></row><row><cell>Multi-Task</cell><cell>1</cell><cell>24.1</cell><cell>57.2</cell><cell>16.6</cell><cell>13.4</cell><cell>42.5</cell><cell cols="3">73.2 84.6 0.58 0.23 62.4 88.2</cell><cell>96.5</cell></row><row><cell>Cross-Stitch</cell><cell>3</cell><cell>25.4</cell><cell>57.6</cell><cell>17.2</cell><cell>14.0</cell><cell>41.4</cell><cell cols="3">70.5 82.9 0.58 0.23 61.4 88.4</cell><cell>95.5</cell></row><row><cell>Sluice</cell><cell>3</cell><cell>23.8</cell><cell>56.9</cell><cell>17.2</cell><cell>14.4</cell><cell>38.9</cell><cell cols="3">71.8 83.9 0.58 0.24 61.9 88.1</cell><cell>96.3</cell></row><row><cell>NDDR-CNN</cell><cell>3.15</cell><cell>21.6</cell><cell>53.9</cell><cell>17.1</cell><cell>14.5</cell><cell>37.4</cell><cell cols="3">73.7 85.6 0.66 0.26 55.7 83.7</cell><cell>94.8</cell></row><row><cell>MTAN</cell><cell>3.11</cell><cell>26.0</cell><cell>57.2</cell><cell>16.6</cell><cell>13.0</cell><cell>43.7</cell><cell cols="3">73.3 84.4 0.57 0.25 62.7 87.7</cell><cell>95.9</cell></row><row><cell>DEN</cell><cell>1.12</cell><cell>23.9</cell><cell>54.9</cell><cell>17.1</cell><cell>14.8</cell><cell>36.0</cell><cell cols="3">73.4 85.9 0.97 0.31 22.8 62.4</cell><cell>88.2</cell></row><row><cell>AdaShare</cell><cell>1</cell><cell>30.2</cell><cell>62.4</cell><cell>16.6</cell><cell>12.9</cell><cell>45.0</cell><cell cols="3">71.7 83.0 0.55 0.20 64.5 90.5</cell><cell>97.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 10 :</head><label>10</label><figDesc>Tiny-Taskonomy 5-Task Learning. AdaShare outperforms the baselines on 3 out of 5 tasks using less than 1/5 parameters of most baselines. While DEN is competitive in terms of number of parameters, AdaShare outperforms DEN with an average improvement of 10.5% over all the metrics.Models# Params ? Seg ? SN ? Depth ? Keypoint ? Edge ?</figDesc><table><row><cell>Single-Task</cell><cell>5</cell><cell>0.575 0.707</cell><cell>0.022</cell><cell>0.197</cell><cell>0.212</cell></row><row><cell>Multi-Task</cell><cell>1</cell><cell>0.596 0.696</cell><cell>0.023</cell><cell>0.197</cell><cell>0.203</cell></row><row><cell>Cross-Stitch</cell><cell>5</cell><cell>0.570 0.679</cell><cell>0.022</cell><cell>0.199</cell><cell>0.217</cell></row><row><cell>Sluice</cell><cell>5</cell><cell>0.596 0.695</cell><cell>0.024</cell><cell>0.196</cell><cell>0.207</cell></row><row><cell>NDDR-CNN</cell><cell>5.41</cell><cell>0.599 0.700</cell><cell>0.023</cell><cell>0.196</cell><cell>0.203</cell></row><row><cell>MTAN</cell><cell>4.51</cell><cell>0.621 0.687</cell><cell>0.023</cell><cell>0.197</cell><cell>0.206</cell></row><row><cell>DEN</cell><cell>1.12</cell><cell>0.737 0.686</cell><cell>0.027</cell><cell>0.192</cell><cell>0.203</cell></row><row><cell>AdaShare</cell><cell>1</cell><cell>0.562 0.702</cell><cell>0.023</cell><cell>0.191</cell><cell>0.200</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 11 :</head><label>11</label><figDesc>FLOPs and Inference Time Comparison among Cross-stitch, Sluice, NDDR-CNN, MTAN, DEN and AdaShare. AdaShare consumes fewer FLOPs and shorter inference time in most scenarios.</figDesc><table><row><cell>Models</cell><cell>NYU v2</cell><cell cols="2">GFLOPs CityScapes NYU v2</cell><cell>Taskonomy</cell><cell>NYU v2</cell><cell cols="2">Inference Time (ms) CityScapes NYU v2</cell></row><row><cell></cell><cell>2-Task</cell><cell>2-Task</cell><cell>3-Task</cell><cell>5-Task</cell><cell>2-Task</cell><cell>2-Task</cell><cell>3-Task</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(a) Seg + SN</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">(b) Seg + Depth</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">(c) Seg + SN + Depth</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Forwarding</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Seg</cell><cell>SN</cell><cell></cell><cell>Depth</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Sharing Patterns of (a)</cell></row><row><cell></cell><cell></cell><cell cols="3">Seg + SN (14/16)</cell><cell cols="2">Seg (2/16)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Sharing Patterns of (b)</cell></row><row><cell></cell><cell></cell><cell cols="3">Seg + Depth (14/16)</cell><cell cols="2">Seg (2/16)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Sharing Patterns of (c)</cell></row><row><cell cols="4">Seg + SN + Depth (12/16)</cell><cell cols="2">Seg + SN (2/16)</cell><cell cols="2">Seg + Depth (2/16)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 12 :</head><label>12</label><figDesc>Ablation Studies on CityScapes 2-Task Learning. T1: Semantic Segmentation, T2: Depth Abs Rel 1.25 1.25 2 1.25 3 ? T2 ?</figDesc><table><row><cell>Prediction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>T 1 : Semantic Seg.</cell><cell></cell><cell></cell><cell cols="2">T 2 : Depth Prediction</cell><cell></cell></row><row><cell>Model</cell><cell cols="2">mIoU ? Pixel Acc ?</cell><cell>? T1 ?</cell><cell>Error ?</cell><cell>?, within ?</cell><cell></cell><cell></cell><cell>? T ?</cell></row><row><cell>Stochastic Depth</cell><cell>41.0</cell><cell>74.2</cell><cell>+0.7</cell><cell cols="2">0.016 0.37 71.0 86.0</cell><cell>92.9</cell><cell>-1.2</cell><cell>-0.3</cell></row><row><cell>Random # 1</cell><cell>40.7</cell><cell>74.6</cell><cell>+0.8</cell><cell cols="2">0.016 0.35 74.7 88.2</cell><cell>94.0</cell><cell>+1.8</cell><cell>+1.3</cell></row><row><cell>Random # 2</cell><cell>41.2</cell><cell>74.9</cell><cell>+1.4</cell><cell cols="2">0.017 0.36 74.1 88.2</cell><cell>93.7</cell><cell>-0.2</cell><cell>+0.6</cell></row><row><cell>w/o curriculum</cell><cell>40.4</cell><cell>74.8</cell><cell>-1.0</cell><cell cols="2">0.017 0.33 75.1 88.9</cell><cell>94.5</cell><cell>0.0</cell><cell>-0.5</cell></row><row><cell>w/o L sparsity</cell><cell>40.8</cell><cell>74.8</cell><cell>+0.8</cell><cell cols="2">0.016 0.34 73.8 89.2</cell><cell>94.7</cell><cell>+2.5</cell><cell>+1.7</cell></row><row><cell>w/o L sharing</cell><cell>41.5</cell><cell>74.9</cell><cell>+1.8</cell><cell cols="2">0.016 0.35 74.0 88.7</cell><cell>94.4</cell><cell>+1.8</cell><cell>+1.8</cell></row><row><cell>AdaShare-Instance</cell><cell>41.5</cell><cell>74.7</cell><cell>+1.6</cell><cell cols="2">0.016 0.33 74.4 89.5</cell><cell>94.9</cell><cell>+3.4</cell><cell>+2.5</cell></row><row><cell>AdaShare-RL</cell><cell>40.2</cell><cell>74.4</cell><cell>-0.2</cell><cell cols="2">0.018 0.36 71.7 87.4</cell><cell>93.7</cell><cell>-2.3</cell><cell>-1.2</cell></row><row><cell>AdaShare</cell><cell>41.5</cell><cell>74.9</cell><cell>+1.8</cell><cell cols="2">0.016 0.33 75.5 89.8</cell><cell>94.9</cell><cell>+3.8</cell><cell>+2.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 13 :</head><label>13</label><figDesc>Ablation Studies in NYU v2 3-Task Learning. Mean Median 11.25 ? 22.5 ? 30 ? Abs Rel 1.25 1.25 2 1.25 3</figDesc><table><row><cell></cell><cell cols="2">Semantic Seg.</cell><cell></cell><cell cols="3">Surface Normal Prediction</cell><cell></cell><cell>Depth Prediction</cell></row><row><cell>Model</cell><cell cols="2">mIoU ? Pixel Acc ?</cell><cell cols="2">Error ?</cell><cell></cell><cell>?, within ?</cell><cell>Error ?</cell><cell>?, within ?</cell></row><row><cell>Stochastic Depth</cell><cell>26.4</cell><cell>58.5</cell><cell>16.6</cell><cell>13.4</cell><cell>42.6</cell><cell cols="3">72.9 84.7 0.60 0.22 58.8 87.7</cell><cell>96.9</cell></row><row><cell>Random #1</cell><cell>26.4</cell><cell>59.7</cell><cell>16.9</cell><cell>13.9</cell><cell>41.5</cell><cell cols="3">71.6 84.2 0.61 0.23 59.0 87.4</cell><cell>96.7</cell></row><row><cell>Random #2</cell><cell>28.4</cell><cell>60.9</cell><cell>16.7</cell><cell>13.0</cell><cell>44.7</cell><cell cols="3">71.9 83.0 0.56 0.21 62.7 89.7</cell><cell>97.6</cell></row><row><cell>w/o curriculum</cell><cell>27.9</cell><cell>60.5</cell><cell>17.0</cell><cell>13.1</cell><cell>44.1</cell><cell cols="3">71.4 82.6 0.58 0.21 63.0 89.0</cell><cell>96.8</cell></row><row><cell>w/o L sparsity</cell><cell>25.8</cell><cell>57.6</cell><cell>16.9</cell><cell>14.0</cell><cell>41.6</cell><cell cols="3">70.9 83.6 0.63 0.23 58.0 86.3</cell><cell>96.5</cell></row><row><cell>w/o L sharing</cell><cell>26.6</cell><cell>59.8</cell><cell>16.5</cell><cell>12.9</cell><cell>44.8</cell><cell cols="3">72.3 83.4 0.56 0.21 64.0 89.7</cell><cell>97.4</cell></row><row><cell>AdaShare-Instance</cell><cell>27.3</cell><cell>55.0</cell><cell>17.0</cell><cell>13.8</cell><cell>41.3</cell><cell cols="3">72.1 83.4 0.56 0.21 64.0 89.7</cell><cell>90.6</cell></row><row><cell>AdaShare-RL</cell><cell>26.9</cell><cell>56.9</cell><cell>18.3</cell><cell>14.7</cell><cell>39.0</cell><cell cols="3">69.0 81.7 0.74 0.27 51.7 83.3</cell><cell>95.7</cell></row><row><cell>AdaShare</cell><cell>30.2</cell><cell>62.4</cell><cell>16.6</cell><cell>12.9</cell><cell>45.0</cell><cell cols="3">71.7 83.0 0.55 0.20 64.5 90.5</cell><cell>97.8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that it would cause the difference in the number of parameters.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is supported by DARPA Contract No. FA8750-19-C-1001, NSF and IBM. It reflects the opinions and conclusions of its authors, but not necessarily the funding agents.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep elastic networks with model selection for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanho</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhwai</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Luc</forename><surname>Bacon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06297</idno>
		<title level="m">Conditional computation in neural networks for faster models</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>L?onard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?me</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Integrated perception with recurrent multi-task neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stochastic filter groups for multi-task cnns: Learning specialist and generalist convolution kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryutaro</forename><surname>Bragman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Tanno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cardoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Journal</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junkun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.07658</idno>
		<title level="m">Xipeng Qiu, and Xuanjing Huang. Exploring shared structures and hierarchies for multiple nlp tasks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo?c</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Chapter of the Association for Computational Linguistics EACL&apos;17</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Blitznet: A real-time deep network for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Shmelkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural architecture search: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Hendrik</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nddr-cnn: Layerwise feature fusing in multi-task cnns by neural discriminative dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spottune: transfer learning through adaptive fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhui</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tajana</forename><surname>Rosing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Temporally distributed networks for fast video semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Caba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Real-time semantic segmentation with fast attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">Caba</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03815</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cross-domain image retrieval with a dual attribute-aware ranking network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junshi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rogerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Clustered multi-task learning: A convex formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Philippe</forename><surname>Vert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis R</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep cross residual learning for multi-task visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM international conference on Multimedia</title>
		<meeting>the 24th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning with whom to share in multi-task feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ubernet: Training a universal convolutional neural network for low-, mid-, and highlevel vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning task grouping and overlap in multi-task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Evolutionary architecture search for deep multitask networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliot</forename><surname>Meyerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risto</forename><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Genetic and Evolutionary Computation Conference</title>
		<meeting>the Genetic and Evolutionary Computation Conference</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Darts: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">End-to-end multi-task learning with attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fullyadaptive feature sharing in multi-task networks with applications in person attribute classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangfei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><surname>Javidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rog?rio</forename><surname>Schmidt Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Chris J Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00712</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A* sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attentive single-tasking of multiple tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevis-Kokitsi</forename><surname>Maninis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ilija Radosavovic, and Iasonas Kokkinos</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Beyond shared hierarchies: Deep multitask learning through soft layer ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliot</forename><surname>Meyerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risto</forename><surname>Miikkulainen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00108</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cross-stitch networks for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Pushmeet Kohli Nathan Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Flexible modeling of latent task structures in multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacques</forename><surname>Wainer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6486</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinxun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xide</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Hyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01249</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Aging evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><forename type="middle">Leon</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kurakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Routing networks: Adaptive selection of non-linear functions for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Riemer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.01239</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">An overview of multi-task learning in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05098</idno>
	</analytic>
	<monogr>
		<title level="m">deep neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Latent multi-task architecture learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Bingel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>S?gaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04671</idno>
		<title level="m">Razvan Pascanu, and Raia Hadsell. Progressive neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Standley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07553</idno>
		<title level="m">Which tasks should be learned together in multi-task learning? arXiv preprint</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Evolving neural networks through augmenting topologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risto</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary computation</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gjorgji</forename><surname>Strezoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Nanne Van Noord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Worring</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12117</idno>
		<title level="m">Many task learning with task routing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Hierarchical multi-scale attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10821</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Learning to learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pratt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Kluwer Academic Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02920</idno>
		<title level="m">Branched multi-task networks: Deciding what layers to share</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Convolutional networks with adaptive inference graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Residual networks behave like ensembles of relatively shallow networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Skipnet: Learning dynamic routing in convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi-Yi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Blockdrop: Dynamic inference paths in residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Liteeval: A coarse-to-fine framework for resource efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Snas: stochastic neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Multi-task learning for classification with dirichlet process priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuejun</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Krishnapuram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Taskonomy: Disentangling task transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Clustered multi-task learning via alternating structure optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
