<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">STN PLAD: A Dataset for Multi-Size Power Line Assets Detection in High-Resolution UAV Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?</forename><surname>Luiz</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buarque</forename><surname>Vieira-E-Silva</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heitor</forename><surname>De</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Castro</forename><surname>Felix</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiago</forename><surname>De</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menezes</forename><surname>Chaves</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Paulo</surname></persName>
							<email>francisco.simoes@ufrpe.br</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magalh?es</forename><surname>Sim?es</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Departamento de Computa??o</orgName>
								<orgName type="institution">Universidade Federal Rural de Pernambuco</orgName>
								<address>
									<settlement>Recife</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronica</forename><surname>Teichrieb</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Mozinho</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santos</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemir</forename><surname>Da</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cunha</forename><surname>Santiago</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Virginia</forename><surname>Ad?lia</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordeiro</forename><surname>Sgotti</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrique</forename><surname>Baptista</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duffles</forename><surname>Teixeira</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lott</forename><surname>Neto</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Sistema de Transmiss?o Nordeste -STN</orgName>
								<address>
									<settlement>Recife</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Centro de Inform?tica</orgName>
								<orgName type="laboratory">Voxar Labs</orgName>
								<orgName type="institution">Universidade Federal de Pernambuco</orgName>
								<address>
									<settlement>Recife</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">In Forma Software</orgName>
								<address>
									<settlement>Recife</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">STN PLAD: A Dataset for Multi-Size Power Line Assets Detection in High-Resolution UAV Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many power line companies are using UAVs to perform their inspection processes instead of putting their workers at risk by making them climb high voltage power line towers, for instance. A crucial task for the inspection is to detect and classify assets in the power transmission lines. However, public data related to power line assets are scarce, preventing a faster evolution of this area. This work proposes the STN Power Line Assets Dataset, containing high-resolution and real-world images of multiple high-voltage power line components. It has 2,409 annotated objects divided into five classes: transmission tower, insulator, spacer, tower plate, and Stockbridge damper, which vary in size (resolution), orientation, illumination, angulation, and background. This work also presents an evaluation with popular deep object detection methods and MS-PAD, a new pipeline for detecting power line assets in hi-res UAV images. The latter outperforms the other methods achieving 89.2% mAP, showing considerable room for improvement. The STN PLAD dataset is publicly available at https://github.com/andreluizbvs/PLAD.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Nowadays, practically all human activities depend on the constant availability of electricity. Power transmission lines, which have an essential role in this task, are constantly exposed to the depreciating action of the environment. They have components that may break, rust, loosen or even go missing. The malfunction of such equipment affects the electricity grid, causing inefficiency in the power transmission and, sometimes, blackouts. According to <ref type="bibr" target="#b0">[1]</ref>, most of the power grids today are interconnected. Thus, these blackouts can initiate others, affecting even larger regions, like a cascade effect <ref type="bibr" target="#b1">[2]</ref>. That can trigger catastrophic consequences such as shutting down hospitals, production at water supplies companies, and telecommunication services <ref type="bibr" target="#b2">[3]</ref>, which leads to significant economic losses for the energy company and, ultimately, severe social impacts <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. According to Bruch et al. <ref type="bibr" target="#b3">[4]</ref>, a power cut of only 30 minutes in the USA results in an average loss of over 15 thousand US dollars for midsize and large industrial clients and a loss of more than 90 thousand US dollars for an eight-hour interruption <ref type="bibr" target="#b0">[1]</ref>. Given this scenario, constant maintenance of the equipment is necessary, replacing defective ones before they can cause any loss. Classically, a team has to check each station personally. That implies having someone climbing the transmission tower and checking the condition of its components, which is dangerous and time-consuming <ref type="bibr" target="#b5">[6]</ref>. In other words, there is a cost to move people, and it takes time to get up at a station, apart from safety issues associated with the service. For example, Rahmani et al. <ref type="bibr" target="#b6">[7]</ref> showed that there were 119 injured workers due to accidents between 2006 and 2012 in an Iranian electricity distribution company, with seven deaths.</p><p>Recent developments in computer vision can be applied in the field of Maintenance &amp; Inspection, improving security and productivity. As an example of application in the security area, automatically detecting power line assets via UAVs eliminates much of the safety risks of manual inspection, as inspectors would not need to climb towers as often as before. Instead, much of the inspection could occur through a direct analysis of the detected components, which a human inspector could perform in a secure environment or by a fault classification method <ref type="bibr" target="#b7">[8]</ref>. In addition to the security gains, there are also financial and time gains, as the frequency of moving teams and providing the necessary equipment would decrease.</p><p>The dataset plays a major role in the training of deep learning networks, where its quality directly influences the accuracy. That is why we should pay more attention to data, as stated by <ref type="bibr" target="#b8">[9]</ref>. Furthermore, public datasets play a fundamental role in a rapidly advancing area. They allow researchers to propose ideas and perform experiments even in scenarios where they cannot get the data by themselves. Also, those datasets usually serve as a benchmark for a specific task, providing a fair comparison among new techniques.</p><p>It is evident how quickly certain areas of computer vision evolved after the introduction of datasets such as CIFAR10 <ref type="bibr" target="#b9">[10]</ref>, ImageNet <ref type="bibr" target="#b10">[11]</ref> and MNIST <ref type="bibr" target="#b11">[12]</ref>. In that sense, public datasets on power line assets for object detection are extremely scarce, and the existing ones are quite limited, typically only supporting one asset type or two, at most <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b14">[15]</ref>. That happens because most of the works in the area are privately funded by companies that want to maintain a competitive advantage by not making their datasets available.</p><p>As the main contribution, this paper introduces a new realworld, high-resolution, and multi-category dataset for multisize power line assets recognition, the Power Line Assets Dataset, or STN PLAD. It serves as publicly available development data and benchmark for the computer vision community working on automatic power line inspection. In addition, experiments with state-of-the-art techniques show the dataset's strengths and limitations. These experiments used two popular general-purpose object detectors, namely SSD and Faster R-CNN. Based on its analysis, a variation of the training pipeline, which we call MS-PAD, is proposed to improve the overall object detection performance in the STN PLAD dataset. This paper is organized as follows. The prior works are presented in section II. The properties of the new STN Power Line Assets Dataset are described in section III. section IV shows the methods used to evaluate the proposed dataset. Next, comparative and performance results of techniques applied in STN PLAD are presented in section V, followed by a discussion about what was seen in the tests in section VI and, lastly, the final remarks in section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>This section is divided into two parts. First, the public datasets closely related to power lines are presented, along with their characteristics and limitations. Then, the existing methods that attempt to detect multi-size power line assets in high-resolution images are shown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Public datasets related to power lines</head><p>A common problem found in the literature when using Deep Learning to detect power line objects is finding data. There are not enough publicly available datasets to feed detectors based on deep learning methods, or they do not cover enough power line components <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Many similar works use private datasets, generally provided by the companies or government agencies financing the projects, which tend not to publish them <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b19">[20]</ref>. Nevertheless, in the literature search presented by Liu et al. <ref type="bibr" target="#b15">[16]</ref> and in the work of Abdelfattah et al. <ref type="bibr" target="#b14">[15]</ref>, a few publicly available datasets were found but with many limitations. <ref type="table" target="#tab_0">Table I</ref> summarizes the main public image datasets related to power lines assets for the object detection task. The last line shows the proposed dataset for comparison purposes.</p><p>As evidenced in <ref type="table" target="#tab_0">Table I</ref>, there is a minimal amount of public datasets related to power line asset detection. They target distinct tasks that can be detection, classification, or segmentation. For instance, the one in <ref type="bibr" target="#b20">[21]</ref> is specifically related to conductor wires in low-resolution images for binary classification. Zhang et al. <ref type="bibr" target="#b21">[22]</ref> propose two datasets of binarized masks of conductor wires of power lines in urban and mountain scenarios, respectively. Abdelfattah et al. <ref type="bibr" target="#b14">[15]</ref> propose a dataset containing pixel-wise annotation (a.k.a. instance segmentation) of both transmission towers and power lines. However, the main competing datasets are CPLID <ref type="bibr" target="#b12">[13]</ref>, and the one from Tomaszewski et al. <ref type="bibr" target="#b13">[14]</ref> as they are the only ones that use bounding box annotations.</p><p>CPLID <ref type="bibr" target="#b12">[13]</ref> is a dataset related only to a specific type of insulator with a specific shape and size. Although it also has annotations of defects in some of those insulators, it lacks a diversity of data since there is only one type of power line asset. The defective samples are also limited because all of them are from data augmentation, i.e., a single faulty insulator was cropped from an image and then pasted into a limited set of backgrounds, like seen in <ref type="figure">Figure 2</ref>(a). On the other hand, STN PLAD provides asset variability in diverse scenarios.</p><p>The dataset in Tomaszewski et al. <ref type="bibr" target="#b13">[14]</ref> has even more limitations. They mainly target data quantity (they reported 2630 images) rather than data variability, as they video recorded a ceramic long rod insulator hanging on an apparatus built by them and then extracted some of the frames using a stationary camera. These images only contain one of nine different backgrounds that do not correspond to real-world power line scenarios. An image from one of these nine variations is shown in <ref type="figure">Figure 2</ref>(b). From the perspective of deep learning techniques, this dataset has a very limited variability of scenes. In summary, although this dataset has a reasonable amount of data, the images taken in the same scenario have a high degree of similarity. Thus, the dataset ends up not being independent and identically distributed (IID) <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, an essential dataset property. In practical terms, it is not feasible to use it in most techniques based on deep learning since it poses little to no challenge to these techniques. In STN PLAD, the images are collected by a drone in the field, providing several real-world scenarios with multiple objects (size, appearance, position, orientation, self-occlusion, background).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Detection of power line assets in high-resolution images</head><p>A few works address the issue of detecting objects related to power lines in high-resolution images. The first one, Zhang et al. <ref type="bibr" target="#b18">[19]</ref>, is a study on object detection in high-resolution images captured through Unmanned Aerial Vehicles (UAVs), using deep learning techniques. In their work, the authors propose the MOHR dataset. This private dataset has over ten  <ref type="bibr" target="#b12">[13]</ref> with an asset inserted in a background, (b) <ref type="bibr" target="#b13">[14]</ref> with an asset in a non-real-world scenario, and (c) our dataset (STN PLAD) with multi-objects in real-world scenes (zoomed in image).</p><p>thousand high-resolution UAV images with five classes: car, truck, building, collapse, and flood damage. The UAV altitudes are high, ranging from 200 to 400 meters, making the objects look quite small. The authors apply six general-purpose object detectors, SSD <ref type="bibr" target="#b24">[25]</ref> and Faster R-CNN <ref type="bibr" target="#b25">[26]</ref> included, to the MOHR dataset. The results suggest that detecting small object instances in high-resolution UAV images remains challenging since they perform poorly. The best mean Average Precision (mAP) achieved was 43.94%, yielded by RFCN-DF <ref type="bibr" target="#b26">[27]</ref>. Those results reinforce the relevance of this task. The works of Kong et al. <ref type="bibr" target="#b27">[28]</ref> and Zhu et al. <ref type="bibr" target="#b28">[29]</ref> share some of their authors and contents, indicating that one is an incremental improvement over the other. The former proposes a technique to detect small objects in high-resolution images. The technique, based on Faster R-CNN, is tested on a private dataset of 3700 high-resolution images. However, the proposed approach here is limited and prone to issues since only small objects inside the context of a large one are attainable. For instance, dampers are usually small independent objects far from larger ones. Another issue is the low Average Precision (AP) of some classes, such as the tower plate (73.2%), which the authors justify by saying they are too small.</p><p>Finally, Zhu et al. <ref type="bibr" target="#b28">[29]</ref> attempts to improve the efficiency of their previous work by merging the two stages in order to share early convolutional layers. They also use a private dataset with high-resolution images with six classes: electric tower, vibration damper, spacer, insulator, bird's nest, and tower plate. Despite the efficiency of this method, the same issues and limitations that existed before regarding object detection are maintained. The only difference is that the objects are not so small as their previous work, which is one of the main factors that positively impact the mAP.</p><p>These last two works, <ref type="bibr" target="#b27">[28]</ref> and <ref type="bibr" target="#b28">[29]</ref>, are not reproducible since the datasets are private, and they are not open-source.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. STN PLAD: DATASET DESCRIPTION</head><p>The images were captured using a DJI Phantom 4 Pro 1 , and <ref type="figure" target="#fig_0">Figure 1</ref> shows some image clippings. A set of policies for data collection was proposed to ensure data variability and consistency. First, the drone was handled by certified drone pilots, who were instructed to capture the images, always maintaining a similar distance to the transmission tower in a wide shot due to the high-resolution nature of the camera. In addition, the drone's viewing angles were varied to ensure better learning by models based on neural networks and diverse daytime, weather, angulation, and illumination conditions. Finally, several transmission towers were captured to obtain background and component variation. This data capture protocol provides images with a wide range and number of power line assets in each one of them, with a mean of 18.1 instances per captured image as can be seen in <ref type="table" target="#tab_0">Table I</ref> .</p><p>The equipped camera is a DJI FC6310 and it can take pictures with a resolution of 5472?3078 (3:2) or 5472?3648 (16:9). Both aspect ratios were used during data collection.For annotating the 2409 objects in all 133 captured images, the LabelImg tool was used <ref type="bibr" target="#b29">[30]</ref>. Two annotators were responsible for carefully surrounding each object with a bounding box. Each person took, on average, 10 minutes to annotate one image. Each image is assigned to only one annotator to perform all its annotations. To maintain the annotation consistency between different annotators, they labeled each assigned image with their highest possible scrutiny and were in touch during the entire annotation process.</p><p>The total amount of images in STN PLAD may appear small but, considering the employed data collection protocol, the camera's resolution, and, more importantly, the total amount of object instances, it can be seen that it has a reasonable amount of data. Images from STN PLAD have considerably more information than regular images from common datasets, such as ImageNet <ref type="bibr" target="#b10">[11]</ref> and MSCOCO <ref type="bibr" target="#b30">[31]</ref>. On average, the STN PLAD has more than 18 objects per image with an average area of at least 2.89?10 3 pixels. This 18 objects/image density is way above the related datasets, as seen in <ref type="table" target="#tab_0">Table I</ref>. Finally, the STN Power Line Assets Dataset is publicly available in https://github.com/andreluizbvs/PLAD 2 .  IV. METHODS This section describes two techniques that are often used to validate object detection datasets <ref type="bibr" target="#b12">[13]</ref>, SSD and Faster R-CNN. Their performance on STN PLAD is presented in the next section and discussed later. The observed limitations in dealing with the proposed dataset inspired the creation of a pipeline called MS-PAD, which is also detailed here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Single Shot MultiBox Detector (SSD)</head><p>In the context of power line inspection, SSD is one of the suggested techniques of two recent reviews <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b15">[16]</ref> to target the problem of detecting assets on power transmission towers. Both reviews have the same context as this work, focusing on inspecting power line assets from UAV images. Moreover, they analyze Deep Learning techniques applied to solve problems in the area. Some of the mentioned problems are assets detection, assets segmentation, assets fault identification.</p><p>The parameters of the SSD used are the same as in the original work by Liu et al. <ref type="bibr" target="#b24">[25]</ref>, such as the backbone, VGG16, and all the parameters and dimensions for the convolutional layers. In the original work, two different input layers were proposed, 300?300 and 512?512. In our experiment, the latter was used since it achieved better accuracy than the former, according to the original results. Also, the images used for this experiment have a higher resolution. This high-resolution implies that the larger the size of the input layer, the less the resizing effect will affect the input image quality. Finally, weights pre-trained with the COCO Dataset <ref type="bibr" target="#b30">[31]</ref> were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Faster R-CNN</head><p>The objective of including the Faster R-CNN <ref type="bibr" target="#b25">[26]</ref> in the tests was to use a recent technique of object detection to obtain results close to the current state of the art. Faster R-CNNbased networks are also suggested to detect and inspect power transmission towers according to the same reviews mentioned in subsection IV-A. They are also well consolidated, have performed well in object detection competitions, and are used by similar works <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b15">[16]</ref>.</p><p>The network used for this experiment was the Feature Pyramid Network (FPN) Faster R-CNN <ref type="bibr" target="#b31">[32]</ref>. FPN aims to improve the detection of small objects, as it uses multi-scale feature maps and higher resolution layers to build new semantically rich layers. Thus, information from the initial layers is used. These layers are traditionally less condensed, but even so, they already have a high semantic level. ResNet-101 was used as a backbone, which had the best detection result in its publication <ref type="bibr" target="#b31">[32]</ref>. Also, the input resolution was chosen in order to decrease the image resizing impact. The input image is resized to 2736?1824, representing a downscaling factor of 4 when compared to the original size, which is much less than the downscaling factor of approximately 19 used in the SSD experiment. All other parameters were kept as the original.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. MS-PAD</head><p>After observing the results related to the SSD and Faster R-CNN methods, it was noticed that a simple pipeline modification could enhance the overall performance of power line assets detection in STN PLAD. This approach takes advantage of the images' high resolution, where information is lost after resizing. In the Multi-Size Power line Asset Detection (MS-PAD) workflow, represented in <ref type="figure">Figure 4</ref>, two independent networks are trained separately. The SSD was chosen because it performed better, as it will be shown in the next section.</p><p>The first of these two networks uses the original pipeline that resizes its input, but this time trained without the Stockbridge damper class. The damper asset was excluded because it has a much smaller size, being harder to identify after image resizing. This strategy can be applied to other assets not contemplated in this work that are small. An important note is that, although the tower plate is also small, it still had enough features after resizing that made it highly recognizable.</p><p>The second network is responsible for detecting small objects, in this case, the Stockbridge damper class. It goes through a different process, where the initial image is split following a grid. The image is divided into 16 smaller ones with a fixed resolution of 1368?769 or 1368?912, depending on the original image, which can be 5472 ? 3078 (3:2) or 5472 ? 3648 (16:9). This 4 ? 4 division is constant since the drone pilots followed a data collection protocol, in which the drone had to stay at similar distances from the transmission towers, as described in section III.</p><p>In summary, only the Stockbridge damper class is submitted to the second step of MS-PAD, which divides the highresolution image in a 4 ? 4 grid. This choice is based on the average area of the classes of <ref type="table" target="#tab_0">Table II</ref> and the AP in the original image-resizing approach. The Stockbridge damper has the lowest average area and was not well detected using the previously mentioned methods, indicating the need to receive extra attention compared to other classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head><p>This section presents the two conducted experiments and their results. The first one is responsible for comparing the performance of the two mentioned object detectors in the <ref type="figure">Fig. 4</ref>. MS-PAD pipeline. The input image is resized for the first network. Meanwhile, the image is divided into a grid, resulting in the second network's inputs that generate different bounding boxes.</p><p>proposed dataset. The second one demonstrates another way to deal with the input data, using MS-PAD, which was detailed in subsection IV-C. In all experiments, the standard metric of Average Precision (AP) is used to evaluate object detection performance on STN PLAD. In order to validate and obtain a greater degree of confidence in the results of the proposed MS-PAD, the Monte Carlo cross-validation method <ref type="bibr" target="#b32">[33]</ref> was chosen and implemented in its experiments presented in subsection V-B. This method creates k random splits of train and test sets of the whole dataset. Then, the model is trained and tested for each k split, and the final result is the average. In the end, this section shows which object detector and which pipeline present the best results in the described scenario according to the considered metric.</p><p>For the experiments, STN PLAD was split in a standard 80/20 proportion for the training and test sets, respectively. Also, to consider that an object was correctly detected, the Intersection over Union (IoU) between the ground truth and the predicted bounding box had to be equal to or larger than 0.5. Also, data augmentation is already an embedded stage in both implementations of the techniques. Finally, the experiments were performed on a desktop running the Ubuntu 18.04 Operating System, powered by an Nvidia RTX 2080 Ti GPU (11 GB of VRAM) and an Intel Core i7 -4790K CPU @ 4.00 GHz with 32 GB of available RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. SSD and Faster R-CNN results</head><p>For this test, both detectors were trained once and for the same period, about two days. The mAP results of using the MS-PAD approach for SSD and Faster R-CNN were 90.2% and 88.6%, respectively, showing that SSD has a slight advantage over Faster R-CNN. These methods were also applied to the two main dataset competitors of the proposed STN PLAD. In CPLID <ref type="bibr" target="#b12">[13]</ref>, SSD and Faster R-CNN achieved 98.17% and 98.31% mAP, respectively. Regarding Tomaszewski et al. <ref type="bibr" target="#b13">[14]</ref>, both detectors reached 100% mAP. <ref type="figure" target="#fig_2">Figure 5</ref> and <ref type="figure" target="#fig_3">Figure 6</ref> show the visual results regarding the detected objects by the SSD and the Faster R-CNN methods, respectively, using the original approach, which only resizes the input image. In the images, the bounding boxes' colors are connected with the dataset classes: blue is for the Insulators; yellow is for the Spacers; green is for the Stockbridge dampers; red is for the Tower plate; white is for the Transmission tower. It is possible to see in both figures how  small the Stockbridge dampers are related to other objects. These images also illustrates failure cases, like the middle insulator in <ref type="figure" target="#fig_2">Figure 5</ref> and the transmission tower in <ref type="figure" target="#fig_3">Figure 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. MS-PAD results</head><p>For this experiment, k = 5, so five splits with randomly selected samples were used, in which each split is used twice: one time for the original image-resizing approach and another time for MS-PAD. The total amount of iterations for each training session is fixed at 20,000. The obtained results regarding AP for each split are shown in <ref type="table" target="#tab_0">Table III</ref>. In addition, <ref type="table" target="#tab_0">Table IV shows the average results from Table III</ref> reached by each approach side-by-side, in a direct comparison. <ref type="figure" target="#fig_4">Figure 7</ref> and <ref type="figure" target="#fig_5">Figure 8</ref> present the qualitative performance of MS-PAD for big and small objects, respectively. The color codes previously mentioned are maintained for these images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DISCUSSION</head><p>This section details and discusses all results presented in section V, also giving insights into the usage of the MS-PAD approach in the proposed STN Power Line Asset Dataset.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. STN PLAD strengths &amp; limitations</head><p>The proposed STN PLAD is the first public power line assets dataset with multiple objects in real-world scenarios. It contains five classes of entirely different objects with multiple instances each, in varied real backgrounds. The data collection protocol allows for a balance in data quantity and variability since the captured images vary in illumination, backgrounds, and weather conditions. Also, the drone position is not fixed in order to obtain objects data from different perspectives. Another STN PLAD challenging characteristic is that there are many objects per image (18.1, on average) compared to the related public datasets, which commonly have a small instance per image rate (1 and 1.9, on average). Thanks to this process, STN PLAD poses a reasonable challenge to recent deep learning techniques, as observed in the section V, in which the best of the tested approaches achieved an 89.2% mAP, leaving considerable room for improvement.</p><p>Although the proposed STN PLAD provides new grounds in the power line area and stimulates the development of power line asset detection methods, it still has limitations. The main one is related to its total amount of images. That prevents some data-hungry object detectors from performing successfully since they would require a more extensive dataset. Another disadvantage is that the images were only collected from one private transmission line. Even though different transmission lines tend to be similar, it would be better to have images of several transmission lines in other places to reduce the bias of background, environment, and electrical assets appearance. Finally, the images belong to a power line in Brazil, which may not apply to other countries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. SSD and Faster R-CNN comparison in STN PLAD</head><p>This discussion is related to the experiment in subsection V-A. According to the proposed methodology, when performing this experiment, it was expected that the results related to the Faster R-CNN would surpass the results from the SSD network considering the applied metrics. However, it can be observed in the results reported in subsection V-A that it did not occur. This result was obtained due to the limitations of the used data. Deep learning techniques benefit from the use of large amounts of data. According to Ng <ref type="bibr" target="#b33">[34]</ref>  <ref type="bibr" target="#b34">[35]</ref> when the number of data limits a deep learning technique, shallower techniques can obtain comparable or even better results than deeper techniques. The used Faster R-CNN is much deeper and has more trainable parameters than the used SSD network. Therefore, for a limited amount of data, the learning of the used Faster R-CNN is limited.</p><p>The other results in this comparison were related to the SSD and Faster R-CNN performance in the competing datasets. In <ref type="bibr" target="#b13">[14]</ref>, both methods achieved 100% mAP, as expected due to the reasons presented in subsection II-A. In <ref type="bibr" target="#b12">[13]</ref>, the original SSD and Faster R-CNN obtained performances above 98% mAP. The high mAP values obtained by both object detectors in both competing datasets showed how well-resolved their challenges already are.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. MS-PAD in STN PLAD</head><p>It is possible to see in <ref type="table" target="#tab_0">Table III</ref> the mAP values reached by MS-PAD are higher than all mAP values of the Original approach in at least ten percentage points (k = 5) and at most 17.5 percentage points (k = 2). <ref type="table" target="#tab_0">Table IV</ref> shows a direct comparison, in which the values for each approach are an average of the five splits showed in <ref type="table" target="#tab_0">Table III</ref>. The best values for each class AP and mAP are in bold. MS-PAD yields the best AP result in three out of the five total classes, and there is a gap of 13.7 percentage points between mAPs. That gap is primarily due to the Stockbridge damper AP improvement, which grew 62.4 percentage points using MS-PAD.</p><p>It is noteworthy that the performance of large objects changes when comparing the Original and the MS-PAD. That may happen because during the MS-PAD resize branch training, one less class is considered (Stockbridge damper). That directly influences how the network learns since there is a different amount of objects and classes, directly impacting the final performance of large assets. Also, it is important to note that there is no guarantee that the performance impact will be positive or negative when training with one less class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS</head><p>This work proposes a new public real-world high-resolution power line asset dataset with multiple assets categories, called STN Power Line Assets Dataset (PLAD). Its images were captured by an Unmanned Aerial Vehicle (UAV) following a data collection protocol to ensure data variability in order to benefit deep learning models. STN PLAD contains 2409 annotated objects across 133 images divided into five classes with different shapes and sizes. It has the biggest amount of power line asset types among all public power line assets datasets, with the highest density of objects per image between them as well. The latter is possible due to its images having far above average resolutions, 5472?3078 and 5472?3648, more precisely. After evaluating STN PLAD in recent generalpurpose object detectors, a different pipeline called MS-PAD is proposed. This pipeline contains a simple modification that allows for an mAP improvement from 75.5% to 89.2%. STN PLAD is publicly available to mitigate the lack of data in the power line inspection area and provide a new challenge to the computer vision community in order to stimulate the proposition of new asset detection methods for power lines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>A few image clippings from the proposed dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Examples of all five classes of power line assets in STN PLAD. Each column shows instances from one class. From left to right: Transmission tower, Insulator, Spacer, Tower plate, and Stockbridge damper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative detection results of the SSD technique using the original image-resizing approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Qualitative detection results of the Faster R-CNN technique using the original image-resizing approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Qualitative detection results of the MS-PAD pipeline for the big object classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Qualitative detection results of the MS-PAD pipeline for the small object classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I MAIN</head><label>I</label><figDesc>PUBLIC IMAGE DATASETS RELATED TO POWER LINES ASSET DETECTION.</figDesc><table><row><cell>Dataset</cell><cell cols="2">#Assets Instances/image (average)</cell><cell>Image size</cell><cell cols="3">Instances Images Background variation</cell></row><row><cell>CPLID [13]</cell><cell>1</cell><cell>1.9</cell><cell>1152?864</cell><cell>1569</cell><cell>848</cell><cell>Limited</cell></row><row><cell>Tomaszewski et al. [14]</cell><cell>1</cell><cell>1</cell><cell>5616?3744</cell><cell>2630 a</cell><cell>2630 b</cell><cell>Very limited</cell></row><row><cell>STN PLAD</cell><cell>5</cell><cell>18.1</cell><cell>5472?3078 or 5472?3648</cell><cell>2409</cell><cell>133</cell><cell>Diverse</cell></row></table><note>a All the instances correspond to the same object.b Images from this dataset are extremely similar and captured from just nine different points of view. Fig. 2. Sample images from public datasets as compared to our dataset STN PLAD in which (a) CPLID</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II STN</head><label>II</label><figDesc>PLAD STATISTICS.</figDesc><table><row><cell>Class name</cell><cell>Label</cell><cell cols="4">Instances Instances per image Average Area (px) Standard Deviation (px)</cell></row><row><cell>Transmission tower</cell><cell>tower</cell><cell>253</cell><cell>1.9</cell><cell>2.61 ? 10 6</cell><cell>3.12 ? 10 6</cell></row><row><cell>Insulator</cell><cell>insulator</cell><cell>312</cell><cell>2.3</cell><cell>8.84 ? 10 4</cell><cell>8.55 ? 10 4</cell></row><row><cell>Spacer</cell><cell>spacer</cell><cell>253</cell><cell>1.9</cell><cell>2.82 ? 10 4</cell><cell>2.41 ? 10 4</cell></row><row><cell>Tower plate</cell><cell>plate</cell><cell>86</cell><cell>0.6</cell><cell>9.42 ? 10 3</cell><cell>1.11 ? 10 4</cell></row><row><cell cols="2">Stockbridge damper damper</cell><cell>1505</cell><cell>11.3</cell><cell>2.89 ? 10 3</cell><cell>5.78 ? 10 3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III COMPARISON</head><label>III</label><figDesc>OF DETECTION RESULTS OF THE ORIGINAL IMAGE-RESIZING APPROACH (ORIGINAL) AND THE MS-PAD PIPELINE (OURS) OF EACH MONTE CARLO CROSS-VALIDATION SPLIT (k) RELATIVE TO THE AVERAGE PRECISION (AP). THE BEST MAP RESULTS ARE IN BOLD.TABLE IV DETECTION AVERAGE RESULTS FROM TABLE III OF BOTH APPROACHES, SIDE-BY-SIDE. THE BEST RESULTS FOR EACH CLASS AND THE MAP IS IN BOLD.</figDesc><table><row><cell></cell><cell>k = 1</cell><cell></cell><cell>k = 2</cell><cell></cell><cell>k = 3</cell><cell></cell><cell>k = 4</cell><cell></cell><cell>k = 5</cell><cell></cell></row><row><cell></cell><cell cols="2">Original Ours</cell><cell cols="2">Original Ours</cell><cell cols="2">Original Ours</cell><cell cols="2">Original Ours</cell><cell cols="2">Original Ours</cell></row><row><cell>Transmission tower</cell><cell>0.885</cell><cell>0.905</cell><cell>0.883</cell><cell>0.901</cell><cell>0.875</cell><cell>0.874</cell><cell>0.920</cell><cell>0.883</cell><cell>0.945</cell><cell>0.938</cell></row><row><cell>Insulator</cell><cell>0.825</cell><cell>0.938</cell><cell>0.924</cell><cell>0.866</cell><cell>0.931</cell><cell>0.893</cell><cell>0.839</cell><cell>0.884</cell><cell>0.874</cell><cell>0.889</cell></row><row><cell>Spacer</cell><cell>0.917</cell><cell>0.810</cell><cell>0.789</cell><cell>0.850</cell><cell>0.914</cell><cell>0.910</cell><cell>0.863</cell><cell>0.805</cell><cell>0.853</cell><cell>0.905</cell></row><row><cell>Tower plate</cell><cell>0.932</cell><cell>0.994</cell><cell>0.830</cell><cell>1.00</cell><cell>0.941</cell><cell>0.990</cell><cell>0.984</cell><cell>0.997</cell><cell>0.995</cell><cell>0.876</cell></row><row><cell>Stockbridge damper</cell><cell>0.189</cell><cell>0.829</cell><cell>0.201</cell><cell>0.882</cell><cell>0.189</cell><cell>0.870</cell><cell>0.264</cell><cell>0.824</cell><cell>0.227</cell><cell>0.787</cell></row><row><cell>mAP</cell><cell>0.750</cell><cell>0.895</cell><cell>0.725</cell><cell>0.900</cell><cell>0.770</cell><cell>0.907</cell><cell>0.774</cell><cell>0.879</cell><cell>0.779</cell><cell>0.879</cell></row><row><cell></cell><cell>Original</cell><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Transmission tower</cell><cell>0.902</cell><cell>0.900</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Insulator</cell><cell>0.879</cell><cell>0.894</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Spacer</cell><cell>0.867</cell><cell>0.856</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Tower plate</cell><cell>0.936</cell><cell>0.971</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Stockbridge damper</cell><cell>0.214</cell><cell>0.838</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mAP</cell><cell>0.755</cell><cell>0.892</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.dji.com/phantom-4-pro<ref type="bibr" target="#b1">2</ref> In case the article is accepted, the dataset will be posted on a web page with a structured presentation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors acknowledge the financial support of STN -Sistema de Transmiss?o Nordeste S.A. through the ANEEL R&amp;D Program for the development of development of the research project entitled: "PD-04825-0006/2019: Inspe??o com Drones por Meio do Acoplamento Eletrost?tico para Carregamento de Baterias em Voo e Uso de Aprendizagem de M?quina para Classifica??o Autom?tica de Defeitos".</p><p>This research was funded in part by the Coordena??o de Aperfei?oamento de Pessoal de N?vel Superior -Brasil (CAPES) -Finance Code 001 and by the Conselho Nacional de Desenvolvimento Cient?fico e Tecnol?gico (CNPq).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic autonomous vision-based power line inspection: A review of current status and the potential role of deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roverso</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0142061517324444" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Electrical Power &amp; Energy Systems</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="107" to="120" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">High level event ontology for multiarea power system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Khaparde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Smart Grid</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Risk analysis and management in power outage and restoration: A literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Castillo</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0378779613002435" />
	</analytic>
	<monogr>
		<title level="j">Electric Power Systems Research</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="9" to="15" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Power blackout risks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>M?nch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aichinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weymann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cro Forum</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A state-failure-network method to identify critical components in power systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electric Power Systems Research</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="page">106192</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Inspection and Monitoring Technologies of Transmission Lines with Remote Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Descriptive study of occupational accidents and their causes among electricity distribution company workers at an eight-year period in iran</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khadem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Madreseh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-A</forename><surname>Aghaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karchani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Safety and health at work</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="160" to="165" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A fault diagnosis system based on case decision technology for uav inspection of power lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IOP Conference Series: Earth and Environmental Science</title>
		<imprint>
			<publisher>IOP Publishing</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">632</biblScope>
			<biblScope unit="page">42077</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">everyone wants to do the model work, not the data work&quot;: Data cascades in high-stakes ai</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sambasivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kapania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Highfill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Akrong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Aroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2021 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Detection of power line insulator defects using aerial images analyzed with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics: Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The collection of images of an insulator taken outdoors in varying lighting conditions with additional laser spots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomaszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ruszczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Michalski</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S2352340918302701" />
	</analytic>
	<monogr>
		<title level="j">Data in Brief</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="765" to="768" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ttpla: An aerial-image dataset for detection and segmentation of transmission towers and power lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Abdelfattah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Review of data analysis in vision inspection of power lines with an in-depth discussion of deep learning technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Intelligent fault detection of high voltage line based on the faster r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sui</surname></persName>
		</author>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0263224119300831" />
	</analytic>
	<monogr>
		<title level="j">Measurement</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="379" to="385" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Insulator self-shattering detection: a deep convolutional neural network approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="10" to="097" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning-based object detection in high resolution uav images: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 17th International Conference on Industrial Informatics (INDIN)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Robust powerline equipment inspection system based on a convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">A</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-J</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">3837</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Powerline image dataset (infrared-ir and visible light-vl)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">?</forename><surname>Emre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">?</forename><surname>Nezih</surname></persName>
		</author>
		<ptr target="https://data.mendeley.com/datasets/n6wrv4ry6v/7" />
	</analytic>
	<monogr>
		<title level="j">Mendeley Data</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Detecting power lines in uav images with convolutional features and structured constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1342</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<meeting>COMPSTAT&apos;2010</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Making gradient descent optimal for strongly convex stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rakhlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Coference on International Conference on Machine Learning</title>
		<meeting>the 29th International Coference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1571" to="1578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>B. Leibe, J. Matas, N. Sebe, and M. Welling</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Context semantics for small target detection in large-field images with two cascaded faster r-CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://doi.org/10.1088%2F1742-6596%2F1069%2F1%2F012138" />
	</analytic>
	<monogr>
		<title level="j">Journal of Physics: Conference Series</title>
		<imprint>
			<biblScope unit="volume">1069</biblScope>
			<biblScope unit="page">12138</biblScope>
			<date type="published" when="2018-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-size object detection assisting fault diagnosis of power systems based on improved cascaded faster R-CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1117/12.2503064</idno>
		<ptr target="https://doi.org/10.1117/12.2503064" />
	</analytic>
	<monogr>
		<title level="m">Tenth International Conference on Digital Image Processing</title>
		<editor>X. Jiang and J.-N. Hwang</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10806</biblScope>
			<biblScope unit="page" from="342" to="351" />
		</imprint>
	</monogr>
	<note>ICDIP 2018</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Labelimg</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tzutalin</surname></persName>
		</author>
		<ptr target="https://github.com/tzutalin/labelImg" />
	</analytic>
	<monogr>
		<title level="m">Git code</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Fundamentals of data mining in genomics and proteomics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dubitzky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Granzow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Berrar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Machine learning yearning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="http://www.mlyearning.org/" />
		<imprint>
			<date type="published" when="2017" />
			<publisher>Stanford Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Canadian association of radiologists white paper on artificial intelligence in radiology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cadrin-Ch?nevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Guest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barfett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chepelev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cairns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Cicero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canadian Association of Radiologists Journal</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="120" to="135" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
