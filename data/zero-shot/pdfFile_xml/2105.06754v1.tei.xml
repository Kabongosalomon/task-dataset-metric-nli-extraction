<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Group Activities from Skeletons without Individual Action Labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Zappardino</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Florence</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiberio</forename><surname>Uricchio</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Florence</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Seidenari</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Florence</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Del Bimbo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Florence</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Group Activities from Skeletons without Individual Action Labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To understand human behavior we must not just recognize individual actions but model possibly complex group activity and interactions. Hierarchical models obtain the best results in group activity recognition but require fine grained individual action annotations at the actor level. In this paper we show that using only skeletal data we can train a state-ofthe art end-to-end system using only group activity labels at the sequence level. Our experiments show that models trained without individual action supervision perform poorly. On the other hand we show that pseudo-labels can be computed from any pre-trained feature extractor with comparable final performance. Finally our carefully designed lean pose only architecture shows highly competitive results versus more complex multimodal approaches even in the self-supervised variant.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Human behavior understanding can hardly be imagined as a task which requires to explain each individual actions in isolation. Human behavior is for the most part induced by social interactions. For a machine to understand the meaning of multiple humans interacting with each other, so called social behavior or group activity, multiple level of reasoning must be enacted. A naive approach could feed the whole frame to a deep convolutional neural network, but we know, that especially when domain shift is present that a large amount of data is required to learn a good hierarchy of features automatically. Recently hierarchical approaches have emerged. With such methodologies a model of the group behavior is built in a bottom-up fashion starting from the detection and tracking of all actors, following with the understanding of their individual behavior and finally building collective models of the whole action.</p><p>As we know a fully supervised system is usually the best bet to obtain high accuracy. Unfortunately such systems must rely on a lot of hand labelling: each person action must be annotated at a not to low frequency, allowing a tracker to propagate such label over time. Considering this issue only few fully annotated datasets are available, limiting the development of group understanding computer vision algorithms. To address this issue in this work we propose a novel approach for group activity recognition based on the concept of pseudo-labels. Loosely inspired by the work of Caron et al. <ref type="bibr" target="#b0">[1]</ref> we propose to replace costly single action labels with pseudo labels obtained via a simple clustering procedure which can be derived at very little cost. Interestingly, we show that such process is enough to provide such mid-level supervision thus enabling group activity recognition. Ground truth labels can therefore limited to the whole activity sequence with order of magnitude of time spared in the annotation phase.</p><p>Recently, the issue of privacy in A.I. applications has been raised especially in the EU, which enforces extremely strict policies regarding acquisition and protection of user personal information and data. Deploying cameras in public or private places to monitor user behavior has the major drawback of requiring the acquisition and possibly the storage and streaming of people images. While reliance on cryptography may offer a solution using highly anonymized human representation such as the skeleton offers many benefits. Human poses can be acquired in real-time with edge computing devices exploiting cloud computing facilities for the more complex task of action recognition. Moreover, dataset acquisition is made easier, not requiring the abidance to privacy policies if only the substantially anonymous 3D skeletal data is stored.</p><p>In this work we propose to relevant contribution to the field of activity recognition:</p><p>? We propose a novel semi-supervised approach allowing to train group activity recognition methods without fine grained ground truth annotation. ? We show how group activity can be efficiently performed using only skeletal representations which have a lower computational burden and have interesting privacy preserving properties.</p><p>II. RELATED WORKS The initial methods for group activity recognition used handcrafted features which were extracted for each actor and The proposed architecture employs three structurally identical branches with independent weights. We use openpose estimated skeletons to create three different representations. A concatenation of actor features is fed to a rather shallow network with two convolutional layers. Finally, single actor features are fed to the individual action classification head and single actor features are pooled to compute a group representation which is fed to the group classification head. combined using probabilistic graphical models <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Then, after the emergence of deep learning and the release of the Volleyball <ref type="bibr" target="#b8">[9]</ref> dataset, there was a renewed interest which remarkably increased performance. Most approaches use RNN-type networks which were introduced by Ibrahim et al. <ref type="bibr" target="#b8">[9]</ref> for the task of group activity recognition. They are trained to understand the action dynamics of individual actors and then predict group activity by taking their aggregation. A combination of RNN with graphical models were proposed in <ref type="bibr" target="#b9">[10]</ref>. A two-level hierarchy of LSTMs were used in <ref type="bibr" target="#b10">[11]</ref> to simultaneously minimize the loss of predictions and maximize the confidence. Bagautdinov et al. <ref type="bibr" target="#b11">[12]</ref> predicted every action of each actor and the group activity with an RNN which is also used to maintain temporal consistency of detected boxes. Intra-group, inter-group interactions and single person dynamics were considered in <ref type="bibr" target="#b12">[13]</ref> and modeled with an LSTM. Each person is also modeled in a relational framework in <ref type="bibr" target="#b13">[14]</ref>. Other works considered different approaches, Azar et al. <ref type="bibr" target="#b14">[15]</ref> exploited activity maps of a CNN and iteratively refined group activity predictions. In <ref type="bibr" target="#b15">[16]</ref>, they capture the appearance and positions between actors to build a graph of actor relationships from a CNN and graph convolutional networks. Gavrilyuk et al. <ref type="bibr" target="#b16">[17]</ref> explicitly modeled spatial and temporal relationships of actors with an actor-transformer model that learns and extract relevant information for group activity recognition. Our approach use CNN to perform classification. We share the use of skeletal data with <ref type="bibr" target="#b16">[17]</ref>, however our approach do not use additional 2D and 3D information to preserve privacy. We also consider the relative position of actors combined with motion and pose to perform individual action classification and group activity recognition.</p><p>The use of human poses for recognizing actions of an actor is a popular approach in the literature. The early approaches were using handcrafted pose features <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, then skeletons <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> and attention based pose estimations <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref> were all explored for the task of action recognition of single actors. We exploit skeletons to model actions of single actors and then combine them into an holistic representation of the entire scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>In the following we show our approach for partially selfsupervised group activity recognition using skeletal data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Input Skeleton Representation</head><p>A group activity model has to consider individual actors representation its temporal evolution and contextual information. Similarly to <ref type="bibr" target="#b8">[9]</ref>, the person bounding boxes are firstly obtained through the object tracker in the Dlib library <ref type="bibr" target="#b23">[24]</ref>. Then we feed each person track frames as input to openpose <ref type="bibr" target="#b24">[25]</ref>, obtaining a group of skeleton sequences GS. GS can be represented with a K ? T ? N ? D tensor, where K is the number of actors in the video clip, T is the number of frames in the sequence, N is the number of joints in the skeleton and D is the coordinate dimension. Given a group of skeletons at time t, GS t = {S t 1 , S t 2 , . . . , S t K }, we represent the skeleton of a person k at time t as</p><formula xml:id="formula_0">S tk = [J tk 1 , J tk 2 , . . . , J tk N ],</formula><p>where J = [x, y, p] is a 2D joint coordinate + precision given by the pose estimator.</p><p>Since skeleton coordinates depend on actor camera distance, bounding box dimension and height we normalize each skeleton sequence by subtracting the mid-hip keypoint from each skeleton joint in order to have this last joint as the center of the coordinates system, then we divide each limb by the torso length.</p><p>Similarly to <ref type="bibr" target="#b25">[26]</ref>, we introduce a representation of skeleton motion. The skeleton motion of a person k at time t is defined as the temporal difference of each joint between two consecutive frames:</p><formula xml:id="formula_1">M tk = S (t+1)k ? S tk = [J (t+1)k 1 ? J tk 1 , J (t+1)k 2 ? J tk 2 , . . . , J (t+1)k N ? J tk N ]</formula><p>(1)</p><p>To model the configuration of each actor k we compute a person-to-person interaction D tk (see <ref type="figure">Fig. 3</ref>), defined as the difference between each pair of joints of two persons at time t. In order to obtain a group representation invariant to camera motion and do not rely on global fram coordinates, we select a pivot actor. We used the pivot as a reference to compute the difference between joint pairs.</p><p>We then represent each skeleton sequence via pivot-actor Joint differences, computed between an actor k and the actor pivot p at time t is formulated as:</p><formula xml:id="formula_2">D tk = S tk ? S tp = [J tk 1 ? J tp 1 , J tk 2 ? J tp 2 , . . . , J tk N ? J tp N ]</formula><p>(2) Motion M k and pivot-actor joint differences D k from all actors are stacked obtaining two tensor GM and GD having the same shape of the group skeleton sequence GS.</p><p>Group sequence of skeletons, motion and pivot-actor joint differences are fed into the network directly as three input streams into three separate network branches sharing the same architecture. However their parameters are not shared and learned separately. Following <ref type="bibr" target="#b25">[26]</ref> feature maps from inputs are learned hierarchically with specific convolution layers, then they are flattened and fused by concatenation obtaining a F ? K matrix to represent feature vectors of actors. Feature vectors are used both for action classifications and max-pooled for group activity classification. The whole model can be trained in an end-to-end manner with back-propagation and the extremely small model size allows us to easily train the network from scratch without the need of pretraining. Combining the two standard cross-entropy losses, the final loss function is formed as</p><formula xml:id="formula_3">L tot = L G (y G ,? G ) + ?L I (y I ,? I )<label>(3)</label></formula><p>where L I and L G are the cross-entropy loss, y G and y I denote the ground-truth labels of group activity and individual action,? G and? I are the predictions to group activity and individual action. The first term corresponds to group activity classification loss, and the second is the loss of the individual action classification. The weight ? is used to balance these two tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Self-Supervised Group Activity Recognition</head><p>The basic idea to provide pseudo-labels for individual actions is to group the representations of persons' crop into an over-segmented set of clusters, taking the cluster ids as annotations of individual actions. The rationale is that we can exploit the manifold induced by any learned representation to label similar individual actions unsupervisedly.</p><p>We use P3D, a pretrained 3D-CNN <ref type="bibr" target="#b26">[27]</ref> initialized on Kinetics sport dataset <ref type="bibr" target="#b27">[28]</ref> to represent individual actor's skeletal data. We use the last fully connected layer output as features from video clips of each actor. We cluster features from the training set using k-means and use the cluster assignments as "pseudo-labels" k I to compute individual action loss term L I (k I ,? I ). This allows to train our model in a self-supervised way. Before the clustering, features are PCA-reduced from 2048 to 256 dimensions, whitened and l2-normalized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we evaluate the performance of the proposed method in both supervised and self-supervised variants with several baselines. We also compare our results with the stateof-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>We evaluate our method on the Volleyball dataset <ref type="bibr" target="#b8">[9]</ref>, since it is the only public available dataset for group activity recognition that is relatively large-scale and contains labels for people locations, as well as their collective and individual actions. This dataset contains 4830 clips of 55 volleyball games. Each clip central frame is annotated at each player level with the bounding box and one of the 9 individual actions, and the whole scene is labeled with one of the 8 collective activity. Since other frames are not annotated, to get the bounding boxes of people, we used DLIB tracker <ref type="bibr" target="#b23">[24]</ref>. We do horizontal flips as data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>We adopt stochastic gradient descent with ADAM to learn the network parameters with fixed hyper-parameters to ? 1 = 0.9, ? 2 = 0.999, = 10 ?8 . We train the network for 100 epochs using a mini-batch size of 64 and a starting learning rate of 0.001 decreasing it by a factor of 10 every 30 epochs. Individual action loss weight ? = 0.7 is used. For training all our models (that include the baseline models) we follow the same training protocol using a Tesla K80 GPU and PyTorch Framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Baseline and Variants for Ablation Studies</head><p>Here we evaluate our model by comparing obtained results with several baselines. First, we describe the baseline model then, results on the Volleyball dataset are presented. Our model is end-to-end trainable but could also be implemented in a 2stage style, splitting the model into an action model and a group model. Training then would consists in learning actions from each skeleton sequence and then use this model to extract actor features, which are pooled over all people and fed to the group model to recognize group activity. We test the following approaches:</p><p>1 Two Stage Model without Pivot-Actor Joint Differences: This baseline is the two stage model with two input streams: skeleton joints and motion.</p><p>2 Two Stage Model: This baseline is an extension of the previous baseline. Here individual action model receive as input a third stream: the pivot-actor joint differences.  three input stream.</p><p>5 End-to-end Model with Data Augmentation: the endto-end final version with its three input stream and data augmentation. In <ref type="table" target="#tab_0">Table I</ref>, the classification results of our method is compared with baselines. A performance increase is obtained thanks to Pivot-Actor Joint Differences as it is including a person-to-person context information. End-to-end training helps significantly the model. Also data augmentation is useful for training the model as it provide consistent data for network's learning. Therefore we choose to use both Pivot-Actor Joint Differences and data augmentation with end-to-end training procedure for our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Self-Supervised Ablation Studies</head><p>As we discussed in previous subsection, the use of Pivot-Actor Joint Differences with end-to-end training is able to achieve the highest performance and we choose this combination as our final model, considering the two variants with and without data augmentation. In order to understand how much performance is affected by the use of our pseudo-labels we conduct the following ablation studies.</p><p>1 Group activity labels only: In this baseline the model has been modified in order to work only with group activity labels. The layers that receive actor feature representation to classify the individual action are removed and 4 End-to-end fully supervised: this method is the endto-end final version, previously seen in <ref type="table" target="#tab_0">Table I</ref>, trained with the full ground truth.</p><p>The difference in results of the first and second baselines illustrate the importance of using instance label annotation, even if from pseudo-labels. Comparing with the second baseline, our self-supervised method considering the time with a 3D-CNN model obtains better performance. Moreover, our selfsupervised method results are very close to the supervised variant, especially when also using data augmentation. <ref type="figure" target="#fig_3">Fig. 5</ref> shows group accuracy results obtained by varying the number of clusters by a step of 10, note that best results are obtained with k=20. Given that we train our model on the Volleyball dataset, one would expect k=9 (actual number of action classes) to yield the best results, but apparently some amount of over-segmentation is beneficial.   III: Comparison of recognition accuracy (%) on Volleyball dataset. "OF" denotes optical flow input, while column "Action Labels" says that a method use ground truth annotations at individual actor level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparison with state-of-the-art methods</head><p>There are only a few works <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref> using pose in group activity recognition reporting results on the Volleyball dataset, thus we compare our method that exploits only poses with other state of-the-art methods that make use of different input feature like RGB and/or optical flow. As shown in <ref type="table" target="#tab_0">Table  III</ref>, our method is very competitive with the state-of-theart methods and even outperforms most of the methods that exploit RGB and optical flow input (including PC-TDM <ref type="bibr" target="#b30">[31]</ref> and SPA+KD+OF <ref type="bibr" target="#b31">[32]</ref>). Although ARG <ref type="bibr" target="#b15">[16]</ref>, CRM <ref type="bibr" target="#b32">[33]</ref>, PRL <ref type="bibr" target="#b33">[34]</ref> and AT <ref type="bibr" target="#b29">[30]</ref> perform somewhat better than our method, note that it is unfair to compare with, because they use RGB, optical flow input and also a much larger model than ours. In addition, even our self-supervised action method outperformed various approaches without using ground truth individual action labels, showing that it is possible to obtain good results using action labels generated with visual feature clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this work we have shown a solution to train group activity recognition systems end-to-end without the use of individual action labels. Our method using only skeletal data is able to reach state-of-the art performance without using RGB or Optical flow and requiring only labels at the sequence level. Compared to previous work, the proposed approach needs less supervision to be trained and using only skeleton data, can also be used in contexts where privacy require to avoid saving RGB images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>We classify group activities using skeletons, motion of single actors and their relative positions to a pivot actor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: The proposed architecture employs three structurally identical branches with independent weights. We use openpose estimated skeletons to create three different representations. A concatenation of actor features is fed to a rather shallow network with two convolutional layers. Finally, single actor features are fed to the individual action classification head and single actor features are pooled to compute a group representation which is fed to the group classification head.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :Fig. 4 : 2 34</head><label>342</label><figDesc>Pivot selection among players. We pick the players closest to the average joint centroid of all players. Pivot is shown in yellow. t e x i t s h a 1 _ b a s e 6 4 = " n j S d O 8 k g P R N V Q 5 P a X f x P W F k O G x Q = " &gt; A A A B 9 X i c b V C 7 T s N A E D z z D O E V H h 3 N i Q i J K r I R C D o i g Q R l E O Q h J S Y 6 X z b h l P P Z u l u D g p V P o I W K D l F C w 8 9 Q U P A n 2 E k K S J h q N L O r n R 0 v l M K g b X 9 a U 9 M z s 3 P z m Y X s 4 t L y y m p u b b 1 i g k h z K P N A B r r m M Q N S K C i j Q A m 1 U A P z P Q l V r 3 u S + t V b 0 E Y E 6 g p 7 I b g + 6 y j R F p x h I l 2 e X n e b u b x d s A e g k 8 Q Z k f z x x / 3 3 2 d t m X G r m v h q t g E c + K O S S G V N 3 7 B D d m G k U X E I / 2 4 g M h I x 3 W Q f q C V X M B + P G g 6 h 9 u h M Z h g E N Q V M h 6 U C E 3 x s x 8 4 3 p + V 4 y 6 T O 8 M e N e K v 7 n 1 S N s H 7 m x U G G E o H h 6 C I W E w S H D t U g 6 A N o S G h B Z m h y o U J Q z z R B B C 8 o 4 T 8 Q o K S W b 9 O G M f z 9 J K n s F Z 7 9 w c O H k i z Y Z I k O 2 y D b Z J Q 4 5 J E V y T k q k T D j p k A f y S J 6 s O + v Z e r F e h 6 N T 1 m h n g / y B 9 f 4 D B Q 2 V 7 w = = &lt; / l a t e x i t &gt; D k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n j S d O 8 k g P R N V Q 5 P a X f x P W F k O G x Q = " &gt; A A A B 9 X i c b V C 7 T s N A E D z z D O E V H h 3 N i Q i J K r I R C D o i g Q R l E O Q h J S Y 6 X z b h l P P Z u l u D g p V P o I W K D l F C w 8 9 Q U P A n 2 E k K S J h q N L O r n R 0 v l M K g b X 9 a U 9 M z s 3 P z m Y X s 4 t L y y m p u b b 1 i g k h z K P N A B r r m M Q N S K C i j Q A m 1 U A P z P Q l V r 3 u S + t V b 0 E Y E 6 g p 7 I b g + 6 y j R F p x h I l 2 e X n e b u b x d s A e g k 8 Q Z k f z x x / 3 3 2 d t m X G r m v h q t g E c + K O S S G V N 3 7 B D d m G k U X E I / 2 4 g M h I x 3 W Q f q C V X M B + P G g 6 h 9 u h M Z h g E N Q V M h 6 U C E 3 x s x 8 4 3 p + V 4 y 6 T O 8 M e N e K v 7 n 1 S N s H 7 m x U G G E o H h 6 C I W E w S H D t U g 6 A N o S G h B Z m h y o U J Q z z R B B C 8 o 4 T 8 Q o K S W b 9 O G M f z 9 J K n s F Z 7 9 w c O H k i z Y Z I k O 2 y D b Z J Q 4 5 J E V y T k q k T D j p k A f y S J 6 s O + v Z e r F e h 6 N T 1 m h n g / y B 9 f 4 D B Q 2 V 7 w = = &lt; / l a t e x i t &gt; S k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 e u w W E 4 t h + 5 q w i k d r f 6/ j N K v B 3 s = " &gt; A A A B 9 X i c b V C 7 T s N A E D y H V w i v 8 O h o T k R I V J G N k I C K S B R Q B o U 8 p C R E 5 8 s m n H I + W 3 d r U L D y C b R Q 0 S F K a P g Z C g r + B N t J A Y G p R j O 7 2 t l x A y k M 2 v a H l Z m Z n Z t f y C 7 m l p Z X V t f y 6 x s 1 4 4 e a Q 5 X 7 0 t c N l x m Q Q k E V B U p o B B q Y 5 0 q o u 4 P T x K / f g D b C V 5 c 4 D K D t s b 4 S P c E Z x l K l c j X o 5 A t 2 0 U 5 B / x J n Q g o n 7 3 d f Z 6 9 b U b m T / 2 x 1 f R 5 6 o J B L Z k z T s Q N s R 0 y j 4 B J G u V Z o I G B 8 w P r Q j K l i H p h 2 l E Y d 0 d 3 Q M P R p A J o K S V M R fm 5 E z D N m 6 L n x p M f w 2 k x 7 i f i f 1 w y x d 9 S O h A p C B M W T Q y g k p I c M 1 y L u A G h X a E B k S X K g Q l H O N E M E L S j j P B b D u J R c 3 I c z / f 1 f U t s v O g f F 4 w u n U L L J G F m y T X b I H n H I I S m R c 1 I m V c J J n 9 y T B / J o 3 V p P 1 r P 1 M h 7 N W J O d T f I L 1 t s 3 H d S W A g = = &lt; / l a t e x i t &gt; S k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 e u w W E 4 t h + 5 q w i k d r f 6 / j N K v B 3 s = " &gt; A A A B 9 X i c b V C 7 T s N A E D y H V w i v 8 O h o T k R I V J G N k I C K S B R Q B o U 8 p C R E 5 8 s m n H I + W 3 d r U L D y C b R Q 0 S F K a P g Z C g r + B N t J A Y G p R j O 7 2 t l x A y k M 2 v a H l Z m Z n Z t f y C 7 m l p Z X V t f y 6 x s 1 4 4 e a Q 5 X 7 0 t c N l x m Q Q k E V B U p o B B q Y 5 0 q o u 4 P T x K / f g D b C V 5 c 4 D K D t s b 4 S P c E Z x l K l c j X o 5 A t 2 0 U 5 B / x J n Q g o n 7 3 d f Z 6 9 b U b m T / 2 x 1 f R 5 6 o J B L Z k z T s Q N s R 0 y j 4 B J G u V Z o I G B 8 w P r Q j K l i H p h 2 l E Y d 0 d 3 Q M P R p A J o K S V M R f m 5 E z D N m 6 L n x p M f w 2 k x 7 i f i f 1 w y x d 9 S O h A p C B M W T Q y g k p I c M 1 y L u A G h X a E B k S X K g Q l H O N E M E L S j j P B b D u J R c 3 I c z / f 1 f U t s v O g f F 4 w u n U L L J G F m y T X b I H n H I I S m R c 1 I m V c J J n 9 y T B / J o 3 V p P 1 r P 1 M h 7 N W J O d T f I L 1 t s 3 H d S W A g = = &lt; / l a t e x i t &gt; M k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s W B e E V T B d G o N + F c I / M 9 4 N 0 X Z I x o = " &gt; A A A B 9 X i c b V C 7 T s N A E D z z D O E V H h 3 N i Q i J K r I R E l A R i Q I a p C D I Q 0 p M d L 5 s w i n n s 3 W 3 B g U r n 0 A L F R 2 i h I a f o a D g T 7 C T F J A w 1 W h m V z s 7 X i i F Q d v + t K a m Z 2 b n 5 j M L 2 c W l 5 Z X V 3 N p 6 x Q S R 5 l D m g Q x 0 z W M G p F B Q R o E S a q E G 5 n s S q l 7 3 J P W r t 6 C N C N Q V 9 k J w f d Z R o i 0 4 w 0 S 6 P L / u N n N 5 u 2 A P Q C e J M y L 5 4 4 / 7 7 9 O 3 z b j U z H 0 1 W g G P f F D I J T O m 7 t g h u j H T K L i E f r Y R G Q g Z 7 7 I O 1 B O q m A / G j Q d R + 3 Q n M g w D G o K m Q t K B C L 8 3 Y u Y b 0 / O 9 Z N J n e G P G v V T 8 z 6 t H 2 D 5 0 Y 6 H C C E H x 9 B A K C Y N D h m u R d A C 0 J T Q g s j Q 5 U K E o Z 5 o h g h a U c Z 6 I U V J K N u n D G f 9 + k l T 2 C s 5 + 4 e j C y R d t M k S G b J F t s k s c c k C K 5 I y U S J l w 0 i E P 5 J E 8 W X f W s / V i v Q 5 H p 6 z R z g b 5 A + v 9 B x R u l f w = &lt; / l a t e x i t &gt; M k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s W B e E V T B d G o N + F c I / M 9 4 N 0 X Z I x o = " &gt; A A A B 9 X i c b V C 7 T s N A E D z z D O E V H h 3 N i Q i J K r I R E l A R i Q I a p C D I Q 0 p M d L 5 s w i n n s 3 W 3 B g U r n 0 A L F R 2 i h I a f o a D g T 7 C T F J A w 1 W h m V z s 7 X i i F Q d v + t K a m Z 2 b n 5 j M L 2 c W l 5 Z X V 3 N p 6 x Q S R 5 l D m g Q x 0 z W M G p F B Q R o E S a q E G 5 n s S q l 7 3 J P W r t 6 C N C N Q V 9 k J w f d Z R o i 0 4 w 0 S 6 P L / u N n N 5 u 2 A P Q C e J M y L 5 4 4 / 7 7 9 O 3 z b j U z H 0 1 W g G P f F D I J T O m 7 t g h u j H T K L i E f r Y R G Q g Z 7 7 I O 1 B O q m A / G j Q d R + 3 Q n M g w D G o K m Q t K B C L 8 3 Y u Y b 0 / O 9 Z N J n e G P G v V T 8 z 6 t H 2 D 5 0 Y 6 H C C E H x 9 B A K C Y N D h m u R d A C 0 J T Q g s j Q 5 U K E o Z 5 o h g h a U c Z 6 I U V J K N u n D G f 9 + k l T 2 C s 5 + 4 e j C y R d t M k S G b J F t s k s c c k C K 5 I y U S J l w 0 i E P 5 J E 8 W X f W s / V i v Q 5 H p 6 z R z g b 5 A + v 9 B x R u l f w = &lt; / l a t e x i t &gt; Architecture of the CNN. Each green block corresponds to a convolutional layer of size WxHxFilters. The layers with /2 have a stride of 2. The entire figure corresponds to the CNN blocks of Fig. End-to-end Model without Pivot-Actor Joint Differences: this baseline is the end-to-end version of first baseline. End-to-end Model: the end-to-end final version with its Method Accuracy Two stage model w/o actor-pivot joint pair difference 78.1 Two stage model 80.9 End-to-end model w/o actor-pivot joint pair difference 85.0 End-to-end model 89.2 End-to-end model with data augmentation 91.0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :3</head><label>5</label><figDesc>Mean Classification Accuracy of group activity varying the number of clusters for self-supervised learning.the individual loss factor ? is set to zero. This baseline is designed to illustrate the importance of individual activity labels in our model.2 Pseudo action labels from 2D-CNN: In this baseline we adopted pseudo-labels instead of ground truth action labels. Visual features are extracted from the central frame of each video clip using a pretrained VGG16 2D-CNN. Number of used cluster k is set to 20. This baseline aims to illustrate the importance of pseudo-action-labels. Pseudo action labels from 3D-CNN: Final Self-Supervised model, where we adopt pseudo-labels instead of ground truth action labels. Visual features are extracted from the whole fixed temporal window of each video clip using a pretrained 3D-CNN. Number of used cluster k is set to 20.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Comparison of our method with baseline methods on the Volleyball dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Comparison of our method with and without action labels. SSAL stands for Self Supervised Action Learning corresponding to centroid indexes assigned by clustering of visual features.</figDesc><table><row><cell>Method</cell><cell>Input</cell><cell cols="2">Action Labels Accuracy</cell></row><row><cell>HDTM [9]</cell><cell>RGB</cell><cell>Yes</cell><cell>81.9</cell></row><row><cell>SSU [12]</cell><cell>RGB</cell><cell>Yes</cell><cell>89.9</cell></row><row><cell>PC-TDM [31]</cell><cell>RGB+OF</cell><cell>Yes</cell><cell>87.7</cell></row><row><cell>MS-CNN [29]</cell><cell>RGB+POSE</cell><cell>Yes</cell><cell>90.5</cell></row><row><cell>stagNet [35]</cell><cell>RGB</cell><cell>Yes</cell><cell>89.3</cell></row><row><cell>RCRG [14]</cell><cell>RGB</cell><cell>Yes</cell><cell>89.5</cell></row><row><cell>SPA+KD [32]</cell><cell>RGB</cell><cell>Yes</cell><cell>89.3</cell></row><row><cell cols="2">SPA+KD+OF [32] RGB+OF</cell><cell>Yes</cell><cell>90.7</cell></row><row><cell>ARG [16]</cell><cell>RGB</cell><cell>Yes</cell><cell>92.6</cell></row><row><cell>CRM [33]</cell><cell>RGB+OF</cell><cell>Yes</cell><cell>93.0</cell></row><row><cell>PRL [34]</cell><cell>RGB</cell><cell>Yes</cell><cell>91.4</cell></row><row><cell>AT [30]</cell><cell>POSE+OF</cell><cell>Yes</cell><cell>94.4</cell></row><row><cell>Ours-SSAL</cell><cell>POSE</cell><cell>No</cell><cell>89.4</cell></row><row><cell>Ours</cell><cell>POSE</cell><cell>Yes</cell><cell>91.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hirf: Hierarchical random field for collective activity recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Amer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="572" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A unified framework for multi-target tracking and collective activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="215" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding collective activities of people from videos</title>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1242" to="1257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning context for collective activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shahid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011. IEEE</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3273" to="3280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Visual recognition by counting instances: A multi-instance cardinality potential kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajimirsadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2596" to="2605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Social roles in hierarchical models for human activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1354" to="1361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Discriminative latent models for recognizing contextual group activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Robinovitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1549" to="1562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A hierarchical deep temporal model for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muralidharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1971" to="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Structure inference machines: Recurrent neural networks for analyzing relations in group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4772" to="4781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cern: confidence-energy recurrent network for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5523" to="5531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Social scene understanding: End-to-end multi-person action localization and collective activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bagautdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4315" to="4324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recurrent modeling of interaction context for collective activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3048" to="3056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hierarchical relational networks for group activity recognition and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="721" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional relational machine for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Atigh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nickabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7892" to="7901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning actor relation graphs for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9964" to="9974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Actortransformers for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gavrilyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sanford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Javan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="839" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint action recognition and pose estimation from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1293" to="1301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An approach to pose-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="915" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An end-to-end spatiotemporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-first AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Action recognition with jointspooled 3d deep convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">P-cnn: Pose-based cnn features for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ch?ron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international</title>
		<meeting>the IEEE international</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3218" to="3226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dlib-ml: A machine learning toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1755" to="1758" />
			<date type="published" when="2009-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Co-occurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06055</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A multi-stream convolutional neural network framework for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Atigh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nickabadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.10328</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Actortransformers for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gavrilyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sanford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Javan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.12737</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Participation-contributed temporal dynamic model for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM international conference on Multimedia</title>
		<meeting>the 26th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1292" to="1300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning semanticspreserving attention and contextual interaction for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4997" to="5012" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Convolutional relational machine for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Atigh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nickabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03308</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Progressive relation learning for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02948</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">stagnet: An attentive semantic rnn for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="101" to="117" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
