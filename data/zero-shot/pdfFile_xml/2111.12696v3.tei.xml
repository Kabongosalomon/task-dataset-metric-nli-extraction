<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Lightweight Graph Transformer Network for Human Mesh Reconstruction from 2D Human Pose A Lightweight Graph Transformer Network for Human Mesh Reconstruc- tion from 2D Human Pose. In Proceedings of Make sure to enter the cor- rect conference title from your rights confirmation emai (ACM Multimedia</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zheng</surname></persName>
							<email>cezheng@knights.ucf.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matias</forename><surname>Mendieta</surname></persName>
							<email>mendieta@knights.ucf.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pu</forename><surname>Wang</surname></persName>
							<email>pu.wang@uncc.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidong</forename><surname>Lu</surname></persName>
							<email>aidong.lu@uncc.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
							<email>chen.chen@crcv.ucf.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matias</forename><surname>Mendieta</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pu</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidong</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of North Carolina at Charlotte</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of North Carolina at Charlotte</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Lightweight Graph Transformer Network for Human Mesh Reconstruction from 2D Human Pose A Lightweight Graph Transformer Network for Human Mesh Reconstruc- tion from 2D Human Pose. In Proceedings of Make sure to enter the cor- rect conference title from your rights confirmation emai (ACM Multimedia</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing deep learning-based human mesh reconstruction approaches have a tendency to build larger networks to achieve higher accuracy. Computational complexity and model size are often neglected, despite being key characteristics for practical use of human mesh reconstruction models (e.g. virtual try-on systems). In this paper, we present GTRS, a lightweight pose-based method that can reconstruct human mesh from 2D human pose. We propose a pose analysis module that uses graph transformers to exploit structured and implicit joint correlations, and a mesh regression module that combines the extracted pose feature with the mesh template to reconstruct the final human mesh. We demonstrate the efficiency and generalization of GTRS by extensive evaluations on the Hu-man3.6M and 3DPW datasets. In particular, GTRS achieves better accuracy than the SOTA pose-based method Pose2Mesh while only using 10.2% of the parameters (Params) and 2.5% of the FLOPs on the challenging in-the-wild 3DPW dataset. Code is available at https://github.com/zczcwh/GTRS ACM Reference Format:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>progressed beyond the estimation of 2D or 3D poses <ref type="bibr" target="#b45">[45]</ref>[42] <ref type="bibr" target="#b26">[27]</ref> with a basic keypoint structure, and the study of reconstructing the entire 3D mesh from a single image has attracted much interest. Mesh representation, which can provide rich human body information and have a better visualization, is more welcomed by real-world applications such as gaming, human-computer interaction, and virtual reality (VR). However, human mesh reconstruction from a single image is a challenging task due to depth ambiguity, occlusion, and complex human body articulation.</p><p>Two general approaches exist in the literature for performing mesh reconstruction. One is the direct image-based method, where the pipeline is trained end-to-end from input image to output mesh. The second is to employ an off-the-shelf 2D pose detector as the front end, and design a mesh reconstruction model using 2D poses as input. Most recent progress has been made in the first category, achieving promising performance. However, the performance gain has come at the cost of ever increasing computational requirements and complex models (for instance, METRO <ref type="bibr" target="#b21">[22]</ref> requires 229M Params and 56.6G FLOPs). In real-world applications such as human-computer interaction, animated avatar, and VR gaming, the human mesh reconstruction task needs to be efficient and deployable on resourceconstrained platforms like VR headsets.</p><p>While less studied, pose-based methods are alternative solutions for human mesh recovery with a few advantages for such applications. First, pose-based methods provide a modular design that can easily be incorporated with any off-the-shelf 2D pose detectors. With speed as the primary goal, fast pose detectors (e.g. <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b44">44]</ref>) can be deployed on a mobile device in real-time with impressive performance. Second, the input to pose-based methods (that is, the detected 2D pose) is extremely sparse data with the size of ? 2, where is the number of joints. Compared to the image input, it gives more flexibility to design a lightweight mesh reconstruction network to achieve computational and memory efficiency with competitive performance. Nonetheless, the existing methods (including the state-of-the-art pose-based method Pose2Mesh <ref type="bibr" target="#b4">[5]</ref>) still incur substantial computational and memory overhead. The efficient design of the model is crucial for practical use, but has been almost entirely ignored in the literature.</p><p>To bridge this gap, we propose a Graph Transformer network for human mesh ReconStruction from 2D human pose (GTRS), which is the first method focusing on the efficiency. GTRS is a pose-based method designed to fully exploit joint correlations for pose and mesh feature representation while minimizing computational complexity and model size. The operational blocks of GTRS are designed with an intentional combination of graph neural networks and transformer operations. Recently, graph convolutional networks (GCNs) have shown promising advances in 3D HPE and mesh reconstruction tasks <ref type="bibr" target="#b62">[62]</ref>. Human pose data is naturally formulated as a graph, and GCNs can extract useful information with relatively little compute and parameters. Therefore, we harness GCNs to form strong representations from these inherent structural priors on the front-end of GTRS block operations. After this, these representations are further refined with lightweight transformer structure to powerfully capture global dependencies via its self-attention mechanism. Thus, these combined operations form our designed graph transformer blocks, which we employ in a parallel fashion for comprehensively exploring human kinematic information from the 2D pose and modeling joint correlations in a lightweight manner. <ref type="figure">Figure 1</ref>: The trade-off between accuracy (MPJPE ?) and model Params/FLOPs. All methods are evaluated on 3DPW dataset. GTRS (Ours) and Pose2Mesh <ref type="bibr" target="#b4">[5]</ref> are pose-based methods. The reported Params/FLOPs include the corresponding front-end 2D pose detector (DARK <ref type="bibr" target="#b54">[54]</ref> or LiteHRNet <ref type="bibr" target="#b53">[53]</ref>). Others are image-based methods.</p><p>As an extremely lightweight pose-based method, GTRS uses only 7.9M parameters (Params) and 0.19G floating-point operations (FLOPs) without considering the front-end 2D pose detector. Compared to SOTA pose-based method Pose2Mesh <ref type="bibr" target="#b4">[5]</ref>, GTRS achieves better results while only requiring 10.2% of the Params and 2.5% of the FLOPs. When also considering the front-end 2D pose detector, GTRS also shows a significant reduction in Params and FLOPs compared to image-based methods in <ref type="figure">Fig. 1</ref> (e.g. 6.9% Params and 1.2% FLOPs compared to I2LMeshNet <ref type="bibr" target="#b34">[34]</ref>). More discussions are provided in Sec. 4.3.</p><p>Our contributions are summarized as follows: ? Observing that existing methods mainly pursue higher accuracy while ignoring computational and memory cost, we present a lightweight pose-based method, GTRS, for efficient human mesh reconstruction from the 2D pose. We hope our work can inspire more research on the efficiency of human mesh reconstruction. ? We introduce our pose analysis module with a parallel design to facilitate improved utilization of human kinematic information.</p><p>Within this module, we propose our graph transformer blocks with fixed and learnable adjacency matrices to simultaneously explore diverse structured and implicit human joint correlations. ? GTRS achieves competitive results compared to previous methods with much fewer parameters and less computational cost on Human3.6M and 3DPW datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Human Mesh Reconstruction: Recovering human mesh from images is a challenging task that has attracted much attention in recent years. Without requiring additional devices such as depth sensors or inertial measurement units, Human Mesh Reconstruction (HMR) from images makes it more efficient and convenient. The majority of previous works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> utilize parametric human model such as SMPL <ref type="bibr" target="#b29">[30]</ref>, ADAM <ref type="bibr" target="#b48">[48]</ref>, STAR <ref type="bibr" target="#b37">[37]</ref> to reconstruct human mesh by training a network to regress model parameters.</p><p>As one of the most popular volumetric models, the SMPL [30] model has been widely used in HMR, e.g., <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b52">52]</ref>. Pavlakos et al. <ref type="bibr" target="#b41">[41]</ref>, and Omran et al. <ref type="bibr" target="#b36">[36]</ref> regress SMPL parameters to reconstruct 3D human mesh. SPIN <ref type="bibr" target="#b18">[19]</ref> revisits optimization approaches within the neural networks that initializes an iterative optimization process (SMPLify). Instead of predicting SMPL parameters, Zhu et al. <ref type="bibr" target="#b60">[60]</ref> combine the SMPL model with a hierarchical mesh deformation framework to enhance the flexibility of free-form 3D deformation. Kocabas et al. <ref type="bibr" target="#b17">[18]</ref> include the large-scale motion capture dataset AMASS <ref type="bibr" target="#b30">[31]</ref> for adversarial training of their SMPL-based method named VIBE (Video Inference for Body Pose and Shape Estimation).</p><p>Compared to previous methods that recover human mesh directly from images, <ref type="bibr" target="#b4">[5]</ref> estimates SMPL parameters from predicted 2D poses and achieves impressive performance. By applying off-theshelf 2D pose detectors such as AlphaPose <ref type="bibr" target="#b7">[8]</ref> and HRNet <ref type="bibr" target="#b46">[46]</ref>, the well-estimated 2D pose can be obtained. Then, a CNN-based PoseNet and MeshNet are proposed to exploit the human mesh topology to recover human mesh based on the input 2D pose. GTRS also follows this pose-based pipeline to reconstruct human mesh.</p><p>Graph Convolution Networks: Recently, graph convolution networks (GCNs) have been widely adopted in 3D human pose estimation (3D HPE) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b62">62]</ref> because of the intuitive modeling of human joints as a graph structure and potential ability to better capture human kinematics. Following this trend, GCNs have also gained much attention in human mesh reconstruction <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24]</ref>. Kolotouros et al. <ref type="bibr" target="#b19">[20]</ref> regress the locations of the SMPL mesh vertices using a GCN architecture. Pose2Mesh <ref type="bibr" target="#b4">[5]</ref> employs a GCN to regress SMPL parameters from estimated 2D and 3D pose.</p><p>Vision Transformer: Transformer architecture is developing rapidly in the field of computer vision. Recent works have demonstrated the powerful global representation ability of transformer attention mechanism in various vision tasks such as object detection <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b61">61]</ref>, image classification <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b27">28]</ref>, segmentation <ref type="bibr" target="#b59">[59]</ref>, human pose estimation <ref type="bibr" target="#b57">[57,</ref><ref type="bibr" target="#b58">58]</ref>, etc. Lin et al. <ref type="bibr" target="#b21">[22]</ref> combine CNNs with transformer networks in their method, named METRO, to regress mesh vertices from a single image.</p><p>MeshGraphormer <ref type="bibr" target="#b22">[23]</ref> is a close related work, which also uses GCN with transformer architecture. However, it is an image-based method that injects GCN with fixed adjacency matrix into the transformer block between multi-head attention and multilayer perceptron (MLP). As a pose-based method, our GTRS utilizes GCNs to model features with prior knowledge, then applies transformers to further explore global dependencies. Moreover, GTRS adopts a paralleled design which enables different graph transformer blocks to explore diverse structured and implicit human kinematic information by using fixed and learnable adjacency metrices. Compared to MeshGraphormer <ref type="bibr" target="#b22">[23]</ref> that requires 226.5M Params and 56.6G FLOPs, GTRS shows significant computational and memory cost reduction. GTRS is more friendly to deploy on mobile devices since it only requires 9.7M Params and 0.89G FLOPs (4.29% and 1.57% of MeshGraphormer).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY 3.1 Baseline</head><p>In order to achieve model efficiency, an intuitive solution is to utilize the existing lightweight architecture such as MobileNetV2 <ref type="bibr" target="#b43">[43]</ref> for the 3D human mesh recovery from the 2D pose input. We refer to this simple design as our baseline. However, these CNN-based lightweight architectures are designed to process an image-like input (with the shape of [ , , ] where is channel, is the height, and is the width). First we embed the 2D pose input of joints given by ? R ?2 to ? R ? ? , then apply MobileNetV2 [43] to model mesh features. Finally, the output mesh parameter ? R 6890?3 can be estimated after feature embedding. However, CNNs do not provide a natural modeling of the graph-like 2D pose input (sparse and meaningfully structured), leaving some representational strength and efficiency on the table.</p><p>In light of the limitations of the baseline, we present our proposed GTRS architecture and elaborate the design components in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overview of GTRS</head><p>The vision transformer architecture is designed to capture the global dependencies cross all patches via self-attention mechanism given the input size of <ref type="bibr">[ , ]</ref> where is the number of patches and is the embedding dimension. Given the 2D pose input ? R ?2 , the joint correlations can be exploited when modeling human pose and mesh features through transformer architecture. Therefore, we design a lightweight transformer architecture which is more suitable for modeling 3D human pose and mesh given the 2D pose input rather than a lightweight CNN architecture. The results in Section 4.3 have proved this claim.</p><p>The overall architecture of GTRS is illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. The input is the estimated 2D pose obtained by off-the-shelf 2D pose detector such as HRNet <ref type="bibr" target="#b45">[45]</ref>, which can be denoted as ? R ?2 , where is the number of joints. A feature embedding layer embeds input ? R ?2 to ? R ? with a high feature dimension . Then, a Pose Analysis Module (PAM) returns modelled feature ? ? R ? . Next, the mesh template ? R 6890?3 (from <ref type="bibr" target="#b18">[19]</ref>) is used to provide initial human mesh information, which is embedded to mesh template feature ? R ? where is the channel number. Then, we feed ? and</p><p>to Mesh Regression Module (MRM), and the output would be ? R ( + )? . Finally, the estimated mesh parameter ? R 6890?3 can be obtained after the regression head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Preliminaries of GTRS</head><p>GCN: The GCN was introduced by <ref type="bibr" target="#b16">[17]</ref>. A graph is defined as G = { , } , where is a set of nodes and is a set of edges. We use GCN to model 2D pose feature, for the input feature ? R ? , where is the number of joints and is the dimension of input feature. Given the adjacency matrix ? R ? based on the joints connectivity, the output ? ? R ? ? of one GCN layer can be represented as:</p><formula xml:id="formula_0">? = ( ) (1)</formula><p>where (?) is the activation function for network non-linearity, and ? R ? ? is the learnable weight matrix which changes the feature dimension from to ? . We use the Gaussian Error Linear Unit (GELU) <ref type="bibr" target="#b9">[10]</ref> as activation function in this work. Transformer: Multi-Head Self-Attention Layer (MHA) is the core function of the transformer blocks, which was proposed by Vaswani et al. <ref type="bibr" target="#b49">[49]</ref>. The input ? R ? is first mapped to three matrices: query matrix , key matrix and value matrix by three linear transformation:</p><formula xml:id="formula_1">= , = , = .</formula><p>(</p><p>where , and ? R ? . The scaled dot product attention can be described as the following mapping function:</p><formula xml:id="formula_3">Attention( , , ) = Softmax( ? / ? ) .<label>(3)</label></formula><p>where 1 ? is the scaling factor for appropriate normalization to prevent extremely small gradients.</p><p>Next, the MHA utilizes multiple heads to model the information jointly from various representation subspaces with different positions. Each head applies scaled dot-product attention in parallel. The MSA output will be the concatenation of ? attention head outputs.</p><formula xml:id="formula_4">MSA( , , ) = Concat( 1 , 2 , . . . , ? ) (4) where = Attention( , , ), ? [1, ..., ?]<label>(5)</label></formula><p>is a linear projection ? R ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Pose Analysis Module in GTRS</head><p>In the PAM, we utilize graph transformers to improve the structured and implicit correlations based on the human kinematic information. We follow <ref type="bibr" target="#b62">[62]</ref> to build our GCN blocks. Then a transformer block is followed to model global dependencies. Our graph transformer block is illustrated in <ref type="figure">Fig. 3</ref> (b). Different from previous transformer architecture that stacks multiple transformer encoders, we form graph transformer blocks parallelly as shown in <ref type="figure">Fig. 3</ref> (a). The transformer embedding dimension is set to be a small number to make the network lightweight. Because GCNs maintain a strong relationship with the input graph structure, we begin each block with such operations to inject structural priors before performing the transformer's self-attention. Based on the human kinematic configuration illustrated in <ref type="figure">Fig. 4 (a)</ref>, the actual adjacency matrix can be obtained as shown in <ref type="figure">Fig. 4</ref> (b). Among these paralleled graph transformer blocks, only one block utilizes the actual adjacency matrix, which means this adjacency matrix is fixed and would not be updated. This graph transformer block is maintained to model pose features with structured human kinematic information using the fixed adjacency matrix. The rest of the graph transformer blocks are responsible to capture implicit correlations from the training data by applying the learnable adjacency matrices. The different patterns of correlations can be discovered through learnable adjacency matrices as shown in <ref type="figure">Fig. 4</ref> (c). The parallel design allows for various structural biases to The mesh template is provided by <ref type="bibr" target="#b18">[19]</ref> and the mesh template figure is from <ref type="bibr" target="#b21">[22]</ref>. A more detailed illustration of the GTRS architecture is provided in the appendix. In PAM, we apply a fixed adjacency matrix and multiple learnable adjacency matrices with a paralleled design to allow the network to explore a diverse set of structured and implicit correlations.</p><p>be applied simultaneously. Not only the kinematic correlations can be provided by the fixed adjacency matrix (based on the body joints structure), but also the unexpected correlations beyond our prior knowledge can be discovered by the learnable adjacency matrices in PAM. Next, a fusion block, which is a convolutional layer, will fuse all paralleled features together to a feature that maintains the same size as the input. Our transformer encoder layer is different from the original transformer encoder in <ref type="bibr" target="#b6">[7]</ref>. The structure of our transformer encoder is illustrated in <ref type="figure">Fig. 3 (b)</ref>. First, we add one convolutional branch parallel to the MHA branch, which is a point-wise convolution with the 1 ? 1 convolutional filters as described in <ref type="bibr" target="#b10">[11]</ref>. The reason we use this pointwise convolution is to create linear combinations of the input channels ( joint channels) while maintaining a low computational cost. Then, instead of the MLP layer, we use an SE block <ref type="bibr" target="#b11">[12]</ref> which is also computationally lightweight. The SE block is designed to recalibrate channel-wise feature responses by modeling channel interdependencies. The output of our transformer blocks given the input ? R ? can be represented as follows:</p><formula xml:id="formula_5">? = MSA(Norm( )) + CONV(Norm( )) + (6) = SE(Norm( ? )) + ?<label>(7)</label></formula><p>where CONV(?) is the convolutional block, Norm(?) is the normalization operator, and SE(?) denotes the SE block. Besides the output pose features , a 3D pose regression head (implemented as an MLP) is used to output a 3D pose prediction 3 ? R ?3 . This supervision on the 3D pose ensures the PAM can exploit pose features well even if the detected 2D pose is noisy (such as missing joints).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Mesh Regression Module in GTRS</head><p>The computational complexity of our transformer structure (as in our PAM) is O ( 2 ) for the input size of ? R ? and is much less than . To maintain a lightweight network, the transformer blocks with small embedding dimension (consistent with the embedding dimension in PAM) are used in MRM. Considering that small embedding dimension may not have enough representation ability, we introduce the mesh template which provides rich human mesh information for a better regression. The effectiveness of adding the mesh template has been verified in Sec. 4.4. We embed the original mesh template to the mesh template feature ? R ? to reduce the computational cost.  Due to different scales of the pose feature ? ? R ? and the mesh template feature ? R ? , we design a dual-branch block structure where two separate transformers are applied first to model these two features and then fuse information by a fusion transformer. The output is ? R ( + )? , which can be split to _ ? R ? and _ ? R ? as the input of next block. Finally, a regression head MLP will upsample the feature ? R ( + )? to the final mesh output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Loss functions</head><p>For the PAM, we add a linear layer to regress the intermediate 3D human pose (the final 3D human pose is obtained from the final mesh) to get a better pose feature. The pose feature is fed into the MRM. To train the PAM, we apply an 1 distance loss between the predicted indeterminate 3D pose 3 _ ? R ?3 and the ground truth 3D pose?3 ? R ?3 , where is the number of joints. The indeterminate 3D joint loss is defined as follows:</p><formula xml:id="formula_6">L _ = 1 ?? =1 ? 3 _ ??3 ? 1 ,<label>(8)</label></formula><p>After pretraining the PAM, we train the entire pipeline includes the PAM and the MRM. The following loss functions are used. Mesh Vertex Loss: We use an 1 distance loss between the predicted 3D mesh vertices coordinates 3 ? R ?3 and the ground truth 3D mesh vertices coordinates?3 ? R ?3 , where is the number of vertices. The mesh vertex loss is defined as follows:</p><formula xml:id="formula_7">L = 1 ?? =1 ? 3 ??3 ? 1 .<label>(9)</label></formula><p>3D Joint Coordinate Loss: After estimating 3D mesh vertices coordinates 3 ? R ?3 , we use the model defined joint regression matrix ? R ? to calculate 3D pose based on the estimated mesh, where is the number of joints and is the number of vertices. We apply an 1 loss between the regressed 3D pose from estimated mesh and the ground truth 3D pose?3 ? R ?3 .</p><formula xml:id="formula_8">L = 1 ?? =1 ? ??3 ? 1<label>(10)</label></formula><p>Surface Normal Loss: Following <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b51">51]</ref>, we supervise normal vectors of the estimated mesh surface with the ground truth unit normal vector. We apply this surface normal loss to improve surface smoothness and local detail as in <ref type="bibr" target="#b51">[51]</ref>. The surface normal loss is defined as follows:</p><formula xml:id="formula_9">L = ?? ?? { , } ? ? ? ? ? ? 2 , * ? ,<label>(11)</label></formula><p>where denotes a triangle face in the human mesh and * denotes a ground truth unit normal vector of . The and denotes the th and th vertices in , respectively. Surface Edge Loss: Following <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b51">51]</ref>, we use a edge length consistency loss between the predicted edges and ground truth edges. The surface edge loss aims to improve the smoothness of hands, feet, and a mouth which is defined as</p><formula xml:id="formula_10">L = ?? ?? { , } ? ? ? ? 2 ? ????? 2 ,<label>(12)</label></formula><p>where denotes a triangle face in the human mesh. The and denotes the th and th vertices in , respectively.</p><p>Based on the four type of loss functions, our overall loss is written as</p><formula xml:id="formula_11">L = L + L + L + L ,<label>(13)</label></formula><p>where = 1, = 0.01, = 0.01, and = 0.01 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Datasets and Evaluation Metrics</head><p>Human3.6M is one of the most widely used large-scale indoor dataset for 3D HPE and mesh reconstruction <ref type="bibr" target="#b12">[13]</ref>. There are 3.6M video frames recorded by 11 professional actors with performing 17 actions. The ground truth 3D pose annotations were captured by an accurate marker-based motion capture system, but no ground truth 3D mesh annotations were provided. The previous works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> used pseudo-ground truth mesh provided by Mosh <ref type="bibr" target="#b28">[29]</ref>. However, they are no longer accessible due to license issues. Now we use the pseudo-ground truth mesh generated by <ref type="bibr" target="#b4">[5]</ref>. Following previous works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>, we select 5 subjects (S1, S5, S6, S7, S8) for training and 2 subjects for testing (S9, S11).</p><p>3DPW is an in-the-wild dataset <ref type="bibr" target="#b50">[50]</ref> that contains 60 video sequences (51K video frames) captured in the outdoor environment. The ground truth 3D pose and mesh annotations are provided. We only use its defined test set for evaluation following <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>We also use MSCOCO <ref type="bibr" target="#b24">[25]</ref> and MuCo-3DHP <ref type="bibr" target="#b32">[33]</ref> for mixed training following <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b34">34]</ref>. For evaluation metrics, we use the three standard metrics below. The unit for these metrics is millimeters (mm).</p><p>MPJPE: Mean-Per-Joint-Position-Error is used to evaluate the estimated 3D human pose. It is computed as the mean Euclidean distance between the estimated joints and the ground truth joints.</p><p>PA-MPJPE: is the MPJPE after Procrustes Analysis <ref type="bibr" target="#b8">[9]</ref>. The rigid alignment using procrustes analysis is performed the estimated 3D pose, then compute MPJPE with the ground truth 3D pose. P-MPJPE aims to measure the errors of the reconstructed structure without considering translations and rotations.</p><p>MPVE: Mean-Per-Vertex-Error is used to evaluate the estimated 3D mesh vertices. It is computed as the mean Euclidean distance between the estimated and the ground truth mesh vertices. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We implemented GTRS with Pytorch <ref type="bibr" target="#b40">[40]</ref> using two NVIDIA RTX 3090 GPUs. GTRS can be trained in an end-to-end manner, but we first pretrain the PAM, then train the entire GTRS after loading those weights for better performance. In the PAM, there are 6 graph transformer blocks (1 with fixed adjacency matrix and 5 with learnable adjacency matrix) and the embedding dimension is 128. We use Adam <ref type="bibr" target="#b15">[16]</ref> optimizer with a learning rate 1 ? 10 ?4 to pretrain the PAM for 120 epochs. In MRM, the embedding dimension is the same as in PAM, which is 128, and the layer number is 4. The mesh template ? R 6890?3 is the mean SMPL parameters provided by <ref type="bibr" target="#b18">[19]</ref>. We also use Adam <ref type="bibr" target="#b15">[16]</ref> optimizer with a learning rate 1 ? 10 ?4 to train the GTRS for 180 epochs. The total training time would be less than one day.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with state-of-the-art results</head><p>Evaluation on Human3.6M: <ref type="table" target="#tab_0">Table 1</ref> compares GTRS with previous SOTA image/video-based and pose-based methods on Hu-man3.6M test set. We follow the same setting as Pose2Mesh <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> for a fair comparison; that is, we only use the Human3.6M training set for training and PA-MPJPE is only measured on the frontal camera set. Same as Pose2Mesh <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b46">[46]</ref> is used as the 2D pose detector. Within the similar total number of parameters and FLOPs, the performance boost from our baseline to GTRS demonstrates that transformer architecture is more suitable than CNN-based architecture when considering model efficiency for pose-based human mesh recovery. GTRS achieves better performances than Pose2Mesh <ref type="bibr" target="#b4">[5]</ref> (MPJPE decreases 0.6 and PA-MPJPE decreases 1.6). Compared with video-based methods, GTRS still achieves similar MPJPE (within 2 points) as a lightweight model. When using ground truth 2D pose as input, GTRS outperforms previous methods both in terms of MPJPE and PA-MPJPE. This results indicates the lower bound of GTRS. As more accurate and robust 2D human pose detectors are proposed, they can be plugged-in to GTRS and close the gap towards this lower bound.</p><p>Evaluation on 3DPW: <ref type="table" target="#tab_1">Table 2</ref> compares GTRS with previous SOTA image/video-based and pose-based methods on 3DPW test set. The training sets include Human3.6M, COCO, MPII <ref type="bibr" target="#b0">[1]</ref>, UP3D <ref type="bibr" target="#b20">[21]</ref>, MPI-INF-3DHP <ref type="bibr" target="#b31">[32]</ref>, and AMASS <ref type="bibr" target="#b30">[31]</ref>. Each method uses a different combination of these datasets. For GTRS, we only use Hu-man3.6M <ref type="bibr" target="#b12">[13]</ref>, COCO <ref type="bibr" target="#b24">[25]</ref>, and MuCo <ref type="bibr" target="#b32">[33]</ref> as the training sets. Following Pose2Mesh <ref type="bibr" target="#b4">[5]</ref>, we use DARK <ref type="bibr" target="#b54">[54]</ref> as the 2D pose detector for 3DPW evaluation. Comparing our baseline with GTRS under similar total number of parameters and FLOPs, we can draw the same conclusion that transformer architecture is more suitable than CNN-based architecture when considering model efficiency for pose-based human mesh recovery. Note here METRO <ref type="bibr" target="#b21">[22]</ref> and MeshGraphormer <ref type="bibr" target="#b22">[23]</ref> used 3DPW training set while others did not. Among those methods without using 3DPW training set, GTRS achieves comparable results with much less computational cost. When using ground truth 2D pose input, the performance of GTRS can be improved significantly. GTRS yields the lowest MPJPE of 53.8, PA-MPJPE of 34.5, and MPVPE of 61.6 (more than 24% reduction compared with MeshGraphormer <ref type="bibr" target="#b22">[23]</ref>). 3DPW dataset is a challenging in-the-wild dataset, distinct from the lab-controlled Human3.6M dataset. Occlusions and atypical human postures in these in-the-wild cases are the biggest challenges for accurate 3D human mesh recovery. Due to the difficulties of obtaining sufficient training data with accurate mesh annotations, these issues are difficult to address directly in 3D pose estimation. However, it is much easier to acquire sufficient training data with accurate 2D pose annotations. By simply plugging-in a robust and more accurate 2D pose detector in the future, GTRS can further improve the performance and approach the lower bound. Therefore, GTRS has the potential to continue staying relevant as an effective approach for 3D mesh reconstruction as 2D pose estimators inevitably improve.</p><p>Model Size and FLOPs Comparison: Previous human mesh reconstruction methods did not pay much attention to model efficiency. These methods mainly pursued higher accuracy without considering computation and memory. <ref type="table" target="#tab_2">Table 3</ref> reports the Params and FLOPs comparison between previous methods and GTRS. Posebased methods can easily select a lightweight 2D pose detector to reduce the computational burden. But for most image/video-based methods, a very large feature extractor is needed for extracting features (e.g. ResNet-50 is used by <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>). GTRS only requires 7.9M Params (10.2% of the Pose2Mesh <ref type="bibr" target="#b4">[5]</ref>) and 0.19G FLOPs (2.5% of the Pose2Mesh <ref type="bibr" target="#b4">[5]</ref>) while achieving better results than Pose2Mesh <ref type="bibr" target="#b4">[5]</ref> when using the same 2D pose input detected by DARK <ref type="bibr" target="#b54">[54]</ref>. When compared with our baseline, GTRS improves the performance while maintaining lower memory and computational cost. When we use Lite-HRNet <ref type="bibr" target="#b53">[53]</ref> as 2D pose detector, the overall Params is 9.7M and FLOPs is 0.89G. GTRS achieves close results with significant Params and FLOPs reduction (6.9% and 1.2% of I2LMeshNet <ref type="bibr" target="#b34">[34]</ref>, 16.5% and 9.2% of VIBE <ref type="bibr" target="#b17">[18]</ref>, 7.9%  and 8.6% of TCMR <ref type="bibr" target="#b3">[4]</ref>). It also can be observed that METRO <ref type="bibr" target="#b21">[22]</ref> and MeshGraphormer <ref type="bibr" target="#b21">[22]</ref> demanded an extremely large number of Params and FLOPs to achieve the SOTA results; they also used the 3DPW training set while other methods did not. Apart from that, METRO <ref type="bibr" target="#b21">[22]</ref> and MeshGraphormer <ref type="bibr" target="#b21">[22]</ref> were trained in 5 days on 8 NVIDIA V100 GPUs, which is incredibly time and resource consuming. GTRS, on the other hand, can be trained in less than one day on two NVIDIA RTX 3090 GPUs. To summarize, GTRS is much more time and resource efficient compared to image-based methods and SOTA pose-based method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>We conduct the ablation study on Human3.6M dataset (training on S1, S5, S6, S7, S8, and testing on S9 and S11) and report the accuracy using MPJPE and PA-MPJPE. Effectiveness of Using Graph Transformer Blocks: We investigate the use of graph transformer blocks in the PAM in <ref type="table" target="#tab_3">Table 4</ref>. All blocks are applied in parallel as shown in <ref type="figure">Fig 3.</ref> When we only use one fixed graph transformer block, the MPJPE is 68.0 and PA-MPJPE is 50.3. By adding learnable graph transformer blocks, and thereby enabling the exploration of implicit correlations, the performance improves. The network achieves the best results (MPJPE is 64.3 and PA-MPJPE is 47.5) when using one graph transformer block with a fixed adjacency matrix and five graph transformer blocks with learnable adjacency matrices, Next, to verify that the observed performance increase is not solely a matter of additional blocks, we also investigate fixing all adjacent matrices in the six graph transformer blocks. This enforces the graph transformer blocks only to learn with structured correlations of human kinematics. The MPJPE is 65.3 and PA-MPJPE is 48.4. On the contrary, we then try setting all adjacent matrices in the six graph transformer blocks to be learnable during the training. This allows the graph transformer blocks to learn implicit correlations of human kinematics, and results in a MPJPE of 66.4 and PA-MPJPE of 48.8. Lastly, we apply six pure transformer blocks (without any GCNs), which gives a MPJPE of 67.2 and PA-MPJPE of 49.5. All these results are worse than applying six graph transformer blocks (one with fixed adjacency matrix and five with learnable adjacency matrices), which verifies the effectiveness of using graph transformer blocks with both fixed and learnable adjacency matrices.</p><p>Impact of 2D Pose Detectors: In <ref type="table">Table 5</ref>, we analyze the impact of the quality of 2D pose on the final mesh performance. When using ground-truth 2D pose as input, GTRS can get 50.3 of MPJPE and 30.4 of PA-MPJPE, which outperform the Pose2Mesh <ref type="bibr" target="#b4">[5]</ref> results (51.3 of MPJPE and 34.9 of PA-MPJPE). Here, we see that accuracy of GTRS can be improved when using more precise 2D pose inputs. We also evaluate the performance of using different 2D pose detectors. Incorporated with <ref type="bibr" target="#b47">[47]</ref>, the MPJPE of GTRS is 64.3 and PA-MPJPE is 47.5, but the Params is 34M and FLOPs is 6.4G. When switching to a more lightweight 2D pose detector <ref type="bibr" target="#b53">[53]</ref>, the entire pipeline is more computational and memory efficient (Params is 9.7M and FLOPs is 0.9G) while preserving the accuracy.</p><p>Impact of 2D Pose Input during Inference: For testing in-thewild images, the quality of the 2D pose would be affected by various fluctuations based on the input image. We evaluate the robustness of our trained model against potential perturbations since GTRS relies on the input pose.</p><p>First, we evaluate if GTRS can still perform well when input joints are missing. During inference, we set a drop probability for the joints and the results are shown in <ref type="figure" target="#fig_5">Fig. 7 (a)</ref>. In PAM, we output the intermediate 3D pose for a 3D pose supervision, which enables GTRS can still achieve acceptable results given a large drop rate.</p><p>Second, we evaluate the robustness of GTRS to noisy 2D pose input. Specifically, we add Gaussian noise N (0, 2 ) to the input 2d pose to simulate in-the-wild 2D pose input during inference. We do not retrain the model (which is trained using GT pose), instead, we directly evaluate the performance of noisy pose input on the trained model. The results are shown in <ref type="figure" target="#fig_5">Fig. 7 (b)</ref>. GTRS consistently outperforms Pose2Mesh <ref type="bibr" target="#b4">[5]</ref>, demonstrating that GTRS is more robust to in-the-wild inference.</p><p>Qualitative Results: <ref type="figure">Fig. 6</ref> shows the qualitative results of GTRS on in-the-wild images from COCO dataset that GTRS can reconstruct acceptable human meshes. More qualitative results are in appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Mesh View 1 View 2 View 3 View 4 <ref type="figure">Figure 6</ref>: Qualitative results of the proposed GTRS. Images are taken from the in-the-wild COCO <ref type="bibr" target="#b24">[25]</ref> dataset. <ref type="table">Table 5</ref>: Ablation study on different 2D pose input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Input MPJPE? PA-MPJPE? Pose2Mesh <ref type="bibr" target="#b4">[5]</ref> GT 2D pose 51.3 34.9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GTRS (Ours)</head><p>GT 2D pose 50.3 30.4 Estimated 2D pose by <ref type="bibr" target="#b47">[47]</ref> 64.3 47.5 Estimated 2D pose by LiteHRNet <ref type="bibr" target="#b53">[53]</ref> 66.9 48.7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND DISCUSSION</head><p>We present a lightweight pose-based method, GTRS, for human mesh reconstruction from 2D human pose that reduces Params and FLOPs significantly. A PAM is introduced to exploit structured and implicit joint correlations by using paralleled graph transformers blocks. Then, a MRM is able to combine the extracted pose feature with the mesh template efficiently to reconstruct the human mesh. Despite GTRS achieving competitive performance, as a posebased approach, GTRS may not be able to recover varied human body shapes using only 2D human poses as input. Although imagebased methods have the potential to reconstruct a more accurate human mesh, pose-based methods are still worth investigating due to their flexibility and lightweight design. In the future, we intend to include another branch that extracts human shape features from  images to improve reconstruction capability while keeping the model structure lightweight. Acknowledgement. This work is supported by the National Science Foundation under Grant No. 1910844</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 Detailed overview of the proposed GTRS architecture</head><p>The detailed overview of GTRS is illustrated in <ref type="figure">Fig. 8</ref>. Given the input image, 2D human pose _2 ? R 17?2 is first detected by an off-the-shelf 2D pose detector, then a feature embedding layer (an MLP layer) embeds input 2D pose to ? R 17?128 . In the pose analysis module, the pose feature ? R 17?128 goes through each parallel graph transformer block without changing the size. A fusion block, consists of convolutional layers, is designed to aggregate these pose features back to ? ? R 17?128 . Before mesh regression module, the mesh template ? R 6890?3 is embedded to the template feature ? R 15?128 . In the mesh regression module, the pose feature ? and template feature are modeled separately, then concatenated to the mesh feature ? ? R 32?128 for a large transformer modeling. The output would be ? R 32?128 . Finally, the estimated mesh parameter ? R 6890?3 can be obtained by the regression head (an MLP layer) for human mesh reconstruction. Our transformer encoder layer is different from the original transformer encoder in <ref type="bibr" target="#b6">[7]</ref>. The structure of our transformer encoder is illustrated in <ref type="figure">Fig. 9 (c)</ref>. If we use the original transformer block in <ref type="figure">Fig. 9 (a)</ref>, the MPJPE is 65.2 and PA-MPJPE is 47.7 as shown in <ref type="table" target="#tab_4">Table 6</ref>. Compared with the original transformer blocks, we add one convolutional branch parallel to the MHA branch as shown in <ref type="figure">Fig. 9</ref> (b). We use this pointwise convolution to create linear combinations of the joint channels while maintaining a low computational cost. The performance has improved since MPJPE is decreased to 64.5 and PA-MPJPE is decreased to 47.5. Then, we replace the MLP with a lightweight SE block <ref type="bibr" target="#b11">[12]</ref> as shown in <ref type="figure">Fig. 9</ref> (c) and achieve the best performance (MPJPE is 64.3 and PA-MPJPE is 47.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Design options of transformer block</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Different locations to inject GCN.</head><p>In GTRS, we utilize GCNs to maintain a strong joint relationship based on human kinematic information. We inject structural priors provided by GCNs before performing the transformer's multi-head self-attention (MSA) as shown in <ref type="figure" target="#fig_7">Fig. 10 (a)</ref>. We also investigate other locations to inject GCNs. The results are reported in <ref type="table" target="#tab_5">Table  7</ref>. When GCN is between the MSA and SE block as illustrated in <ref type="figure" target="#fig_7">Fig. 10 (b)</ref>, the MPJPE is 64.9 and PA-MPJPE is 48.2. When GCN is behind the SE block in <ref type="figure" target="#fig_7">Fig. 10 (c)</ref>, the MPJPE is 65.2 and PA-MPJPE is 48.3. We observe that GCN before the MHA achieves the best performance (MPJPE is 64.3 and PA-MPJPE is 47.5), indicating that GCNs do provide structural priors to help final mesh regression. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Impact of different backbones</head><p>As a pose-based method, GTRS can easily choose a lightweight 2D pose detector to reduce the computational burden. But for most image/video-based methods, a very large feature extractor is needed for extracting features (e.g. ResNet-50 is used by <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>). We retrain the I2LMeshNet <ref type="bibr" target="#b34">[34]</ref> with a small backbone (ResNet18), the results are shown in <ref type="table" target="#tab_6">Table 8</ref>. Although selecting a small backbone can reduce the total Params and FLOPs, the performance also drops. GTRS with DARK <ref type="bibr" target="#b54">[54]</ref> is much more time and resource efficient compared to I2LMeshNet. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Impact of different losses</head><p>We apply multiple loss introduced in Section 3.6 when training GTRS. Here we evaluate the impact of different losses combination in Table. Without 3D joint supervision, the MPJPE is 71.9 and MPVE is 86.3 (2D pose is detected by <ref type="bibr" target="#b42">[42]</ref>). After adding 3D joint supervision, the performances are boosted to 65.3 of MPJPE and 83.4 of MPVE. The normal loss and edge loss can further improve the performance slightly as shown in the table below.</p><p>A.6 Evaluation on the inference speed  We compare the inference speed between GTRS (end-to-end) and the state-of-the-art image-based mesh reconstruction method METRO <ref type="bibr" target="#b21">[22]</ref>. The frame per second (fps) is reported in <ref type="table" target="#tab_5">Table 7</ref>. We use a single NVIDIA RTX 3090 GPU and an AMD Ryzen 3970X 32-Core Processor CPU for testing. Our proposed GTRS is a posebased method, which means any off-the-shelf 2D pose detector can be easily adopted. Here we use the lightweight OpenPose <ref type="bibr" target="#b39">[39]</ref> as the 2D pose detector. The fps numbers of lightweight OpenPose on the testing GPU and CPU are 133.73 and 19.87, respectively. For GTRS, the fps numbers on the GPU and CPU are 32.41 and 24.11, respectively. Overall, GTRS can achieve 26.27 fps on GPU and 10.86 fps on CPU, which is much faster than METRO (the fps is 15.65 on GPU and 1.81 on CPU) on the same computing hardware. Compared to METRO, GTRS gains more advantages on resource-constrained devices since it is significantly more computational efficient.</p><p>A.7 More qualitative results of GTRS In <ref type="figure">Fig. 11</ref>, we show the qualitative results of GTRS on in-the-wild images. We observe that GTRS achieves acceptable performance by reconstructing reasonable human mesh on these challenging in-thewild cases. However, there are still some failure cases of our method due to challenging pose and heavy occlusion as shown in <ref type="figure" target="#fig_0">Fig. 12</ref>.</p><p>We also compare with image-based method I2LMeshNet <ref type="bibr" target="#b34">[34]</ref> in <ref type="figure">Fig. 13</ref>. Our GTRS is comparable with I2LMeshNet while reducing memory and computational cost significantly (only 6.9 % Params and 1.2 % FLOPs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Mesh View 1 View 2 View 3 View 4 <ref type="figure">Figure 11</ref>: Qualitative results of the proposed GTRS on in-the-wild images. Images are taken from MSCOCO <ref type="bibr" target="#b24">[25]</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Overview of the proposed GTRS architecture. Given the image, 2D human pose is first detected by an off-the-shelf 2D pose detector. Then, the Pose Analysis Module outputs the pose feature and intermediate 3D pose which is supervised by the ground truth 3D pose. Next, the pose feature is modeled with template mesh feature in the Mesh Regression Module. Finally, a regression head will output human mesh parameters for reconstruction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>The Pose Analysis Module architecture is shown in (a), which consists of paralleled graph transformer blocks. 'fixed adj' means a fixed adjacency matrix is used in this graph transformer block, and 'learnable adj' means a learnable adjacency matrix is used. (b) is the architecture of one graph transformer block. The normalized adjacency matrices used in PAM. (a) is the joint index of Human3.6M dataset. (b) is a fixed adjacency matrix directly from the joint connectivity to model structured correlations. (c) shows learnable adjacency matrices that learned from training data to capture implicit correlations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>The Mesh Regression Module architecture is shown in (a), which is consists of dual-branch transformer blocks. (b) is the architecture of one transformer block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>(a) Impact of missing joints during inference. Each joint has a drop probability to simulate missing joints by the 2D pose detector during inference. (b) Impact of noisy 2D pose input during inference. Various degrees of Gaussian noise (i.e. variance 2 ) are added to the input 2d pose to simulate in-thewild 2D pose input during inference. 2 = 0 means GT pose provided in the Human3.6M dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Overview of the proposed GTRS architecture. The mesh template figure is from<ref type="bibr" target="#b21">[22]</ref>. Three design choices for building our proposed transformer block. (a) is the original transformer block from<ref type="bibr" target="#b6">[7]</ref>. (b) is the transformer block with adding a convolutional branch parallel to the MHA branch. (c) is the transformer block that replaces MLP by SE block based on (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Three different locations to inject GCN to transformer blocks. (a) GCN is in front of the MHA block. (b) GCN is between the MHA and SE block. (c) GCN is behind the SE block. Results are evaluated on Human3.6M dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 :Figure 13 :</head><label>1213</label><figDesc>Failure cases due to challenging pose and heavy occlusion. Qualitative comparison with image-based method I2LMeshNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison with SOTA methods on Hu-man3.6M. " ?" denotes using GT (ground truth) 2D pose as input.</figDesc><table><row><cell></cell><cell>Methods</cell><cell></cell><cell cols="2">Human3.6M MPJPE? PA-MPJPE?</cell></row><row><cell></cell><cell>HMR [15]</cell><cell>CVPR 2018</cell><cell>88.0</cell><cell>56.8</cell></row><row><cell></cell><cell>GraphCMR [20]</cell><cell>CVPR 2019</cell><cell>-</cell><cell>50.1</cell></row><row><cell>Image</cell><cell>SPIN [19]</cell><cell>ICCV 2019</cell><cell>-</cell><cell>41.1</cell></row><row><cell>Based</cell><cell>METRO [22]</cell><cell>CVPR 2021</cell><cell>54.0</cell><cell>36.7</cell></row><row><cell></cell><cell>MeshGraphormer [23]</cell><cell>ICCV 2021</cell><cell>51.2</cell><cell>34.5</cell></row><row><cell></cell><cell>PyMAF [55]</cell><cell>ICCV 2021</cell><cell>57.7</cell><cell>40.5</cell></row><row><cell>Video</cell><cell>VIBE [18]</cell><cell>CVPR 2020</cell><cell>65.6</cell><cell>41.4</cell></row><row><cell>Based</cell><cell>TCMR [4]</cell><cell>CVPR 2021</cell><cell>62.3</cell><cell>41.1</cell></row><row><cell>Pose</cell><cell>Pose2Mesh [5]</cell><cell>ECCV 2020</cell><cell>64.9</cell><cell>47.0</cell></row><row><cell>Based</cell><cell>Baseline</cell><cell>-</cell><cell>68.3</cell><cell>50.0</cell></row><row><cell>(detected by [46])</cell><cell>GTRS</cell><cell>-</cell><cell>64.3</cell><cell>45.4</cell></row><row><cell>Pose</cell><cell>Pose2Mesh [5] ?</cell><cell>ECCV 2020</cell><cell>51.3</cell><cell>34.9</cell></row><row><cell>Based</cell><cell>Baseline ?</cell><cell>-</cell><cell>57.5</cell><cell>39.6</cell></row><row><cell>(GT 2D)</cell><cell>GTRS ?</cell><cell>-</cell><cell>50.3</cell><cell>30.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison with SOTA methods on 3DPW dataset. * indicates 3DPW training set is used during training. " ?" denotes using GT 2D pose as input.</figDesc><table><row><cell></cell><cell>Methods</cell><cell></cell><cell>MPJPE?</cell><cell>3DPW dataset PA-MPJPE?</cell><cell>MPVE?</cell></row><row><cell></cell><cell>HMR [15]</cell><cell>CVPR 2018</cell><cell>-</cell><cell>81.3</cell><cell>-</cell></row><row><cell></cell><cell>GraphCMR [20]</cell><cell>CVPR 2019</cell><cell>-</cell><cell>70.2</cell><cell>-</cell></row><row><cell>Image</cell><cell>SPIN [19]</cell><cell>ICCV 2019</cell><cell>-</cell><cell>59.2</cell><cell>116.4</cell></row><row><cell>Based</cell><cell>I2LMeshNet [34]</cell><cell>ECCV 2020</cell><cell>93.2</cell><cell>57.7</cell><cell>-</cell></row><row><cell></cell><cell>PyMAF [55]</cell><cell>ICCV 2021</cell><cell>92.8</cell><cell>58.9</cell><cell>110.1</cell></row><row><cell></cell><cell>MeshGraphormer* [23]</cell><cell>ICCV 2021</cell><cell>74.7</cell><cell>45.6</cell><cell>87.7</cell></row><row><cell>Video Based</cell><cell>VIBE [18] VIBE* [18] TCMR [4]</cell><cell>CVPR 2020 CVPR 2021 CVPR 2021</cell><cell>93.5 82.9 95.0</cell><cell>56.5 51.9 55.8</cell><cell>113.4 99.1 111.5</cell></row><row><cell>Pose</cell><cell>Pose2Mesh [5]</cell><cell>ECCV 2020</cell><cell>88.9</cell><cell>58.3</cell><cell>106.3</cell></row><row><cell>Based</cell><cell>Baseline</cell><cell>-</cell><cell>95.6</cell><cell>61.3</cell><cell>129.8</cell></row><row><cell>(detected by [54])</cell><cell>GTRS</cell><cell>-</cell><cell>88.5</cell><cell>58.9</cell><cell>106.2</cell></row><row><cell>Pose</cell><cell>Pose2Mesh [5] ?</cell><cell>ECCV 2020</cell><cell>65.1</cell><cell>34.6</cell><cell>-</cell></row><row><cell>Based</cell><cell>Baseline ?</cell><cell>-</cell><cell>67.7</cell><cell>36.1</cell><cell>70.3</cell></row><row><cell>(GT 2D)</cell><cell>GTRS ?</cell><cell>-</cell><cell>53.8</cell><cell>34.5</cell><cell>61.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The Params and FLOPs comparison on 3DPW dataset. * indicates 3DPW training set is used during training.</figDesc><table><row><cell></cell><cell>Methods</cell><cell></cell><cell cols="2">Feature extractor Params (M) FLOPs (G)</cell><cell cols="2">Proposed model Params (M) FLOPs (G)</cell><cell cols="2">Overall Params (M) FLOPs (G)</cell><cell>MPJPE on 3DPW</cell></row><row><cell>Image Based</cell><cell>I2LMeshNet [34] METRO* [22] MeshGraphormer* [23]</cell><cell>ECCV2020 CVPR2021 ICCV2021</cell><cell>---</cell><cell>---</cell><cell>---</cell><cell>---</cell><cell>140.5 229.2 226.5</cell><cell>73.2 56.6 56.6</cell><cell>93.2 77.1 74.7</cell></row><row><cell>Video</cell><cell>VIBE [18]</cell><cell>CVPR2020</cell><cell>25.6</cell><cell>8.2</cell><cell>33.0</cell><cell>1.3</cell><cell>58.6</cell><cell>9.6</cell><cell>93.5</cell></row><row><cell>Based</cell><cell>TCMR [4]</cell><cell>CVPR2021</cell><cell>25.6</cell><cell>8.2</cell><cell>97.3</cell><cell>2.1</cell><cell>122.9</cell><cell>10.3</cell><cell>95.0</cell></row><row><cell></cell><cell>Methods</cell><cell></cell><cell cols="2">2D pose detector Param (M) FLOPs (G)</cell><cell cols="2">Proposed model Param (M) FLOPs (G)</cell><cell cols="2">Overall Param (M) FLOPs (G)</cell><cell>MPJPE on 3DPW</cell></row><row><cell></cell><cell>Pose2Mesh with DARK [54]</cell><cell>ECCV2020</cell><cell>63.6</cell><cell>3.6</cell><cell>77.1</cell><cell>7.5</cell><cell>140.7</cell><cell>11.1</cell><cell>88.9</cell></row><row><cell></cell><cell>Baseline with DARK [54]</cell><cell>-</cell><cell>63.6</cell><cell>3.6</cell><cell>8.3</cell><cell>0.32</cell><cell>71.9</cell><cell>3.9</cell><cell>97.7</cell></row><row><cell>Pose</cell><cell>GTRS with DARK [54]</cell><cell>-</cell><cell>63.6</cell><cell>3.6</cell><cell>7.9</cell><cell>0.19</cell><cell>71.5</cell><cell>3.8</cell><cell>88.5</cell></row><row><cell>Based</cell><cell>Pose2Mesh with LiteHRNet [53]</cell><cell>ECCV2020</cell><cell>1.8</cell><cell>0.7</cell><cell>77.1</cell><cell>7.5</cell><cell>78.9</cell><cell>8.2</cell><cell>-</cell></row><row><cell></cell><cell>Baseline with LiteHRNet [53]</cell><cell>-</cell><cell>1.8</cell><cell>0.7</cell><cell>8.3</cell><cell>0.32</cell><cell>11.1</cell><cell>1.1</cell><cell>103.4</cell></row><row><cell></cell><cell>GTRS with LiteHRNet [53]</cell><cell>-</cell><cell>1.8</cell><cell>0.7</cell><cell>7.9</cell><cell>0.19</cell><cell>9.7</cell><cell>0.89</cell><cell>93.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on different components in PAM.</figDesc><table><row><cell></cell><cell>Architecture in PAM</cell><cell></cell><cell></cell></row><row><cell>graph transformer blocks with fixed adj</cell><cell>graph transformer blocks with learnable adj</cell><cell>MPJPE?</cell><cell>PA-MPJPE?</cell></row><row><cell>1</cell><cell>0</cell><cell>68.0</cell><cell>50.3</cell></row><row><cell>1</cell><cell>1</cell><cell>66.9</cell><cell>48.9</cell></row><row><cell>1</cell><cell>3</cell><cell>65.1</cell><cell>48.2</cell></row><row><cell>1</cell><cell>5</cell><cell>64.3</cell><cell>47.5</cell></row><row><cell>1</cell><cell>7</cell><cell>64.6</cell><cell>47.6</cell></row><row><cell>6</cell><cell>0</cell><cell>65.3</cell><cell>48.4</cell></row><row><cell>0</cell><cell>6</cell><cell>66.4</cell><cell>48.8</cell></row><row><cell cols="2">pure transformer blocks</cell><cell></cell><cell></cell></row><row><cell>6</cell><cell></cell><cell>67.2</cell><cell>49.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Ablation study on different design options of transformer block. Results are evaluated on Human3.6M dataset.</figDesc><table><row><cell></cell><cell>MPJPE?</cell><cell>PA-MPJPE?</cell></row><row><cell>Original</cell><cell>65.2</cell><cell>47.7</cell></row><row><cell>Add CONV</cell><cell>64.5</cell><cell>47.5</cell></row><row><cell>Add CONV and replace MLP by SE</cell><cell>64.3</cell><cell>47.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Ablation study on different locations to inject GCN. Results are evaluated on Human3.6M dataset.</figDesc><table><row><cell>MPJPE?</cell><cell>PA-MPJPE?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Impact of different backbones. Results are evaluated on 3DPW dataset.</figDesc><table><row><cell></cell><cell>Backbone</cell><cell>total Params(M)</cell><cell>FLOPs(G)</cell><cell>MPJPE</cell><cell>PA-MPJPE</cell></row><row><cell>I2LMeshNet</cell><cell>ResNet50</cell><cell>140.5</cell><cell>73.2</cell><cell>93.2</cell><cell>57.7</cell></row><row><cell>I2LMeshNet (small)</cell><cell>ResNet18</cell><cell>94.6</cell><cell>68.3</cell><cell>101.8</cell><cell>62.9</cell></row><row><cell>GTRS with DARK [54]</cell><cell></cell><cell>71.5</cell><cell>3.8</cell><cell>88.5</cell><cell>58.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 :</head><label>9</label><figDesc>Comparison of the inference speed. The frame per second (fps) is obtained by using batch size 1 on a single GPU/CPU.</figDesc><table><row><cell cols="2">Human3.6M Human3.6M</cell></row><row><cell>MPJPE</cell><cell>MPVE</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 :</head><label>10</label><figDesc>Comparison of the inference speed. The frame per second (fps) is obtained by using batch size 1 on a single GPU/CPU.</figDesc><table><row><cell></cell><cell></cell><cell>FPS on GPU</cell><cell></cell><cell></cell><cell>FPS on CPU</cell><cell></cell></row><row><cell></cell><cell>2D pose detection</cell><cell>3D mesh regression</cell><cell>overall</cell><cell>2D pose detection</cell><cell>3D mesh regression</cell><cell>overall</cell></row><row><cell>METRO [22]</cell><cell>-</cell><cell>-</cell><cell>15.65</cell><cell>-</cell><cell>-</cell><cell>1.81</cell></row><row><cell>GTRS</cell><cell>133.73</cell><cell>32.41</cell><cell>26.17</cell><cell>19.87</cell><cell>24.11</cell><cell>10.86</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Beyond Static Features for Temporally Consistent 3D Human Pose and Shape From a Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsuk</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1964" to="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pose2Mesh: Graph Convolutional Network for 3D Human Pose and Mesh Recovery from a 2D Human Pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsuk</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Optimizing Network Structure for 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<title level="m">Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqin</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generalized procrustes analysis: a tool for exploring aggregates and persons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimberly</forename><forename type="middle">K</forename><surname>Grice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Assad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Multivariate Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="93" to="112" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian Error Linear Units (GELUs)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<title level="m">Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments. IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2014-07" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Coherent Reconstruction of Multiple Humans From a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end Recovery of Human Shape and Pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">VIBE: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to Reconstruct 3D Human Pose and Shape via Model-Fitting in the Loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Convolutional Mesh Regression for Single-Image Human Shape Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3d and 2d human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6050" to="6059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-end human pose and mesh reconstruction with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1954" to="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">2021. Mesh Graphormer. In ICCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mesh Graphormer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12939" to="12948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Comprehensive Study of Weight Sharing in Graph Networks for 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenkun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongqi</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sen-ching Cheung, and Vijayan Asari. 2020. Attention mechanism exploits temporal contexts: Real-time 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="page" from="5064" to="5073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<title level="m">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">MoSh: Motion and shape capture from sparse markers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">SMPL: A Skinned Multi-Person Linear Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">AMASS: Archive of Motion Capture as Surface Shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nikolaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Troje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Single-Shot Multi-Person 3D Pose Estimation From Monocular RGB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<ptr target="http://gvv.mpi-inf.mpg.de/projects/SingleShotMultiPerson" />
		<title level="m">Sixth International Conference on. IEEE</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">I2L-MeshNet: Image-to-Lixel Prediction Network for Accurate 3D Human Pose and Mesh Estimation from a Single RGB Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">EfficientHRNet: efficient and scalable high-resolution networks for real-time multi-person 2D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Neff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aneri</forename><surname>Sheth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Furgurson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Middleton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Tabkhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Real-Time Image Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Neural Body Fitting: Unifying Deep Learning and Model-Based Human Pose and Shape Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">STAR: A Spare Trained Articulated Human Body Regressor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Real-time 2d multi-person pose estimation on CPU</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniil</forename><surname>Osokin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12004</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Lightweight OpenPose. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Real-time 2D Multi-Person Pose Estimation on CPU: Lightweight OpenPose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniil</forename><surname>Osokin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12004</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to Estimate 3D Human Pose and Shape from a Single Color Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semisupervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7753" to="7762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Towards Fast and Accurate Multi-Person Pose Estimation on Mobile Devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiexiong</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhi</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.15304</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Total Capture: 3D Human Pose Estimation Fusing Video and Inertial Sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Malleson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Collomosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Recovering Accurate 3D Human Pose in The Wild Using IMUs and a Moving Camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pixel2mesh: Generating 3d mesh models from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="52" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">3D Human Shape and Pose from a Single Low-Resolution Image with Self-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>L?szl?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Nong Sang, and Jingdong Wang. 2021. Lite-HRNet: A Lightweight High-Resolution Network</title>
		<imprint/>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Distribution-Aware Coordinate Representation for Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">PyMAF: 3D Human Pose and Shape Regression with Pyramidal Mesh Alignment Feedback Loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yating</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Semantic Graph Convolutional Networks for 3D Human Pose Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubbasir</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">GraFormer: Graph Convolution Transformer for 3D Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqiang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.08364</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">3D Human Pose Estimation With Spatial and Temporal Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matias</forename><surname>Mendieta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taojiannan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengming</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11656" to="11665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6881" to="6890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Detailed human shape estimation from a single image by hierarchical mesh deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Modulated Graph Convolutional Network for 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11477" to="11487" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
