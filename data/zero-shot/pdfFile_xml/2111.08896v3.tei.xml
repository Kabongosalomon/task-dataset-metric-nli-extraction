<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ACHIEVING HUMAN PARITY ON VISUAL QUESTION ANSWERING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Bi</surname></persName>
							<email>b.bi@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhe</forename><surname>Xu</surname></persName>
							<email>xianzhe.xxz@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wang</surname></persName>
							<email>fan.w@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Zhang</surname></persName>
							<email>zhangzhicheng.zzc@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiyu</forename><surname>Zhang</surname></persName>
							<email>qiyu.zhang@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
							<email>songfang.hsf@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
							<email>f.huang@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
							<email>luo.si@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Jin</surname></persName>
							<email>jinrong.jr@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ACHIEVING HUMAN PARITY ON VISUAL QUESTION ANSWERING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Visual Question Answering (VQA) task utilizes both visual image and language analysis to answer a textual question with respect to an image. It has been a popular research topic with an increasing number of real-world applications in the last decade. This paper describes our recent research of AliceMind-MMU (ALIbaba's Collection of Encoder-decoders from Machine IntelligeNce lab of Damo academy -MultiMedia Understanding) that obtains similar or even slightly better results than human being does on VQA. This is achieved by systematically improving the VQA pipeline including: (1) pre-training with comprehensive visual and textual feature representation; (2) effective cross-modal interaction with learning to attend; and (3) A novel knowledge mining framework with specialized expert modules for the complex VQA task. Treating different types of visual questions with corresponding expertise needed plays an important role in boosting the performance of our VQA architecture up to the human level. An extensive set of experiments and analysis are conducted to demonstrate the effectiveness of the new research work.</p><p>Despite immense progress on VQA in the research community over the past years, human parity has remained out of reach even though the gap is reduced significantly over the last a few years. This paper describes our efforts to achieve the unprecedented human-level performance on the VQA task by systematically improving the components of the VQA architecture, which is illustrated in <ref type="figure">Figure 2</ref>. This work addresses a number of limitations of existing VQA studies, and make the following major contributions: 1. Most existing VQA approaches rely on a single class of features to represent visual signals. The homogeneous feature representation is insufficient to capture the diversity of visual signal needed to answer open-ended Pre-training Tasks The pre-training tasks of the three types (language, vision and cross-modality) are introduced in the pre-training stage, following LXMERT [Tan and Bansal, 2019].</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent years have witnessed human-level performance reached or surpassed by well trained computer programs in tasks ranging from games such as Go <ref type="bibr" target="#b0">[Silver et al., 2016]</ref> to classification of images in ImageNet <ref type="bibr" target="#b1">[Deng et al., 2009a</ref>] to natural language understanding on the GLUE benchmark <ref type="bibr" target="#b2">[Wang et al., 2018]</ref>. Vision and language are two fundamental capabilities of human intelligence. We have seen dramatic progress in the area of representation learning across these two modalities. Inspired by the success of pre-training in both computer vision (CV) <ref type="bibr" target="#b3">[Sharif Razavian et al., 2014]</ref> and natural language processing (NLP) <ref type="bibr" target="#b4">[Devlin et al., 2018]</ref>, a number of vision-and-language (V&amp;L) models have been proposed in the last couple of years to tackle challenges at the intersection of these two key areas of AI. Despite superhuman performance achieved respectively in some vision (e.g., ImageNet) and natural language tasks (e.g., GLUE), joint learning across these two modalities, which is essential to human cognition, has demonstrated limited human-level performance by prevalent V&amp;L approaches.</p><p>A compelling reason to study vision and language jointly is the promise of language as a universal and natural interface for visual reasoning problems -useful both in specifying a wide range of problems and in communicating AI responses. Visual reasoning has long been recognized most challenging for cross-modal learning owing to its requirement of higherorder cognition and commonsense reasoning intelligence. With the systematic design of our reasoning architecture, this research work unprecedentedly surpasses human performance in the popular Visual Question Answering (VQA) task <ref type="bibr" target="#b5">[Agrawal et al., 2017]</ref>.</p><p>This paper summarizes how we achieve the human parity in VQA. Most of the presented techniques are not specific to VQA and can be transferable to tackling other visual reasoning tasks. Our work builds upon the significant progress made on CV and NLP over the past few decades.  <ref type="figure" target="#fig_0">Figure 1</ref>. The open-ended questions require a potentially vast set of AI capabilities to answer, including question understanding, commonsense reasoning, activity recognition, object counting, and visually-grounded language understanding, etc.. Therefore, achieving human performance in VQA would be an important milestone in artificial intelligence. In pursuit of this goal, a new VQA architecture is designed by improving the individual capabilities of visual reasoning. Different from the prevalent VQA methods that rely on a single kind of features with standard Transformer, the new VQA architecture exploits more comprehensive visual and textual feature representation with pre-training, and more effective cross-modal interaction with learning to attend.</p><p>The key to our success in VQA is tackling the diverse challenges with different capabilities. In particular, we introduce a novel knowledge mining framework with the Mixture-of-Experts (MoE) model for the complex VQA task. Most existing methods for VQA treat different types of visual questions in the same manner. Different from these methods, following the divide-and-conquer strategy, our new framework first decomposes the complex VQA task into different sub-tasks by a clustering-based method, which allows to identify the types of questions difficult to address. Each type of these questions are then resolved by a specialized expert module. All these expert modules are put together by the MoE paradigm. Beyond a patchwork of models like ensemble, the MoE paradigm learns which module to call upon by exploiting the expertise of each module, and thus intelligently delegates every question to a proper expert module. According to our quantitative analysis, the new knowledge mining with MoE plays an important role in boosting the performance of our VQA architecture up to the human level, significantly outperforming existing methods without explicit task decomposition by a large margin.</p><p>The rest of this paper is organized as follows. Section 2 presents the design of our new VQA architecture with the knowledge mining framework. The empirical evaluation and quantitative analysis are given in Section 3. Section 4 introduces the prior work related to VQA. Finally, the paper is concluded in Section 5 by discussing our findings and limitations. questions as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. To address this limitation, our VQA architecture exploits a diverse set of visual representations: region features, grid features and patch features, each of which is used to capture visual signals of a specific type. Specifically, region features are good at locating the salient objects in the image, e.g., slices of pizza in the second case of <ref type="figure" target="#fig_0">Figure 1</ref>, which is more suitable in tasks like object counting. Grid features are more skilled in the global or background information in the image, e.g., in the fourth case of <ref type="figure" target="#fig_0">Figure 1</ref>, where background information of weather is identified. With the heterogeneous feature set, the model is able to answer different types of questions by leveraging desired visual signals.</p><p>2. The semantic gap between visual and textual modalities has always been treated as one of the most important challenges in cross-modality research. There exist two families of V&amp;L models that bridge the cross-modal semantic gap: single-stream architecture <ref type="bibr" target="#b6">[Su et al., 2019</ref> and dual-stream architecture <ref type="bibr" target="#b8">[Tan and</ref><ref type="bibr">Bansal, 2019, Yu et al., 2021]</ref>. The single-stream architecture essentially treats the two input modalities equally, and thus does not make full use of the signal from each modality. On the other hand, the dual-stream architecture is insufficient to capture the fine-grained interaction between visual and textual hints. To address the limitations of these architectures, our VQA architecture fuses visual and textual modalities by learning how the features should attend to each other. To allow for fine-grained cross-modal interaction, our model is built upon the single-stream architecture. The original self-attention in Transformer is replaced with a weighted self-attention, where intra-modal attention and inter-modal attention are dynamically adjusted. This leads to effective alignment of the cross-modal semantics.</p><p>3. In the VQA task, visual questions are open-ended and require various kinds of knowledge and capabilities to answer. We, therefore, propose a new knowledge mining framework with MoE to address the diverse questions.</p><p>Besides the general V&amp;L understanding and reasoning in VQA, the new framework is able to identify two types of questions (text-reading questions and clock-reading questions) that are difficult to address by the general-purpose VQA techniques, due to the lack of text reading and clock reading abilities. A specialized expert module is introduced for each of the two question types: 1) Text Reading Expert: answering questions by reasoning about text in images. 2) Clock Reading Expert: answering questions on the time shown by clocks. An outstanding VQA architecture needs to appropriately mix the multiple experts to delegate each question to a proper expert module. Hence our VQA architecture employs the MoE paradigm to distill the expert knowledge required to answer each type of questions and mix the results of the experts to derive final answers. In the future, we will exploit techniques that automatically discover challenging case sets. New expert modules devoted to the discovered cases are then autonomously learned from specialized data. This learn-and-evolve process will lead to evolutionary intelligence that can rapidly adapt to any new task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Comprehensive Feature Representation</head><p>Feature representation for vision and text is fundamental for cross-modal learning. Different kinds of features can help capture diverse data characteristics, which complements with each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Visual Features</head><p>For comprehensive visual feature understanding, three kinds of visual features are considered: region feature, grid feature and patch feature.</p><p>Region Feature With the discovery of 'bottom-up' attention <ref type="bibr" target="#b10">[Anderson et al., 2018]</ref>, region-based visual features have been the de facto standard for vision and language tasks. Unlike normal 'top-down' attention that directly focuses on semantically irrelevant parts of visual input, bottom-up attention uses pre-trained object detectors <ref type="bibr" target="#b11">[Ren et al., 2015]</ref> to identify salient regions based on the visual input. As a result, images are represented by a collection of region-based features, which provide better localization of individual objects and capture the detailed semantics within the image content. Generally, region-based visual encoders such as BUTD <ref type="bibr" target="#b10">[Anderson et al., 2018]</ref>   <ref type="figure">Figure 3</ref>: The framework of ALICEMIND-MMU .</p><p>data like Visual Genome <ref type="bibr" target="#b12">[Krishna et al., 2017]</ref>. Recently, VinVL  has been built on a large-scale pre-trained object-attribute detection model with much larger amounts of data on four public object detection datasets, which helps better capture both coarse-level and fine-grained visual semantic information in images. The object detector from VinVL  is used in this work to extract a collection of object-level region features with more detailed visual semantics, where each object o j is represented as a 2048-dimensional feature vector r j . To capture the spatial information of the object, box-level location features for each object are also encoded via a 4-dimensional vector l j = ( x1 W , y1 H , x2 W , y2 H ) as in SemVLP <ref type="bibr" target="#b14">[Li et al., 2021a]</ref> and LXMERT <ref type="bibr" target="#b8">[Tan and Bansal, 2019]</ref>. The r j and l j are concatenated to form a position-sensitive object feature vector, which is further transformed to a lower dimension of D using a linear projection to ensure that it has the same vector dimension as that of token embeddings.</p><p>Despite the superior performance obtained via region-based visual features, this kind of features suffer from several problems. Firstly, the region-based methods heavily rely on a pre-trained object detector, where the performance may be bounded by the capability of the object detector and its predefined visual vocabulary. Besides, only salient regions of image are used in region-based methods, where the global or background information may be missing.</p><p>Grid Feature To address the limitations of region-based features like locality, some work such as PixelBERT <ref type="bibr" target="#b15">[Huang et al., 2020]</ref>, E2E-VLP , Grid-VLP  and <ref type="bibr" target="#b18">[Jiang et al., 2020]</ref> have been proposed to revisit grid-based convolutional features for multi-modal learning, skipping the expensive region-related steps. The advantage lies in that: 1) the grid-based feature allows to introduce more flexible architectural designs for vision and language tasks, which makes it possible to support end-to-end training and efficient online inference; 2) it operates on a full image instead of a collection of semantic regions, so it can better capture global information of an image such as the background information; 3) it does not rely on a pre-trained object detector with limited visual vocabulary. Specifically, starting from the raw image I img ? R 3?H0?W0 with 3 color channels, a fixed CNN-based image encoder such as ResNet  generates a lower-resolution activation map F img ? R C?H?W , where C is the channel width and H = H0 32 , W = W0 32 . As the cross-modal fusion network expects a sequence as input, the spatial dimensions of F img are collapsed into one dimension, resulting in a HW ? C feature map. Finally, a linear projection layer is used to reduce the channel dimension of the high-level feature map from C to a smaller dimension D for matching the dimension of token embeddings. To distinguish between different modalities, the grid feature map is supplemented with a learnable modal type embedding which is added to the output of linear projection layer.</p><p>To generate good grid features, it is very important to pre-train a strong CNN-based image encoder, to which the visual semantic information is incorporated. In terms of the data used to pre-train the image encoder, there are mainly two ways along this line: 1) Supervised Pre-training: the image encoder is pre-trained with image classification data such as ImageNet <ref type="bibr" target="#b20">[Deng et al., 2009b]</ref> or detection data such as Visual Genome <ref type="bibr" target="#b12">[Krishna et al., 2017]</ref>. As found in <ref type="bibr" target="#b18">[Jiang et al., 2020]</ref>, the large-scale object and attribute annotations collected in the Visual Genome are very helpful to provide the grid feature with visual semantics incorporated; 2) Unsupervised Pre-training: the image encoder is pre-trained with a large amount of unlabeled image-text pairs without human supervision such as CLIP <ref type="bibr" target="#b21">[Radford et al., 2021]</ref>, where about 400M aligned image-text pairs are used. It belongs to the line of research that learns visual representations from natural language supervision <ref type="bibr">[Jia et al., 2021, Desai and</ref><ref type="bibr" target="#b23">Johnson, 2021]</ref>. In this way, the image encoder is naturally aligned with the textual semantics to facilitate the cross-modal fusion. It is well recognized that fully supervised pre-trained CNN model shows promising performance on in-domain or near-domain datasets, while it may not yield best performance when coming to transfer learning on out-domain datasets. Features derived from different ways can well complement with each other, which adapts to different kinds of questions.</p><p>Patch Feature Vision Transformer (ViT) <ref type="bibr" target="#b24">[Dosovitskiy et al., 2020]</ref> has achieved outstanding performance in various visual tasks <ref type="bibr" target="#b26">, Touvron et al., 2021</ref>. It firstly splits an image into fixed-size patches, then uses a simple linear projection of a patch before feeding them into transformers. ViLT  is the first to explore patch-based features for multi-modal learning, and achieves up to dozens of times faster inference than previous region-based VLP methods. The advantages of patch-based features are following: 1) its simple framework can be more efficient than grid-based convolutional features in the online inference phrase; 2) it is more effective in capturing the global structure of a full image with the self-attention mechanism, which can provide complementary visual features different from region-based and grid-based ones. Specifically, the 2D image I img ? R 3?H0?W0 is reshaped into a sequence of flattened 2D patches x p ? R N ?(P 2 ?C) , where (H 0 , W 0 ) is the resolution of the original image, C is the number of channels,(P, P ) is the resolution of each image patch, and N = HW/P 2 is the resulting number of patches and also serves as the input sequence length for the Transformer. Then, the patches are flatten and embedded to D dimensions with a trainable linear projectionm, and an appropriate position encoding is introduced to capture the geometric relationship among different patches. Finally, the sequence of patch embeddings serves as input of the visual transformer encoder to pretrain.</p><p>With the rapid development of various ViT variants, there are also different ways to generate diverse patch features as in grid feature extraction: 1) Supervised Pre-training: the image patch encoder is pre-trained with image classification data such as in ViT <ref type="bibr" target="#b24">[Dosovitskiy et al., 2020]</ref> or object detection data such as in Swin Transformer ; 2) Unsupervised Pre-training: the image patch encoder is pre-trained with a large amount of unlabeled image-text pairs. For example, CLIP <ref type="bibr" target="#b21">[Radford et al., 2021]</ref> pretrains the image patch encoder of ViT with 400M aligned image-text pairs and <ref type="bibr" target="#b29">[Changpinyo et al., 2021]</ref> provides a large dataset of 12M image-text pairs CC12M and conducts image-text pre-training to recognize long-tail visual concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Textual Features</head><p>This research work utilizes the method in BERT <ref type="bibr" target="#b4">[Devlin et al., 2018]</ref> with the WordPiece tokenizer to tokenize the input text sentence into a sequence of sub-word tokens {w 1 , ? ? ? , w m }. Then each token w i is assigned three kinds of learnable embeddings: token, modal type and position embeddings. The three embeddings are summed and layernormalized to represent input sentence as a sequence of embedding vectors E emb = {e CLS , e 1 , ? ? ? , e m , e SEP }, where [CLS] and [SEP ] are the special tokens in BERT.</p><p>To provide better textual features, text stream parameters were initialized with three different pre-trained language models: BERT <ref type="bibr" target="#b4">[Devlin et al., 2018]</ref>, RoBERTa <ref type="bibr" target="#b30">[Liu et al., 2019]</ref> and StructBERT <ref type="bibr" target="#b31">[Wang et al., 2019]</ref>. RoBERTa trains on a larger corpus for more steps, and StructBERT incorporates more word ordering and sentence ordering information into pre-training language structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Cross-Modal Interaction with Learning to Attend</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Vision-and-Language Pre-training (VLP)</head><p>The interaction of multiple modalities has always been treated as one of the most significant problems in cross-modality research. In VLP literature, there are two mainstream architectures to bridge the cross-modal feature gap: single-stream architecture and dual-stream architecture. The former such as VL-BERT <ref type="bibr" target="#b6">[Su et al., 2019]</ref> and UNITER  assume simple and clear underlying semantics behind the two modalities and thus simply concatenate imageregion features and text features as input to a single Transformer <ref type="bibr" target="#b32">[Vaswani et al., 2017]</ref> network for early fusion in a straightforward manner. This paradigm learns the cross-modal semantic alignment from a bottom feature level by using the self-attention mechanism. Nevertheless, the design of single-stream structure treats both modality inputs equally, leaving the inherent different peculiarity of each modality not fully exploited. In contrast, the latter like LXMERT <ref type="bibr" target="#b8">[Tan and Bansal, 2019]</ref> and ERNIE-ViL  first use separate Transformer encoders to learn high-level abstraction of image and sentence representation respectively, and then combine the two modalities together with a cross-modal Transformer. This kind of design explicitly distinguishes between different modality inputs and aligns the cross-modal representations at a higher semantic level, but is usually parameter-inefficient and may ignore fundamental feature-level association. <ref type="bibr" target="#b14">[Li et al., 2021a]</ref> propose SemVLP to learn the joint representation of vision and language, which aligns cross-modal semantics at multiple levels. It builds upon a shared Transformer encoder with specific self-attention masks for cross-modal interaction. However, the interaction between the two modalities are controlled by fixed self-attention mask with only two modes: interactive or non-interactive. Therefore, our VQA architecture uses two learnable self-attention weights for each layer to dynamically control the inter-modal and intra-modal interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Learning to Attend</head><p>In single-stream models, the input to a Transformer layer is the concatenation of both modalities, X = [X L |X V ]. As a result, in each single-stream attention head, the query representation is given by:</p><formula xml:id="formula_0">Q = XW Q = X L X V * W Q = Q L Q V<label>(1)</label></formula><p>where ?L ?V are the language and visual sub-matrices of the input and the resulting output. As shown in <ref type="figure">Figure 3</ref>, the score matrix S can be defined in terms of four sub-matrices:</p><formula xml:id="formula_1">S = QK T = Q L Q V K T L K T V = S LL S LV S V L S V V<label>(2)</label></formula><p>Then, two learnable self-attention weights ? 1 and ? 2 are introduced for intra-modal attention score sub-matrices (diagonal of S) and inter-modal attention score sub-matrices (anti-diagonal of S), respectively. In each Transformer layer, the learnable weights are multiplied by the attention score matrix to obtain the new attention score matrix:</p><formula xml:id="formula_2">S ? = ? 1 S LL ? 2 S LV ? 2 S V L ? 1 S V V<label>(3)</label></formula><p>The following two methods are investigated to learn the self-attention weights ? 1 and ? 2 :</p><p>1. The weights are derived from a single-layer feed-forward network with the sigmoid activation function. V CLS (the representation of [CLS]) is used as the input feature to reflect how well an image matches with text. It gives a useful signal to measure intra-modal and inter-modal interaction.</p><formula xml:id="formula_3">(? 1 , ? 2 ) = F F N (V CLS )<label>(4)</label></formula><p>2. The self-attention weights are learned directly as the two parameters with specified initial values:</p><p>(? 1 , ? 2 ) = nn.P arameter(init_value 1 , init_value 2 ) (5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">VLP with Learning to Attend</head><p>Section 2.2.1 discusses the use of three classes of visual features (Region, Grid and Patch). Each class of the visual features and text feature are fused by the novel learning to attend mechanism for cross-modal interaction, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Embeddings</head><p>The input to Transformer is the image feature and its associated sentence (e.g. caption text). Each image is represented as a sequence of image features {o 1 , ..., o n }, and each sentence is represented as a sequence of words {w 1 , ..., w m }. The image and text embedding features are concatenated as input to the Transformer with learning to attend. The image representation for each kind of feature is described in Section 2.2.1. To capture both local and global semantics of image and obtain diverse visual feature representation, different kinds of image features are fused by concatenating them together. It is then combined with the text embedding features as input to the Transformer with learning to attend. This method is referred to as Fusion-VLP.</p><p>2. Masked Object Prediction. Similarly, the vision side is pre-trained by randomly masking objects. In particular, 15% of image objects are randomly masked, and the model is then asked to predict properties of these masked objects with the output object representations O L . 3. Image-Text Matching (ITM). This task randomly samples 50% of mismatched image-text pairs and 50% matched pairs, and trains a classifier to predict whether an image and a sentence match with each other on the representation. 4. Image Question Answering (QA). The image question answering task is cast as a classification problem where the model is pre-trained with image QA data as in LXMERT <ref type="bibr" target="#b8">[Tan and Bansal, 2019]</ref>. A classifier is then built on top of the representation h L CLS in the model. For region-based features, the Region-VLP model is pre-trained with all the four pre-training tasks as in LXMERT <ref type="bibr" target="#b8">[Tan and Bansal, 2019]</ref>, and the four losses are added up with equal weights. For grid feature, the Grid-VLP model is pre-trained with the pre-training tasks except masked object prediction, since the grid feature does not capture explicit semantics. Besides, to accelerate the pre-training process, a random sampling strategy is adopted to dynamically sample 100 image grids for each image following PixelBERT <ref type="bibr" target="#b15">[Huang et al., 2020]</ref>. The masked object prediction task is also removed for Patch-VLP and Fusion-VLP.</p><p>During fine-tuning, the complete region/grid/patch features are used to retain all the extracted visual information. The hidden state h L CLS of the last layer is used for cross-modality calculation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Knowledge-guided Mixture of Experts</head><p>Due to the complexity of the VQA task, there exist questions that are difficult to address by combining the diverse feature representation and V&amp;L pre-training. To address these questions and enable the model to evolve constantly, we further propose a knowledge-guided framework using the Mixture of Experts (MoE) model, as shown in <ref type="figure">Figure 3</ref>. Starting from a pre-trained V&amp;L model (the base Vision Understanding Expert in our case), a knowledge mining module is introduced to automatically discover the types of the questions that are not well-addressed by the Vision Understanding Expert, such as text-reading questions and clock-reading questions. These questions are then addressed by two extra expert modules newly introduced: Text Reading Expert and Clock Reading Expert, respectively. Finally, the three expert modules are combined together and routed to the right questions by the MoE model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Knowledge Mining</head><p>On top of the comprehensive study of diverse feature representation and specific design of cross-modal interaction, we propose a continual learning framework to boost the power of the pre-trained V&amp;L model. It includes two stages: 1) identify new sub-tasks which require extra knowledge to learn; 2) learn expert models for these sub-tasks with the knowledge collected from domain experts or internet.</p><p>To identify new sub-tasks, we adopt a clustering-based method, which considers the low-confidence examples from a base model and discovers groups of these examples by their similarity to form new sub-tasks. Given a base model M (i.e., Vision Understanding Expert in our case), we first collect examples which the model M is difficult to give correct answers with high confidence. The model is unable to address these examples well with existing knowledge, which calls for specialized expert models with extra knowledge to handle them. Specifically, given an example t, the base model M is designed to give a prediction with confidence score s. The output score on the predicted label of the Visual Understanding Expert is used to calculate this score s. The examples with low confidence scores (s &lt; 0.1) thus indicate the cases that the base model finds difficult to handle. Then, the typical clustering algorithm K-Means <ref type="bibr" target="#b33">[MacQueen et al., 1967</ref>] is used to partition the set of these low-confidence examples into sub-task clusters. Under our V&amp;L circumstances, clustering is conducted on both the textual and visual content of examples. Therefore, the cross-modal representation of [CLS] in the last layer of the Transformer are used as input to the clustering algorithm.</p><p>Clustering the low-confidence examples allows us to identify new sub-tasks. In the VQA task, the clustering discovers the two types of visual questions (text-reading questions and clock-reading questions) that need OCR ability and clock-reading ability to deal with, respectively. Since the existing Vision Understanding Expert is incapable of resolving the two sub-tasks well, two extra expert modules: Text Reading Expert and Clock Reading Expert, are trained to deal with the low-confidence examples. Finally, the three expert modules work together as a complete VQA solution via the Mixture of Expert model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Text Reading Expert</head><p>Text-reading VQA is an important VQA sub-task, which focuses on the questions that require to read the textual content shown in an input image. The existing models on the VQA dataset perform classification with frequent answers as labels. This classification modeling does not work well on text-reading samples where the answers are often not frequent enough to be included in the label set. Therefore, a specially designed deep LM that aims to capture structure information from texts, the StructuralLM model <ref type="bibr">[Li et al., 2021b]</ref> is utilized on these text-reading samples to extract answers from the text in images recognized by OCR. StructuralLM introduces cell-level 2D-positional embeddings and a new pre-training objective that classifies cells' positions. The pre-trained StructuralLM model is adopted to the text-reading VQA samples in the following way.</p><p>In order to adapt our StructureLM to image texts, we fine-tuned a pre-trained StructuralLM by text-reading samples.</p><p>In particular, an OCR tool is first used to recognize text and serialize the cells (bounding boxes) from top-left to bottom-right in images. Each image is represented as a sequence of cells {c 1 , ..., c n }, each of which contains a sequence of words</p><formula xml:id="formula_4">c i = {w 1 i , ..., w m i }. A separator [SEP ]</formula><p>is added between every two bounding boxes to separate them, which gives an input sequence {q 1 , ..., q e , [SEP ], c 1 , [SEP ], c 2 .., <ref type="bibr">[SEP ]</ref>, c n }. The StructuralLM is pre-trained subsequently in the same way as it is pre-trained on document images. A token-level span prediction classifier is then built upon the token representation to perform an extractive QA task, as often did for machine reading comprehension <ref type="bibr" target="#b35">[Rajpurkar et al., 2016</ref>. Finally, the added separator is removed from the predicted answer span.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3">Clock Reading Expert</head><p>With the powerful VQA features aforementioned, many questions can get satisfactory answers. However, it still suffers from reading precise time from clocks, as clock reading requires specific prior knowledge. Hence, a clock reading expert is introduced to address such kind of problems. The clock reading expert consists of a clock detector and a clock reader. The clock detector is used to detect clocks in images, which is essentially an object detection task. The Cascade-RCNN <ref type="bibr" target="#b37">[Cai and Vasconcelos, 2018]</ref> is used as the backbone network for the clock detector. A binary classification loss and a bounding box regression loss are applied for training as the standard detection framework does <ref type="bibr" target="#b37">[Cai and Vasconcelos, 2018]</ref>. The detected bounding boxes from the clock detector are fed into the clock reader, which reads the precise time in the clocks. The clock reading is modeled as both a classification task as well as a regression task.</p><p>In the clock reader, Resnet50-IBN <ref type="bibr" target="#b38">[Pan et al., 2018</ref>] is adopted as our backbone, and two specific branches are introduced for hour and minute prediction respectively. Furthermore, as the hour and minute hands in the clock are the keys to predict the time, attention modules were introduced to force the focus of the model on the hands. A SE-layer <ref type="bibr" target="#b39">[Hu et al., 2018]</ref> is employed after the backbone for channel-wise attention, and a spatial attention module which consists of convolution layers and ReLU activation is employed in the beginning of hour and minute branch respectively for spatial-wise attention. Such a corporation of channel and spatial wise attention is able to adapt to the individual bias of hour and minute prediction. The feature outputs from two branches are listed as following:</p><formula xml:id="formula_5">f m = E m (Attn sp (F ) * F ), f h = E h (Attn sp (F ) * F )<label>(6)</label></formula><p>where I is the image, and E, E h , E m are the backbone, hour branch and minute branch respectively. F is the feature map from the backbone after SE-layer, F = Attn se (E(I)).</p><p>As the clock reader is formulated as both a classification task and a regression task, it introduces loss from two perspectives. From the classification aspect, a 12-category classification loss is used for both hour and minute 2 prediction. The cross-entropy loss is adopted as follows:</p><formula xml:id="formula_6">L cls = ? N i g i ? log p i<label>(7)</label></formula><p>where N is the number of categories and set to 12.</p><p>As 2:00 is closer to 3:00 than to 9:00, it is also important to solve the problem from a regression perspective. The regression loss is listed as below:</p><formula xml:id="formula_7">L reg = B i cos 2? C ? (p i ? g i ) ? ? + 1 (8)</formula><p>where B is the batch size. The cosine formulation is used for the periodicity constraint of the clock prediction. C is the periodicity of hour or minute, which is set to 60 moves. p i and g i is the prediction and ground truth.</p><p>Because one full turn of the minute hand corresponds to 5 moves of the hour hand, a self-supervised loss is introduced. The self-supervision of hour and minute is regarded as a regularization loss to improve the generalization of clock reader.</p><formula xml:id="formula_8">L self = B i Smooth L1 (C ? (p h ? [p h ]) ? p m )<label>(9)</label></formula><p>Finally, the total loss is:</p><formula xml:id="formula_9">L = L cls + L self + ?L reg<label>(10)</label></formula><p>where ? is used to weight the self-supervised loss, and set to 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.4">Visual Understanding Expert</head><p>Vison-and-Language Pre-training (VLP) models with different visual features can help capture diverse visual semantics from images, which facilitates deep vision-and-language understanding. For example, region features are good at capturing objects in an image, and thus very useful for answering questions on object counting. Grid features retain global or background information of an image, which can help to answer descriptive questions. Therefore, a diverse feature ensemble method is used to construct our visual understanding expert, which ensembles multiple VLP models with different types of visual features: region feature, grid feature and patch feature. For each kind of features, different VLP models are trained separately. A simple maximum voting strategy is then utilized to ensemble the different VLP models based on the prediction score. Our experiments demonstrate the advantage of the diverse feature ensemble over a single class of visual features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.5">Putting It All Together by MoE</head><p>The methodology of Mixture of Experts (MoE) <ref type="bibr" target="#b40">[Jacobs et al., 1991</ref> essentially decomposes a task into sub-tasks, and develops an expert on each sub-task. A gating network is then used to coordinate multiple experts for task completion. We follow the recent work of Switch Transformers <ref type="bibr" target="#b42">[Fedus et al., 2021]</ref>, and adopt the simple routing strategy that the the gating network routes to only a single expert. We use the simplest form to preserves model quality and reduce routing computation. In our framework, the VQA task can be decomposed into three sub-tasks according to the analysis of the visual questions via proposed knowledge mining. A multi-layer perception network is trained as the gating network, which performs three-class classification to determine which expert to choose for each instance.</p><p>Given an expert M t for sub-task t, the expert will give an answer and we compute a reward score s t between the predicted answer and the human annotated labels using Equation 13. The reward score s t is used as supervision for training, where the network is trained to route each instance to its best-match expert. At training time, we propose to maximize the Binary Cross Entropy (BCE) loss L as follows:</p><formula xml:id="formula_10">L M oE = t s t log? t + (1 ? s t ) log (1 ?? t )<label>(11)</label></formula><p>where s t denotes the ground-truth reward score of sub-task t,? t stands for the prediction score of the MoE network. At test time, we choose the single routed expert with the maximum prediction scoret = arg max? t . The prediction scor? s is calculated using a Multi-Layer Perception network (MLP) as follows:</p><formula xml:id="formula_11">s = W 3 (tanh(W 2 tanh(W 1 x + b 1 ) + b 2 )) + b 3<label>(12)</label></formula><p>where x is the input feature and W i , b i are the learnable parameters.</p><p>The following features are derived for training the gating network:</p><p>? Each expert's confidence: for Visual Understanding Expert, the maximum prediction score is used for confidence score. For Text Reading Expert and Clock Reading Expert, the output score is used for confidence score, and if an image does not have text or any clock, the score is set to ?1.</p><p>? Question type: A three-class classifier is trained to predict whether a question is asked about text reading, clock reading or visual understanding. To train the classifier, OCR-labeled data is collected from TextVQA <ref type="bibr" target="#b43">[Singh et al., 2019</ref><ref type="bibr">] &amp; STVQA [Biten et al., 2019</ref>, and clock-labeled data from the VQA dataset by retrieving the keywords clock and what time. Other cases from VQA data are sampled as negative samples by the ratio of 1:2. The prediction scores of these three classes are used as the input features.</p><p>Even though the current process is manually designed, in the future, we will be exploiting techniques that allow us to automatically discover subset of challenging cases, and incrementally add more experts to address the discovered cases.  <ref type="bibr" target="#b45">[Lin et al., 2014]</ref>, Visual Genome <ref type="bibr" target="#b12">[Krishna et al., 2017]</ref>, image question answering data from VQA 2.0 <ref type="bibr" target="#b46">[Antol et al., 2015]</ref>, GQA balanced version <ref type="bibr" target="#b47">[Hudson and Manning, 2019]</ref> and VG-QA <ref type="bibr" target="#b48">[Zhu et al., 2016]</ref>. The total amount of the dataset is 9.18M image-and-sentence pairs on 180K distinct images. Also, additional out-of-domain data from Conceptual Captions <ref type="bibr" target="#b49">[Sharma et al., 2018]</ref> for model pre-training, which consists of about 3M image-text pairs on 3M images.</p><p>For Text Reading Expert, the StructuralLM <ref type="bibr">[Li et al., 2021b]</ref> is used as the base model, which is pre-trained on the IIT-CDIP Test Collection 1.0 <ref type="bibr" target="#b50">[Lewis et al., 2006]</ref>. It is a large-scale scanned document image dataset containing more than 6 million documents, with more than 11 million scanned document images.  Evaluation Metric Following <ref type="bibr" target="#b46">[Antol et al., 2015]</ref>, an evaluation metric robust to inter-human variability is used in phrasing the answers:</p><p>Acc(ans) = min # human that said ans 3 , 1</p><p>In order to be consistent with "human accuracies", machine accuracies are averaged over all 10-choose-9 sets of human annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Setup</head><p>VLP The maximum sequence length for the sentence is set as 20. For the VLP models, the pre-trained Transformer encoder with 12 layers is used as our base architecture, and the one with 24 layers as the large architecture. The basic settings of the Transformer are the same as BERT <ref type="bibr" target="#b4">[Devlin et al., 2018]</ref>, and the Transformer encoder is initialized with StructBERT <ref type="bibr" target="#b31">[Wang et al., 2019]</ref> for its good performance. For the method of learning to attend, the two learnable parameters are initialized with init_value 1 = 1.0 and init_value 2 = L s /L, where L is the number of Transformer layers and L s is the corresponding layer number. The base model is pre-trained with a total batch size of 512 for 30 epochs on 8 A100 GPUs and the AdamW optimizer with the initial learning rate of 1e-4. The 24-layer large architecture is pre-trained with the total batch size of 512 on 8 A100 GPUs. To deal with over-fitting, two-stage pre-training strategy is employed as in LXMERT <ref type="bibr" target="#b8">[Tan and Bansal, 2019]</ref>. Specifically, the model is first pre-trained without the question answering task with the initial learning rate of 5e-5 for 20 epochs, and then pre-trained with all the tasks together with the initial learning rate of 2e-5 for another 10 epochs. The detailed settings for the three VLP methods are listed as below:</p><p>1. Region-VLP: The detection model is used in VinVL  to detect objects and extract region features. It is a large-scale object-attribute detection model based on the ResNeXt-152 C4 architecture. 100 objects is retained for each image to maximize the pre-training compute utilization by avoiding padding.</p><p>2. Grid-VLP: It follows the basic settings in Grid-VLP  . ResNeXt is chosen to be the visual encoder with different sizes <ref type="bibr" target="#b51">[Xie et al., 2017]</ref> as in <ref type="bibr" target="#b18">[Jiang et al., 2020</ref><ref type="bibr" target="#b15">, Huang et al., 2020</ref>. The shorter side of every input image is resized to 600, and the longer side is limit to at most 1000. A fixed number of 100 grids are randomly selected each time during pre-training 4 .</p><p>3. Patch-VLP: It uses the visual Transformer encoder of the Swin detector  and CLIP <ref type="bibr" target="#b21">[Radford et al., 2021]</ref>. The ViT-B/32 pre-trained model is chosen, which has 12 Transformers layers with input patches of size 32 ? 32. Every input images is resized to 224 ? 224 as CLIP does, resulting in 7 ? 7 = 49 patches.</p><p>4. Fusion-VLP: It fuses the three classes of image features (Region, Grid and Patch) by concatenating them together as the visual input to the Transformer. A two-stage strategy is employed to pre-train Fusion-VLP, which first trains the region-grid model initialized with Region-VLP, and then continues to train the regiongrid-patch model.</p><p>Fine-tuning on VQA Following <ref type="bibr" target="#b10">[Anderson et al., 2018]</ref>, our architecture treats VQA as a multi-class classification task by picking an answer from a shared set of 3,129 answers. The hidden state of h L CLS is used to map the representation into 3,129 possible answers with an additional MLP layer. The model is trained with a binary cross-entropy loss on the soft target scores. The pre-trained models are fine-tuned based on the three classes of features on the VQA training data for 3 epochs with the batch size of 32, and the BERT Adam optimizer is employed with the initial learning rate of 1e-4 for base models and 2e-5 for large models. At inference, a softmax function is used for prediction.</p><p>Text Reading Expert The text reading expert follows the basic settings in StructuralLM <ref type="bibr">[Li et al., 2021b]</ref> and uses the pre-trained StructuralLM-large as the backbone model. In particular, StructuralLM is pre-trained with a batch size of 16 for 50K steps. The question tokens and the OCR tokens of an image are concatenated as an input sequence, of which the maximum length is set as 128. For fine-tuning, the three kinds of text-reading VQA datasets are merged and split with 10-fold cross-validation. The StructuralLM is fine-tuned with the total batch size of 16 for 4 epochs, and the AdamW optimizer is employed with the initial learning rate of 3e-5. Accuracy and ANLS (Average Normalized Levenshtein Similarity) are used as the metrics to evaluate the text reading expert.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clock Reading Expert</head><p>The clock detector of the clock reading expert is trained following the basic settings of Cascade-RCNN <ref type="bibr" target="#b37">[Cai and Vasconcelos, 2018]</ref>. The clock reader is trained with the batch size of 96 in 2 GPUs, and the initial learning rate is set as 0.02. It is trained for 150 epochs with the learning rate multiplied by 0.1 at 90-th and 120-th epochs. The data augmentation pipeline consists of 256 ? 256 random resized cropping, random color jittering, random gray-scale conversion, Gaussian blurring and random rotation within ?45 ? .</p><p>Visual Understanding Expert The visual understanding expert ensembles 46 models in total, including 14 Region-VLP models, 21 Grid-VLP models, 4 Patch-VLP models and 7 Fusion-VLP models. Simple maximum voting is adopted to ensemble all the models based on their prediction scores. Mixture of Experts The MoE adopts Multi-layer Perceptron (MLP) as the gating network to determine experts for given questions. The MLP has two hidden layers of 100 neurons and 50 neurons, respectively. It uses tanh as the activation function, and the Adam optimizer with the initial learning rate of 1e-3. The network is trained for 5 epochs with the batch size of 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Main Results</head><p>Table 3 presents our main results compared with all the previous public and unpublic best results on the VQA Challenge Leaderboard. From the results, it can be observed that: 1) Our VQA architecture ALICEMIND-MMU represents the first to achieve human parity on VQA Challenge Leaderboard outperforming all the previous state-of-the-art methods by a large margin, which demonstrates the effectiveness of our framework. 2) With regard to a breakdown of performance on different question types, ALICEMIND-MMU performs much better on the "Other" type than human do, and gives comparable results on "Yes/No" questions. ALICEMIND-MMU performs worse than human do on type "Number" for  the two reasons: a) in the "Number" type, there are many questions about reading OCR text, which are easier for human to answer; and b) there are many object counting questions that are more difficult for ALICEMIND-MMU to answer. <ref type="table" target="#tab_5">Table 4</ref> presents the detailed results of our single VLP models compared with other state-of-the-art methods. From the results, it is observed that: 1) the proposed VLP model outperforms the others on every kind of visual feature (region / grid / patch), respectively. It demonstrates the effectiveness of the proposed cross-modal interaction with learning to attend mechanism. 2) The methods with self-attention on patch feature perform worse than the region-based and grid-based methods do. There are two weaknesses of patch-based methods: a) the visual semantic information is not well-captured in existing patch-based VLP methods. How to inject visual semantics into patch representation remains largely unexplored; b) the image-text pre-training data is not enough for large-scale patch-based pre-training; 3) Fusion-VLP gives the best performance by fusing all the three classes of visual features as input, which validates the effectiveness of comprehensive feature representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Analysis by Modules</head><p>Visual Feature Importance Here presents the ablation study to assess the importance of different visual features for VLP on the VQA test-dev set. The results shown in <ref type="table" target="#tab_6">Table 5</ref> indicate that: 1) The VLP methods based on region and grid features achieve much better performance than the ones based on patch feature do, as stated in Section 3.3. When examining by individual question types, Region-VLP performs better on the "Number" type, while Grid-VLP does better on the "Yes/No" and "Other" types. The difference can be attributed to the fact that region feature captures more local information of an image at the object level, and thus is more effective in address the visual counting problem by identifying local objects in an image. On the other hand, grid feature captures globally visual context in an image, which helps to answer the "Yes/No" and "Other" questions; 2) by combining the three classes of features in the way of early fusion, Fusion-VLP performs the best among all the single models. It shows that the different kinds of features can complement well with each other.</p><p>Learning to Attend Here presents the ablation study to assess the importance of the learning to attend mechanism on the VQA test-dev set. The 24-layer Region-VLP is used as the baseline model, which is pre-trained and fine-tuned based on the original Transformer. The ablation applies the same pre-training and fine-tuning settings, and only modifies the self-attention block with the two ways of learning to attend stated in Section 2.3.2. From <ref type="table" target="#tab_7">Table 6</ref>, it can be seen that the model with either way of learning to attend outperforms the best Region-VLP baseline. Among the two different ways, the one with two learnable parameters performs slightly better than the other one. The reason may lie in: 1) learning two unrestricted parameters for each layer allowing for more parameter freedom, so as to better align cross-modal semantics, 2) the discrepancy between pre-training and fine-tuning, where the representation of [CLS] is learnt to model the semantic relation between a caption and an image during pre-training, while it is repurposed for question answering in fine-tuning. <ref type="figure" target="#fig_1">Figure 4</ref> illustrates the clustering result. We choose the number of the clusters as 5, which gives the best performance on our quantitative test. For each cluster, we showcase two examples in the cluster and the percentage of the examples in this cluster. From the results, we can see that the proposed knowledge mining can  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge Mining</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cluster Examples Percent</head><p>What number is on the train?</p><p>What number is on the license plate?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>15.1%</head><p>Where is the cat's head? What is the cat sitting on?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>36.5%</head><p>--- Cluster 2 is about counting problems. Cluster 3 is about reading texts from the wall or the clothes. Cluster 4 is about reading number texts on the vehicles. We found that there are two new tasks: clock-reading task (Cluster 1) and text-reading task (Cluster 3,4), both of which require specific prior knowledge. We also use a three-class classifier in Section 2.4.5 to classify the filtered candidate examples, so as to measure the consistency between the clustering result and classification result. To measure the consistency of the clustering result to the classification labels, we also provide detailed quantitative analysis on different clustering methods. We manually build a three-label classifier (OCR, clock and vision) with 95% accuracy as in Section 2.4.5 and apply it to evaluate the consistence of each cluster. We project each cluster to the corresponding label heuristically. For example, in <ref type="figure" target="#fig_1">Figure 4</ref>, Cluster 1 is assigned to clock label, Cluster 3 and Cluster 4 are to OCR label, and Cluster 5 is to vision label. We then compare the assigned label of each cluster to that of the classification label (95% accuracy). We use accuracy, macro-precision, macro-recall and macro-f1 to measure how consistent the compared label in each cluster is. As list in <ref type="table" target="#tab_9">Table 8</ref>, K-Means (K=5) achieves the best performance with 0.8448 accuracy and 0.8739 macro-F1, which shows that the clustering result is highly consistent with the assumed classification labels on OCR/clock/vision.</p><p>Text Reading Expert As stated in Section 2.4.2, the pre-trained StructuralLM model is adapted on text-reading VQA samples in different ways. <ref type="table" target="#tab_8">Table 7</ref> shows the ablation results on the VQA test-dev set. The visual understanding expert 5 is used as the baseline method, upon which all the ablation experiments for text reading expert is conducted. First, it is  <ref type="figure">Figure (a)</ref> shows the clustering results and the label 1/2/3/4/5 is cluster id. <ref type="figure">Figure (b)</ref> shows the classification results and the label ocr/clock/vision is classification label. The classifier is manually built and the accuracy of it is 95.0%. observed that text reading expert greatly improves the performance on the "Number" type by over 6%, where many questions are asked about reading numbers from OCR text, such as a bus number and a football player number. On the "Other" type, the performance can be improved by over 1%. Answering many questions of this type requires the ability to reason with both visual and textual OCR information. Adding the separator between textual bounding boxes and continual pre-training on domain data can lead to further improvement, demonstrating the effectiveness of adapting the pre-trained StructuralLM for text-reading VQA.</p><p>Clock Reading Expert The ablation study of clock reading expert is shown in <ref type="table" target="#tab_10">Table 9</ref>, where only the results on the "Number" and "Overall" types are given, because questions on reading clocks are only present in the "Number" type. Adding clock reading expert results in more than 4.5% performance improvement on the "Number" type (from 59.93 to 62.65), which demonstrates the effectiveness of proposed ideas in the clock reading expert. Specifically, the proposed regression loss is prone to provide a larger gradient when there is a bigger difference between the predicted time and the ground truth, which benefits prediction of the clock reader. Moreover, it can be observed that the self-supervised loss boosts the performance significantly, as the relationship prior constrains hour and minute branches both, which eliminates the confusion of hour and minute hands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixture of Experts (MoE)</head><p>The ablation study of MoE is shown in <ref type="table" target="#tab_1">Table 10</ref>. With only visual understanding expert, the model gives a strong performance of accuracy 80.05 on the VQA Test-dev set. Adding text reading expert increases the overall performance by more than 1%, which already achieves human parity of 80.83. Adding clock reading expert further boosts the performance to 81.27, where the performance on the "Number" type increases by more than 4%. The gating network of MoE mimics human who is able to identify domain experts based on the nature of tasks. This knowledge-guided MoE framework can also be easily extended to incorporate more specialized experts for continual self-evolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">VQA Dataset Analysis</head><p>This subsection provides more detailed analysis of our VQA results. To gain an understanding of types of questions and answers, 1000 examples are randomly sampled from the validation set for analysis. The 1000 examples are classified  into the categories listed below by manual examination based on the abilities required. The categorization is multi-label in the sense that every example is classified into all applicable categories. <ref type="figure">Figure 6</ref> provides an estimate of the proportion for each category. Commonsense Knowledge, Relational Reasoning and Object Counting are the top three categories in the overall distribution. Commonsense Knowledge accounts for over 80% of the Yes/No type. In the Number type, Object Counting and Textual Recognition are the two most popular categories compared with the other two types. The type Other has a similar distribution as that of the overall distribution. <ref type="figure" target="#fig_5">Figure 7</ref> presents representative examples from each category.</p><p>? Commonsense Knowledge This category contains questions inquiring commonsense knowledge from our daily life, such as colors, weathers, food and furniture. ? Other This category contains the questions that are ambiguous or cannot be answered based on given images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">ALICEMIND-MMU vs. Human</head><p>A comparative study of ALICEMIND-MMU and human on visual question answering has been conducted. <ref type="table" target="#tab_1">Table  11</ref> and <ref type="table" target="#tab_1">Table 12</ref> show the overall and per-category performance of ALICEMIND-MMU and human on the val split,   respectively, from which there are the following observations: (i) ALICEMIND-MMU outperforms human annotators on the two largest categories, Commonsense Knowledge and Relational Reasoning. It shows ALICEMIND-MMU 's superiority of identifying common scene objects in daily life and leveraging commonsense knowledge such as colors and weathers. This result also demonstrates the power of ALICEMIND-MMU in reasoning over relative positions, such as the left sign on a wall, to answer a spatial reasoning question. Besides, it is surprising that ALICEMIND-MMU can reason over simple comparison, such as which object is the tallest. (ii) The questions in the Object Counting category seem rather difficult for ALICEMIND-MMU to answer. ALICEMIND-MMU is found to be good at counting a small number (&lt;10) of objects. It would give an incorrect count when encountering a large number of small objects and/or requiring reasoning over them. (iii) ALICEMIND-MMU significantly surpasses human performance on Visual Recognition which requires specialized knowledge. It is expected that ALICEMIND-MMU , as a machine learner trained with large data, is skilled in memorizing specialized/professional knowledge with visual recognition, compared with non-professional human annotators. (iv) ALICEMIND-MMU is more capable of reading time shown in a clock than human, as demonstrated by the result of Clock Reading. On text reading, however, there is still a big gap between ALICEMIND-MMU and human in recognizing and understanding text in an image, as shown by the result of Textual Recognition. Some research progress has been made on text-reading VQA tasks, such as TextVQA <ref type="bibr" target="#b43">[Singh et al., 2019]</ref>.  . These examples are studied by category as follows:</p><p>Commonsense Knowledge ALICEMIND-MMU is knowledgeable in many aspects of daily life. As shown in <ref type="figure" target="#fig_6">Figure 8</ref>, ALICEMIND-MMU is able to tell not only weather and the sentiments of people, but also classic sports and electronic products as an ordinary person does. Also, it is skilled in geography and understands the food around the world. For example, ALICEMIND-MMU recognizes the small English flag and Big Ben in the image, by which the country is identified as England. As another example, based on the rice and dishes from the image, Chinese food can be identified by ALICEMIND-MMU and people familiar with Chinese cuisine. The other people may not tell cuisine of the food . Our experiments show that ALICEMIND-MMU trained on adequate data can capture the commonsense knowledge (the largest category in the VQA dataset) in our daily life.</p><p>Visual Recognition Except for Clock Reading, ALICEMIND-MMU shows much better performance than an ordinary person does in this category. As shown in <ref type="figure" target="#fig_6">Figure 8</ref>, it is relatively easy for an AI model trained adequately to memorize specialized knowledge, while rather difficult for people unfamiliar with the specific domain. For example, ALICEMIND-MMU can better identify the specific categories of the animals, such as dog and bird, and the historical style of the furniture, which requires specialized knowledge. By locating and recognizing the barely visible logo from the motor bike, ALICEMIND-MMU correctly recognizes its brand, while human may miss the details in the image and give an answer based on their best guesses. On the other hand, ALICEMIND-MMU has a slim chance of being fooled by the activity present in the image. It incorrectly identifies that the boy is playing baseball based on his motion, while it is actually a Wii game. As a result, by incorporating relevant specialized knowledge and visual recognition capability, ALICEMIND-MMU can outperform human by a large margin in this category.</p><p>Object Counting As shown in <ref type="figure" target="#fig_8">Figure 9</ref>, ALICEMIND-MMU achieves human parity when counting a small number of objects, but fails in more complicated cases where there are occlusions or a great quantity of objects. It surprises us that ALICEMIND-MMU can give a count very close to the correct answer in the example of counting trees. However, the object counting ability is still quite limited compared with ordinary people. One reason may lie in that the visual detection is too weak to detect all the objects in an image when the number of objects is large. There are few cases with more than 10 objects in the training set, and thus the model is not fully trained with sufficient data. This is shown in the third example where ALICEMIND-MMU fails to identify all people from the image. Another possible reason is that the object detector is difficult to count in the presence of occlusion. The second example shows that ALICEMIND-MMU counts the racing people incorrectly due to the occluded person.</p><p>Relational Reasoning <ref type="figure" target="#fig_8">Figure 9</ref> shows that ALICEMIND-MMU has the abilities to reason over the relationship of positions, comparison and exclusion. It is observed that ALICEMIND-MMU may be more capable than human in precise position identification and knowledge reasoning for relational reasoning questions. Specifically, 1) position: the first two examples show the power of ALICEMIND-MMU in distinguishing the positions of the left-right and front-back, and conducting one-step reasoning over the positional relationship; 2) comparison: the third example demonstrates that ALICEMIND-MMU can even compare the colors of two objects, which is a simple one-step reasoning over the comparison between attributes of two objects; 3) exclusion: the last example shows that ALICEMIND-MMU is able to identify the exclusion relationship, and reason over it with commonsense knowledge.</p><p>Textual Recognition (OCR) As shown in <ref type="figure" target="#fig_0">Figure 10</ref>, the text reading expert (StructuralLM) is able to identify text and layout in simple cases. In the first example, the model correctly answers with the words displayed on the man's shirt.</p><p>In addition, StructuralLM is capable of learning the interactions between text and layout, which makes StructuralLM aware of location of text present in an image. This is shown in the second example, where the model predicts the answer correctly when asked about the sign on the left. However, the model fails in the two cases: 1) OCR errors; 2) answering complex questions which requires visual feature and reasoning abilities. As shown in the third example, when asked the words on the man's shirt, the model can predict only "3" because the OCR tool cannot recognize the word "cardinals". In the fourth example, given the question about the number present on the white shirt, the model answers incorrectly due to the lack of visual feature of colors and the reasoning ability. Currently, the text reading expert utilizes only the layout and textual information to answer a text-reading question, without leveraging visual signals. There is an urgent need for deep interaction between visual information and OCR textual information in images, which is left for future work.</p><p>Clock Reading As shown in <ref type="figure" target="#fig_0">Figure 10</ref>, the clock reading expert is able to read the clock time accurately at a five-minute level. One important problem to be addressed is distinguishing the hour hand (generally shorter) from the minute hand (generally longer). The clock reading expert is trained well on this objective. Therefore, in the first example, the model predicts the correct time "8:05", while some human annotators misread the hour hand and minute hand, and thus give the wrong time reading "1:40". In the second example, the clock reading expert can tell the time accurately even when the hour hand and minute hand overlap. There also exist limitations for the current clock reading expert, that it can't tell more accurate time at a minute level. In the fourth example, the model only recognizes the time is about 12:10, but cannot tell the exact time of 12:12. The reason comes from casting the problem as detection and then classification, of which the clock training data is not adequate to support training of 1-minute level clock reading.  4 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Visual Question Answering</head><p>First released in 2016, the VQA task and research on the dataset have been developing for more than five years. A large number of new methods have been proposed to push the limit of this task. Existing approaches on the VQA task mostly put efforts on four key aspects: 1) better visual feature representation, 2) effective cross-modal fusion, 3) large-scale vision-and-language pre-training, and 4) task-specific expert optimization. Region-based visual features have long been treated as the de facto standard for vision and language tasks such as VQA, where visual semantics and salient image objects are captured with bottom-up attention <ref type="bibr" target="#b10">[Anderson et al., 2018]</ref>. The proposed bottom-up attention method with region features won the VQA Challenge of 2017, which has been largely adopted since then. Recently, Li et al.  has further pushed the limit of the region-based features by pre-training on extremely large object detection datasets. On the other hand, <ref type="bibr" target="#b18">[Jiang et al., 2020]</ref> studied the key factors that contributed to the effectiveness of existing bottom-up attention features, and found that the grid-based features from convolutional neural networks can yield comparable or even better results on the VQA task. It won the VQA Challenge of 2020. More recently, inspired by the popular vision Transformer <ref type="bibr" target="#b24">[Dosovitskiy et al., 2020]</ref>, some pioneer work such as ViLT  began to study the use of patch-based visual feature for its efficiency.</p><p>Cross-modal fusion or interaction has always been treated as one of the most important challenges in cross-modality research. At first, the cross-modal fusion on VQA is simply element-wise product of textual and visual representation <ref type="bibr" target="#b5">[Agrawal et al., 2017]</ref>. Then, it gradually improves from a linear model <ref type="bibr" target="#b5">[Agrawal et al., 2017]</ref> to bilinear pooling ones <ref type="bibr" target="#b62">[Fukui et al., 2016</ref><ref type="bibr" target="#b63">, Yu et al., 2017</ref>, and attention-based fusion ones <ref type="bibr" target="#b64">[Lu et al., 2016</ref><ref type="bibr" target="#b66">, Yu et al., 2019b</ref>, where questions and image features are fully interacted with each other. Recently, with the popularity of pre-train and fine-tune paradigm such as BERT <ref type="bibr" target="#b4">[Devlin et al., 2018]</ref> in NLP, large-scale vision-and-language pre-training <ref type="bibr" target="#b8">[Tan and Bansal, 2019</ref><ref type="bibr" target="#b6">, Su et al., 2019</ref><ref type="bibr" target="#b67">, Li et al., 2020c</ref><ref type="bibr" target="#b59">, Li et al., 2020b</ref><ref type="bibr" target="#b15">, Huang et al., 2020</ref> has been used to better align the vision-language representations. It has established the new state-of-the-art performance on the VQA task since 2020, by pre-training on a large amount of unlabeled image-text pairs.</p><p>To further improve the VQA performance, some studies analyze the weakness of existing models on the VQA task, and address it with specific expert optimization. For example, MoVie <ref type="bibr" target="#b68">[Nguyen et al., 2020]</ref> revisited modulated convolutions for visual counting, which is more efficient and advances the state-of-the-art on counting-specific VQA tasks. <ref type="bibr" target="#b69">[Wu et al., 2016]</ref> propose a knowledge-enhanced VQA method to combine an internal representation of image content with information extracted from a general knowledge base to answer a broad range of image-based questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Visual Features for Vision-and-Language tasks</head><p>Feature representations for vision have played a key role in the advancement of vision and language tasks. Different kinds of features can capture diverse data characteristics, which complements with each other. In most of the visionand-language tasks, visual feature is the bottleneck of existing multi-modal models and will set the upper bound for the final performance. To capture both the local and global information in the image, this work takes a comprehensive visual feature understanding on three kinds of typical features.</p><p>Region Feature Popularized as bottom-up attention <ref type="bibr" target="#b10">[Anderson et al., 2018]</ref>, region-based visual features have been treated as the de facto standard for vision and language tasks and achieved dominant performance in tasks such as visual question answering (VQA) and image captioning <ref type="bibr" target="#b70">[Chen et al., 2015]</ref>. It uses pre-trained object detectors <ref type="bibr" target="#b11">[Ren et al., 2015]</ref> to identify salient regions based on the vision input. A region proposal network (RPN) is first used to propose regions of interest (RoI) based on the grid features pooled from the CNN backbone. Then non-maximum suppression (NMS) is used to select a small collection of proper RoIs, and RoI head is used to extract region feature for each selected RoI. As a result, images are represented by a collection of region-based features. Most current VLP approaches such as LXMERT <ref type="bibr" target="#b8">[Tan and Bansal, 2019]</ref>, UNITER , ERNIE-ViL  and OSCAR <ref type="bibr" target="#b59">[Li et al., 2020b]</ref> adopt the region feature-based VLP paradigm. Recently, Li et al.  have proposed an extreme large version of the region-based object detector, which is pretrained on four object detection datasets, including MS COCO <ref type="bibr" target="#b45">[Lin et al., 2014]</ref>, Visual Genome <ref type="bibr" target="#b12">[Krishna et al., 2017]</ref>, OpenImages <ref type="bibr" target="#b71">[Kuznetsova et al., 2020]</ref>, and Object365 <ref type="bibr" target="#b72">[Shao et al., 2019]</ref>. It greatly improved the performance of VLP models, creating new state-of-the-art results on seven public benchmarks.</p><p>Grid Feature Besides the local region-based features, the grid-like feature map from convolutional neural networks such as ResNet  can also be used as visual features for visual-and-language tasks. One of the largest advantages of grid feature is that it removes all the time-consuming region-related steps, which can support more flexible architecture design of end-to-end vision-and-language models. Successful use of grid feature was first proposed by GridFeat <ref type="bibr" target="#b18">[Jiang et al., 2020]</ref> on VQA and image captioning task. It adopts the same pre-training settings as in the region-based method, discarding all the region-related steps from the detector and using only the grid convolutional features during inference. In this way, it greatly reduces the inference time of visual encoder and obtains comparable or even better accuracy on VQA task. Grid-VLP  further uses grid features for vision-language pre-training, and by using only in-domain datasets, Grid-VLP outperforms most state-of-the-art region-based VLP methods. Pixel-BERT <ref type="bibr" target="#b15">[Huang et al., 2020]</ref> is the first end-to-end grid-based VLP method, which uses a ResNet image encoder and jointly optimizes both ResNet image encoder and cross-modality Transformer in an end-to-end manner. E2E-VLP  builds upon the detection transformer (DETR) <ref type="bibr">[Carion et al., 2020]</ref>, and proposes an end-to-end grid-based VLP model for both V+L understanding and generation. It enhances learning of the pre-trained model by capturing more semantic-related visual representation with object detection and image caption pretext tasks.</p><p>Patch Feature Patch feature gains its popularity with the rise of Vision Transformer (ViT) <ref type="bibr" target="#b24">[Dosovitskiy et al., 2020]</ref>, which firstly splits an image into fixed-size patches, then uses a simple linear projection of a patch before feeding them into transformers. Vision Transformer (ViT) <ref type="bibr" target="#b24">[Dosovitskiy et al., 2020]</ref> is the first successful use of self-attention architecture to replace the convolutional neural network architecture in computer vision, after which a series of related work <ref type="bibr" target="#b26">, Touvron et al., 2021</ref> has been proposed to promote the development of this new direction. Swin Transformer ] further builds the model on multi-scale feature maps and uses a shifted windowing scheme on image patches, which makes it compatible with a broad range of vision tasks such as image classification and object detection. Most of the work in this ViT series adopts the image patch feature so as to reserve high efficiency in self attention-based Transformer modeling. CLIP <ref type="bibr" target="#b21">[Radford et al., 2021]</ref> proposes a simple contrastive pre-training method to learn the visual models based on a dataset of 400 million image-text pairs, which obtains superior zero-shot performance on various existing computer vision datasets. Recently, ViLT  has proposed the pioneering work of vision-language pre-training on image patch feature, and achieved up to tens of times faster than previous VLP models with a convolution-free scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Vision-and-Language Pre-training</head><p>Inspired by the breakthrough of language pre-training in NLP field, the research community begins to pay more attention to vision-language pre-training on large-scale image-text pairs, which also shows great effectiveness and achieves state-of-the-art performance across a variety of vision-language (VL) tasks <ref type="bibr" target="#b46">[Antol et al., 2015</ref><ref type="bibr" target="#b75">, Suhr et al., 2018</ref><ref type="bibr" target="#b47">, Hudson and Manning, 2019</ref>.</p><p>Existing approaches to VLP <ref type="bibr" target="#b8">[Tan and Bansal, 2019</ref><ref type="bibr" target="#b6">, Su et al., 2019</ref><ref type="bibr" target="#b67">, Li et al., 2020c</ref><ref type="bibr" target="#b59">, Li et al., 2020b</ref><ref type="bibr" target="#b15">, Huang et al., 2020</ref> mainly take a two-step training pipeline, which first extracts semantic visual features by specific object detector and then learns a cross-modal pre-training model to align text and visual features. Current research about this topic can be roughly divided into two lines. The first line adopts a single-stream transformer architecture to model both image and text representations in a unified semantic space such as VLBERT <ref type="bibr" target="#b6">[Su et al., 2019]</ref>, UNITER  and OSCAR <ref type="bibr" target="#b59">[Li et al., 2020b]</ref>. In contrast, the other line uses a two-stream Transformer architecture that first encodes the image and text modalities separately, and then fuses the cross-modal representations with another Transformer network, such as LXMERT <ref type="bibr" target="#b8">[Tan and Bansal, 2019]</ref> and ERNIE-ViL . Recently, VinVL  pre-trained a large-scale object-attribute detection model with much larger amounts of data on four public object detection datasets for extracting better region feature, and creating new state-of-the-art results on seven public benchmarks. In addition to image-text pairs, UNIMO <ref type="bibr" target="#b57">[Li et al., 2020a]</ref> also employed large scale of free text corpus and image collections for enhancing the cross-modal learning. These methods rely heavily on a task-specific bounding box (or region) based object detector, which impose unnecessary constraints on model designs and limit potential applications of existing vision and language systems. Therefore, PixelBERT <ref type="bibr" target="#b15">[Huang et al., 2020]</ref> and E2E-VLP  further proposed end-to-end VLP method, by jointly learning both the visual encoder and cross-modal Transformer simultaneously. Besides, the object detector is removed from the whole process, and the VLP model directly conducts on the grid-based feature map from convolutional visual encoder. To further improve the training and inference speed of VLP model, ViLT  removed both the region supervision and convolutional visual encoder, and conducted vision-language pre-training directly on image patch feature with linear projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Limitation</head><p>This paper describes our new research work on improving the full pipeline of the VQA task, which has achieved human parity on this challenging task for the first time. The key to the breakthrough lies in three aspects: 1) more comprehensive textual and visual feature representation with pre-trained technologies, 2) more effective cross-modal interaction with learning to attend, and 3) more knowledge-guided optimization with mixture of expert modules. It demonstrates the power of an AI model to achieve human parity on the challenging cross-modal understanding task under a closed-set scenario, which requires AI to have the ability to understand both visual and textual information. This makes it possible to further conduct higher-order cognition and commonsense reasoning intelligence.</p><p>Despite the success, the current AI technology on V&amp;L understanding still has notable limitations, and bridging the gap between machine intelligence and real human intelligence still has a long way to go. For the VQA task, there still exist certain weaknesses for our model: 1) object counting is still a very difficult problem for the current VLP model, especially the case when a large number of different objects with a small or tiny size exist. Besides, some objects can even overlap with each other, which introduces more complexity; 2) composite reading of both OCR text and visual content is still challenging for the current VLP model, while it is relatively easy for human to answer; 3) the current model can only work in a closed-set scenario, which is not applicable to the open-domain or unseen set. In the future, we hope to witness more breakthrough on open-set learning and more intelligent AI models, which can evolve itself by acquiring and reasoning about new knowledge.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Examples of free-form, open-ended questions and images in the VQA dataset A VQA system takes as input an image and a free-form, open-ended, natural-language question about the image, and produces a natural-language answer as the output. Example questions are shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Illustration examples of clustering results. Percent is the percentage number of each cluster's examples. actually mine certain meaningful topic clusters, where similar examples are clustered together. For example, Cluster 1 is about asking questions about time and clocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 5gives t-SNE visualization of the clustering result and classification result. From the result, we can see high consistency between the clustering and classification result on the measured topics. The knowledge mining method can properly separate part of the clock-reading examples and OCR-reading examples from the other examples, although for the OCR-related exampled there still exist limited examples mixed up in the common vision category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>The t-SNE visualization of clustering results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>?</head><label></label><figDesc>Visual Recognition This category requires the ability to acquire specialized knowledge with visual recognition to answer questions in this category. ? Relational Reasoning This category requires understanding and reasoning over certain relationships of objects in an image, such as positional relationship, comparison relationship, etc. ? Textual Recognition (OCR) This category requires the ability to recognize and utilize text together with the positions or visual information in an image (e.g., road signs, ads on a bus). ? Object Counting This category contains the examples that test the ability of counting objects in an image. ? Clock Reading This category contains the examples that test the ability of reading a clock.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Representative examples from each category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 ,</head><label>8</label><figDesc>Figure 9and Figure 10 present ALICEMIND-MMU 's predictions together with ground truth on each category for case study. In particular, a couple of representative examples are listed for each category, each containing a question, an image and the answer predicted by ALICEMIND-MMU . The scores of human annotators and ALICEMIND-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Case Study for Commonsense Knowledge and Visual Recognition. The scores of Human and ALICEMIND-MMU are calculated with Equation 13. Ground Truth gives the top three annotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Case Study for Object Counting and Relational Reasoning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Case Study for Textural Recognition (OCR) and Clock Reading.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Visual Feature(Sec 2.2 ) Vision and Languge Pretraining(Sec 2.3)</head><label></label><figDesc>are pre-trained with detection</figDesc><table><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Lear ning to Attend</cell><cell></cell><cell></cell></row><row><cell cols="2">Image-Text</cell><cell cols="2">Masked Language</cell><cell>Image Question</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Matching</cell><cell></cell><cell>Modeling</cell><cell></cell><cell>Answering</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Match</cell><cell></cell><cell>dog</cell><cell></cell><cell>Yes</cell><cell></cell><cell></cell><cell>*</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Attention weight</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Transformer Encoder</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Answer</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[CLS]</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Mixture of</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Experts</cell></row><row><cell>0 0</cell><cell>0 1</cell><cell>0 2</cell><cell>1 0</cell><cell>1 1</cell><cell>1 2</cell><cell cols="2">Modal Type</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Embedding</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Token Embedding</cell><cell></cell><cell cols="2">Visual Encoder</cell><cell cols="2">Token/Visual Position Embedding</cell><cell></cell><cell>Vision Understanding Expert</cell><cell>Text Reading Expert</cell><cell>Clock Reading Expert</cell><cell>Gating Network</cell></row><row><cell cols="3">[CLS] a cute curly [mask]</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Token/Visual Feature</cell><cell></cell><cell></cell><cell>Multi-layer Perceptron</cell></row><row><cell cols="2">in the wood</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Knowledge</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Minining</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Input</cell></row><row><cell cols="3">Region-Based</cell><cell></cell><cell cols="2">Grid-Based</cell><cell cols="2">Patch-Based</cell><cell></cell><cell></cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell cols="4">Vision and Language Pr etr aining</cell></row><row><cell cols="5">Compr ehensive Featur e Repr esentation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Task-specific Optimization</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Visual Featur e</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Textual</cell><cell>Vision</cell><cell>Text Reading</cell><cell>Clock Reading</cell></row><row><cell cols="2">Region</cell><cell>Grid</cell><cell></cell><cell>Patch</cell><cell></cell><cell>Region</cell><cell>Grid</cell><cell>Patch</cell><cell>Featur e</cell><cell>Expert</cell><cell>Expert</cell><cell>Expert</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Feature</cell><cell>Feature</cell><cell>Feature</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Object Detection</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Knowledge-guided MoE</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Learning to Attend</cell><cell></cell></row></table><note>? Figure 2: The overview of the new VQA architecture.? Knowledge-guided MoE (Sec 2.4)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>VQA Data Statistics.</figDesc><table><row><cell></cell><cell cols="6">Images Questions Yes/No Number Other Answers</cell></row><row><cell>Training</cell><cell>80K</cell><cell>443K</cell><cell>169K</cell><cell>58K</cell><cell>219K</cell><cell>4.4M</cell></row><row><cell>Validation</cell><cell>40K</cell><cell>214K</cell><cell>81K</cell><cell>28K</cell><cell>106K</cell><cell>2.1M</cell></row><row><cell>Test</cell><cell>80K</cell><cell>447K</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>3 Experiments</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3.1 Data</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Pre-training Data The same in-domain data is used as in LXMERT [Tan and Bansal, 2019] for pre-training. It consists of the image caption data from MS COCO</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Text-reading VQA Data Statistics. Clock Reading Expert, the images are collected from two sources. One is from open-access datasets. Specifically, a total of 4863 images are collected from COCO2017<ref type="bibr" target="#b45">[Lin et al., 2014]</ref> 3 and 2691 images from Ima-geNet<ref type="bibr" target="#b20">[Deng et al., 2009b]</ref> for clock labeling, both of which are widely used open-access datasets. Annotators are required to give the bounding boxes and the precision time of clocks in images. After labeling, 4236 and 3271 valid clock bounding boxes are obtained from COCO2017<ref type="bibr" target="#b45">[Lin et al., 2014]</ref> and ImageNet<ref type="bibr" target="#b20">[Deng et al., 2009b]</ref> respectively. 785 clock bounding boxes are randomly sampled from COCO2017 images for validation. The other source is Internet images. To further increase the generalization and capacity of our clock reader, 2878 images from internet with various clocks are collected. After careful annotation, 2314 valid clocks are obtained. Note that this data is only used for the training of the clock reader.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Images Questions</cell></row><row><cell>VQA-Subset</cell><cell>20k</cell><cell>21k</cell></row><row><cell>TextVQA</cell><cell>25k</cell><cell>39k</cell></row><row><cell>ST-VQA</cell><cell>19k</cell><cell>26k</cell></row><row><cell>For training</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>VQA Challenge Leaderboard.</figDesc><table><row><cell cols="3">VQA Challenge Leaderboard (Test-std)</cell><cell></cell><cell></cell></row><row><cell>Models</cell><cell cols="4">Overall Yes/No Number Other</cell></row><row><cell>Human</cell><cell>80.83</cell><cell>95.48</cell><cell>81.29</cell><cell>67.97</cell></row><row><cell>LXMERT (Tan and Bansal [2019])</cell><cell>74.34</cell><cell>89.45</cell><cell>56.69</cell><cell>65.22</cell></row><row><cell>MCAN (Yu et al. [2019a])</cell><cell>75.23</cell><cell>90.36</cell><cell>59.17</cell><cell>65.75</cell></row><row><cell>VILLA (Gan et al. [2020])</cell><cell>75.85</cell><cell>91.30</cell><cell>59.23</cell><cell>66.20</cell></row><row><cell>BGN (Guo et al. [2019])</cell><cell>75.92</cell><cell>90.89</cell><cell>61.13</cell><cell>66.28</cell></row><row><cell>InterBERT (Lin et al. [2020])</cell><cell>76.10</cell><cell>91.67</cell><cell>59.24</cell><cell>66.40</cell></row><row><cell>GridFeat+MoVie (Jiang et al. [2020])</cell><cell>76.29</cell><cell>90.81</cell><cell>61.53</cell><cell>67.04</cell></row><row><cell>VinVL (Zhang et al. [2021])</cell><cell>77.45</cell><cell>92.38</cell><cell>62.55</cell><cell>67.87</cell></row><row><cell>ROSITA (Cui et al. [2021])</cell><cell>78.34</cell><cell>92.66</cell><cell>63.24</cell><cell>69.33</cell></row><row><cell>UNIMO (Li et al. [2020a])</cell><cell>78.40</cell><cell>93.10</cell><cell>63.06</cell><cell>69.12</cell></row><row><cell>VQA Challenge 2021 winner</cell><cell>79.34</cell><cell>93.28</cell><cell>65.36</cell><cell>70.40</cell></row><row><cell>PASH-SFE</cell><cell>79.47</cell><cell>92.45</cell><cell>76.57</cell><cell>68.82</cell></row><row><cell>SimVLM (Wang et al. [2021])</cell><cell>80.34</cell><cell>93.29</cell><cell>66.54</cell><cell>72.23</cell></row><row><cell>ALICEMIND-MMU</cell><cell>81.26</cell><cell>93.55</cell><cell>72.01</cell><cell>72.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison with other single models.</figDesc><table><row><cell></cell><cell cols="2">Performance of Single Models</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Models</cell><cell>Feature Type</cell><cell cols="5">BASE Params Test-dev Test-std Params Test-dev Test-std LARGE</cell></row><row><cell>VLBERT (Su et al. [2019])</cell><cell>Region</cell><cell>110M 71.16</cell><cell>-</cell><cell cols="3">345M 71.79 72.22</cell></row><row><cell>UNITER (Chen et al. [2020])</cell><cell>Region</cell><cell cols="5">110M 72.70 72.91 345M 73.82 74.02</cell></row><row><cell>OSCAR (Li et al. [2020b])</cell><cell>Region</cell><cell cols="5">110M 73.16 73.44 345M 73.61 73.82</cell></row><row><cell>UNIMO (Li et al. [2020a])</cell><cell>Region</cell><cell cols="5">110M 73.79 74.02 345M 75.06 75.27</cell></row><row><cell>VinVL (Zhang et al. [2021] )</cell><cell>Region</cell><cell cols="5">110M 75.95 76.12 345M 76.52 76.60</cell></row><row><cell>ViLBERT (Lu et al. [2019])</cell><cell>Region</cell><cell cols="2">221M 70.55 70.92</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>12-in-1 (Lu et al. [2020])</cell><cell>Region</cell><cell>221M 73.15</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>LXMERT (Tan and Bansal [2019] )</cell><cell>Region</cell><cell cols="2">183M 72.42 72.54</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell>ERNIE-ViL (Yu et al. [2021] )</cell><cell>Region</cell><cell cols="5">250M 73.18 73.36 510M 74.95 75.10</cell></row><row><cell>PixelBERT (Huang et al. [2020])</cell><cell>Grid</cell><cell cols="2">170M 74.45 74.55</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ViLT (Kim et al. [2021])</cell><cell>Patch</cell><cell>110M 71.26</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Region-VLP</cell><cell>Region</cell><cell>110M 76.25</cell><cell>-</cell><cell cols="2">345M 77.17</cell><cell>-</cell></row><row><cell>Grid-VLP</cell><cell>Grid</cell><cell>110M 76.50</cell><cell>-</cell><cell cols="2">345M 77.13</cell><cell>-</cell></row><row><cell>Patch-VLP</cell><cell>Patch</cell><cell>110M 71.61</cell><cell>-</cell><cell>345M</cell><cell>-</cell><cell>-</cell></row><row><cell>Fusion-VLP</cell><cell cols="6">Region+Grid+Patch 110M 76.80 76.78 345M 77.59 77.61</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Ablation study of visual features on VQA Test-dev.</figDesc><table><row><cell></cell><cell cols="4">Overall Yes/No Number Other</cell></row><row><cell>Fusion-VLP</cell><cell>77.59</cell><cell>91.91</cell><cell>64.29</cell><cell>68.33</cell></row><row><cell>Region-VLP</cell><cell>77.17</cell><cell>91.62</cell><cell>63.69</cell><cell>67.84</cell></row><row><cell>Grid-VLP</cell><cell>77.13</cell><cell>92.20</cell><cell>59.99</cell><cell>68.15</cell></row><row><cell>Patch-VLP</cell><cell>71.61</cell><cell>88.17</cell><cell>49.44</cell><cell>62.54</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Ablation study of learning to attend on VQA Test-dev.</figDesc><table><row><cell></cell><cell cols="4">Overall Yes/no Number Other</cell></row><row><cell>Region-VLP (Baseline)</cell><cell>76.75</cell><cell>91.28</cell><cell>63.31</cell><cell>67.34</cell></row><row><cell>+ Learning to Attend (FFN)</cell><cell>77.09</cell><cell>91.58</cell><cell>63.54</cell><cell>67.74</cell></row><row><cell>+ Learning to Attend (Param)</cell><cell>77.17</cell><cell>91.62</cell><cell>63.69</cell><cell>67.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Ablation study of text reading expert on the VQA Test-dev.</figDesc><table><row><cell></cell><cell cols="5">Overall Yes/No Number Other ANLS</cell></row><row><cell>Visual Understanding Expert</cell><cell>79.44</cell><cell>93.31</cell><cell>65.70</cell><cell>71.16</cell><cell>-</cell></row><row><cell>+ Text-reading VQA data</cell><cell>80.35</cell><cell>93.31</cell><cell>69.81</cell><cell cols="2">71.49 79.85</cell></row><row><cell>+ add separator</cell><cell>80.41</cell><cell>93.31</cell><cell>69.82</cell><cell cols="2">71.64 79.96</cell></row><row><cell>+ continue pre-training</cell><cell>80.63</cell><cell>93.31</cell><cell>69.97</cell><cell cols="2">72.01 80.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Quantitative analysis on the clustering results of different clustering methods.</figDesc><table><row><cell></cell><cell>Acc</cell><cell>P</cell><cell>R</cell><cell>Macro-F1</cell></row><row><cell cols="4">DBSCAN (eps=0.5) 0.1544 0.4605 0.3861</cell><cell>0.1466</cell></row><row><cell>K-Means (K=3)</cell><cell cols="3">0.4969 0.6487 0.662</cell><cell>0.6163</cell></row><row><cell>K-Means (K=4)</cell><cell cols="3">0.7894 0.8239 0.8219</cell><cell>0.8195</cell></row><row><cell>K-Means (K=5)</cell><cell cols="3">0.8448 0.8659 0.8898</cell><cell>0.8739</cell></row><row><cell>K-Means (K=6)</cell><cell cols="3">0.8443 0.8668 0.8918</cell><cell>0.8740</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Ablation study of clock reading expert.</figDesc><table><row><cell>Clock Detector</cell><cell></cell><cell></cell><cell>Clock Reader</cell><cell></cell><cell cols="2">VQA Test-dev</cell></row><row><cell cols="7">Detection(mAP) Cls Regression Loss Self-supervised Loss Clock Accuracy Number Overall</cell></row><row><cell>Baseline</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>59.93</cell><cell>76.51</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>72.5</cell><cell>62.52</cell><cell>76.79</cell></row><row><cell>79.30</cell><cell></cell><cell></cell><cell></cell><cell>73.0</cell><cell>62.59</cell><cell>76.80</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>74.7</cell><cell>62.65</cell><cell>76.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Ablation study of MoE on the VQA Test-dev.</figDesc><table><row><cell>Overall Yes/No Number Other</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>The distribution of abilities in each answer type.</figDesc><table><row><cell>Category</cell><cell cols="2">Examples</cell><cell></cell><cell>Category</cell><cell cols="2">Examples</cell></row><row><cell>Common Sense</cell><cell></cell><cell></cell><cell></cell><cell>Visual Recognition</cell><cell></cell></row><row><cell>Knowledge</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Q: Is there snow on the ground?</cell><cell cols="2">Q: What type of food is being</cell><cell cols="2">Q: What kind of bear is this?</cell><cell>Q: What type of flowers are</cell></row><row><cell cols="2">A: yes</cell><cell>sold?</cell><cell></cell><cell cols="2">A: grizzly</cell><cell>those?</cell></row><row><cell></cell><cell></cell><cell cols="2">A: donuts</cell><cell></cell><cell></cell><cell>A: daffodils</cell></row><row><cell>Object Counting</cell><cell></cell><cell></cell><cell></cell><cell>Relational Reasoning</cell><cell></cell></row><row><cell cols="2">Q: How many couches?</cell><cell cols="2">Q: How many trees are in the</cell><cell cols="2">Q: What is the dish on the left?</cell><cell>Q: Which elephant is tallest?</cell></row><row><cell>A: 2</cell><cell></cell><cell>picture?</cell><cell></cell><cell cols="2">A: sandwich</cell><cell>A: left</cell></row><row><cell></cell><cell></cell><cell>A: 37</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Textual Recognition</cell><cell></cell><cell></cell><cell></cell><cell>Clock</cell><cell></cell></row><row><cell>(OCR)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Q: What number is the player?</cell><cell cols="2">Q: What does the sign say?</cell><cell cols="2">Q: What time is on the clock?</cell><cell>Q: What time is it?</cell></row><row><cell>A: 46</cell><cell></cell><cell cols="2">A: one way</cell><cell cols="2">A: 12:18</cell><cell>A: 10:20</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Clock</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Q: What time is the clock</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">showing?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">A: 2:55</cell></row><row><cell cols="2">Overall Distribution</cell><cell cols="2">Yesno Type</cell><cell cols="2">Number Type</cell><cell>Other Type</cell></row><row><cell>Relational Reasoning 13.4%</cell><cell cols="2">Clock 0.4% Other Visual Recognition Textual Recognition (OCR) Object Counting 0.6% 5.9% 6.2% 8.7%</cell><cell cols="2">Clock Object Counting 0.5%Other 0.2% Visual Recognition Textual Recognition 2.4% Relational Reasoning 10.4% Textual Recognition 15.0%</cell><cell cols="2">Other Visual Recognition Clock Common Sense 1.3% Knowledge Relational 0.6% 2.5% 7.5% Reasoning 11.9% Reasoning Relational 15.9%</cell><cell>0.7% 6.6% 9.6%</cell><cell>Object Counting 0.5% Other Textual Recognition Visual Recognition</cell></row><row><cell>64.7%</cell><cell></cell><cell>83.7%</cell><cell></cell><cell>61.3%</cell><cell></cell><cell>66.6%</cell></row><row><cell>Common Sense Knowledge</cell><cell cols="6">Common Sense Knowledge Figure 6: Object Object Counting Common Sense Knowledge Counting</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Q: What does the sign s</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>A: one way</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>The overall performance of ALICEMIND-MMU and human on val split.</figDesc><table><row><cell></cell><cell>Test-std</cell><cell>Val</cell></row><row><cell></cell><cell cols="2">Overall Overall Yes/no Number Other</cell></row><row><cell>VLP</cell><cell cols="2">81.26 79.54 92.47 70.63 72.00</cell></row><row><cell cols="3">Human 80.83 78.69 94.87 78.79 66.34</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 :</head><label>12</label><figDesc>The performance of ALICEMIND-MMU and human by category.</figDesc><table><row><cell></cell><cell>Commonsense Knowledge</cell><cell>Relational Reasoning</cell><cell cols="2">Object Counting Visual Recognition</cell><cell>Textual Recognition (OCR)</cell><cell cols="2">Clock Reading Other</cell></row><row><cell></cell><cell>767</cell><cell>159</cell><cell>103</cell><cell>70</cell><cell>74</cell><cell>7</cell><cell>5</cell></row><row><cell>VLP</cell><cell>83.60</cell><cell>71.19</cell><cell>77.76</cell><cell>68.14</cell><cell>52.03</cell><cell>86.00</cell><cell>70.00</cell></row><row><cell>Human</cell><cell>80.04</cell><cell>70.20</cell><cell>81.29</cell><cell>59.76</cell><cell>76.62</cell><cell>60.66</cell><cell>49.52</cell></row><row><cell cols="8">MMU , as well as the top three ground-truth annotations are also given for comparison. The scores of Human and</cell></row><row><cell cols="4">ALICEMIND-MMU are calculated based on Equation 13</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">the minute task is divided into 12 bins by 5 moves per bin.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The COCO images used in the VQA test set are left unlabeled and excluded from our training data.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We also tested with 64 and 128 selected grids. It did not lead to significantly different results.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">An early version of visual understanding expert for VQA Challenge 2021 is used as the baseline.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth:</head><p>Ground Truth:</p><p>Ground Truth:</p><p>Ground Truth:</p><p>Ground Truth: Ground Truth: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veda</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Grewe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Nham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madeleine</forename><surname>Leach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thore Graepel, and Demis Hassabis</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="page" from="484" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="806" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="31" />
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08530</idno>
		<title level="m">Vl-bert: Pre-training of generic visual-linguistic representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Uniter: Universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="104" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07490</idno>
		<title level="m">Lxmert: Learning cross-modality encoder representations from transformers</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ernie-vil: Knowledge enhanced visionlanguage representations through scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiji</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3208" to="3216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Vinvl: Revisiting visual representations in vision-language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5579" to="5588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Semvlp: Vision-language pre-training by aligning semantics at multiple levels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuli</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07829</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Pixel-bert: Aligning image pixels with text by deep multi-modal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00849</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">E2e-vlp: End-to-end vision-language pre-training enhanced by visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenming</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.01804</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Grid-vlp: Revisiting grid features for vision-language pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">In defense of grid features for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10267" to="10276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhsuan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05918</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Virtex: Learning visual representations from textual annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11162" to="11173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pre-trained image processing transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12299" to="12310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Vilt: Vision-and-language transformer without convolution or region supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bokyung</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.03334</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3558" to="3568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuyi</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangnan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.04577</idno>
		<title level="m">Structbert: incorporating language structures into pre-training for deep language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth Berkeley symposium on mathematical statistics and probability</title>
		<meeting>the fifth Berkeley symposium on mathematical statistics and probability<address><addrLine>Oakland, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1967" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Structurallm: Structural pretraining for form understanding. CoRR, abs/2105.11210, 2021b</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2105.11210" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Reading wikipedia to answer open-domain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00051</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Two at once: Enhancing learning and generalization capacities via ibn-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="464" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Adaptive mixtures of local experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="87" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06538</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03961</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Towards VQA models that can read. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meet</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1904.08920" />
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Scene text visual question answering. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rub?n</forename><surname>Ali Furkan Biten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?s</forename><surname>Tito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llu?s</forename><surname>Mafla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mar?al</forename><surname>G?mez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernest</forename><surname>Rusi?ol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Valveny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karatzas</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1905.13648" />
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Gqa: A new dataset for real-world visual reasoning and compositional question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6700" to="6709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Visual7w: Grounded question answering in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4995" to="5004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2556" to="2565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Building a test collection for complex document information processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Agam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Argamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Frieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heard</surname></persName>
		</author>
		<idno type="DOI">10.1145/1148170.1148307</idno>
		<ptr target="https://doi.org/10.1145/1148170.1148307" />
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;06</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="665" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep modular co-attention networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6281" to="6290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Large-scale adversarial training for vision-and-language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Bilinear graph networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.09815</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Interbert: Vision-and-language interaction for multi-modal pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13198</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Rosita: Enhancing vision-and-language semantic alignments via cross-and intra-modal knowledge integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongzhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guocheng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15409</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Simvlm: Simple visual language model pretraining with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<idno>abs/2108.10904</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">12-in-1: Multi-task vision and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01847</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Multi-modal factorized bilinear pooling with co-attention learning for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1821" to="1830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deep modular co-attention networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6281" to="6290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuejian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11336" to="11344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedanuj</forename><surname>Duy-Kien Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11883</idno>
		<title level="m">Movie: Revisiting modulated convolutions for visual counting and beyond</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Ask me anything: Free-form visual question answering based on knowledge from external sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4622" to="4630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Microsoft coco captions: Data collection and evaluation server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">The open images dataset v4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1956" to="1981" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Objects365: A large-scale, high-quality dataset for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8430" to="8439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">A corpus for reasoning about natural language grounded in photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ally</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00491</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
