<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GAIN: Missing Data Imputation using Generative Adversarial Nets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsung</forename><surname>Yoon</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Jordon</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihaela</forename><surname>Van Der Schaar</surname></persName>
						</author>
						<title level="a" type="main">GAIN: Missing Data Imputation using Generative Adversarial Nets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel method for imputing missing data by adapting the well-known Generative Adversarial Nets (GAN) framework. Accordingly, we call our method Generative Adversarial Imputation Nets (GAIN). The generator (G) observes some components of a real data vector, imputes the missing components conditioned on what is actually observed, and outputs a completed vector. The discriminator (D) then takes a completed vector and attempts to determine which components were actually observed and which were imputed. To ensure that D forces G to learn the desired distribution, we provide D with some additional information in the form of a hint vector. The hint reveals to D partial information about the missingness of the original sample, which is used by D to focus its attention on the imputation quality of particular components. This hint ensures that G does in fact learn to generate according to the true data distribution. We tested our method on various datasets and found that GAIN significantly outperforms state-of-the-art imputation methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Missing data is a pervasive problem. Data may be missing because it was never collected, records were lost or for many other reasons. In the medical domain, the respiratory rate of a patient may not have been measured (perhaps because it was deemed unnecessary/unimportant) or accidentally not recorded <ref type="bibr" target="#b24">(Yoon et al., 2017;</ref><ref type="bibr" target="#b0">Alaa et al., 2018)</ref>. It may also be the case that certain pieces of information are difficult or even dangerous to acquire (such as information gathered from a biopsy), and so these were not gathered for those reasons <ref type="bibr" target="#b26">(Yoon et al., 2018b</ref>). An imputation algorithm can be used to estimate missing values based on data that was observed/measured, such as the systolic blood pressure and Missing data can be categorized into three types: (1) the data is missing completely at random (MCAR) if the missingness occurs entirely at random (there is no dependency on any of the variables), (2) the data is missing at random (MAR) if the missingness depends only on the observed variables 1 , (3) the data is missing not at random (MNAR) if the missingness is neither MCAR nor MAR (more specifically, the data is MNAR if the missingness depends on both observed variables and the unobserved variables; thus, missingness cannot be fully accounted for by the observed variables). In this paper we provide theoretical results for our algorithm under the MCAR assumption, and compare to other stateof-the-art methods in this setting 2 .</p><p>State-of-the-art imputation methods can be categorized as either discriminative or generative. Discriminative methods include MICE <ref type="bibr" target="#b5">(Buuren &amp; Oudshoorn, 2000;</ref><ref type="bibr" target="#b4">Buuren &amp; Groothuis-Oudshoorn, 2011)</ref>, MissForest <ref type="bibr" target="#b21">(Stekhoven &amp; B?hlmann, 2011)</ref>, and matrix completion <ref type="bibr" target="#b15">(Mazumder et al., 2010a;</ref><ref type="bibr" target="#b28">Yu et al., 2016;</ref><ref type="bibr" target="#b20">Schnabel et al., 2016;</ref><ref type="bibr" target="#b16">Mazumder et al., 2010b)</ref>; generative methods include algorithms based on Expectation Maximization <ref type="bibr" target="#b7">(Garc?a-Laencina et al., 2010)</ref> and algorithms based on deep learning (e.g. denoising autoencoders (DAE) and generative adversarial nets (GAN)) <ref type="bibr" target="#b23">(Vincent et al., 2008;</ref><ref type="bibr" target="#b8">Gondara &amp; Wang, 2017;</ref><ref type="bibr" target="#b1">Allen &amp; Li, 2016)</ref>. However, current generative methods for imputation have various drawbacks. For instance, the approach for data imputation based on <ref type="bibr" target="#b7">(Garc?a-Laencina et al., 2010)</ref> makes assumptions about the underlying distribution and fails to generalize well when datasets contain mixed categorical and continuous variables. In contrast, the approaches based on DAE <ref type="bibr" target="#b23">(Vincent et al., 2008)</ref>  LG] 7 Jun 2018 many circumstances, missing values are part of the inherent structure of the problem so obtaining a complete dataset is impossible. Another approach with DAE <ref type="bibr" target="#b8">(Gondara &amp; Wang, 2017)</ref> allows for an incomplete dataset; however, it only utilizes the observed components to learn the representations of the data. <ref type="bibr" target="#b1">(Allen &amp; Li, 2016)</ref> uses Deep Convolutional GANs for image completion; however, it also requires complete data for training the discriminator.</p><p>In this paper, we propose a novel imputation method, which we call Generative Adversarial Imputation Nets (GAIN), that generalizes the well-known GAN <ref type="bibr" target="#b9">(Goodfellow et al., 2014)</ref> and is able to operate successfully even when complete data is unavailable. In GAIN, the generator's goal is to accurately impute missing data, and the discriminator's goal is to distinguish between observed and imputed components. The discriminator is trained to minimize the classification loss (when classifying which components were observed and which have been imputed), and the generator is trained to maximize the discriminator's misclassification rate. Thus, these two networks are trained using an adversarial process. To achieve this goal, GAIN builds on and adapts the standard GAN architecture. To ensure that the result of this adversarial process is the desired target, the GAIN architecture provides the discriminator with additional information in the form of "hints". This hinting ensures that the generator generates samples according to the true underlying data distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Problem Formulation</head><p>Consider a d-dimensional space X = X 1 ? ... ? X d . Suppose that X = (X 1 , ..., X d ) is a random variable (either continuous or binary) taking values in X , whose distribution we will denote P (X). Suppose that M = (M 1 , ..., M d ) is a random variable taking values in {0, 1} d . We will call X the data vector, and M the mask vector.</p><p>For each i ? {1, ..., d}, we define a new spaceX i = X i ? { * } where * is simply a point not in any X i , representing an unobserved value. LetX =X 1 ? ... ?X d . We define a new random variableX = (X 1 , ...,X d ) ?X in the following way:</p><formula xml:id="formula_0">X i = X i , if M i = 1 * , otherwise<label>(1)</label></formula><p>so that M indicates which components of X are observed. Note that we can recover M fromX.</p><p>Throughout the remainder of the paper, we will often use lower-case letters to denote realizations of a random variable and use the notation 1 to denote a vector of 1s, whose dimension will be clear from the context (most often, d). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Imputation</head><p>In the imputation setting, n i.i.d. copies ofX are realized, denotedx 1 , ...,x n and we define the dataset D =</p><formula xml:id="formula_1">{(x i , m i )} n i=1 , where m i is simply the recovered realiza- tion of M corresponding tox i .</formula><p>Our goal is to impute the unobserved values in eachx i . Formally, we want to generate samples according to P (X|X = x i ), the conditional distribution of X givenX =x i , for each i, to fill in the missing data points in D. By attempting to model the distribution of the data rather than just the expectation, we are able to make multiple draws and therefore make multiple imputations allowing us to capture the uncertainty of the imputed values <ref type="bibr" target="#b5">(Buuren &amp; Oudshoorn, 2000;</ref><ref type="bibr" target="#b4">Buuren &amp; Groothuis-Oudshoorn, 2011;</ref><ref type="bibr" target="#b19">Rubin, 2004)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Generative Adversarial Imputation Nets</head><p>In this section we describe our approach for simulating P (X|X =x i ) which is motivated by GANs. We highlight key similarities and differences to a standard (conditional) GAN throughout. <ref type="figure" target="#fig_0">Fig. 1</ref> depicts the overall architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Generator</head><p>The generator, G, takes (realizations of)X, M and a noise variable, Z, as input and outputsX, a vector of imputations. Let G :X ? {0, 1} d ? [0, 1] d ? X be a function, and Z = (Z 1 , ..., Z d ) be d-dimensional noise (independent of all other variables).</p><p>Then we define the random variablesX,X ? X b?</p><formula xml:id="formula_2">X = G(X, M, (1 ? M) Z)</formula><p>(2)</p><formula xml:id="formula_3">X = M X + (1 ? M) X<label>(3)</label></formula><p>where denotes element-wise multiplication.X corresponds to the vector of imputed values (note that G outputs a value for every component, even if its value was observed) andX corresponds to the completed data vector, that is, the vector obtained by taking the partial observationX and replacing each * with the corresponding value ofX.</p><p>This setup is very similar to a standard GAN, with Z being analogous to the noise variables introduced in that framework. Note, though, that in this framework, the target distribution, P (X|X), is essentially ||1 ? M|| 1 -dimensional and so the noise we pass into the generator is (1 ? M) Z, rather than simply Z, so that its dimension matches that of the targeted distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Discriminator</head><p>As in the GAN framework, we introduce a discriminator, D, that will be used as an adversary to train G. However, unlike in a standard GAN where the output of the generator is either completely real or completely fake, in this setting the output is comprised of some components that are real and some that are fake. Rather than identifying that an entire vector is real or fake, the discriminator attempts to distinguish which components are real (observed) or fake (imputed)this amounts to predicting the mask vector, m. Note that the mask vector M is pre-determined by the dataset.</p><p>Formally, the discriminator is a function D : X ? [0, 1] d with the i-th component of D(x) corresponding to the probability that the i-th component ofx was observed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Hint</head><p>As will be seen in the theoretical results that follow, it is necessary to introduce what we call a hint mechanism. A hint mechanism is a random variable, H, taking values in a space H, both of which we define. We allow H to depend on M and for each (imputed) sample (x, m), we draw h according to the distribution H|M = m. We pass h as an additional input to the discriminator and so it becomes a function D : X ? H ? [0, 1] d , where now the i-th compo-nent of D(x, h) corresponds to the probability that the i-th component ofx was observed conditional onX =x and H = h.</p><p>By defining H in different ways, we control the amount of information contained in H about M and in particular we show (in Proposition 1) that if we do not provide "enough" information about M to D (such as if we simply did not have a hinting mechanism), then there are several distributions that G could reproduce that would all be optimal with respect to D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Objective</head><p>We train D to maximize the probability of correctly predicting M. We train G to minimize the probability of D predicting M. We define the quantity V (D, G) to be</p><formula xml:id="formula_4">V (D, G) = EX ,M,H M T log D(X, H) (4) + (1 ? M) T log 1 ? D(X, H) ,</formula><p>where log is element-wise logarithm and dependence on G is throughX.</p><p>Then, as with the standard GAN, we define the objective of GAIN to be the minimax problem given by</p><formula xml:id="formula_5">min G max D V (D, G).<label>(5)</label></formula><p>We define the loss function L :</p><formula xml:id="formula_6">{0, 1} d ? [0, 1] d ? R by L(a, b) = d i=1 a i log(b i ) + (1 ? a i ) log(1 ? b i ) . (6)</formula><p>WritingM = D(X, H), we can then rewrite <ref type="formula" target="#formula_5">(5)</ref> as</p><formula xml:id="formula_7">min G max D E L(M,M) .<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Theoretical Analysis</head><p>In this section we provide a theoretical analysis of (5). Given</p><formula xml:id="formula_8">a d-dimensional space Z = Z 1 ? ... ? Z d , a (probability) density 3 p over Z corresponding to a random variable Z, and a vector b ? {0, 1} d we define the set A b = {i : b i = 1}, the projection ? b : Z ? ? i?A b Z i by ? b (z) = (z i ) i?A and the density p b to be the density of ? b (Z).</formula><p>Throughout this section, we make the assumption that M is independent of X, i.e. that the data is MCAR.</p><p>We will write p(x, m, h) to denote the density of the random variable (X, M, H) and we will writep, p m and p h to denote the marginal densities (of p) corresponding toX, M and H, respectively. When referring to the joint density of two of the three variables (potentially conditioned on the third), we will simply use p, abusing notation slightly.</p><p>It is more intuitive to think of this density through its decomposition into densities corresponding to the true data generating process, and to the generator defined by <ref type="formula">(2)</ref>,</p><formula xml:id="formula_9">p(x, m, h) =p m (m)p m (? m (x|m)) (8) ?p 1?m (? 1?m (x)|m, ? m (x))p h (h|m).</formula><p>The first two terms in (8) are both defined by the data,</p><formula xml:id="formula_10">wherep m (? m (x)|m) is the density of ? m (X)|M = m which corresponds to the density of ? m (X) (i.e. the true data distribution), since conditional on M = m, ? m (X) = ? m (X) (see equations 1 and 3). The third term,p 1?m (? 1?m (x)|m, ? m (x))</formula><p>, is determined by the generator, G, and is the density of the random variable</p><formula xml:id="formula_11">? 1?m (G(x, m, Z)) = ? 1?m (X)|X =x, M = m wher? x is determined by m and ? m (x).</formula><p>The final term is the conditional density of the hint, which we are free to define (its selection will be motivated by the following analysis).</p><p>Using this decomposition, one can think of drawing a sample fromp as first sampling m according to p m (?), then sampling the "observed" components, x obs , according t? p m (?) (we can then constructx from x obs and m), then generating the imputed values, x imp , from the generator according top 1?m (?|m, x obs ) and finally sampling the hint according to p h (?|m).</p><p>Lemma 1. Let x ? X . Let p h be a fixed density over the hint space H and let h ? H be such that p(x, h) &gt; 0. Then for a fixed generator, G, the i-th component of the optimal discriminator, D * (x, h) is given by</p><formula xml:id="formula_12">D * (x, h) i = p(x, h, m i = 1) p(x, h, m i = 1) + p(x, h, m i = 0) (9) = p m (m i = 1|x, h)<label>(10)</label></formula><p>for each i ? {1, ..., d}.</p><p>Proof. All proofs are provided in Supplementary Materials.</p><p>We now rewrite (4), substituting for D * , to obtain the following minimization criterion for G:</p><formula xml:id="formula_13">C(G) =EX ,M,H i:Mi=1 log p m (m i = 1|X, H) (11) + i:Mi=0 log p m (m i = 0|X, H) ,</formula><p>where dependence on G is through p m (?|X).</p><p>Theorem 1. A global minimum for C(G) is achieved if and only if the densityp satisfie?</p><formula xml:id="formula_14">p(x|h, m i = t) =p(x|h)<label>(12)</label></formula><p>for each i ? {1, ..., d}, x ? X and h ? H such that p h (h|m i = t) &gt; 0.</p><p>The following proposition asserts that if H does not contain "enough" information about M, we cannot guarantee that G learns the desired distribution (the one uniquely defined by the (underlying) data </p><formula xml:id="formula_15">B j = 1 if j = k 0 if j = k.<label>(13)</label></formula><p>Let H = {0, 0.5, 1} d and, given M, define</p><formula xml:id="formula_16">H = B M + 0.5(1 ? B).<label>(14)</label></formula><p>Observe first that H is such that H i = t =? M i = t for t ? {0, 1} but that H i = 0.5 implies nothing about M i . In other words, H reveals all but one of the components of M to D. Note, however, that H does contain some information about M i since M i is not assumed to be independent of the other components of M.</p><p>The following lemma confirms that the discriminator behaves as we expect with respect to this hint mechanism. Lemma 2. Suppose H is defined as above. Then for h such that h i = 0 we have D * (x, h) i = 0 and for h such that h i = 1 we have D * (x, h) i = 1, for all x ? X , i ? {1, ..., d}.</p><p>The final proposition we state tells us that H as specified above ensures the generator learns to replicate the desired distribution. Proposition 2. Suppose H is defined as above. Then the solution to (12) is unique and satisfie?</p><formula xml:id="formula_17">p(x|m 1 ) =p(x|m 2 )<label>(15)</label></formula><p>for all m 1 , m 2 ? {0, 1} d . In particular,p(x|m) =p(x|1) and since M is independent of X,p(x|1) is the density of X. The distribution ofX is therefore the same as the distribution of X.</p><p>For the remainder of the paper, B and H will be defined as in equations <ref type="formula" target="#formula_0">(13)</ref> and <ref type="formula" target="#formula_0">(14)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">GAIN Algorithm</head><p>Using an approach similar to that in <ref type="bibr" target="#b9">(Goodfellow et al., 2014)</ref>, we solve the minimax optimization problem (5) in an iterative manner. Both G and D are modeled as fully connected neural nets.</p><p>We first optimize the discriminator D with a fixed generator G using mini-batches of size k D 4 . For each sample in the mini-batch, (x(j), m(j)) 5 , we draw k D independent samples, z(j) and b(j), of Z and B and computex(j) and h(j) accordingly. Lemma 2 then tells us that the only outputs of D that depend on G are the ones corresponding to b i = 0 for each sample. We therefore only train D to give us these outputs (if we also trained D to match the outputs specified in Lemma 2 we would gain no information about G, but D would overfit to the hint vector). We define</p><formula xml:id="formula_18">L D : {0, 1} d ? [0, 1] d ? {0, 1} d ? R by L D (m,m, b) = i:bi=0 m i log(m i )<label>(16)</label></formula><formula xml:id="formula_19">+ (1 ? m i ) log(1 ?m i ) .</formula><p>D is then trained according to</p><formula xml:id="formula_20">min D ? k D j=1 L D (m(j),m(j), b(j))<label>(17)</label></formula><p>recalling thatm(j) = D(x(j), m(j)).</p><p>Second, we optimize the generator G using the newly updated discriminator D with mini-batches of size k G . We first note that G in fact outputs a value for the entire data vector (including values for the components we observed). Therefore, in training G, we not only ensure that the imputed values for missing components (m j = 0) successfully fool the discriminator (as defined by the minimax game), we also ensure that the values outputted by G for observed components (m j = 1) are close to those actually observed. This is justified by noting that the conditional distribution of X givenX =x obviously fixes the components of X corresponding to M i = 1 to beX i . This also ensures that the representations learned in the hidden layers ofX suitably capture the information contained inX (as in an auto-encoder).</p><p>To achieve this, we define two different loss functions. The first,</p><formula xml:id="formula_21">L G : {0, 1} d ? [0, 1] d ? {0, 1} d ? R, is given by L G (m,m, b) = ? i:bi=0 (1 ? m i ) log(m i ),<label>(18)</label></formula><p>4 Details of hyper-parameter selection can be found in the Supplementary Materials. <ref type="bibr">5</ref> The index j now corresponds to the j-th sample of the minibatch, rather than the j-th sample of the entire dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Pseudo-code of GAIN</head><p>while training loss has not converged do (1) Discriminator optimization Draw k D samples from the dataset</p><formula xml:id="formula_22">{(x(j), m(j))} k D j=1 Draw k D i.i.d. samples, {z(j)} k D j=1 , of Z Draw k D i.i.d. samples, {b(j)} k D j=1 , of B for j = 1, ..., k D d? x(j) ? G(x(j), m(j), z(j)) x(j) ? m(j) x(j) + (1 ? m(j)) x(j) h(j) = b(j) m(j) + 0.5(1 ? b(j)) end for Update D using stochastic gradient descent (SGD) ? D ? k D j=1 L D (m(j), D(x(j), h(j)), b(j)) (2) Generator optimization Draw k G samples from the dataset {(x(j), m(j))} k G j=1 Draw k G i.i.d. samples, {z(j)} k G j=1 of Z Draw k G i.i.d. samples, {b(j)} j=1 of B for j = 1, ..., k G do h(j) = b(j) m(j) + 0.5(1 ? b(j)) end for Update G using SGD (for fixed D) ? G k G j=1 L G (m(j),m(j), b(j)) + ?L M (x(j),x(j)) end while and the second, L M : R d ? R d ? R, by L M (x, x ) = d i=1 m i L M (x i , x i ),<label>(19)</label></formula><p>where</p><formula xml:id="formula_23">L M (x i , x i ) = (x i ? x i ) 2 , if x i is continuous, ?x i log(x i ), if x i is binary.</formula><p>As can be seen from their definitions, L G will apply to the missing components (m i = 0) and L M will apply to the observed components (m i = 1). L G (m,m) is smaller whenm i is closer to 1 for i such that m i = 0. That is, L G (m,m) is smaller when D is less able to identify the imputed values as being imputed (it falsely categorizes them as observed). L M (x,x) is minimized when the reconstructed features (i.e. the values G outputs for features that were observed) are close to the actually observed features. G is then trained to minimize the weighted sum of the two losses as follows:</p><formula xml:id="formula_24">min G k G j=1 L G (m(j),m(j), b(j)) + ?L M (x(j),x(j)),</formula><p>where ? is a hyper-parameter.</p><p>The pseudo-code is presented in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>In this section, we validate the performance of GAIN using multiple real-world datasets. In the first set of experiments we qualitatively analyze the properties of GAIN. In the second we quantitatively evaluate the imputation performance of GAIN using various UCI datasets <ref type="bibr" target="#b13">(Lichman, 2013)</ref>, giving comparisons with state-of-the-art imputation methods.</p><p>In the third we evaluate the performance of GAIN in various settings (such as on datasets with different missing rates).</p><p>In the final set of experiments we evaluate GAIN against other imputation algorithms when the goal is to perform prediction on the imputed dataset.</p><p>We conduct each experiment 10 times and within each experiment we use 5-cross validations. We report either RMSE or AUROC as the performance metric along with their standard deviations across the 10 experiments. Unless otherwise stated, missingness is applied to the datasets by randomly removing 20% of all data points (MCAR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Source of gain</head><p>The potential sources of gain for the GAIN framework are: the use of a GAN-like architecture (through L G ), the use of reconstruction error in the loss (L M ), and the use of the hint (H). In order to understand how each of these affects the performance of GAIN, we exclude one or two of them and compare the performances of the resulting architectures against the full GAIN architecture. <ref type="table" target="#tab_2">Table 1</ref> shows that the performance of GAIN is improved when all three components are included. More specifically, the full GAIN framework has a 15% improvement over the simple auto-encoder model (i.e. GAIN w/o L G ). Furthermore, utilizing the hint vector additionally gives improvements of 10%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Quantitative analysis of GAIN</head><p>We use five real-world datasets from UCI Machine Learning Repository <ref type="bibr" target="#b13">(Lichman, 2013</ref>) (Breast, Spam, Letter, Credit, and News) to quantitatively evaluate the imputation performance of GAIN. Details of each dataset can be found in the Supplementary Materials.</p><p>In table 2 we report the RMSE (and its standard deviation) for GAIN and 5 other state-of-the-art imputation methods: MICE <ref type="bibr" target="#b5">(Buuren &amp; Oudshoorn, 2000;</ref><ref type="bibr" target="#b4">Buuren &amp; Groothuis-Oudshoorn, 2011)</ref>, MissForest <ref type="bibr" target="#b21">(Stekhoven &amp; B?hlmann, 2011)</ref>, Matrix completion (Matrix) <ref type="bibr" target="#b15">(Mazumder et al., 2010a)</ref>, Auto-encoder <ref type="bibr" target="#b8">(Gondara &amp; Wang, 2017)</ref> and Expectation-maximization (EM) <ref type="bibr" target="#b7">(Garc?a-Laencina et al., 2010)</ref>. As can be seen from the table, GAIN significantly outperforms each benchmark. Results for the imputation quality of categorical variables in this experiment are given in the Supplementary Materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">GAIN in different settings</head><p>To better understand GAIN, we conduct several experiments in which we vary the missing rate, the number of samples, and the number of dimensions using Credit dataset. <ref type="figure" target="#fig_1">Fig.  2</ref> shows the performance (RMSE) of GAIN within these   <ref type="figure" target="#fig_1">Fig. 2 (a)</ref> shows that, even though the performance of each algorithm decreases as missing rates increase, GAIN consistently outperforms the benchmarks across the entire range of missing rates. <ref type="figure" target="#fig_1">Fig. 2 (b)</ref> shows that as the number of samples increases, the performance improvements of GAIN over the benchmarks also increases. This is due to the large number of parameters in GAIN that need to be optimized, however, as demonstrated on the Breast dataset (in <ref type="table" target="#tab_3">Table 2</ref>), GAIN is still able to outperform the benchmarks even when the number of samples is relatively small. <ref type="figure" target="#fig_1">Fig. 2 (c)</ref> shows that GAIN is also robust to the number of feature dimensions. On the other hand, the discriminative model (MissForest) cannot as easily cope when the number of feature dimensions is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Prediction Performance</head><p>We now compare GAIN against the same benchmarks with respect to the accuracy of post-imputation prediction. For this purpose, we use Area Under the Receiver Operating Characteristic Curve (AUROC) as the measure of performance. To be fair to all methods, we use the same predictive model (logistic regression) in all cases.</p><p>Comparisons are made on all datasets except Letter (as it has multi-class labels) and the results are reported in <ref type="table" target="#tab_4">Table  3</ref>.</p><p>As <ref type="table" target="#tab_4">Table 3</ref> shows, GAIN, which we have already shown to achieve the best imputation accuracy (in <ref type="table" target="#tab_3">Table 2</ref>), yields the best post-imputation prediction accuracy. However, even in cases where the improvement in imputation accuracy is large, the improvements in prediction accuracy are not always significant. This is probably due to the fact that there is sufficient information in the (80%) observed data to predict the label.</p><p>Prediction accuracy with various missing rates: In this experiment, we evaluate the post-imputation prediction performance when the missing rate of the dataset is varied. Note that every dataset (except Letter) has their own binary label.</p><p>The results of this experiment (for GAIN and the two most competitive benchmarks) are shown in <ref type="figure">Fig. 3</ref>. In particular, the performance of GAIN is significantly better than the other two for higher missing rates, this is due to the fact that as the information contained in the observed data decreases (due to more values being missing), the imputation quality becomes more important, and GAIN has already been shown  <ref type="figure">Figure 3</ref>. The AUROC performance with various missing rates with Credit dataset to provide (significantly) better quality imputations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Congeniality of GAIN</head><p>The congeniality of an imputation model is its ability to impute values that respect the feature-label relationship <ref type="bibr" target="#b17">(Meng, 1994;</ref><ref type="bibr" target="#b3">Burgess et al., 2013;</ref><ref type="bibr" target="#b6">Deng et al., 2016)</ref>. The congeniality of an imputation model can be evaluated by measuring the effects on the feature-label relationships after the imputation. We compare the logistic regression parameters, w, learned from the complete Credit dataset with the parameters,?, learned from an incomplete Credit dataset by first imputing and then performing logistic regression.</p><p>We report the mean and standard deviation of both the mean bias (||w ??|| 1 ) and the mean square error (||w ??|| 2 ) for each method in <ref type="table" target="#tab_5">Table 4</ref>. These quantities being lower indicates that the imputation algorithm better respects the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We propose a generative model for missing data imputation, GAIN. This novel architecture generalizes the well-known GAN such that it can deal with the unique characteristics of the imputation problem. Various experiments with realworld datasets show that GAIN significantly outperforms state-of-the-art imputation techniques. The development of a new, state-of-the-art technique for imputation can have transformative impacts; most datasets in medicine as well as in other domains have missing data. Future work will investigate the performance of GAIN in recommender systems, error concealment as well as in active sensing <ref type="bibr" target="#b29">(Yu et al., 2009)</ref>. Preliminary results in error concealment using the MNIST dataset (LeCun &amp; Cortes, 2010) can be found in the Supplementary Materials -see <ref type="figure">Fig. 4</ref> and 5.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The architecture of GAIN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>RMSE performance in different settings: (a) Various missing rates, (b) Various number of samples, (c) Various feature dimensions different settings in comparison to the two most competitive benchmarks (MissForest and Auto-encoder).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>have been shown to work well in practice but require complete data during training. In</figDesc><table /><note>1 A formal definition of MAR can be found in the Supplemen- tary Materials.2 Empirical results for the MAR and MNAR settings are shown in the Supplementary Materials.arXiv:1806.02920v1 [cs.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Source of gains in GAIN algorithm (Mean ? Std of RMSE (Gain (%))) ? .0006 .0513? .0016 .1198? .0005 .1858 ? .0010 .1441 ? .0007 GAIN w/o .0701 ? .0021 .0676 ? .0029 .1344 ? .0012 .2436 ? .0012 .1612 ? .0024 .0015 .0672 ? .0036 .1586 ? .0024 .2533 ? .0048 .2522 ? .0042 .0018 .0582 ? .0008 .1249 ? .0011 .2173 ? .0052 .1521 ? .0008</figDesc><table><row><cell>Algorithm</cell><cell>Breast</cell><cell>Spam</cell><cell>Letter</cell><cell>Credit</cell><cell>News</cell></row><row><cell cols="2">GAIN .0546 L G (22.1%)</cell><cell>(24.1%)</cell><cell>(10.9%)</cell><cell>(23.7%)</cell><cell>(10.6%)</cell></row><row><cell cols="2">GAIN w/o .0767 ? L M (28.9%)</cell><cell>(23.7%)</cell><cell>(24.4%)</cell><cell>(26.7%)</cell><cell>(42.9%)</cell></row><row><cell cols="2">GAIN w/o .0639 ? Hint (14.6%)</cell><cell>(11.9%)</cell><cell>(4.1%)</cell><cell>(14.5%)</cell><cell>(5.3%)</cell></row><row><cell>GAIN w/o</cell><cell cols="5">.0782 ? .0016 .0700 ? .0064 .1671 ? .0052 .2789 ? .0071 .2527 ? .0052</cell></row><row><cell>Hint &amp; L M</cell><cell>(30.1%)</cell><cell>(26.7%)</cell><cell>(28.3%)</cell><cell>(33.4%)</cell><cell>(43.0%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Imputation performance in terms of RMSE (Average ? Std of RMSE) ? .0006 .0513? .0016 .1198? .0005 .1858 ? .0010 .1441 ? .0007 MICE .0646 ? .0028 .0699 ? .0010 .1537 ? .0006 .2585 ? .0011 .1763 ? .0007 MissForest .0608 ? .0013 .0553 ? .0013 .1605 ? .0004 .1976 ? .0015 .1623 ? 0.012 Matrix .0946 ? .0020 .0542 ? .0006 .1442 ? .0006 .2602 ? .0073 .2282 ? .0005 Auto-encoder .0697 ? .0018 .0670 ? .0030 .1351 ? .0009 .2388 ? .0005 .1667 ? .0014 EM .0634 ? .0021 .0712 ? .0012 .1563 ? .0012 .2604 ? .0015 .1912 ? .0011</figDesc><table><row><cell>Algorithm</cell><cell>Breast</cell><cell>Spam</cell><cell>Letter</cell><cell>Credit</cell><cell>News</cell></row><row><cell>GAIN</cell><cell>.0546</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Prediction performance comparison .0073 .9529 ? .0023 .7527 ? .0031 .9711 ? .0027 MICE .9914 ? .0034 .9495 ? .0031 .7427 ? .0026 .9451 ? .0037 MissForest .9860 ? .0112 .9520 ? .0061 .7498 ? .0047 .9597 ? .0043 Matrix .9897 ? .0042 .8639 ? .0055 .7059 ? .0150 .8578 ? .0125 Auto-encoder .9916 ? .0059 .9403 ? .0051 .7485 ? .0031 .9321 ? .0058 EM .9899 ? .0147 .9217 ? .0093 .7390 ? .0079 .8987 ? .0157</figDesc><table><row><cell>Algorithm</cell><cell></cell><cell cols="2">AUROC (Average ? Std)</cell></row><row><cell></cell><cell>Breast</cell><cell>Spam</cell><cell>Credit</cell><cell>News</cell></row><row><cell>GAIN</cell><cell>.9930 ?</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Congeniality of imputation models ? 0.0017 1.6660 ? 0.0015 Auto-encoder 0.3500 ? 0.1503 0.5608 ?0.1697 EM 0.8418 ? 0.2675 0.9369 ? 0.2296 relationship between feature and label. As can be seen in the table, GAIN achieves significantly lower mean bias and mean square error than other state-of-the-art imputation algorithms (from 8.9% to 79.2% performance improvements).</figDesc><table><row><cell>Algorithm</cell><cell>Mean Bias (||w ??|| 1 )</cell><cell>MSE (||w ??|| 2 )</cell></row><row><cell>GAIN</cell><cell cols="2">0.3163? 0.0887 0.5078? 0.1137</cell></row><row><cell>MICE</cell><cell cols="2">0.8315 ? 0.2293 0.9467 ? 0.2083</cell></row><row><cell>MissForest</cell><cell cols="2">0.6730 ? 0.1937 0.7081 ? 0.1625</cell></row><row><cell>Matrix</cell><cell>1.5321</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">For ease of exposition, we use the term density even when referring to a probability mass function.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The authors would like to thank the reviewers for their helpful comments. The research presented in this paper was supported by the Office of Naval Research (ONR) and the NSF (Grant number: ECCS1462245, ECCS1533983, and ECCS1407712).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Personalized risk scoring for critical care prognosis using mixtures of gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Alaa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Generative Adversarial Denoising Autoencoder for Face Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://www.cc.gatech.edu/?hays/7476/projects/Avery_Wenchen/" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Applications of multiple imputation in medical studies: from aids to nhanes. Statistical methods in medical research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-L</forename><surname>Meng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="17" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Combining multiple imputation and meta-analysis with individual participant data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">R</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Resche-Rigon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics in medicine</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">26</biblScope>
			<biblScope unit="page" from="4499" to="4514" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">mice: Multivariate imputation by chained equations in r</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buuren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Groothuis-Oudshoorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of statistical software</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Multivariate imputation by chained equations: Mice v1. 0 user&apos;s manual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V</forename><surname>Buuren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Oudshoorn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>TNO</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multiple imputation for general missing data patterns in the presence of high-dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">21689</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pattern classification with missing data: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Garc?a-Laencina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Sancho-G?mez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="282" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Multiple imputation using deep denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gondara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02737</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The effects of the irregular sample and missing data in time series analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Kreindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Lumsden</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Nonlinear Dynamical Systems Analysis for the Behavioral Sciences Using Real Data</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">135</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">MNIST handwritten digit database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lichman</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The use and reporting of multiple imputation in medical research-a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mackinnon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of internal medicine</title>
		<imprint>
			<biblScope unit="volume">268</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="586" to="593" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spectral regularization algorithms for learning large incomplete matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2287" to="2322" />
			<date type="published" when="2010-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spectral regularization algorithms for learning large incomplete matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2287" to="2322" />
			<date type="published" when="2010-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multiple-imputation inferences with uncongenial sources of input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-L</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="page" from="538" to="558" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Hybrid prediction model with missing value imputation for medical data. Expert Systems with Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Purwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="5621" to="5631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Multiple imputation for nonresponse in surveys</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">81</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Swaminatan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chandak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachims</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<title level="m">Recommendations as treatments: debiasing learning and evolution. ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Missforestnonparametric missing value imputation for mixed-type data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Stekhoven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>B?hlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="112" to="118" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multiple imputation for missing data in epidemiological and clinical research: potential and pitfalls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Sterne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">R</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Spratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Royston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Kenward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Carpenter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMJ</title>
		<imprint>
			<biblScope unit="volume">338</biblScope>
			<biblScope unit="page">2393</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International conference on Machine learning</title>
		<meeting>the 25th International conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Discovery and clinical decision support for personalized healthcare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Davtyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE journal of biomedical and health informatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1133" to="1145" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Estimation of individualized treatment effects using generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Der Schaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganite</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ByKWUeWA-" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Personalized survival predictions via trees of predictors: An application to cardiac transplantation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Zame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cadeiras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Alaa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">194985</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep sensing: Active sensing using multi-directional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Zame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Der Schaar</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1SnX5xCb" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Temporal regularized matrix factorization for high-dimensional time series prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Active sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krishnapuram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rosales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="639" to="646" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
