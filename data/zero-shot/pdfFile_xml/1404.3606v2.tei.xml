<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PCANet: A Simple Deep Learning Baseline for Image Classification?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Han</forename><surname>Chan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zinan</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
						</author>
						<title level="a" type="main">PCANet: A Simple Deep Learning Baseline for Image Classification?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>JOURNAL OF L A T E X CLASS FILES 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Convolution Neural Network</term>
					<term>Deep Learning</term>
					<term>PCA Network</term>
					<term>Random Network</term>
					<term>LDA Network</term>
					<term>Face Recognition</term>
					<term>Handwritten Digit Recognition</term>
					<term>Object Classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we propose a very simple deep learning network for image classification which comprises only the very basic data processing components: cascaded principal component analysis (PCA), binary hashing, and block-wise histograms. In the proposed architecture, PCA is employed to learn multistage filter banks. It is followed by simple binary hashing and block histograms for indexing and pooling. This architecture is thus named as a PCA network (PCANet) and can be designed and learned extremely easily and efficiently. For comparison and better understanding, we also introduce and study two simple variations to the PCANet, namely the RandNet and LDANet. They share the same topology of PCANet but their cascaded filters are either selected randomly or learned from LDA. We have tested these basic networks extensively on many benchmark visual datasets for different tasks, such as LFW for face verification, MultiPIE, Extended Yale B, AR, FERET datasets for face recognition, as well as MNIST for hand-written digits recognition. Surprisingly, for all tasks, such a seemingly naive PCANet model is on par with the state of the art features, either prefixed, highly hand-crafted or carefully learned (by DNNs). Even more surprisingly, it sets new records for many classification tasks in Extended Yale B, AR, FERET datasets, and MNIST variations. Additional experiments on other public datasets also demonstrate the potential of the PCANet serving as a simple but highly competitive baseline for texture classification and object recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Image classification based on visual content is a very challenging task, largely because there is usually large amount of intra-class variability, arising from different lightings, misalignment, non-rigid deformations, occlusion and corruptions. Numerous efforts have been made to counter the intra-class variability by manually designing low-level features for classification tasks at hand. Representative examples are Gabor features and local binary patterns (LBP) for texture and face classification, and SIFT and HOG features for object recognition. While the low-level features can be hand-crafted with great success for some specific data and tasks, designing effective features for new data and tasks usually requires new domain knowledge since most hand-crafted features cannot be simply adopted to new conditions <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>.</p><p>Learning features from the data of interest is considered as a plausible way to remedy the limitation of hand-crafted features. An example of such methods is learning through deep neural networks (DNNs), which draws significant attention recently <ref type="bibr" target="#b0">[1]</ref>. The idea of deep learning is to discover multiple levels of representation, with the hope that higher-level features represent more abstract semantics of the data. Such abstract representations learned from a deep network are expected to provide more invariance to intra-class variability. One key ingredient for success of deep learning in image classification is the use of convolutional architectures <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b9">[10]</ref>. A convolutional deep neural network (ConvNet) architecture <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> consists of multiple trainable stages stacked on top of each other, followed by a supervised classifier. Each stage generally comprises of "three layers" -a convolutional filter bank layer, a nonlinear processing layer, and a feature pooling layer.</p><p>To learn a filter bank in each stage of ConvNet, a variety of techniques has been proposed, such as restricted Boltzmann machines (RBM) <ref type="bibr" target="#b6">[7]</ref> and regularized autoencoders or their variations; see <ref type="bibr" target="#b1">[2]</ref> for a review and references therein. In general, such a network is typically learned by stochastic gradient descent (SGD) method. However, learning a network useful for classification critically depends on expertise of parameter tuning and some ad hoc tricks. While many variations of deep convolutional networks have been proposed for different vision tasks and their success is usually justified empirically, arguably the first instance that has led to clear mathematical justification is the wavelet scattering networks (ScatNet) <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>. The only difference there is that the convolutional filters in ScatNet are prefixed -they are simply wavelet operators, hence no learning is needed at all. Somewhat surprisingly, such a pre-fixed filter bank, once utilized in a similar multistage architecture of ConvNet or DNNs, has demonstrated superior performance over ConvNet and DNNs in several challenging vision tasks such as handwritten digit and texture recognition <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>. However, as we will see in this paper, such a prefixed architecture does not generalize so well to tasks  such as face recognition where the intra-class variability includes significant illumination change and corruption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Motivations</head><p>An initial motivation of our study is trying to resolve some apparent discrepancies between ConvNet and ScatNet. We want to achieve two simple goals: First, we want to design a simple deep learning network which should be very easy, even trivial, to train and to adapt to different data and tasks. Second, such a basic network could serve as a good baseline for people to empirically justify the use of more advanced processing components or more sophisticated architectures for their deep learning networks.</p><p>The solution comes as no surprise: We use the most basic and easy operations to emulate the processing layers in a typical (convolutional) neural network mentioned above: The data-adapting convolution filter bank in each stage is chosen to be the most basic PCA filters; the nonlinear layer is set to be the simplest binary quantization (hashing); for the feature pooling layer, we simply use the block-wise histograms of the binary codes, which is considered as the final output features of the network. For ease of reference, we name such a data-processing network as a PCA Network (PCANet). As example, <ref type="figure" target="#fig_1">Figure  1</ref> illustrates how a two-stage PCANet extracts features from an input image.</p><p>At least one characteristic of the PCANet model seem to challenge common wisdoms in building a deep learning network such as ConvNet <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref> and ScatNet <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>: No nonlinear operations in early stages of the PCANet until the very last output layer where binary hashing and histograms are conducted to compute the output features. Nevertheless, as we will see through extensive experiments, such drastic simplification does not seem to undermine performance of the network on some of the typical datasets.</p><p>A network closely related to PCANet could be twostage oriented PCA (OPCA), which was first proposed for audio processing <ref type="bibr" target="#b10">[11]</ref>. Noticeable differences from PCANet lie in that OPCA does not couple with hashing and local histogram in the output layer. Given covariance of noises, OPCA gains additional robustness to noises and distortions. The baseline PCANet could also incorporate the merit of OPCA, likely offering more invariance to intra-class variability. To this end, we have also explored a supervised extension of PCANet, we replace the PCA filters with filters that are learned from linear discriminant analysis (LDA), called LDANet. As we will see through extensive experiments, the additional discriminative information does not seem to improve performance of the network; see Sections 2.3 and 3. Another, somewhat extreme, variation to PCANet is to replace the PCA filters with totally random filters (say the filter entries are i.i.d. Gaussian variables), called RandNet. In this work, we conducted extensive experiments and fair comparisons of these types of networks with other existing networks such as ConvNet and ScatNet. We hope our experiments and observations will help people gain better understanding of these different networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Contributions</head><p>Although our initial intention of studying the simple PCANet architecture is to have a simple baseline for comparing and justifying other more advanced deep learning components or architectures, our findings lead to some pleasant but thought-provoking surprises: The very basic PCANet, in fair experimental comparison, is already quite on par with, and often better than, the state-of-the-art features (prefixed, hand-crafted, or learned from DNNs) for almost all image classification tasks, including face images, hand-written digits, texture images, and object images. More specifically, for face recognition with one gallery image per person, it achieves 99.58% accuracy in Extended Yale B dataset, and over 95% accuracy for across disguise/illumination subsets in AR dataset. In FERET dataset, it obtains the state-of-the-art average accuracy 97.25% and achieves the best accuracy of 95.84% and 94.02% in Dup-1 and Dup-2 subsets, respectively. <ref type="bibr" target="#b0">1</ref> In LFW dataset, it achieves competitive 86.28% face verification accuracy under "unsupervised setting". In MNIST datasets, it achieves the state-of-the-art results for subtasks such as basic, background random, and background image. See Section 3 for more details. Overwhelming empirical evidences demonstrate the effectiveness of the proposed PCANet in learning robust invariant features for various image classification tasks. <ref type="bibr" target="#b0">1</ref>. The results were obtained by following FERET standard training CD, and could be marginally better when the PCANet is trained on MultiPIE database.</p><p>The method hardly contains any deep or new techniques and our study so far is entirely empirical. <ref type="bibr" target="#b1">2</ref> Nevertheless, a thorough report on such a baseline system has tremendous value to the deep learning and visual recognition community, sending both sobering and encouraging messages: On one hand, for future study, PCANet can serve as a simple but surprisingly competitive baseline to empirically justify any advanced designs of multistage features or networks. On the other hand, the empirical success of PCANet (even that of RandNet) confirms again certain remarkable benefits from cascaded feature learning or extraction architectures. Even more importantly, since PCANet consists of only a (cascaded) linear map, followed by binary hashing and block histograms, it is now amenable to mathematical analysis and justification of its effectiveness. That could lead to fundamental theoretical insights about general deep networks, which seems in urgent need for deep learning nowadays.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">CASCADED LINEAR NETWORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Structures of the PCA Network (PCANet)</head><p>Suppose that we are given N input training images {I i } N i=1 of size m ? n, and assume that the patch size (or 2D filter size) is k 1 ? k 2 at all stages. The proposed PCANet model is illustrated in <ref type="figure" target="#fig_3">Figure 2</ref>, and only the PCA filters need to be learned from the input images</p><formula xml:id="formula_0">{I i } N i=1 .</formula><p>In what follows, we describe each component of the block diagram more precisely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">The first stage: PCA</head><p>Around each pixel, we take a k 1 ? k 2 patch, and we collect all (overlapping) patches of the ith image; i.e., x i,1 , x i,2 , ..., x i,mn ? R k1k2 where each x i,j denotes the jth vectorized patch in I i . We then subtract patch mean from each patch and obtainX i = [x i,1 ,x i,2 , ...,x i,mn ], wherex i,j is a mean-removed patch. By constructing the same matrix for all input images and putting them together, we get</p><formula xml:id="formula_1">X = [X 1 ,X 2 , ...,X N ] ? R k1k2?N mn .<label>(1)</label></formula><p>Assuming that the number of filters in layer i is L i , PCA minimizes the reconstruction error within a family of orthonormal filters, i.e.,</p><formula xml:id="formula_2">min V ?R k 1 k 2 ?L 1 X ? V V T X 2 F , s.t. V T V = I L1 ,<label>(2)</label></formula><p>where I L1 is identity matrix of size L 1 ?L 1 . The solution is known as the L 1 principal eigenvectors of XX T . The PCA filters are therefore expressed as</p><formula xml:id="formula_3">W 1 l . = mat k1,k2 (q l (XX T )) ? R k1?k2 , l = 1, 2, ..., L 1 ,<label>(3)</label></formula><p>where mat k1,k2 (v) is a function that maps v ? R k1k2 to a matrix W ? R k1?k2 , and q l (XX T ) denotes the lth principal eigenvector of XX T . The leading principal 2. We would be surprised if something similar to PCANet or variations to OPCA <ref type="bibr" target="#b10">[11]</ref> have not been suggested or experimented with before in the vast learning literature. eigenvectors capture the main variation of all the meanremoved training patches. Of course, similar to DNN or ScatNet, we can stack multiple stages of PCA filters to extract higher level features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">The second stage: PCA</head><p>Almost repeating the same process as the first stage. Let the lth filter output of the first stage be</p><formula xml:id="formula_4">I l i . = I i * W 1 l , i = 1, 2, ..., N,<label>(4)</label></formula><p>where * denotes 2D convolution, and the boundary of I i is zero-padded before convolving with W 1 l so as to make I l i having the same size of I i . Like the first stage, we can collect all the overlapping patches of I l i , subtract patch mean from each patch, and form</p><formula xml:id="formula_5">Y l i = [? i,l,1 ,? i,l,2 , ...,? i,l,mn ] ? R k1k2?mn , where? i,l,j is the jth mean-removed patch in I l i . We further define Y l = [? l 1 ,? 1 2 , .</formula><p>..,? l N ] ? R k1k2?N mn for the matrix collecting all mean-removed patches of the lth filter output, and concatenate Y l for all the filter outputs as</p><formula xml:id="formula_6">Y = [Y 1 , Y 2 , ..., Y L1 ] ? R k1k2?L1N mn .<label>(5)</label></formula><p>The PCA filters of the second stage are then obtained as</p><formula xml:id="formula_7">W 2 . = mat k1,k2 (q (Y Y T )) ? R k1?k2 , = 1, 2, ..., L 2 . (6)</formula><p>For each input I l i of the second stage, we will have L 2 outputs, each convolves I l i with W 2 for = 1, 2, ..., L 2 :</p><formula xml:id="formula_8">O l i . = {I l i * W 2 } L2 =1 .<label>(7)</label></formula><p>The number of outputs of the second stage is L 1 L 2 . One can simply repeat the above process to build more (PCA) stages if a deeper architecture is found to be beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Output stage: hashing and histogram</head><p>For each of the L 1 input images I l i for the second stage, it has L 2 real-valued outputs {I l i * W 2 } L2 =1 from the second stage. We binarize these outputs and get</p><formula xml:id="formula_9">{H(I l i * W 2 )} L2 =1 , where H(?)</formula><p>is a Heaviside step (like) function whose value is one for positive entries and zero otherwise.</p><p>Around each pixel, we view the vector of L 2 binary bits as a decimal number. This converts the L 2 outputs in O l i back into a single integer-valued "image":</p><formula xml:id="formula_10">T l i . = L2 =1 2 ?1 H(I l i * W 2 ),<label>(8)</label></formula><p>whose every pixel is an integer in the range 0, 2 L2 ? 1 .</p><p>The order and weights of for the L 2 outputs is irrelevant as we here treat each integer as a distinct "word." For each of the L 1 images T l i , l = 1, . . . , L 1 , we partition it into B blocks. We compute the histogram (with 2 L2 bins) of the decimal values in each block, and concatenate all the B histograms into one vector and denote as Bhist(T l i ). After this encoding process, the  "feature" of the input image I i is then defined to be the set of block-wise histograms; i.e.,</p><formula xml:id="formula_11">f i . = [Bhist(T 1 i ), . . . , Bhist(T L1 i )] T ? R (2 L 2 )L1B .<label>(9)</label></formula><p>The local blocks can be either overlapping or nonoverlapping, depending on applications. Our empirical experience suggests that non-overlapping blocks are suitable for face images, whereas the overlapping blocks are appropriate for hand-written digits, textures, and object images. Furthermore, the histogram offers some degree of translation invariance in the extracted features, as in hand-crafted features (e.g., scale-invariant feature transform (SIFT) <ref type="bibr" target="#b11">[12]</ref> or histogram of oriented gradients (HOG) <ref type="bibr" target="#b12">[13]</ref>), learned features (e.g., bag-of-words (BoW) model <ref type="bibr" target="#b13">[14]</ref>), and average or maximum pooling process in ConvNet <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. The model parameters of PCANet include the filter size k 1 , k 2 , the number of filters in each stage L 1 , L 2 , the number of stages, and the block size for local histograms in the output layer. PCA filter banks require that k 1 k 2 ? L 1 , L 2 . In our experiments in Section 3, we always set L 1 = L 2 = 8 inspired from the common setting of Gabor filters <ref type="bibr" target="#b14">[15]</ref> with 8 orientations, although some fine-tuned L 1 , L 2 could lead to marginal performance improvement. Moreover, we have noticed empirically that two-stage PCANet is in general sufficient to achieve good performance and a deeper architecture does not necessarily lead to further improvement. Also, larger block size for local histograms provides more translation invariance in the extracted feature f i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4">Comparison with ConvNet and ScatNet</head><p>Clearly, PCANet shares some similarities with ConvNet <ref type="bibr" target="#b4">[5]</ref>. The patch-mean removal in PCANet is reminiscent of local contrast normalization in ConvNet. <ref type="bibr" target="#b2">3</ref> This operation moves all the patches to be centered around the origin of the vector space, so that the learned PCA filters can better catch major variations in the data. In addition, PCA can be viewed as the simplest class of auto-encoders, which minimizes reconstruction error.</p><p>The PCANet contains no non-linearity process between/in stages, running contrary to the common wisdom of building deep learning networks; e.g., the absolute rectification layer in ConvNet <ref type="bibr" target="#b4">[5]</ref> and the modulus layer in ScatNet <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>. We have tested the PCANet with an absolute rectification layer added right after the first stage, but we did not observe any improvement on the final classification results. The reason could be that the use of quantization plus local histogram (in the output layer) already introduces sufficient invariance and robustness in the final feature.</p><p>The overall process prior to the output layer in PCANet is completely linear. One may wonder what if we merge the two stages into just one that has an equivalently same number of PCA filters and size of receptive field. To be specific, one may be interested in how the single-stage PCANet with L 1 L 2 filters of size (k 1 + k 2 ? 1) ? (k 1 + k 2 ? 1) could perform, in comparison to the two-stage PCANet we described in Section 2.1. We have experimented with such settings on faces and handwritten digits and observed that the two-stage PCANet outperforms this single-stage alternative in most cases; see the last rows of Tables 2, 9, and 10. In comparison to the filters learned by the single-stage alternative, the resulting two-stage PCA filters essentially has a lowrank factorization, possibly having lower chance of overfitting the dataset. As for why we need the deep structure, from a computational perspective, the single-stage alternative requires learning filters with L 1 L 2 (k 1 +k 2 ?1) 2 variables, whereas the two-stage PCANet only learns filters with totally L 1 k 2 1 +L 2 k 2 2 variables. Another benefit of the two-stage PCANet is the larger receptive field as it contains more holistic observations of the objects in images and learning invariance from it can essentially capture more semantic information. Our comparative experiments validates that hierarchical architectures with large receptive fields and multiple stacked stages are more efficient in learning semantically related representations, which coincides with what have been observed in <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Computational Complexity</head><p>The components for constructing the PCANet are extremely basic and computationally efficient. To see how light the computational complexity of PCANet would be, let us take the two-stage PCANet as an example. In each stage of PCANet, forming the patch-mean-removed matrix X costs k 1 k 2 + k 1 k 2 mn flops; the inner product XX T has complexity of 2(k 1 k 2 ) 2 mn flops; and the complexity of eigen-decomposition is O((k 1 k 2 ) 3 ). The PCA filter convolution takes L i k 1 k 2 mn flops for stage i. In the output layer, the conversion of L 2 binary bits to a decimal number costs 2L 2 mn, and the naive histogram operation is of complexity O(mnBL 2 log 2). Assuming mn max(k 1 , k 2 , L 1 , L 2 , B), the overall complexity of PCANet is easy to be verified as</p><formula xml:id="formula_12">O(mnk 1 k 2 (L 1 + L 2 ) + mn(k 1 k 2 ) 2 ).</formula><p>The above computational complexity applies to training phase and testing phase of PCANet, as the extra computational burden in training phase from testing phase is the eigen-decomposition, whose complexity is ignorable when mn max(k 1 , k 2 , L 1 , L 2 , B). In comparison to ConvNet, the SGD for filter learning is also a simple gradient-based optimization solver, but the overall training time is still much longer than PCANet. For example, training PCANet on around 100,000 images of 80?60 pixel dimension took only half a hour, but CNN-2 took 6 hours, excluding the fine-tuning process; see Section 3.1.1.D for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Two Variations: RandNet and LDANet</head><p>The PCANet is an extremely simple network, requiring only minimum learning of the filters from the training data. One could immediately think of two possible variations of the PCANet towards two opposite directions: 1) We could further eliminate the necessity of training data and replace the PCA filters at each layer with random filters of the same size. Be more specific, for random filters, i.e., the elements of W 1 l and W 2 l , are generated following standard Gaussian distribution. We call such a network Random Network, or RandNet as a shorthand. It is natural to wonder how much degradation such a randomly chosen network would perform in comparison with PCANet. 2) If the task of the learned network is for classification, we could further enhance the supervision of the learned filters by incorporating the information of class labels in the training data and learn the filters based on the idea of multi-class linear discriminant analysis (LDA). We called so composed network LDA Network, or LDANet for ease of reference. Again we are interested in how much the enhanced supervision would help improve the performance of the network. To be more clear, we here describe with more details how to construct the LDANet. Suppose that the N training images are classified into C classes {I i } i?Sc , c = 1, 2, ..., C where S c is the set of indices of images in class c, and the mean-removed patches associated with each image of distinct classesX i ? R k1k2?mn , i ? S c (in the same spirit ofX i in (1)) are given. We can first compute the class mean ? c and the intra-class variability ? c for all the patches as follows,</p><formula xml:id="formula_13">? c = i?ScX i /|S c |,<label>(10)</label></formula><formula xml:id="formula_14">? c = i?Sc (X i ? ? c )(X i ? ? c ) T /|S c |.<label>(11)</label></formula><p>Each column of ? c denotes the mean of patches around each pixel in the class c, and ? c is the sum of all the patch-wise sample covariances in the class c. Likewise, the inter-class variability of the patches is defined as</p><formula xml:id="formula_15">? = C c=1 (? c ? ?)(? c ? ?) T /C,<label>(12)</label></formula><p>where ? is the mean of class means. The idea of LDA is to maximize the ratio of the inter-class variability to sum of the intra-class variability within a family of orthonormal filters; i.e.,</p><formula xml:id="formula_16">max V ?R k 1 k 2 ?L 1 Tr(V T ?V ) Tr(V T ( C c=1 ? c )V ) , s.t. V T V = I L1 ,<label>(13)</label></formula><p>where Tr(?) is the trace operator. The solution is known as the L 1 principal eigenvectors of? = ( C c=1 ? c ) ? ?, where the superscript ? denotes the pseudo-inverse. The pseudo inverse is to deal with the case when C c=1 ? c is not of full rank, though there might be another way of handling this with better numeric stability <ref type="bibr" target="#b15">[16]</ref>. The LDA filters are thus expressed as W 1 l = mat k1,k2 (q l (?)) ? R k1?k2 , l = 1, 2, ..., L 1 . A deeper network can be built by repeating the same process as above. .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>We now evaluate the performance of the proposed PCANet and the two simple variations (RandNet and LDANet) in various tasks, including face recognition, face verification, hand-written digits recognition, texture discrimination, and object recognition in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Face Recognition on Many Datasets</head><p>We first focus on the problem of face recognition with one gallery image per person. We use part of MultiPIE dataset to learn PCA filters in PCANet, and then apply such trained PCANet to extract features of new subjects in MultiPIE dataset, Extended Yale B, AR, and FERET datasets for face recognition. Generic faces training set. MultiPIE dataset <ref type="bibr" target="#b16">[17]</ref> contains 337 subjects across simultaneous variation in pose, expression, and illumination. Of these 337 subjects, we select the images of 129 subjects that enrolled all the four sessions. The images of a subject under all illuminations and all expressions at pose ?30 ? to +30 ? with step size 15 ? , a total of 5 poses, were collected. We manually select eye corners as the ground truth for registration, and down-sample the images to 80?60 pixels. The distance between the two outer eye corners is normalized to be 50 pixels. This generic faces training set comprises around 100,000 images, and all images are converted to gray scale.</p><p>We use these assembled face images to train the PCANet and together with data labels to learn LDANet, and then apply the trained networks to extract features of the new subjects in Multi-PIE dataset. As mentioned above, 129 subjects enrolling all four sessions are used for PCANet training. The remaining 120 new subjects in Session 1 are used for gallery training and testing. Frontal view of each subject with neutral expression and frontal illumination is used in gallery, and the rest is for testing. We classify all the possible variations into 7 test sets, namely cross illumination, cross expression, cross pose, cross expression-plus-pose, cross illumination-plus-expression, cross illuminationplus-pose, and cross illumination-plus-expression-andpose. The cross-pose test set is specifically collected over the poses ?30</p><formula xml:id="formula_17">? , ?15 ? , +15 ? , +30 ? .</formula><p>A. Impact of the number of filters. Before comparing RandNet, PCANet, and LDANet with existing methods on all the 7 test sets, we first investigate the impact of the number of filters of these networks on the crossillumination test set only. The filter size of the networks is k 1 = k 2 = 5 and their non-overlapping blocks is of size 8?6. We vary the number of filters in the first stage L 1 from 2 to 12 for one-stage networks. When considering two-stage networks, we set L 2 = 8 and vary L 1 from 4 to 24. The results are shown in <ref type="figure">Figure 3</ref>. One can see that PCANet-1 achieves the best results for L 1 ? 4 and PCANet-2 is the best for all L 1 under test. Moreover, the accuracy of PCANet and LDANet (for both one-stage and two-stage networks) increases for larger L 1 , and the RandNet also has similar performance trend. However, some performance fluctuation is observed for RandNet due to the filters' randomness.</p><p>B. Impact of the the block size. We next examine the impact of the block size (for histogram computation) on robustness of PCANet against image deformations. We use the cross-illumination test set, and introduce artificial deformation to the testing image with a translation, inplane rotation or scaling; see <ref type="figure">Figure 4</ref>. The parameters of PCANet are set to k 1 = k 2 = 5 and L 1 = L 2 = 8. Two block sizes 8?6 and 12?9 are considered. <ref type="figure" target="#fig_6">Figure 5</ref> shows the recognition accuracy for each artificial deformation. It is observed that PCANet-2 achieves more than 90 percent accuracy with translation up to 4 pixels in all directions, up to 8 ? in-plane rotation, or with scale varying from 0.9 to 1.075. Moreover, the results suggest that PCANet-2 with larger block size provides more robustness against various deformations, but a larger block side may sacrifice some performance for PCANet-1.</p><p>C. Impact of the number of generic faces training samples. We also report the recognition accuracy of the PCANet for differen number of the generic faces training images. Again, we use cross-illumination test set. We randomly select S images from the generic training set to train the PCANet, and varies S from 10 to 50, 000. The parameters of PCANet are set to k 1 = k 2 = 5, L 1 = L 2 = 8, and block size 8?6. The results are tabulated in <ref type="table" target="#tab_1">Table  1</ref>. Surprisingly, the accuracy of PCANet is less-sensitive to the number of generic training images. The performance of PCANet-1 gradually improves as the number of generic training samples increases, and PCANet-2 keeps perfect recognition even when there are only 100 generic training samples.</p><p>D. Comparison with state of the arts. We compare the   RandNet, PCANet, and LDANet with Gabor 4 <ref type="bibr" target="#b14">[15]</ref>, LBP 5 <ref type="bibr" target="#b17">[18]</ref>, and two-stage ScatNet (ScatNet-2) <ref type="bibr" target="#b5">[6]</ref>. We set the parameters of PCANet to the filter size k 1 = k 2 = 5, the number of filters L 1 = L 2 = 8, and 8?6 block size, and the learned PCANet filters are shown in <ref type="figure" target="#fig_7">Figure 6</ref>. The number of scales and the number of orientations in ScatNet-2 are set to 3 and 8, respectively. We use the nearest neighbor (NN) classifier with the chi-square distance for RandNet, PCANet, LDANet and LBP, or with the cosine distance for Gabor and ScatNet. The NN classifier with different distance measure is to secure the best performances of respective features.</p><p>We also compare with CNN. Since we could not find any work that successfully applies CNN to the same face recognition tasks, we use Caffe framework <ref type="bibr" target="#b18">[19]</ref> to pretrain a two-stage CNN (CNN-2) on the generic faces training set. The CNN-2 is fully-supervised network with filter size 5?5; 20 channels for the first stage and 50 <ref type="bibr" target="#b3">4</ref>. Each face is convolved with a family of Gabor kernels with 5 scales and 8 orientations. Each filter response is down-sampled by a 3 ? 3 uniform lattice, and normalized to zero mean and unit variance.</p><p>5. Each face is divided into several blocks, each of size the same as PCANet. The histogram of 59 uniform binary patterns is then computed, where the patterns are generated by thresholding 8 neighboring pixels in a circle of radius 2 using the central pixel value. channels for the second stage. Each convolution output is followed by a rectified linear function relu(x) = max(x, 0) and 2?2 max-pooling. The output layer is a softmax classifier. After pre-training the CNN-2 on the generic faces training set, the CNN-2 is also fine-tuned on the 120 gallery images for 500 epochs.</p><p>The performance of all methods are given in <ref type="table" target="#tab_4">Table  2</ref>. Except for cross-pose test set, the PCANet yields the best precision. For all test sets, the performance of RandNet and LDANet is inferior to that of PCANet, and LDANet does not seem to take advantage of discriminative information. One can also see that whenever there is illumination variation, the performance of LBP drops significantly. The PCANet overcomes this drawback and offers comparable performance to LBP for cross-pose and cross-expression variations. As a final note, ScatNet and CNN seem not performing well. <ref type="bibr" target="#b5">6</ref> This is the case for all face-related experiments below, and therefore ScatNet and CNN are not included for comparison in these experiments. We also do not include RandNet and LDANet in the following face-related experiments, as they did not show performance superior over PCANet.</p><p>The last row of <ref type="table" target="#tab_4">Table 2</ref> shows the result of PCANet-1 with L 1 L 2 filters of size (k 1 + k 2 ? 1) ? (k 1 + k 2 ? 1). The PCANet-1 with such a parameter setting is to mimic the reported PCANet-2 in a single-stage network, as both have the same number of PCA filters and size of receptive field. PCANet-2 outperforms the PCANet-1 alternative, showing the advantages of deeper networks. Another issue worth mentioning is the efficiency of the PCANet. Training PCANet-2 on the generic faces <ref type="bibr" target="#b5">6</ref>. The performance of CNN could be further promoted if the model parameters are more fine-tuned.  training set (i.e., around 100,000 face images of 80?60 pixel dimension) took only half a hour, but CNN-2 took 6 hours, excluding the fine-tuning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Testing on Extended Yale B Dataset.</head><p>We then apply the PCANet with the PCA filters learned from MultiPIE to Extended Yale B dataset <ref type="bibr" target="#b19">[20]</ref>. Extended Yale B dataset consists of 2414 frontal-face images of 38 individuals. The cropped and normalized 192?168 face images were captured under various laboratorycontrolled lighting conditions. For each subject, we select frontal illumination as the gallery images, and the rest for testing. To challenge ourselves, in the test images, we also simulate various levels of contiguous occlusion, from 0 percent to 80 percent, by replacing a randomly located square block of each test image with an unrelated image; see <ref type="figure" target="#fig_8">Figure 7</ref> for example. The size of nonoverlapping blocks in the PCANet is set to 8?8. We compare with LBP <ref type="bibr" target="#b17">[18]</ref> and LBP of the test images being processed by illumination normalization, P-LBP <ref type="bibr" target="#b20">[21]</ref>. We use the NN classifier with the chi-square distance measure.</p><p>The experimental results are given in <ref type="table" target="#tab_2">Table 3</ref>. One can see that the PCANet outperforms the P-LBP for different levels of occlusion. It is also observed that the PCANet is not only illumination-insensitive, but also robust against block occlusion. Under such a single sample per person setting and various difficult lighting conditions, the PCANet surprisingly achieves almost perfect recognition 99.58%, and still sustains 86.49% accuracy when 60% pixels of every test image are occluded! The reason could be that each PCA filter can be seen as a detector with the maximum response for patches from a face. In other words, the contribution from occluded patches would somehow be ignored after PCA filtering and are not passed onto the output layer of the PCANet, thereby yielding striking robustness to occlusion. We further evaluate the ability of the MultiPIE-learned PCANet to cope with real possibly malicious occlusions using AR dataset <ref type="bibr" target="#b21">[22]</ref>. AR dataset consists of over 4,000 frontal images for 126 subjects. These images include different facial expressions, illumination conditions and disguises. In the experiment, we chose a subset of the data consisting of 50 male subjects and 50 female subjects. The images are cropped with dimension 165?120 and converted to gray scale. For each subject, we select the face with frontal illumination and neural expression in the gallery training, and the rest are all for testing. The size of non-overlapping blocks in the PCANet is set to 8?6. We also compare with LBP <ref type="bibr" target="#b17">[18]</ref> and P-LBP <ref type="bibr" target="#b20">[21]</ref>. We use the NN classifier with the chi-square distance measure.</p><p>The results are given in <ref type="table" target="#tab_3">Table 4</ref>. For test set of illumination variations, the recognition by PCANet is again almost perfect, and for cross-disguise related test sets, the accuracy is more than 95%. The results are consistent with that on MultiPIE and Extended Yale B datasets: PCANet is insensitive to illumination and robust to occlusions. To the best of our knowledge, no single feature with a simple classifier can achieve such performances, even if in extended representation-based classification (ESRC) [23]!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Testing on FERET Dataset.</head><p>We finally apply the MultiPIE-learned PCANet to the popular FERET dataset <ref type="bibr" target="#b23">[24]</ref>, which is a standard dataset used for facial recognition system evaluation. FERET contains images of 1,196 different individuals with up to 5 images of each individual captured under different lighting conditions, with non-neural expressions and over the period of three years. The complete dataset is partitioned into disjoint sets: gallery and probe. The probe set is further subdivided into four categories: Fb with different expression changes; Fc with different lighting conditions; Dup-I taken within the period of three to four months; Dup-II taken at least one and a half year apart. We use the gray-scale images, cropped to image size of 150?90 pixels. The size of non-overlapping blocks in the PCANet is set to 15?15. To compare fairly with prior methods, the dimension of the PCANet features are reduced to 1000 by a whitening PCA (WPCA), <ref type="bibr" target="#b6">7</ref> where the projection matrix is learned from the features of 7. The PCA projection directions are weighted by the inverse of their corresponding square-root energies, respectively.   <ref type="table" target="#tab_5">Table 5</ref>. Surprisingly, both simple MultiPIE-learned PCANet-2 and FERET-learned PCANet-2 (with Trn. CD in a parentheses) achieve the state-of-the-art accuracies 97.25% and 97.26% on average, respectively. As the variations in MultiPIE database are much richer than the standard FERET training set, it is nature to see that the MultiPIE-learned PCANet slightly outperforms FERET-learned PCANet. More importantly, PCANet-2 breaks the records in Dup-I and Dup-II.</p><p>Conclusive remarks on face recognition. A prominent message drawn from the above experiments in sections 3.1.1, 3.1.2, 3.1.3, and 3.1.4 is that training PCANet from a face dataset can be very effective to capture the abstract representation of new subjects or new datasets. After the PCANet is trained, extracting PCANet-2 feature for one test face only takes 0.3 second in Matlab. We can anticipate that the performance of PCANet could be further improved and moved toward practical use if the PCANet is trained upon a wide and deep dataset that collect sufficiently many inter-class and intra-class variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Face Verification on LFW Dataset</head><p>Besides tests with laboratory face datasets, we also evaluate the PCANet on the LFW dataset <ref type="bibr" target="#b30">[31]</ref> for unconstrained face verification. LFW contains 13,233 face images of 5,749 different individuals, collected from the web with large variations in pose, expression, illumination, clothing, hairstyles, etc. We consider "unsupervised setting", which is the best choice for evaluating the learned features, for it does not depend on metric learning and discriminative model learning. The aligned version of the faces, namely LFW-a, provided by Wolf et al. <ref type="bibr" target="#b31">[32]</ref> is used, and the face images were cropped into 150 ? 80 pixel dimensions. We follow the standard evaluation protocal, which splits the View 2 dataset into 10 subsets with each subset containing 300 intra-class pairs and 300 inter-class pairs. We perform 10-fold cross validation using the 10 subsets of pairs in View 2. In PCANet, the filter size, the number of filters, and the (non-overlapping) block size are set to k 1 = k 2 = 7, L 1 = L 2 = 8, and 15?13, respectively. The performances are measured by averaging the 10-fold cross validation. We project the PCANet features onto 400 and 3,200 dimensions using WPCA for PCANet-1 and PCANet-2, respectively, and use NN classifier with cosine distance. <ref type="table">Table 6</ref> tabulates the results. <ref type="bibr" target="#b7">8</ref> Note that PCANet followed by sqrt in a parentheses represents the PCANet feature taking square-root operation. One can see that the square-root PCANet outperforms PCANet, and this performance boost from square-root operation has also been observed in other features for this dataset <ref type="bibr" target="#b32">[33]</ref>. Moreover, the square-root PCANet-2 that achieves 86.28% accuracy is quite competitive to the current state-of-the-art methods. This shows that the proposed PCANet is also effective in learning invariant features for face images captured in less controlled conditions.</p><p>In preparation of this paper, we are aware of two concurrent works <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref> that employ ConvNet for LFW face verification. While both works achieve very impressive results on LFW, their experimental setting <ref type="bibr" target="#b7">8</ref>. For fair comparison, we only report results of single descriptor. The best known LFW result under unsupervised setting is 88.57% <ref type="bibr" target="#b32">[33]</ref>, which is inferred from four different descriptors. differs from ours largely. These two works require some outside database to train the ConvNet and the face images have to be more precisely aligned; e.g., <ref type="bibr" target="#b33">[34]</ref> uses 3-dimensional model for face alignment and <ref type="bibr" target="#b34">[35]</ref> extracts multi-scale features based on detected landmark positions. On the contrary, we only trained PCANet based on LFW-a <ref type="bibr" target="#b31">[32]</ref>, an aligned version of LFW images using the commercial alignment system of face.com.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 6</head><p>Comparison of verification rates (%) on LFW under unsupervised setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Accuracy POEM <ref type="bibr" target="#b25">[26]</ref> 82.70?0.59 High-dim. LBP <ref type="bibr" target="#b35">[36]</ref> 84.08 High-dim. LE <ref type="bibr" target="#b35">[36]</ref> 84.58 SFRD <ref type="bibr" target="#b36">[37]</ref> 84.81 I-LQP <ref type="bibr" target="#b26">[27]</ref> 86.20?0.46 OCLBP <ref type="bibr" target="#b32">[33]</ref> 86.66?0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Digit Recognition on MNIST Datasets</head><p>We now move forward to test the proposed PCANet, along with RandNet and LDANet, on MNIST <ref type="bibr" target="#b3">[4]</ref> and MNIST variations <ref type="bibr" target="#b37">[38]</ref>, a widely-used benchmark for testing hierarchical representations. There are 9 classification tasks in total, as listed in <ref type="table" target="#tab_8">Table 8</ref>. All the images are of size 28 ? 28. In the following, we use MNIST basic as the dataset to investigate the influence of the number of filters or different block overlap ratios for RandNet, PCANet and LDANet, and then compare with other state-of-the-art methods on all the MNIST datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Impact of the number of filters</head><p>We vary the number of filters in the first stage L 1 from 2 to 12 for one-stage networks. Regarding two-stage networks, we set L 2 = 8 and change L 1 from 4 to 24.</p><p>The filter size of the networks is k 1 = k 2 = 7, block size is 7?7, and the overlapping region between blocks is half of the block size. The results are shown in <ref type="figure" target="#fig_9">Figure  8</ref>. The results are consistent with that for MultiPIE face database in <ref type="figure">Figure 3</ref>; PCANet outperforms RandNet and LDANet for almost all the cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Impact of the block overlap ratio</head><p>The number of filters is fixed to L 1 = L 2 = 8, and the filter size is again k 1 = k 2 = 7 and block size is 7?7. We only vary the block overlap ratio (BOR) from 0.1 to 0.7. <ref type="table" target="#tab_7">Table 7</ref> tabulates the results of RandNet-2, PCANet-2, and LDANet-2. Clearly, PCANet-2 and LDANet-2 achieve their minimum error rates for BOR equal to 0.5 and 0.6, respectively, and PCANet-2 performs the best for all conditions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Comparison with state of the arts</head><p>We compare RandNet, PCANet, and LDANet with Con-vNet <ref type="bibr" target="#b4">[5]</ref>, 2-stage ScatNet (ScatNet-2) <ref type="bibr" target="#b5">[6]</ref>, and other existing methods. In ScatNet, the number of scales and the number of orientations are set to 3 and 8, respectively. Regarding the parameters of PCANet, we set the filter size k 1 = k 2 = 7, the number of PCA filters L 1 = L 2 = 8; the block size is tuned by a cross-validation for MNIST, and the validation sets for MNIST variations <ref type="bibr" target="#b8">9</ref> . The overlapping region between blocks is half of the block size. Unless otherwise specified, we use linear SVM classifier for ScatNet and RandNet, PCANet and LDANet for the 9 classification tasks. The testing error rates of the various methods on MNIST are shown in <ref type="table" target="#tab_10">Table 9</ref>. For fair comparison, we do not include the results of methods using augmented training samples with distortions or other information, for that the best known result is 0.23% <ref type="bibr" target="#b38">[39]</ref>. We see that RandNet-2, PCANet-2, and LDANet-2 are comparable with the state-of-the-art methods on this standard MNIST task. However, as MNIST has many training data, all methods perform very well and very closethe difference is not so statistically meaningful.</p><p>Accordingly, we also report results of different methods on MNIST variations in <ref type="table" target="#tab_1">Table 10</ref>. To the best of our knowledge, the PCANet-2 achieves the state-of-the-art results for four out of the eight remaining tasks: basic, bg-img, bg-img-rot, and convex. Especially for bg-img, the error rate reduces from 12.25% <ref type="bibr" target="#b39">[40]</ref> to 10.95%. <ref type="table" target="#tab_1">Table 10</ref> also shows the result of PCANet-1 with L 1 L 2 filters of size (k 1 + k 2 ? 1) ? (k 1 + k 2 ? 1). The PCANet-1 with such a parameter setting is to mimic the reported PCANet-2 in a single-stage structure. PCANet-2 still outperforms this PCANet-1 alternative.</p><p>Furthermore, we also draw the learned PCANet filters in <ref type="figure" target="#fig_10">Figure 9</ref> and <ref type="figure" target="#fig_1">Figure 10</ref>. An intriguing pattern is observed in the filters of rect and rect-img datasets. For rect, we can see both horizontal and vertical stripes, for these patterns attempt to capture the edges of the rectangles. When there is some image background in rect-img, several filters become low-pass, in order to secure the responses from background images. <ref type="bibr" target="#b8">9</ref>. Using either cross-validation or validation set, the optimal block size is obtained as 7?7 for MNIST, basic, rec-img, 4?4 for rot, bg-img, bg-rnd, bg-img-rot, 14?14 for rec, and 28?28 for convex.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Texture Classification on CUReT Dataset</head><p>The CUReT texture dataset contains 61 classes of image textures. Each texture class has images of the same material with different pose and illumination conditions. Other than the above variations, specularities, shadowing and surface normal variations also make this classification challenging. In this experiment, a subset of the dataset with azimuthal viewing angle less than 60 Methods MNIST HSC <ref type="bibr" target="#b40">[41]</ref> 0.77 K-NN-SCM <ref type="bibr" target="#b41">[42]</ref> 0.63 K-NN-IDM <ref type="bibr" target="#b42">[43]</ref> 0.54 CDBN <ref type="bibr" target="#b6">[7]</ref> 0.82 ConvNet <ref type="bibr" target="#b4">[5]</ref> 0.53 Stochastic pooling ConvNet <ref type="bibr" target="#b43">[44]</ref> 0.47 Conv. Maxout + Dropout <ref type="bibr" target="#b2">[3]</ref> 0.45 ScatNet-2 (SVM rbf ) <ref type="bibr" target="#b5">[6]</ref> 0.  <ref type="bibr" target="#b46">[47]</ref>. The PCANet is trained with filter size k 1 = k 2 = 5, the number of filters L 1 = L 2 = 8, and block size 50?50. We use linear SVM classifier. The testing error rates averaged over 10 different random splits are shown in <ref type="table" target="#tab_1">Table 11</ref>. We see that the PCANet-1 outperforms ScatNet-1, but the improvement from PCANet-1 to PCANet-2 is not as large as that of ScatNet.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Error rates Textons <ref type="bibr" target="#b47">[48]</ref> 1.50 BIF <ref type="bibr" target="#b48">[49]</ref> 1.40 Histogram <ref type="bibr" target="#b49">[50]</ref> 1.00 ScatNet-1 (PCA) <ref type="bibr" target="#b5">[6]</ref> 0.50 ScatNet-2 (PCA) <ref type="bibr" target="#b5">[6]</ref> 0. Note that ScatNet-2 followed by a PCA-based classifier gives the best result <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Object Recognition on CIFAR10</head><p>We finally evaluate the performance of PCANet on CIFAR10 database for object recognition. CIFAR10 is a set of natural RGB images of 32?32 pixels. It contains 10 classes with 50000 training samples and 10000 test samples. Images in CIFAR10 vary significantly not only in object position and object scale within each class, but also in colors and textures of these objects. The motivation here is to explore the limitation of such a simple PCANet on a relatively complex database, in comparison to the databases of faces, digits, and textures we have experimented with, which could somehow be roughly aligned or prepared. To begin with, we extend PCA filter learning so as to accommodate the RGB images in object databases. In the same spirit of constructing the data matrix X in (1), we gather the same individual matrix for RGB channels of the images, denoted by X r , X g , X b ? R k1k2?N mn , respectively. Following the key steps in Section 2.1.1, the multichannel PCA filters can be easily verified as</p><formula xml:id="formula_18">W r,g,b l . = mat k1,k2,3 (q l ( X X T )) ? R k1?k2?3 ,<label>(14)</label></formula><p>where X = [X T r , X T g , X T b ] T and mat k1,k2,3 (v) is a function that maps v ? R 3k1k2 to a tensor W ? R k1?k2?3 . An example of the learned multichannel PCA filters is demonstrated in <ref type="figure" target="#fig_1">Figure 12</ref>. In addition to the modification above, we also connect spatial pyramid pooling (SPP) <ref type="bibr" target="#b50">[51]</ref>- <ref type="bibr" target="#b52">[53]</ref> to the output layer of PCANet, with the aim of extracting information invariant to large poses and complex backgrounds, usually seen in object databases. The SPP essentially helps object recognition, but finds no significant improvement in the previous experiments on faces, digits and textures.</p><p>We use linear SVM classifier in the experiments. In the first experiment, we train PCANet on CIFAR10 with filter size k 1 = k 2 = 5, the number of filters L 1 = 40, L 2 = 8, and block size equal to 8 ? 8. Also, we set the overlapping region between blocks to half of the block size, and connected SPP to the output layer of PCANet; i.e., the maximum response in each bin of block histograms is pooled in a pyramid of 4?4, 2?2, and 1?1 subregions. This yields the 21 pooled histogram feature of dimension L 1 2 L2 . The dimension of each pooled feature is reduced to 1280 by PCA.</p><p>In the second experiment, we concatenate PCANet features learned with different filter size k 1 = k 2 = 3 and k 1 = k 2 = 5. All the processes and model parameters are fixed identical to the single descriptor mentioned in last paragraph, except L 1 = 12 and L 1 = 28 set for filter size equal to 3 and 5, respectively. This is to ensure that the combined features are of the same dimension with the single descriptor, for fairness.</p><p>The results are shown in <ref type="table" target="#tab_1">Table 12</ref>. PCANet-2 achieves accuracy 77.14% and gains 1.5% improvement when combining two features learned with different filter sizes (marked with combined in a parenthesis). While PCANet-2 has around 11% accuracy degradation in comparison to state-of-the-art method (with no data augmentation), the performance of the fully unsupervised and extremely simple PCANet-2 shown here is still encouraging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSION</head><p>In this paper, we have proposed arguably the simplest unsupervised convolutional deep learning network-PCANet. The network processes input images by cascaded PCA, binary hashing, and block histograms. Like the most ConvNet models, the network parameters such as the number of layers, the filter size, and the number of filters have to be given to PCANet. Once the parameters are fixed, training PCANet is extremely simple and efficient, for the filter learning in PCANet does not involve regularized parameters and does not require numerical optimization solver. Moreover, building the Methods Accuracy Tiled CNN <ref type="bibr" target="#b53">[54]</ref> 73.10 Improved LCC <ref type="bibr" target="#b54">[55]</ref> 74.50 KDES-A <ref type="bibr" target="#b55">[56]</ref> 76.00 K-means (Triangle, 4000 features) <ref type="bibr" target="#b56">[57]</ref> 79.60 Cuda-convnet2 <ref type="bibr" target="#b57">[58]</ref> 82.00 Stochastic pooling ConvNet <ref type="bibr" target="#b43">[44]</ref> 84.87 CNN + Spearmint <ref type="bibr" target="#b58">[59]</ref> 85.02 Conv. Maxout + Dropout <ref type="bibr" target="#b2">[3]</ref> 88.32 NIN <ref type="bibr" target="#b59">[60]</ref> 89.59 PCANet-2 77.14 PCANet-2 (combined) 78.67</p><p>PCANet comprises only a cascaded linear map, followed by a nonlinear output stage. Such a simplicity offers an alternative and yet refreshing perspective to convolutional deep learning networks, and could further facilitate mathematical analysis and justification of its effectiveness.</p><p>A couple of simple extensions of PCANet; that is, RandNet and LDANet, have been introduced and tested together with PCANet on many image classification tasks, including face, hand-written digit, texture, and object. Extensive experimental results have consistently shown that the PCANet outperforms RandNet and LDANet, and is generally on par with ScatNet and variations of ConvNet. Furthermore, the performance of PCANet is closely comparable and often better than highly engineered hand-crafted features (such as LBP and LQP). In tasks such as face recognition, PCANet also demonstrates remarkable robustness to corruption and ability to transfer to new datasets.</p><p>The experiments also convey that as long as the images in databases are somehow well prepared; i.e., images are roughly aligned and do not exhibit diverse scales or poses, PCANet is able to eliminate the image variability and gives reasonably competitive accuracy. In challenging image databases such as Pascal and ImageNet, PCANet might not be sufficient to handle the variability, given its extremely simple structure and unsupervised learning method. An intriguing research direction will then be how to construct a more complicated (say more sophisticated filters possibly with discriminative learning) or deeper (more number of stages) PCANet that could accommodate the aforementioned issues. Some preprocessing of pose alignment and scale normalization might be needed for good performance guarantee. The current bottleneck that keeps PCANet from growing deeper (e.g., more than two stages) is that the dimension of the resulted feature would increase exponentially with the number of stages. This fortunately seems able to be fixed by replacing the 2-dimensional convolution filters with tensor-like filters as in <ref type="bibr" target="#b13">(14)</ref>, and it will be our future study. Furthermore, we will also leave as future work to augment PCANet with a simple, scalable baseline classifier, readily applicable to much larger scale datasets or problems.</p><p>Regardless, extensive experiments given in this paper sufficiently conclude two facts: 1) the PCANet is a very simple deep learning network, effectively extracting useful information for classification of faces, digits, and texture images; 2) the PCANet can be a valuable baseline for studying advanced deep learning architectures for large-scale image classification tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>?</head><label></label><figDesc>Tsung-Han Chan, Kui Jia, Shenghua Gao, Jiwen Lu, and Zinan Zeng are with Advanced Digital Sciences Center (ADSC), Singapore. ? Yi Ma is with the School of Information Science and Technology of ShanghaiTech University, and with the ECE Department of the University of Illinois at Urbana-Champaign</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Illustration of how the proposed PCANet extracts features from an image through three simplest processing components: PCA filters, binary hashing, and histogram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>The detailed block diagram of the proposed (two-stage) PCANet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3. 1 . 1</head><label>11</label><figDesc>Training and Testing on MultiPIE Dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Recognition accuracy of PCANet on MultiPIE cross-illumination test set for varying number of filters in the first stage. (a) PCANet-1; (b) PCANet-2 with L 2 = 8. (-6,-6)-translated 10 o -rotated 0.9-scaled Input face Original image and its artificially deformed images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Recognition rate of PCANet on MultiPIE cross-illumination test set, for different PCANet block size and deformation to the test image. Two block sizes [8 6] and [12 9] for histogram aggregation are tested. (a) Simultaneous translation in x and y directions. (b) Translation in x direction. (c) Translation in y direction. (d) In-plane rotation. (e) Scale variation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>The PCANet filters learned on MultiPIE dataset. Top row: the first stage. Bottom row: the second stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Illustration of varying level of an occluded test face image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Error rate of PCANet on MNIST basic test set for varying number of filters in the first stage. (a) PCANet-1; (b) PCANet-2 with L 2 = 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>The PCANet filters learned on MNIST dataset. Top row: the first stage. Bottom row: the second stage. The filter size k 1 = k 2 = 7 are set in RandNet, PCANet, and LDANet unless specified otherwise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 .</head><label>10</label><figDesc>The PCANet filters learned on various MNIST datasets. For each dataset, the top row shows the filters of the first stage; the bottom row shows the filters of the second stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 11 .</head><label>11</label><figDesc>The PCANet filters learned on CUReT database. Top row: the first stage. Bottom row: the second stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 12 .</head><label>12</label><figDesc>The PCANet filters learned on Cifar10 database. Top: the first stage. Bottom: the second stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:1404.3606v2 [cs.CV] 28 Aug 2014</figDesc><table><row><cell cols="2">Stage 1</cell><cell cols="2">Stage 2</cell></row><row><cell>PCA filter</cell><cell>bank 1</cell><cell>PCA filter</cell><cell>bank 2</cell></row><row><cell>Input Image</cell><cell></cell><cell>PCA filter</cell><cell>bank 2</cell></row><row><cell>Output layer</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Composed</cell><cell></cell><cell></cell></row><row><cell cols="2">block-wise</cell><cell></cell><cell></cell></row><row><cell cols="2">histogram</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Binarization &amp;</cell></row><row><cell>.. .</cell><cell>histogram block-wise Composed</cell><cell></cell><cell>Conversion Decimal Binary to</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 Face</head><label>1</label><figDesc></figDesc><table><row><cell>S</cell><cell>100</cell><cell>500</cell><cell>1,000</cell><cell>5,000</cell><cell>10,000</cell><cell>50,000</cell></row><row><cell>PCANet-1</cell><cell>98.01</cell><cell>98.44</cell><cell>98.61</cell><cell>98.65</cell><cell>98.70</cell><cell>98.70</cell></row><row><cell cols="2">PCANet-2 100.00</cell><cell>100.00</cell><cell>100.00</cell><cell>100.00</cell><cell>100.00</cell><cell>100.00</cell></row></table><note>recognition rates (%) of PCANet on MultiPIE cross-illumination test set, with respect to different amount of generic faces training images (S).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3</head><label>3</label><figDesc>Recognition rates (%) on Extended Yale B dataset.</figDesc><table><row><cell>Percent occluded</cell><cell>0%</cell><cell>20%</cell><cell>40%</cell><cell>60%</cell><cell>80%</cell></row><row><cell>LBP [18]</cell><cell>75.76</cell><cell>65.66</cell><cell>54.92</cell><cell>43.22</cell><cell>18.06</cell></row><row><cell>P-LBP [21]</cell><cell>96.13</cell><cell>91.84</cell><cell>84.13</cell><cell>70.96</cell><cell>41.29</cell></row><row><cell>PCANet-1</cell><cell>97.77</cell><cell>96.34</cell><cell>93.81</cell><cell>84.60</cell><cell>54.38</cell></row><row><cell>PCANet-2</cell><cell>99.58</cell><cell>99.16</cell><cell>96.30</cell><cell>86.49</cell><cell>51.73</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4</head><label>4</label><figDesc>Recognition rates (%) on AR dataset.</figDesc><table><row><cell>Test sets</cell><cell>Illum.</cell><cell cols="3">Exps. Disguise Disguise + Illum.</cell></row><row><cell>LBP [18]</cell><cell>93.83</cell><cell>81.33</cell><cell>91.25</cell><cell>79.63</cell></row><row><cell>P-LBP [21]</cell><cell>97.50</cell><cell>80.33</cell><cell>93.00</cell><cell>88.58</cell></row><row><cell>PCANet-1</cell><cell>98.00</cell><cell>85.67</cell><cell>95.75</cell><cell>92.75</cell></row><row><cell>PCANet-2</cell><cell>99.50</cell><cell>85.00</cell><cell>97.00</cell><cell>95.00</cell></row><row><cell cols="3">3.1.3 Testing on AR Dataset.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 2</head><label>2</label><figDesc>Comparison of face recognition rates (%) of various methods on MultiPIE test sets. The filter size k 1 = k 2 = 5 are set in RandNet, PCANet, and LDANet unless specified otherwise.</figDesc><table><row><cell>Test Sets</cell><cell>Illum.</cell><cell>Exps.</cell><cell>Pose</cell><cell cols="2">Exps.+Pose Illum.+Exps.</cell><cell cols="2">Illum.+Pose Illum.+Exps.+Pose</cell></row><row><cell>Gabor [15]</cell><cell>68.75</cell><cell>94.17</cell><cell>84.17</cell><cell>64.70</cell><cell>38.09</cell><cell>39.76</cell><cell>25.92</cell></row><row><cell>LBP [18]</cell><cell>79.77</cell><cell>98.33</cell><cell>95.63</cell><cell>86.88</cell><cell>53.77</cell><cell>50.72</cell><cell>40.55</cell></row><row><cell>ScatNet-2 [6]</cell><cell>20.88</cell><cell>66.67</cell><cell>71.46</cell><cell>54.37</cell><cell>14.51</cell><cell>15.00</cell><cell>14.47</cell></row><row><cell>CNN-2 [8]</cell><cell>46.71</cell><cell>75.00</cell><cell>73.54</cell><cell>57.50</cell><cell>23.38</cell><cell>25.05</cell><cell>18.74</cell></row><row><cell>RandNet-1</cell><cell>80.88</cell><cell>98.33</cell><cell>87.50</cell><cell>75.62</cell><cell>46.57</cell><cell>42.80</cell><cell>31.85</cell></row><row><cell>RandNet-2</cell><cell>97.64</cell><cell>97.50</cell><cell>83.13</cell><cell>75.21</cell><cell>63.87</cell><cell>53.50</cell><cell>42.47</cell></row><row><cell>PCANet-1</cell><cell>98.70</cell><cell>99.17</cell><cell>94.17</cell><cell>87.71</cell><cell>72.40</cell><cell>65.76</cell><cell>53.80</cell></row><row><cell>PCANet-2</cell><cell>100.00</cell><cell>99.17</cell><cell>93.33</cell><cell>87.29</cell><cell>87.89</cell><cell>75.29</cell><cell>66.49</cell></row><row><cell>LDANet-1</cell><cell>99.95</cell><cell>98.33</cell><cell>92.08</cell><cell>82.71</cell><cell>77.89</cell><cell>68.55</cell><cell>57.97</cell></row><row><cell>LDANet-2</cell><cell>96.02</cell><cell>99.17</cell><cell>93.33</cell><cell>83.96</cell><cell>65.78</cell><cell>60.14</cell><cell>46.72</cell></row><row><cell>PCANet-1 (k 1 = 9)</cell><cell>100</cell><cell>99.17</cell><cell>89.58</cell><cell>81.46</cell><cell>75.74</cell><cell>67.59</cell><cell>56.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5</head><label>5</label><figDesc>Recognition rates (%) on FERET dataset.</figDesc><table><row><cell>Probe sets</cell><cell>Fb</cell><cell>Fc</cell><cell cols="2">Dup-I Dup-II</cell><cell>Avg.</cell></row><row><cell>LBP [18]</cell><cell>93.00</cell><cell>51.00</cell><cell>61.00</cell><cell>50.00</cell><cell>63.75</cell></row><row><cell>DMMA [25]</cell><cell>98.10</cell><cell>98.50</cell><cell>81.60</cell><cell>83.20</cell><cell>89.60</cell></row><row><cell>P-LBP [21]</cell><cell>98.00</cell><cell>98.00</cell><cell>90.00</cell><cell>85.00</cell><cell>92.75</cell></row><row><cell>POEM [26]</cell><cell>99.60</cell><cell>99.50</cell><cell>88.80</cell><cell>85.00</cell><cell>93.20</cell></row><row><cell>G-LQP [27]</cell><cell>99.90</cell><cell>100</cell><cell>93.20</cell><cell>91.00</cell><cell>96.03</cell></row><row><cell>LGBP-LGXP [28]</cell><cell>99.00</cell><cell>99.00</cell><cell>94.00</cell><cell>93.00</cell><cell>96.25</cell></row><row><cell>sPOEM+POD [29]</cell><cell>99.70</cell><cell>100</cell><cell>94.90</cell><cell>94.00</cell><cell>97.15</cell></row><row><cell>GOM [30]</cell><cell>99.90</cell><cell>100</cell><cell>95.70</cell><cell>93.10</cell><cell>97.18</cell></row><row><cell cols="2">PCANet-1 (Trn. CD) 99.33</cell><cell>99.48</cell><cell>88.92</cell><cell>84.19</cell><cell>92.98</cell></row><row><cell cols="2">PCANet-2 (Trn. CD) 99.67</cell><cell>99.48</cell><cell>95.84</cell><cell>94.02</cell><cell>97.25</cell></row><row><cell>PCANet-1</cell><cell>99.50</cell><cell>98.97</cell><cell>89.89</cell><cell>86.75</cell><cell>93.78</cell></row><row><cell>PCANet-2</cell><cell>99.58</cell><cell>100</cell><cell>95.43</cell><cell>94.02</cell><cell>97.26</cell></row><row><cell cols="6">gallery samples. The NN classifier with cosine distance</cell></row><row><cell cols="6">is used. Moreover, in addition to PCANet trained from</cell></row><row><cell cols="6">MultiPIE database, we also train PCANet on the FERET</cell></row><row><cell cols="6">generic training set, consisting of 1,002 images of 429</cell></row><row><cell cols="5">people listed in the FERET standard training CD.</cell><cell></cell></row><row><cell cols="6">The results of the PCANet and other state-of-the-</cell></row><row><cell cols="2">art methods are listed in</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 7</head><label>7</label><figDesc>Error rates (%) of PCANet-2 on basic dataset for varying block overlap ratios (BORs).</figDesc><table><row><cell>BOR</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell></row><row><cell cols="2">RandNet-2 1.31</cell><cell>1.35</cell><cell>1.23</cell><cell>1.34</cell><cell>1.18</cell><cell cols="2">1.14 1.24</cell></row><row><cell>PCANet-2</cell><cell>1.12</cell><cell>1.12</cell><cell>1.07</cell><cell>1.06</cell><cell>1.06</cell><cell cols="2">1.02 1.05</cell></row><row><cell>LDANet-2</cell><cell>1.14</cell><cell>1.14</cell><cell>1.11</cell><cell>1.05</cell><cell>1.05</cell><cell cols="2">1.05 1.06</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 8</head><label>8</label><figDesc>Details of the 9 classification tasks on MNIST and MNIST variations.</figDesc><table><row><cell>Data Sets</cell><cell>Description</cell><cell>Num. of classes</cell><cell>Train-Valid-Test</cell></row><row><cell>MNIST</cell><cell>Standard MNIST</cell><cell>10</cell><cell>60000-0-10000</cell></row><row><cell>basic</cell><cell>Smaller subset of MNIST</cell><cell>10</cell><cell>10000-2000-50000</cell></row><row><cell>rot</cell><cell>MNIST with rotation</cell><cell>10</cell><cell>10000-2000-50000</cell></row><row><cell>bg-rand</cell><cell>MNIST with noise background</cell><cell>10</cell><cell>10000-2000-50000</cell></row><row><cell>bg-img</cell><cell>MNIST with image background</cell><cell>10</cell><cell>10000-2000-50000</cell></row><row><cell cols="2">bg-img-rot MNIST with rotation and image background</cell><cell>10</cell><cell>10000-2000-50000</cell></row><row><cell>rect</cell><cell>Discriminate between tall and wide rectangles</cell><cell>2</cell><cell>1000-200-50000</cell></row><row><cell>rect-img</cell><cell>Dataset rect with image background</cell><cell>2</cell><cell>10000-2000-50000</cell></row><row><cell>convex</cell><cell>Discriminate between convex and concave shape</cell><cell>2</cell><cell>6000-2000-50000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 10</head><label>10</label><figDesc>Comparison of testing error rates (%) of the various methods on MNIST variations.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Methods</cell><cell>basic</cell><cell>rot</cell><cell>bg-rand</cell><cell>bg-img bg-img-rot</cell><cell>rect</cell><cell>rect-img</cell><cell>convex</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CAE-2 [45]</cell><cell>2.48</cell><cell>9.66</cell><cell>10.90</cell><cell>15.50</cell><cell>45.23</cell><cell>1.21</cell><cell>21.54</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TIRBM [46]</cell><cell>-</cell><cell>4.20</cell><cell>-</cell><cell>-</cell><cell>35.50</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">PGBM + DN-1 [40]</cell><cell>-</cell><cell>-</cell><cell>6.08</cell><cell>12.25</cell><cell>36.76</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ScatNet-2 [6]</cell><cell>1.27</cell><cell>7.48</cell><cell>12.30</cell><cell>18.40</cell><cell>50.48</cell><cell>0.01</cell><cell>8.02</cell><cell>6.50</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">RandNet-1</cell><cell>1.86</cell><cell>14.25</cell><cell>18.81</cell><cell>15.97</cell><cell>51.82</cell><cell>0.21</cell><cell>15.94</cell><cell>6.78</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">RandNet-2</cell><cell>1.25</cell><cell>8.47</cell><cell>13.47</cell><cell>11.65</cell><cell>43.69</cell><cell>0.09</cell><cell>17.00</cell><cell>5.45</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">PCANet-1</cell><cell>1.44</cell><cell>10.55</cell><cell>6.77</cell><cell>11.11</cell><cell>42.03</cell><cell>0.15</cell><cell>25.55</cell><cell>5.93</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">PCANet-2</cell><cell>1.06</cell><cell>7.37</cell><cell>6.19</cell><cell>10.95</cell><cell>35.48</cell><cell>0.24</cell><cell>14.08</cell><cell>4.36</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">LDANet-1</cell><cell>1.61</cell><cell>11.40</cell><cell>7.16</cell><cell>13.03</cell><cell>43.86</cell><cell>0.15</cell><cell>23.63</cell><cell>6.89</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">LDANet-2</cell><cell>1.05</cell><cell>7.52</cell><cell>6.81</cell><cell>12.42</cell><cell>38.54</cell><cell>0.14</cell><cell>16.20</cell><cell>7.22</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">PCANet-1 (k 1 = 13)</cell><cell>1.21</cell><cell>8.30</cell><cell>6.88</cell><cell>11.97</cell><cell>39.06</cell><cell>0.03</cell><cell>13.94</cell><cell>6.75</cell></row><row><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">RandNet?1</cell><cell></cell><cell></cell></row><row><cell>Error rate (%)</cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell>PCANet?1 LDANet?1</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2 0</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>12</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="6">Number of filters in the first stage (L 1 )</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 9</head><label>9</label><figDesc>Comparison of error rates (%) of the methods on MNIST, excluding methods that augment the training data. The filter size k 1 = k 2 = 7 are set in RandNet, PCANet, and LDANet unless specified otherwise.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 11</head><label>11</label><figDesc>Comparison of error rates (%) on CUReT.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 12</head><label>12</label><figDesc>Comparison of accuracy (%) of the methods on CIFAR10 with no data augmentation.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">. We have tested the PCANet without patch-mean removal and the performance degrades significantly.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Representation learning: a review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Maxout networks,&quot; in ICML</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">What is the best multi-stage architecture for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Invariant scattering convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1872" to="1886" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional deep belief networks for scalable unsupervised learnig of hierachical representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rananth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning convolutional feature hierarchies for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rotation, scaling and deformation invariant scattering for texture discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distortion discriminant analysis for audio fingerprinting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TSAP</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="165" to="174" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Bayesian hierarchical model for learning natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gabor feature based classification using the enhanced fisher linear discriminant model for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="476" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A direct LDA algorithm for high-dimensional data-with application to face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2067" to="2069" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-pie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Face description with local binary patterns: application to face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2037" to="2041" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Caffe: An open source convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="http://caffe.berkeleyvision.org/" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">From few to many: Illumination cone models for face recognition under variable lighting and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Georghiades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="643" to="660" />
			<date type="published" when="2001-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Enhanced local texture feature sets for face recognition under difficult lighting conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1635" to="1650" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benavente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVC Technical Report</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Extended SRC: Undersampled face recognition via intraclass variant dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1864" to="1870" />
			<date type="published" when="2012-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The FERET database and evaluaion procedure for face-recognition algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Rauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vision Comput</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="295" to="306" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Discriminative multi-manifold analysis for face recognition from a single training sample per person</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="51" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Enhanced patterns of oriented edge magnitudes for face recognition and image matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-S</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caplier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1352" to="1368" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Face recognition using local quantized patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Napoleon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fusing local patterns of gabor magnitude and phase for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1349" to="1361" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploring patterns of gradient orientations and magnitudes for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-S</forename><surname>Vu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="304" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Gabor ordinal measures for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mndez-Vzquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="26" />
			<date type="published" when="2014-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: a database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Effective face recognition by combining multiple descriptors and learned background statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast high dimensional vector multiplication face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Barkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Aronowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning deep face representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1403.2802v1</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Blessing of dimensionality: high-dimentional feature and its efficient compression for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fusing robust face region descriptors via multiple metric learning for face recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An empirical evaluation of deep architectures on problems with many factors of variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning and selecting features jointly with point-wise gated Boltzmann machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning image representations from the pixel level via hierarchical sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Shape matching and object recognition using shape contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="509" to="522" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deformation models for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gollan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1422" to="1435" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Stochastic pooling for regularization of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Contractive auto-encoders: explicit invariance during feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rifai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning invariant representations with local transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A statistical approach to material classification using image patch examplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2032" to="2047" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">On the significance of real-world conditions for material classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hayman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">O</forename><surname>Eklundh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Using basic image features for texture classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Crosier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Griffin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="447" to="460" />
		</imprint>
	</monogr>
	<note type="report_type">IJCV</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Statistical estimation of histogram variation for texture classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Broadhust</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop on Texture Analysis and Synthesis</title>
		<meeting>Workshop on Texture Analysis and Synthesis</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The pyramid match kernel: Discriminative classification with sets of image features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Beyond bags of features: spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Tiled convolutional neural networks,&quot; in NIPS</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Improved local coordinate coding using local tangents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Kernel descriptors for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<ptr target="http://code.google.com/p/cuda-convnet/" />
	</analytic>
	<monogr>
		<title level="j">cuda-convnet</title>
		<imprint>
			<date type="published" when="2014-07-18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400v3</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
