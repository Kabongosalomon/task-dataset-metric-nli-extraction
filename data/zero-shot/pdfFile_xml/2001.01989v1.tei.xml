<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Latent Opinions Transfer Network for Target-Oriented Opinion Words Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Collaborative Innovation Center of Novel Software Technology and Industrialization</orgName>
								<orgName type="laboratory">National Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<postCode>210023, 210023</postCode>
									<settlement>Nanjing, Nanjing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Zhao</surname></persName>
							<email>zhaof@smail.nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Collaborative Innovation Center of Novel Software Technology and Industrialization</orgName>
								<orgName type="laboratory">National Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<postCode>210023, 210023</postCode>
									<settlement>Nanjing, Nanjing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin-Yu</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Collaborative Innovation Center of Novel Software Technology and Industrialization</orgName>
								<orgName type="laboratory">National Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<postCode>210023, 210023</postCode>
									<settlement>Nanjing, Nanjing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Collaborative Innovation Center of Novel Software Technology and Industrialization</orgName>
								<orgName type="laboratory">National Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<postCode>210023, 210023</postCode>
									<settlement>Nanjing, Nanjing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
							<email>chenjj@nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Collaborative Innovation Center of Novel Software Technology and Industrialization</orgName>
								<orgName type="laboratory">National Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<postCode>210023, 210023</postCode>
									<settlement>Nanjing, Nanjing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Latent Opinions Transfer Network for Target-Oriented Opinion Words Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Target-oriented opinion words extraction (TOWE) is a new subtask of ABSA, which aims to extract the corresponding opinion words for a given opinion target in a sentence. Recently, neural network methods have been applied to this task and achieve promising results. However, the difficulty of annotation causes the datasets of TOWE to be insufficient, which heavily limits the performance of neural models. By contrast, abundant review sentiment classification data are easily available at online review sites. These reviews contain substantial latent opinions information and semantic patterns. In this paper, we propose a novel model to transfer these opinions knowledge from resource-rich review sentiment classification datasets to low-resource task TOWE. To address the challenges in the transfer process, we design an effective transformation method to obtain latent opinions, then integrate them into TOWE. Extensive experimental results show that our model achieves better performance compared to other state-of-the-art methods and significantly outperforms the base model without transferring opinions knowledge. Further analysis validates the effectiveness of our model. * Authors contributed equally.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Target-oriented opinion words extraction (TOWE) <ref type="bibr" target="#b1">(Fan et al. 2019</ref>) is a new subtask of aspect-level sentiment analysis (ABSA) <ref type="bibr" target="#b17">(Pang and Lee 2008;</ref><ref type="bibr" target="#b15">Liu 2012;</ref><ref type="bibr" target="#b19">Pontiki et al. 2014)</ref>, which aims to extract the corresponding opinion words for a given opinion target from a review sentence. Opinion targets, also known as aspect terms, are the words or phrases in the sentence representing features or entities toward which users show attitudes. Opinion words refer to those terms of a sentence used to express attitudes or opinions explicitly. <ref type="figure">Figure 1</ref> shows an example of TOWE. In the sentence "waiters are very friendly and the pasta is out of this world.", the terms "waiters" and "pasta" are two given opinion targets. TOWE needs to extract the word "friendly" as the opinion word of the opinion target "waiters" and the opinion words span "out of this world" for the target "pasta". Figure 1: An example of TOWE. The words highlighted in red are two given opinion targets. TOWE task aims to extract the spans in blue as opinion words for the given targets. The arrows indicate the correspondence between opinion targets and opinion words. Note that opinion targets are given beforehand in the TOWE task.</p><p>Many downstream sentiment analysis tasks, e.g., targetoriented sentiment classification <ref type="bibr" target="#b25">(Tang et al. 2016;</ref><ref type="bibr" target="#b29">Wang et al. 2016b;</ref><ref type="bibr" target="#b32">Xue and Li 2018)</ref> and pair-wise opinion summarization <ref type="bibr" target="#b6">(Hu and Liu 2004;</ref><ref type="bibr" target="#b33">Zhuang, Jing, and Zhu 2006;</ref><ref type="bibr" target="#b10">Li et al. 2010)</ref>, can benefit from TOWE as it provides explicit opinion pairs information. To study this task, <ref type="bibr" target="#b1">Fan et al. (2019)</ref> released a benchmark corpus including four datasets and formalized TOWE as a problem of sequence labeling for given targets. Furthermore, they proposed a targetfused neural sequence labeling model and achieved state-ofthe-art performance.</p><p>Despite the promising results of neural network methods, the lack of annotated data still heavily restricts the performance of TOWE. In practical scenarios, users usually refer to a considerable number of opinion targets in a review. It is extremely labor-intensive and time-consuming for annotators to identify all targets of a sentence and locate their corresponding opinion words. The difficulty of annotation causes the datasets of TOWE to be relatively scarce, which finally limits the effectiveness of neural models. In contrast, abundant labeled data of review sentiment classification are easily accessible at online review sites such as Amazon, Yelp and IMDB. Substantial opinions information and semantic patterns are naturally embodied in these reviews. Thus we propose to transfer them from large-scale sentiment classification datasets to the low-resource task TOWE. Although latent opinion knowledge is beneficial for TOWE, there are still two challenges remaining:</p><p>? The opinions information such as opinion words in sentiment classification datasets are latent and unannotated, we need to find them explicitly before transferring them. ? Since sentiment classification for reviews does not consider the target information, the latent-opinion information obtained is global and independent of the target. Thus, this information cannot be used directly by TOWE.</p><p>To address the above issues, we propose a novel model Latent Opinions Transfer Network (LOTN) leveraging latent opinions knowledge from resource-rich review sentiment classification datasets to improve TOWE task. Specifically, we first pre-train an attention-based BiLSTM model on review sentiment classification datasets. The attention mechanism <ref type="bibr" target="#b0">(Bahdanau, Cho, and Bengio 2015)</ref> is employed to extract possible opinion words through probabilistic weights. To solve the second issue, we design an effective transformation method to convert the global attention distribution over words in the sentiment classification model to latent target-dependent opinion words. Finally, we integrate these captured opinions words into TOWE model via an auxiliary learning signal. Additionally, we incorporate the encoder of the pretrained model to further guide TOWE model to learn latent opinions, which proves effective.</p><p>We evaluate the LOTN model on the four benchmark datasets. Results from extensive experiments indicate that our model achieves new state-of-the-art performance for TOWE and performs significantly better than our base model that does not transfer opinion knowledge. Further indepth analysis also validates the effectiveness of our model. To the best of our knowledge, it is the first work to improve TOWE by transferring latent opinions knowledge of review sentiment classification datasets.</p><p>The main contributions of this work include: ? In tackling the problem of insufficient annotated data, we are the first to propose transferring latent opinion knowledge from resource-rich review sentiment classification datasets to the low-resource task of TOWE. ? To transfer opinion information effectively, we propose a novel model that obtains latent opinion words from a sentiment classification model and integrates them into TOWE via an auxiliary learning signal. ? The experiment results indicate that our model achieves better results compared to state-of-the-art methods. Extensive analysis validates the effectiveness of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminary</head><p>In this section, we will introduce the task formalization of TOWE and the pretrained sentiment classification model that is used for transferring latent opinions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TOWE Formalization</head><p>TOWE aims to extract the corresponding opinion words for a given target from a sentence, which can be formalized as a task of sequence labeling for given targets. Specifically, given a review sentence s = {w 1 , w 2 , ? ? ? , w n } consisting n words and an opinion target w t in the sentence s (Note that, we notate an opinion target as one word for simplicity and t is the position of the target in the sentence), the goal is to tag each word w i in s with a label y i ? {B, I, O} (B: Beginning, I: Inside, O: Others). The spans composed by the tags B and I represent the corresponding opinion words of the target w t . For example, the sentence in <ref type="figure">Figure 1</ref> is tagged as w i /y i for different opinion targets as shown in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pretraining Sentiment Classification Model</head><p>Review sentiment classification aims to detect overall sentiment polarity (e.g., positive or negative) of a review text. Before transferring latent opinions, we first pretrain an attention-based BiLSTM model on large-scale review sentiment classification datasets. Specifically, we regard a review from sentiment classification datasets as a long sentence {w 1 , w 2 , ? ? ? , w m } consisiting m words, and map them into the corresponding vector representations {w 1 , w 2 , ? ? ? , w m } by looking up an embedding table. Then a BiLSTM network is applied to encode the word representations {w 1 , w 2 , ? ? ? , w m } and genenrate the context representations {h sc 1 , h sc 2 , ? ? ? , h sc m }. The attention mechanism is employed to capture the latent and global opinion words that are significant to sentiment classification. The attention weight ? i of h sc i is defined as:</p><formula xml:id="formula_0">u(h sc i , h sc avg ) = h sc i ? W u ? h sc avg + b u ,<label>(1)</label></formula><formula xml:id="formula_1">? i = exp(u(h sc i , h sc avg )) m j=1 exp(u(h sc j , h sc avg )) ,<label>(2)</label></formula><p>where h sc avg is the average of all hidden states, i.e., h sc avg = m j=1 h sc j /m, W u and b u are the weight matrix and bias. The review representation r sc is a weighted sum of all hidden states:</p><formula xml:id="formula_2">r sc = m i=1 ? i h sc i .<label>(3)</label></formula><p>Finally, the representation r sc is fed to a linear layer and a softmax layer to predict the sentiment label of the review. We train the sentiment classification model by minimizing the cross-entropy loss between the predicted sentiment distribution and the ground truth. After pretraining is finished, all parameters in sentiment classification model are fixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Latent Opinions Transfer Network</head><p>In this section, we introduce our model Latent Opinions Transfer Network in details. We will present the overall architecture of the model, our base TOWE model and two proposed latent opinions transfer methods, as well as the final decoding and training. Overall Description <ref type="figure" target="#fig_0">Figure 2</ref> shows the overall architecture of Latent Opinions Transfer Network (LOTN). It consists of two components mainly: a TOWE module and a pretrained review sentiment classification module. We design a simple and effective network as our base TOWE module, namely position embedding based BiLSTM (PE-BiLSTM). The pretrained sentiment classification module is the aforementioned attentionbased BiLSTM network. LOTN transfers latent opinions from the sentiment classification module to the TOWE module through two different perspectives. Firstly, the BiLSTM layer of the pretrained sentiment classification module contains substantial implicit opinion information and semantic patterns. We integrate this information into the encoding layer of TOWE module to introduce external opinion knowledge.</p><p>Secondly, the latent opinion words captured by the attention process of the pretrained module are global and targetindependent since review sentiment classification does not consider the target information. To address this issue, we propose a heuristic transformation method to convert global attention weights over words to latent target-dependent opinion words by considering the position information of the target and other words. Then we incorporate them into the TOWE module via an auxiliary learning signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Position Embedding based BiLSTM</head><p>In the base model, we use position embedding to model the target information instead the state-of-the-art model IOG <ref type="bibr" target="#b1">(Fan et al. 2019)</ref> because IOG employs six different positional and directional LSTMs simultaneously and suffers from high model complexity. In contrast to IOG, position embedding is a simple and effective method of model-ing position information and widely used in Natural Language Processing <ref type="bibr" target="#b11">(Lin et al. 2016;</ref><ref type="bibr">Gehring et al. 2017;</ref><ref type="bibr" target="#b26">Vaswani et al. 2017;</ref><ref type="bibr" target="#b4">Gu et al. 2018)</ref>.</p><p>Given the sentence s = {w 1 , w 2 , ? ? ? , w n } and the opinion target w t in the sentence, we first generate the relative distance index l i = |i ? t| of each word w i in s by calculating the relative distance from w i to the target w t . Then the distance index l i is mapped into the positional representation by using a position embedding tabel E pos ? R L?d1 , where d 1 is the embedding dimension and L is the maximal position index. In addition, we also use a word embedding table E emb ? R |V |?d2 to obtain the semantic representation of word. The representation e i of each word w i is formed by concatenating the word vector and the corresponding position vector:</p><formula xml:id="formula_3">e i = [E emb (w i ); E pos (l i )] ,<label>(4)</label></formula><p>where the [?; ?] denotes the vector concatenation operation. Finally, we employ a BiLSTM network to capture the contextual information of each word. The simplified update rule can be written as follows:</p><formula xml:id="formula_4">h t i = BiLSTM(h t i?1 , e i , ? t ),<label>(5)</label></formula><p>where ? t represents the parameters of BiLSTM. In the base model, the context representation h t can be used for predicting the opinion words of the given target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transferring Pretrained Encoder</head><p>In order to transfer latent opinions knowledge, we also feed the sentence s of TOWE task to the pretrained sentiment classification module to yield the corresponding hidden states {h sc 1 , h sc 2 , ? ? ? , h sc n } and attention weights {? 1 , ? 2 , ? ? ? , ? n }.</p><p>From the semantic level view, the encoder of the pretrained sentiment classification module holds substantial implicit opinion information, and thus we integrate it into the TOWE module by concatenating two hidden states:</p><formula xml:id="formula_5">r i = h t i ; h sc i ,<label>(6)</label></formula><p>where r i contains both task-specific context representations and external opinions knowledge from review sentiment classification datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transferring Latent Opinion Words</head><p>To effectively transfer latent opinion words from sentiment classification module to TOWE module, we design a heuristic transformation method and an auxiliary learning signal respectively to capture and integrate them into TOWE.</p><p>Transformation Method As we have mentioned, the attention mechanism in the sentiment classification module captures latent opinion words in the style of probabilistic weights. However. these probabilistic opinions information are global and target-independent. Intuitively, the word that is closer to the opinion target is more likely to be the opinion word of the target. Thus we introduce the opinion target information into the attention distribution by a target-relevant distance weight c i :</p><formula xml:id="formula_6">? i = c i ? ? i ,<label>(7)</label></formula><formula xml:id="formula_7">c i = 1 ? |i ? t| n ,<label>(8)</label></formula><p>where n is the length of input sentence, t indicates the position of the opinion target w t in the sentence, and |i ? t| denotes the relative distance between the word w i and the target w t . It can be observed that a closer word to the target has a bigger distance weight. To regain the probabilistic attention distribution, the target-dependent attention weight ? i is re-normalized:</p><formula xml:id="formula_8">? i = ? i n j=1 ? j .<label>(9)</label></formula><p>Finally, we use a heuristic strategy to convert the normalized attention weight ? i into the binary latent opinion words by the threshold 1 n :</p><formula xml:id="formula_9">y a i = 1 if ? i ? 1 n , 0 otherwise ,<label>(10)</label></formula><p>where y a i = 1 denotes that the word w i is a latent and target-relevant opinion word from the perspective of sentiment classification module and 0 indicates not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Auxiliary Learning Signal</head><p>In fact, the y a i is a pseudo label and represents the opinions knowledge from the sentiment classifcation module. We integrate these latent opinions into TOWE module by auxiliary learning signal:</p><formula xml:id="formula_10">y a i = softmax(W a r i + b a ),<label>(11)</label></formula><formula xml:id="formula_11">L a = ? n i=1 1 k=0 I(y a i = k) log(? a i,k ),<label>(12)</label></formula><p>where W a and b a are the weight matrix and the bias,? a i represents the prediction probability and I(?) is the indicator function. LOTN embraces these latent opinions knowledge by optimizing the auxiliary loss L a , which helps TOWE moduel decode the opinion words of the target better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decoding and Training</head><p>Since the context representation r i contains both taskspecfic opinions and tranferred opinions knowledge, LTON finally use r i to predict the tag y i ? {B, I, O} of the word w i . It can regarded as a three-class classfication problem at each position of the sentence s. We use a linear layer and a softmax layer to compute prediction probaility? i :</p><formula xml:id="formula_12">y i = softmax(W t r i + b t ),<label>(13)</label></formula><p>where W t is the weight matrix and b t denotes the bias. The cross-entropy loss of TOWE task can be defined as follows:</p><formula xml:id="formula_13">L t = ? n i=1 2 k=0 I(y i = k) log(? i,k ),<label>(14)</label></formula><p>here the tags {O, B, I} are correspondingly converted into labels {0, 1, 2} and y i denotes the ground truth label. LOTN also integrates latent opinions through auxiliary learning signal L a . Thus the final loss is defined as follows:</p><formula xml:id="formula_14">J = L t + ?L a ,<label>(15)</label></formula><p>where ? measures the importance of auxiliary learning and can be adjusted. We minimize the loss J to optimize the LOTN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets and Metrics</head><p>We evaluate our model on four benchmark datasets <ref type="bibr" target="#b1">(Fan et al. 2019)</ref>. The statistics of the datasets are summarized in <ref type="table" target="#tab_1">Table 2</ref>. The datasets 14res and 14lap are derived from Se-mEval Challenge 2014 task 4 <ref type="bibr" target="#b19">(Pontiki et al. 2014)</ref>, 15res and 16res are respectively from SemEval Challenge 2015 task 12 <ref type="bibr" target="#b20">(Pontiki et al. 2015</ref>) and SemEval Challenge 2016 task 5 <ref type="bibr" target="#b21">(Pontiki et al. 2016</ref>). The suffixes "res" and "lap" respectively represent reviews from restaurant domain or laptop domain. The original SemEval Challenge datasets are very popular benchmarks for ABSA subtasks. They provide the annotation of opinion targets for each sentence, but not the corresponding opinion words. To perform TOWE task, <ref type="bibr" target="#b1">Fan et al. (2019)</ref> annotate the corresponding opinion words for the given targets from sentences and remove the cases without explicit opinion words.</p><p>To pretrain the sentiment classification model, we use the two datasets respectively from Amazon Review and Yelp Review. The Yelp Review is applied to transfer latent opinions for TOWE datasets 14res, 15res, and 16res. The Amazon Review is used for the dataset 14lap. <ref type="table" target="#tab_2">Table 3</ref> shows the statistics of Amazon Review and Yelp Review.</p><p>Following the state-of-the-art work <ref type="bibr" target="#b1">(Fan et al. 2019)</ref>, we use the metrics precision, recall, and F1-score to measure the performance of different methods. An opinion word/phrase is deemed to be correct on the condition that the starting and ending positions of the prediction are both the same as those of the golden word/phrase.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Settings</head><p>We initialize word vectors with 300-dimension word embeddings from GloVe <ref type="bibr" target="#b18">(Pennington, Socher, and Manning 2014)</ref>. The word vectors are fixed and not fine-tuned during the training stage. We set the dimension of position embeddings to be 300. The position embeddings, all weight matrices and biases are randomly initialized by a uniform distribution U (?0.01, 0.01). The dimension of LSTM cell is set to 200. We adopt Adam optimizer <ref type="bibr" target="#b8">(Kingma and Ba 2015)</ref> to update parameters of models. The initial learning rate is 0.001 and mini-batch size is set to 25. The dropout <ref type="bibr" target="#b5">(Hinton et al. 2012)</ref> is applied on embedding layer with probability 0.5. We randomly select 20% of training set as validation set for tuning hyper-parameters and early stopping. Finally, we run each model five times and report the average result of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compared Methods</head><p>We compare our model with the following methods:</p><p>? Distance-rule first performs POS tagging on the sentence, then regards the nearest adjective to the opinion target as the corresponding opinion word <ref type="bibr" target="#b6">(Hu and Liu 2004)</ref>.</p><p>? Dependency-rule collects the POS tags of opinion targets and opinion words and the dependency path between them as rule templates from the training set. Then the hign-frequency dependency templates are applied to detect the corresponding opinion words of opinion targets for the testing set <ref type="bibr" target="#b33">(Zhuang, Jing, and Zhu 2006)</ref>.</p><p>? LSTM/BiLSTM employs word embeddings to represent words, then uses LSTM/BiLSTM network to capture the context information of input. Finally, each hidden state is fed to a softmax classifier for three-class classification to extract the opinion words of the given target <ref type="bibr" target="#b13">(Liu, Joty, and Meng 2015)</ref>.</p><p>? Pipeline is a combination method of BiLSTM and Distance-rule method <ref type="bibr" target="#b1">(Fan et al. 2019)</ref>. It first trains a BiLSTM model on the training set. During the testing stage, the Pipeline method uses BiLSTM model to extract all the opinion words spans, then select the nearest span to the target as the extraction result.</p><p>? TC-BiLSTM follows the design of the work for targetoriented sentiment classification <ref type="bibr" target="#b25">(Tang et al. 2016)</ref>. This method adopts the average pooling to obtain dimensionfixed target vector, then concatenate it with word vector at each position of the sentence. Finally, the concatenation of target vectors and word vectors are fed to BiLSTM for sequence labeling.</p><p>? IOG adopts six different positional and directional LSTMs to extract the opinion words of the target. This model achieves very powerful performance and is the state-of-the-art method in TOWE <ref type="bibr" target="#b1">(Fan et al. 2019</ref>).</p><p>? PE-BiLSTM is our base model. It incorporates the target information into TOWE by position embedding, then uses a BiLSTM to extract the opinion words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Results</head><p>The main experiment results are shown in <ref type="table" target="#tab_3">Table 4</ref>. We can observe that pure rule-based methods perform very poorly compared to the supervised learning models. The method Distance-rule achieves the worst recall and F1-score in most of the datasets since it only detects the single word as opinion word. Dependency-rule method obtains some improvements, but it is still worse than the sequence labeling models. Compared to other neural network methods, LSTM and BiLSTM achieve poor performance because they are targetindependent. They extract the same span as opinion words for different targets in a sentence. Pipeline uses BiLSTM to extract the spans of opinion words, then selects the closest span to the target as the final result. In fact, it is a targetdependent method and the distance strategy makes Pipeline method obtain nearly 20% improvement of precision over BiLSTM in the dataset 14res. By contrast, TC-BiLSTM performs worse than Pipeline and even is inferior to BiLSTM in 14lap. One possible reason for this is that concatenating the same target vector at each position is not a good approach to incorporate the target information. IOG employs six different positional and directional LSTMs to generate rich targetdependent context representations, achieving very powerful results in all datasets. However, it also suffers from high model complexity.</p><p>PE-BiLSTM is our base method that adopts position embedding to incorporate the target information. We can observe that this simple method obtain competitive performance and is only inferior to IOG and LOTN. The experiment results show that our model LOTN achieves the best F1-score in all datasets. Compared to its base version PE-BiLSTM, LOTN obtains about 4%?5% improvements in F1-score. In addition, LOTN outperforms the previous stateof-the-art method IOG by 1.98% and 2.02% F1-score respectively in the datasets 14res and 16res. These observations demonstrate that our model can effectively transfer the latent opinions knowledge from sentiment classification datasets to the TOWE task.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Effects of Tranferring Encoder and Latent Opinion Words</head><p>To investigate the effects of transferring the encoder and latent opinion words, we conduct the following experiments:</p><p>? PE-BiLSTM+transferred encoder: We only transfer the encoder of the sentiment classification model to TOWE based on the model PE-BiLSTM.</p><p>? PE-BiLSTM+auxiliary learning: This method only incorporates the latent opinion words from sentiment classification model into PE-BiLSTM via auxiliary learning. <ref type="table" target="#tab_4">Table 5</ref> shows the experiment results. Compared to the base model PE-BiLSTM, we can find that PE-BiLSTM+transferred encoder and PE-BiLSTM+auxiliary learning both achieve significant and consistent improvements on all datasets. The comparisons validate the effectiveness of transferring the encoder and latent opinion words from sentiment classification model for the TOWE task. After integrating both the transferred encoder and auxiliary learning, the model LOTN obtains further improvements.</p><p>The results indicate that the proposed two methods are useful for the final model LOTN and they transfer opinions knowledge from different perspectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Effect of the Hyper-parameter ?</head><p>To analyze the effect of different ? on our model, we adjust ? of Equation 15 in (0, 1) to conduct experiments and the step is 0.05. <ref type="figure" target="#fig_1">Figure 3</ref> shows the results of LOTN with different ? on four datasets.</p><p>We can observe that LOTN achieves the relatively stable performance with varying ? on the datasets 14res, 15res and 16res, which indicates the robustness of our method. In general, the performance of LOTN has a downward trend with an increase of ? since the bigger ? has a negative effect on the decoding of the model. Finally, we set ? to be 0.1 on 14lap and 0.2 on other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case Study</head><p>In order to compare different methods and validate the effectiveness of our model, we present some extracted results of the dataset 14res in <ref type="table" target="#tab_5">Table 6</ref>. The first example shows that the Distance-rule method cannot extract opinion phrase for the given target and thus makes the wrong prediction. Comparing the second and third examples, we can find that BiLSTM gives the same predictions for the different opinion targets since it ignores the target information. In the last two examples, the stateof-the-art model IOG and our base model PE-BiLSTM both make incorrect predictions in complicated cases, while our proposed model LOTN extracts target-dependent opinion words successfully. The results demonstrate that our model can leverage latent opinions information from sentiment </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Error Analysis</head><p>We count the distribution of different error types in the dataset 14res to analyze the weakness of LOTN. The results are given in <ref type="table" target="#tab_6">Table 7</ref>. The "NULL" represents the prediction is empty. The "Under-extracted" and "Over-extracted" respectively mean that LOTN extracts the part of the ground truth and extra words besides the golden opinion words. It can be observed that PE-BiLSTM and LOTN do not extract any opinion words in more than a quarter of error cases. In two models, the under-extracted error is the main error type. Compared to PE-BiLSTM, LOTN makes fewer mistakes in the NULL and under-extracted type. In contrast, PE-BiLSTM makes fewer over-extracted predictions. The three comparisons consistently indicate that LOTN tends to decode more opinion words under the influence of latent opinions from the sentiment classification model.</p><p>In addition, we find that the sentence is often quite long and the golden opinion words are far away from the target in NULL error. As for under-extracted cases, the models usually ignore modifiers such as the word "most" in "most delicious" or negators, e.g., only extracting "best" from "not the best". The over-extracted predictions are common in the samples containing multiple targets.</p><p>Although our model improves the overall performance of TOWE, LOTN makes some wrong predictions due to transferring some noise opinions. For example, about 5.7% samples of the dataset 14res are predicted successfully by the model PE-BiLSTM but incorrectly by LOTN. The heuristic transformation method is unable to consistently yield correct opinion words for a given target, thus we inevitably introduce some noise during converting global attention knowledge into latent target-dependent opinion words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>The early works devote to the research of opinion targets extraction, including unsupervised methods <ref type="bibr" target="#b22">(Popescu and Etzioni 2005;</ref><ref type="bibr" target="#b27">Wang and Wang 2008;</ref><ref type="bibr" target="#b24">Qiu et al. 2011;</ref><ref type="bibr" target="#b14">Liu, Xu, and Zhao 2012)</ref> and supervised methods <ref type="bibr" target="#b7">(Jin, Ho, and Srihari 2009;</ref><ref type="bibr" target="#b10">Li et al. 2010;</ref><ref type="bibr" target="#b13">Liu, Joty, and Meng 2015;</ref><ref type="bibr" target="#b23">Poria, Cambria, and Gelbukh 2016;</ref><ref type="bibr" target="#b31">Xu et al. 2018</ref>). Recently, some works extract opinion targets and opinion words jointly in a unified framework and achieve promising results <ref type="bibr" target="#b24">(Qiu et al. 2011;</ref><ref type="bibr" target="#b14">Liu, Xu, and Zhao 2012;</ref><ref type="bibr" target="#b12">Liu et al. 2013;</ref><ref type="bibr" target="#b28">Wang et al. 2016a;</ref><ref type="bibr" target="#b9">Li and Lam 2017)</ref>. However, these works does not extract the corresponding relation between targets and opinion words.</p><p>In fact, there are only a few works focusing on the paired opinion relations. <ref type="bibr" target="#b6">Hu and Liu (2004)</ref> propose to use association rule mining for extracting opinion targets and regard the nearest adjective of targets as the corresponding opinion words. <ref type="bibr" target="#b33">Zhuang, Jing, and Zhu (2006)</ref> adopt WordNet <ref type="bibr" target="#b16">(Miller 1995)</ref> and human-built word lists to find targets and opinion words, then apply dependency-tree templates to extract the valid target-opinion pairs. These two unsupervised methods heavily depend on pre-defined rules and external resources such as syntax parser. Because of the significance of the paired relations of targets and opinion words for downstream sentiment task, <ref type="bibr" target="#b1">Fan et al. (2019)</ref> propose a new subtask of ABSA, target-oriented opinion words extraction (TOWE), to extract the corresponding opinion words for a given opinion target from a review. They also design a target-fused neural sequence labeling model and achieve competitive results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Insufficiency of labeled data heavily restricts the effectiveness of the neural models for TOWE. In this paper, we propose a novel model to transfer latent opinions knowledge from resource-rich review sentiment classification datasets to improve the low-resource task TOWE. Specifically, we propose an effective method to convert the attention knowledge in the sentiment classification model into target-dependent opinion words, then integrate them into TOWE via auxiliary learning signal. In addition, we also integrate the encoder of the sentiment classification model to further improve TOWE. Results from numerous experiments indicate that our approach achieves better performance than other state-of-the-art methods. Extensive analysis also demonstrates the effectiveness of our model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The architecture of Latent Opinions Transfer Network. Different opinion targets in a sentence have different position embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The effect of different hyper-parameter ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Different labeling results of a sentence when given different opinion targets. The opinion targets are highlighted in underline and the opinion words/phrases are in bold.</figDesc><table><row><cell>1. Waiters/O are/O very/O friendly/B and/O the/O pasta/O</cell></row><row><cell>is/O out/O of/O this/O world/O ./O</cell></row><row><cell>2. Waiters/O are/O very/O friendly/O and/O the/O pasta/O</cell></row><row><cell>is/O out/B of/I this/I world/I ./O</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Statistics of TOWE datasets. A sentence may contain multiple opinion targets.</figDesc><table><row><cell cols="2">Datasets</cell><cell cols="2">#sentences #targets</cell></row><row><cell>14res</cell><cell>Train Test</cell><cell>1,627 500</cell><cell>2,643 864</cell></row><row><cell>14lap</cell><cell>Train Test</cell><cell>1,158 343</cell><cell>1,634 482</cell></row><row><cell>15res</cell><cell>Train Test</cell><cell>754 325</cell><cell>1,076 436</cell></row><row><cell>16res</cell><cell>Train Test</cell><cell>1,079 329</cell><cell>1,512 457</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Statistics of the two datasets Amazon Review and Yelp Review.</figDesc><table><row><cell>Datasets</cell><cell cols="2">#positive #negative</cell><cell>#total</cell></row><row><cell>Yelp Review</cell><cell>266,041</cell><cell>177,218</cell><cell>443,259</cell></row><row><cell>Amazon Review</cell><cell>277,228</cell><cell>277,769</cell><cell>554,997</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Main experiment results(%). Best results are in bold (P, R, and F1-score, the larger is the better). The marker ? represents that LOTN outperforms other methods significantly (p &lt; 0.01) . ? 80.52 ? 82.21 ? 77.08 ? 67.62 72.02 ? 76.61 ? 70.29 73.29 ? 86.57 ? 80.89 ? 83.62 ?</figDesc><table><row><cell>Models</cell><cell>P</cell><cell>14res R</cell><cell>F1</cell><cell>P</cell><cell>14lap R</cell><cell>F1</cell><cell>P</cell><cell>15res R</cell><cell>F1</cell><cell>P</cell><cell>16res R</cell><cell>F1</cell></row><row><cell>Distance-rule</cell><cell>58.39</cell><cell>43.59</cell><cell>49.92</cell><cell cols="3">50.13 33.86 40.42</cell><cell cols="3">54.12 39.96 45.97</cell><cell>61.90</cell><cell>44.57</cell><cell>51.83</cell></row><row><cell cols="2">Dependency-rule 64.57</cell><cell>52.72</cell><cell>58.04</cell><cell cols="3">45.09 31.57 37.14</cell><cell cols="3">65.49 48.88 55.98</cell><cell>76.03</cell><cell>56.19</cell><cell>64.62</cell></row><row><cell>LSTM</cell><cell>52.64</cell><cell>65.47</cell><cell>58.34</cell><cell cols="3">55.71 57.53 56.52</cell><cell cols="3">57.27 60.69 58.93</cell><cell>62.46</cell><cell>68.72</cell><cell>65.33</cell></row><row><cell>BiLSTM</cell><cell>58.34</cell><cell>61.73</cell><cell>59.95</cell><cell cols="3">64.52 61.45 62.71</cell><cell cols="3">60.46 63.65 62.00</cell><cell>68.68</cell><cell>70.51</cell><cell>69.57</cell></row><row><cell>Pipeline</cell><cell>77.72</cell><cell>62.33</cell><cell>69.18</cell><cell cols="3">72.58 56.97 63.83</cell><cell cols="3">74.75 60.65 66.97</cell><cell>81.46</cell><cell>67.81</cell><cell>74.01</cell></row><row><cell>TC-BiLSTM</cell><cell>67.65</cell><cell>67.67</cell><cell>67.61</cell><cell cols="3">62.45 60.14 61.21</cell><cell cols="3">66.06 60.16 62.94</cell><cell>73.46</cell><cell>72.88</cell><cell>73.10</cell></row><row><cell>IOG</cell><cell>82.38</cell><cell>78.25</cell><cell>80.23</cell><cell cols="3">73.43 68.74 70.99</cell><cell cols="3">72.19 71.76 71.91</cell><cell>84.36</cell><cell>79.08</cell><cell>81.60</cell></row><row><cell>PE-BiLSTM</cell><cell>80.10</cell><cell>76.51</cell><cell>78.26</cell><cell cols="3">72.01 64.20 67.83</cell><cell cols="3">70.36 65.73 67.96</cell><cell>82.27</cell><cell>74.95</cell><cell>78.43</cell></row><row><cell>LOTN</cell><cell>84.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Experiment results of adding the transferred encoder or auxiliary learning on PE-BiLSTM(%).</figDesc><table><row><cell>Models</cell><cell>P</cell><cell>14res R</cell><cell>F1</cell><cell>P</cell><cell>14lap R</cell><cell>F1</cell><cell>P</cell><cell>15res R</cell><cell>F1</cell><cell>P</cell><cell>16res R</cell><cell>F1</cell></row><row><cell>PE-BiLSTM</cell><cell cols="12">80.10 76.51 78.26 72.01 64.20 67.83 70.36 65.73 67.96 82.27 74.95 78.43</cell></row><row><cell cols="13">+transferred encoder 84.57 79.54 81.97 77.50 67.47 72.13 75.90 69.00 72.26 86.05 79.81 82.79</cell></row><row><cell>+auxiliary learning</cell><cell cols="12">84.10 77.20 80.49 75.63 66.42 70.71 76.31 68.67 72.29 86.77 79.46 82.93</cell></row><row><cell>LOTN</cell><cell cols="12">84.00 80.52 82.21 77.08 67.62 72.02 76.61 70.29 73.29 86.57 80.89 83.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Examples of the extracted results in different methods. The opinion targets are in underline and the corresponding golden opinion words are in bold. The "NULL" represents that the prediction is empty.</figDesc><table><row><cell>Sentence</cell><cell>Distance-rule</cell><cell>BiLSTM</cell><cell>IOG</cell><cell>PE-BiLSTM</cell><cell cols="2">LOTN Latent Opinion Words Target Decoding</cell></row><row><cell>The bread is top notch as well.</cell><cell>top%</cell><cell>top notch!</cell><cell>top notch!</cell><cell>top notch!</cell><cell>top notch</cell><cell>top notch!</cell></row><row><cell>Great food but the service was dreadful!</cell><cell>Great!</cell><cell>dreadful%</cell><cell>Great!</cell><cell>Great!</cell><cell>Great</cell><cell>Great!</cell></row><row><cell>Great food but the service was dreadful!</cell><cell>dreadful!</cell><cell>dreadful!</cell><cell>dreadful!</cell><cell>dreadful!</cell><cell>dreadful</cell><cell>dreadful!</cell></row><row><cell>Good for a quick sushi lunch.</cell><cell>quick%</cell><cell>Good, quick!</cell><cell>quick%</cell><cell>quick%</cell><cell>Good</cell><cell>Good, quick!</cell></row><row><cell>Their twist on pizza is healthy, but full of flavor.</cell><cell>full!</cell><cell>healthy%</cell><cell>healthy, full%</cell><cell>NULL%</cell><cell>full</cell><cell>full!</cell></row><row><cell cols="2">classification datasets to improve the TOWE task.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Statistics of different error types for PE-BiLSTM and LOTN in the dataset 14res.</figDesc><table><row><cell>Models</cell><cell cols="5">NULL Under-extracted Over-extracted Others Total</cell></row><row><cell>PE-BiLSTM</cell><cell>76</cell><cell>107</cell><cell>49</cell><cell>34</cell><cell>266</cell></row><row><cell>LOTN</cell><cell>65</cell><cell>85</cell><cell>62</cell><cell>31</cell><cell>243</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Robert Ridley for his comments and suggestions on this paper, and the anonymous reviewers for their valuable feedback. This work was supported by the NSFC <ref type="figure">(No. 61976114, 61936012)</ref> and National Key R&amp;D Program of China (No. 2018YFB1005102).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Target-oriented opinion words extraction with target-fused neural sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2509" to="2518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A convolutional encoder model for neural machine translation</title>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<biblScope unit="page" from="123" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A positionaware bidirectional attention network for aspect-level sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="774" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>abs/1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A novel lexicalized hmm-based learning framework for web opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srihari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="465" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep multi-task learning for aspect term extraction with memory interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2886" to="2892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Structure-aware review mining and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="653" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2124" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Opinion target extraction using partially-supervised word alignment model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2134" to="2140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fine-grained opinion mining with recurrent neural networks and word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1433" to="1443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Opinion target extraction using word-based translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1346" to="1356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Synthesis Lectures on Human Language Technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
	<note>Sentiment Analysis and Opinion Mining</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Opinion mining and sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1" to="135" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semeval-2014 task 4: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Se-mEval@COLING 2014</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semeval-2015 task 12: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval@NAACL-HLT</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="486" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semeval-2016 task 5: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Al-Smadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Al-Ayyoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">D</forename><surname>Clercq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Apidianaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tannier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Loukachevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">V</forename><surname>Kotelnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M J</forename><surname>Zafra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Eryigit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval@NAACL-HLT</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="19" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Extracting product features and opinions from reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT/EMNLP</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="339" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Aspect extraction for opinion mining with a deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl.-Based Syst</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="42" to="49" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Opinion word expansion and target extraction through double propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="27" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Effective lstms for target-dependent sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3298" to="3307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bootstrapping both product features and opinion words from chinese customer reviews with cross-inducing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="289" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recursive neural conditional random fields for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="616" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention-based LSTM for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Coupled multi-layer attentions for co-extraction of aspect and opinion terms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3316" to="3322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Double embeddings and cnn-based sequence labeling for aspect extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="592" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Aspect based sentiment analysis with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2514" to="2523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Movie review mining and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM CIKM</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
