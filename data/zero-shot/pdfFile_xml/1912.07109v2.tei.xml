<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SDFDiff: Differentiable Rendering of Signed Distance Fields for 3D Shape Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Jiang</surname></persName>
							<email>yuejiang@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dantong</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Zwicker</surname></persName>
							<email>zwicker@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SDFDiff: Differentiable Rendering of Signed Distance Fields for 3D Shape Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose SDFDiff, a novel approach for image-based shape optimization using differentiable rendering of 3D shapes represented by signed distance functions (SDFs). Compared to other representations, SDFs have the advantage that they can represent shapes with arbitrary topology, and that they guarantee watertight surfaces. We apply our approach to the problem of multi-view 3D reconstruction, where we achieve high reconstruction quality and can capture complex topology of 3D objects. In addition, we employ a multi-resolution strategy to obtain a robust optimization algorithm. We further demonstrate that our SDF-based differentiable renderer can be integrated with deep learning models, which opens up options for learning approaches on 3D objects without 3D supervision. In particular, we apply our method to single-view 3D reconstruction and achieve state-of-the-art results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The "vision as inverse graphics" or "inverse rendering" strategy has long been attractive as a conceptual framework to solve inverse problems such as recovering shape or appearance models from images. In this analysis-by-synthesis approach, the goal is to reproduce given input images by synthesizing them using an image formation model, possibly including shape, appearance, illumination, and camera parameters. Solving this problem implies finding suitable model parameters (shape, appearance, illumination, camera) that describe the underlying scene. While conceptually simple, this approach can be challenging to use in practice, because it requires a suitable parameterization of a powerful image formation model, and effective numerical techniques to solve the resulting optimization problem. Recently, automatic differentiation has attracted renewed attention to implement differentiable renderers or image formation models that can be used in gradient-based optimization techniques. In particular, it is attractive to combine differentiable ren-dering with neural networks to solve highly ill-posed inverse problems, such as single-view 3D reconstruction.</p><p>In this paper, we advocate using signed distance fields (SDFs) in a differentiable image formation model because they have several advantages over other geometry representations. In contrast to triangle meshes, the surface topology is not fixed in SDFs and can adapt to the actual scene topology during optimization. Point clouds can also represent arbitrary topologies but they do not provide continuous surface reconstructions. Instead, SDFs inherently represent continuous, watertight surfaces, which are required for downstream applications such as 3D printing and physicsbased simulation. In addition, SDFs can easily be used in a multi-resolution framework, which is important to avoid undesired local minima during optimization.</p><p>The main contribution of our paper is SDFDiff, a differentiable renderer based on ray-casting SDFs. Our renderer is integrated with a deep learning framework such that it can be combined with neural networks to learn how to solve highly ill-posed inverse problems. Finally, we provide an effective multi-resolution strategy to improve the robustness of gradient-based optimization. We demonstrate the usefulness of our approach using several application studies, including multi-view 3D reconstruction and learning-based single-view 3D reconstruction. Our results demonstrate the advantages of SDFs over other surface representations.</p><p>In summary, we make the following contributions:</p><p>? We introduce a differentiable renderer based on raycasting SDFs, and we describe an implementation that is integrated into a standard deep learning framework. Advantages of using SDFs for differentiable rendering and shape optimization include that we can adapt the topology freely, and that the resulting shapes are guaranteed to consist of watertight surfaces.</p><p>? We present results of a multi-view 3D reconstruction approach using shape optimization via differentiable rendering. Using a multi-resolution approach, our gradient-descent optimization reliably converges to high quality solutions. Our approach is able to reconstruct geometry with high level of detail and complex topology, even with few input views.</p><p>? We leverage the SDF-based differentiable renderer to train deep neural networks to perform single-view 3D shape reconstruction without 3D supervision. We demonstrate the advantages of our approach by recovering accurate 3D shapes with arbitrary topology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Signed Distance Functions. A distance function is a level set representation <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b41">42]</ref> that, at each point in 3D, stores the distance to the closest point on the 3D surface. Signed distance fields (SDFs) <ref type="bibr" target="#b7">[8]</ref> store a signed distance to distinguish between the inside and outside of objects. SDFs are often discretized using uniform voxel grids <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b37">38]</ref>. Compared to meshes or parametric surfaces, the implicit surface representation <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b5">6]</ref> of SDFs has the advantage that it can represent arbitrary topologies. In contrast to point clouds, SDFs always represent watertight surfaces. SDFs recently started attracting interest for shape analysis via deep learning. DeepSDF <ref type="bibr" target="#b40">[41]</ref> was proposed to learn continuous SDF representation of a class of shapes. Similarly, deep level sets <ref type="bibr" target="#b32">[33]</ref> were introduced as an end-to-end trainable model that directly predicts implicit surfaces of arbitrary topology. However, these methods require 3D supervision during training, such as pairs of 3D coordinates and their corresponding SDF values <ref type="bibr" target="#b40">[41]</ref>, or voxel occupancy maps <ref type="bibr" target="#b32">[33]</ref>.</p><p>Learning-based 3D Reconstruction. Reconstructing 3D models from 2D images is a classic problem in computer vision. Compared to traditional multi-view stereo and shading-based approaches <ref type="bibr" target="#b46">[47]</ref>, learning-based 3D reconstruction can achieve impressive performance even with very few input views. Deep learning models for 3D shape understanding have been proposed for different kinds of 3D representations, including multiple views <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, point clouds <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b17">18]</ref>, triangle meshes <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, voxel grids <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b50">51]</ref>, and signed distance functions (SDFs) <ref type="bibr" target="#b40">[41]</ref>. However, most learning-based methods <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23]</ref> require 3D supervision. Although some methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref> do not require supervised learning, they are often limited by specific settings, such as restricted lighting conditions or annotation of object orientation. In contrast, predicting 3D shapes with differentiable renderers <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37]</ref> has recently attracted increasing attention as it enables 3D reconstruction without 3D supervision, that is, by optimizing neural networks only using images as training data.</p><p>Differentiable Rendering Voxel-based differentiable renderers <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b35">36]</ref> first drew attention performing volumetric ray marching, however, they are limited to low-resolution voxel grids. To render SDFs we use sphere tracing, also used in scene representation networks (SRNs) <ref type="bibr" target="#b48">[49]</ref>. SRNs learn how to sphere trace novel views, but cannot produce full 3D shapes in a single step. They do not reconstruct a view independent shape representation and focus more on novel view synthesis rather than 3D watertight surface reconstruction, which is our goal. Starting with Loper and Black's OpenDR <ref type="bibr" target="#b29">[30]</ref>, much recent work focused on mesh-based differentiable renderers <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b39">40]</ref>. Kato et al. <ref type="bibr" target="#b20">[21]</ref> proposed a neural 3D mesh renderer with hand-designed gradients. Paparazzi <ref type="bibr" target="#b26">[27]</ref> employed analytically computed gradients to adjust the location of vertices. Similarly, SoftRas <ref type="bibr" target="#b27">[28]</ref> assigned each pixel to all faces of a mesh in a probabilistic rasterization framework. Although these methods enable to learn 3D mesh reconstruction without 3D supervision, they are restricted to a fixed, usually spherical mesh topology. Many of these mesh-based rendering approaches are differentiable with respect to geometry <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b27">28]</ref>, lighting models <ref type="bibr" target="#b4">[5]</ref>, textures <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b1">2]</ref>, or materials <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b1">2]</ref>. Mesh-based differentiable rendering <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b24">25]</ref> has also been applied to real images, although current results are rather limited and applying differentiable rendering for real photographs remains an open challenge as it requires a comprehensive global illumination model. Differentiable rendering has also been applied to Monte Carlo ray tracing <ref type="bibr" target="#b24">[25]</ref> and point cloud rendering <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34]</ref>. Insafutdinov et al. <ref type="bibr" target="#b17">[18]</ref> proposed a point cloud-based differentiable renderer with visibility modeling by conducting orthogonal projection on voxelized 3D space holding the point cloud. Surface splatting <ref type="bibr" target="#b52">[53]</ref> was employed to model the visibility in point cloud rendering. Although point clouds can be easily acquired using range sensing technology, such as Microsoft Kinect and LIDAR, they do not explicitly represent topology and require post-processing to produce watertight surfaces. Concurrent works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b36">37]</ref> proposed differentiable rendering based on SDFs and on occupancy networks, further improving the quality of 3D reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overview</head><p>We propose a novel approach for image-based 3D shape optimization by leveraging SDFs as the geometric representation to perform differentiable rendering. Given a set of parameters ? representing the geometry description, lighting model, camera position, etc, a renderer R can be written as a forward operator that produces an image I by computing I = R(?). In contrast, optimizing geometry and other scene parameters from images is a backward process. Given a desired target image I, our goal is to get the set of parameters ? = R ?1 (I) that produces the target image. The rendering process itself is not invertible. Hence, instead of solving the inverse rendering problem directly, we can formulate it as an energy minimization problem,</p><formula xml:id="formula_0">? * = arg min ? L img (R(?), I) + L reg (?) (1)</formula><p>where L img is a loss function measuring the distance between the target image and the rendered image from the 3D object. In practice, the loss is typically accumulated over multiple target images. Getting the desired parameters ? * is equivalent to minimizing the loss L. While all rendering parameters including geometry, illumination, camera pose, and surface appearance could in theory be recovered from images this way, we focus on shape optimization in this paper and assume the other parameters are known. To enable gradient-based optimization, a key issue is to obtain the gradient of L img (R(?), I) with respect to the parameters ?. A differentiable renderer achieves this by producing not only images from a description of the scene, but also the derivatives of pixel values with respect to scene parameters. In this paper, we propose a novel differentiable renderer which uses signed distance functions (SDFs) and camera pose as inputs and renders an image. Our SDF-based differentiable renderer leverages the ray casting algorithm and uses automatic differentiation to compute the derivatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Differentiable SDF Rendering</head><p>We represent discrete SDFs by sampling SDF values on regular grids, and apply a standard ray casting algorithm based on sphere tracing <ref type="bibr" target="#b14">[15]</ref> to find the intersection points between rays and the object surface. For this purpose we employ trilinear interpolation to reconstruct continuous SDFs that can be evaluated at any desired location. This allows us to continuously represent the object surface, which is given by the zero level set of the interpolated SDF.</p><p>A key observation is that the derivatives of a given pixel with respect to rendering parameters only depend on a local neighborhood of eight SDF samples that define the value of the trilinearly interpolated SDF at the surface intersection point. In other words, the sphere tracing process itself does not need to be differentiable. Instead, only the local computations involving the local set of eight SDF samples around the surface intersection need to be differentiable. Therefore, our approach proceeds in two stages: first, we apply sphere tracing to identify the eight samples nearest to the surface intersection. This step is not differentiable. Second, we locally compute the pixel color based on the local set of SDF samples. This step is implemented using an automatic differentiation framework to obtain the derivatives.</p><p>While differentiable ray marching of voxel grids has been used before <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b43">44]</ref>, these approaches are based on voxel opacities, given by binary variables or continuous occupancy probabilities. In these cases ray marching through the entire volume needs to be differentiated be-cause all voxels along a ray may influence the corresponding pixel.</p><p>Sphere Tracing. We perform ray casting via sphere tracing <ref type="bibr" target="#b14">[15]</ref> in the first stage by starting from the ray origin, and evaluating the SDF using trilinear interpolation to find the minimum distance from the current point on the ray to the object. Then we move along the ray by that distance. Moving along the ray by the minimum distance to the object guarantees that we will never move across the boundary of the object, while allowing us to make a possibly large step towards the surface. We repeat this process until we reach the surface of the object, that is, until the SDF value at our current position on the ray is small enough, or until we leave the bounding box of the object. While the efficiency of sphere tracing can be improved by increasing the step size <ref type="bibr" target="#b28">[29]</ref>, we implemented sphere tracing directly in CUDA without support for automatic differentiation. Hence, the computation cost of this step is negligible in our approach.</p><p>Differentiable Shading. In the second stage, we compute the pixel color as a function of the local SDF samples that define the SDF at the intersection point, as determined by the first stage. These computations are implemented in a framework that supports automatic differentiation, allowing us to easily obtain the derivatives of the pixel. For each pixel, the input consists of the light and camera parameters, and the eight SDF samples closest to the ray-surface intersection point. The computations include: getting the intersection point and the surface normal at the intersection point as a function of the trilinear basis coefficients (i.e., the eight SDF samples), and evaluating a shading model.</p><p>To take into account the dependence of the pixel value on the ray-surface intersection point, we express the intersection point as a function of the eight local SDF samples. Let us denote the local SDF values by d 0 , . . . , d 7 , the current position on the ray (obtained from the ray casting stage) by s ? R 3 , and the unit ray direction by v ? R 3 . To express the intersection point as a function of d 0 , . . . , d 7 , we use the same approximation as in the ray casting stage, that is, the approximate intersection is p(d 0 , . . . , d 7 ) = s+trilinear(d 0 , . . . , d 7 ; s)v, where trilinear(d 0 , . . . , d 7 ; s) is the trilinear interpolation of the SDF at location s and considered as a function of d 0 , . . . , d 7 . This approximation is conservative in the sense that it is accurate only if the SDF represents a plane that is perpendicular to the ray direction v. Otherwise, p(d 0 , . . . , d 7 ) is guaranteed not to cross the true intersection along the ray.</p><p>As an alternative to our conservative approximation, one could express the intersection point exactly as the solution of the intersection of the ray s + tv and the local trilinear interpolation of the SDF. That is, we could express the solution of trilinear(d 0 , . . . , d 7 ; s + tv) = 0 with respect to t ? R as a function of d 0 , . . . d 7 . However, this involves finding roots of a cubic polynomial, and we found that our much simpler approach works more robustly in practice.</p><p>To evaluate a shading model, we need the surface normal at the intersection point p(d 0 , . . . , d 7 ). Considering that the surface normal corresponds to the gradient of the SDF, we first compute gradients at the grid vertices using central finite differencing, and then trilinearly interpolate them at the intersection point p(d 0 , . . . , d 7 ; s). In summary, this leads to an expression of the normal at the intersection point as a function of SDF coefficients within an 4 ? 4 ? 4 neighborhood around the intersection (because of central finite differencing). Surface normals are normalized after trilinear interpolation. Finally, in our current implementation we evaluate a simple diffuse shading model. Implementation. We implemented SDF ray casting using CUDA to leverage the computational power of GPUs. Differentiable shading is implemented with the Pytorch library, which supports automatic differentiation and allows seamless integration of the renderer with neural network training. Pytorch also leverages the GPU, and our implementation directly accesses the output of the ray casting stage that is stored in GPU memory, avoiding any unnecessary memory transfers. Our code is available at https://github.com/YueJiang-nj/CVPR2020-SDFDiff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Multi-view 3D Reconstruction</head><p>In this section we describe how to perform multi-view 3D reconstruction using our differentiable renderer. This is a proof of concept, where we assume known camera poses, illumination, and surface appearance, and we only optimize over the 3D shape represented by the SDF. Our inputs are synthetically rendered images from a fixed set of camera poses. We set the camera poses to point from the center of each face and edge, and from each vertex of the object bounding box towards its center, where the bounding box is a cube. Since the cube has 6 faces, 8 vertices, and 12 edges, we obtain 26 camera poses in total. <ref type="figure" target="#fig_0">Figure 1</ref> shows the input images we used to reconstruct the bunny in our experiments. In addition, we initialize the SDF to a sphere.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Energy Function</head><p>For simplicity we choose the L 2 distance between the rendered and the target images as our image-based loss, that is L img (R(?), I) = ||R(?) ? I|| 2 . The loss is summed over all target views. In this proof of concept scenario, the optimization parameters ? include only the SDF values, as we assume the other rendering parameters are known. Minimizing the image-based loss by optimizing SDF values requires differentiable rendering, where we compute the gradient of the image loss w.r.t. the SDF values as in Section 4. In addition, we impose a regularization loss that ensures that the SDF values ? represent a valid signed distance function, that is, its gradient should have unit magnitude. Writing the SDF represented by ? as a function f (x; ?), where x is a point in 3D, the regularization loss is</p><formula xml:id="formula_1">L reg = ||1 ? ||?f (x; ?)|| 2 || 2 dx<label>(2)</label></formula><p>In practice, we obtain the gradients via finite differencing and we compute a discrete sum over the SDF grid vertices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Iterative Optimization</head><p>We apply gradient descent optimization using ADAM <ref type="bibr" target="#b21">[22]</ref> to iteratively optimize our SDF to match the target images. Compared to straightforward gradient descent, ADAM is more robust and faster in convergence. In addition, we accelerate convergence by greedily selecting a single view in each gradient descent step to compute the gradient, similar to active mini batch sampling. The intuition is that some parts of the 3D model may have more complex structures so it is more difficult to optimize SDF values using some views than others. Different views may incur image losses of varying magnitude, and we should focus on the views with large losses to make sure all parts of the object can be well-reconstructed. Our approach first calculates the average loss for all the camera views from the result of the previous iteration. If a loss for a view is greater than the average loss, then during the current iteration, we update the SDF until the loss for this view is less than the average (with max. 20 updates). For the other views, we update the SDF five times. If one update increases the loss, then we switch to the next view directly. We stop our optimization process when the loss is smaller than a given tolerance or the step length is too small.</p><p>Reconstructing high-resolution 3D objects is challenging because gradient descent takes many iterations to eliminate low frequency errors. Therefore, we apply a coarse-to-fine multi-resolution approach. We start by initializing the SDF GT Ours Error DSS <ref type="bibr" target="#b52">[53]</ref> Error SMVS <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b47">48]</ref> Error <ref type="figure">Figure 2</ref>. Multi-view reconstruction results comparing to DSS <ref type="bibr" target="#b52">[53]</ref> and SMVS <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b47">48]</ref> with the corresponding error visualizations based on Hausdorff distance (red means zero and blue high error). grid at a resolution of 8 3 to the SDF of a sphere. We then iterate between performing gradient descent optimization as described above, and increasing the grid resolution. We increase the resolution simply by performing trilinear interpolation and stop at a resolution of 64 3 .</p><p>To further improve the efficiency of the multiresolution scheme, we choose an appropriate image resolution for rendering corresponding to the SDF resolution at each resolution level. We determine the appropriate resolution by ensuring that a sphere with a radius equivalent to the grid spacing, and placed at the corner of the bounding box of the SDF furthest from the camera, has a projected footprint of at most the size of a 2 ? 2 pixel block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object</head><p>Ours </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Experimental Results</head><p>Qualitative Results. We compare our results with DSS <ref type="bibr" target="#b52">[53]</ref>, which is a differentiable renderer for point clouds based on surface splatting <ref type="bibr" target="#b56">[57]</ref>. We let both systems deform a sphere to fit the target object given as input. When running DSS, we adopt the same settings used in their original experiments: the system randomly selects 12 from a set of 26 views of the target in each optimization cycle, and optimizes for up to 16 cycles. We experimented with different numbers of 3D points and report the best result. For SDFDiff we use our optimization technique from Section 5.2 using the same set of 26 views. <ref type="figure">Figure 2</ref> shows the comparison between SDFDiff and DSS. DSS cannot recover geometric details as accurately as SDFDiff.</p><p>We also compare our result with SMVS <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b47">48]</ref>, which is a state-of-the-art shading-aware multi-view 3D reconstruction approach. We use the default settings, and provide 1000 randomly sampled views of the dragon rendered by our SDF renderer as input. Note that SMVS automatically recovers camera parameters from the input images and estimates surface albedo and illumination, hence the comparison is not entirely fair. As shown in <ref type="figure">Figure 2</ref>, however, even with a large number of input views the SMVS output can be overly smooth and lack details. SMVS may also fail with fewer views due to inaccurate camera pose estimation. In contrast, SDFDiff can obtain better results using only 26 views (with known camera poses, albedo, and illumination).</p><p>Quantitative Results. <ref type="table" target="#tab_0">Table 1</ref> compares the symmetric Hausdorff distance between ground truth and reconstructed meshes for torus, bunny and dragon. The visual results of torus and bunny are in supplementary materials. For a fair comparison, we report errors relative to the size of the bounding boxes. We observe that SDFDiff leads to smaller symmetric Hausdorff distances, which means our reconstruction results are closer to the ground truth than the other two approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Parameter Study</head><p>Initial Resolution. <ref type="figure">Figure 3</ref> shows the impact of the initial resolution in our multi-resolution scheme. We fix the number of multi-resolution steps and our target resolution being 64, and then set the initial resolution to be 8, 16, 32, and 48 respectively. We find that a lower initial resolu-init res=8 init res=16 init res=32 init res=48 <ref type="figure">Figure 3</ref>. Given different initial resolutions, with 4 resolution stages, we can find that our 3D reconstruction results are better if the initial resolution is lower.</p><p>1 step 4 steps 8 steps 15 steps <ref type="figure">Figure 4</ref>. We fix the initial and target resolution to 8 and 64 respectively, but use different numbers of intermediate resolution stages. We find that more resolution stages can give us better results.</p><p>tion can reconstruct qualitatively better 3D shapes because it more robustly captures large scale structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Multi-Resolution</head><p>Steps. <ref type="figure">Figure 4</ref> shows that given fixed initial (init res=8) and target (target res=64) resolutions, adding more multi-resolution steps can give us better results. In particular, single-resolution optimization (1 step) cannot reconstruct the object successfully, further justifying our multi-resolution setup.</p><p>Image Resolution. <ref type="figure">Figure 5</ref> shows that image resolution does not significantly affect the quality of the results, where we use images with various resolutions for optimization.</p><p>Noisy Data. As shown in <ref type="figure" target="#fig_1">Figure 6</ref>, when some noise is added to target images or camera poses, our approach can still maintain its robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Learning-based Single-view Reconstruction</head><p>In the following experiments, we leverage SDFDiff to train a neural network to perform single-view 3D reconstruction without 3D supervision. We use the same dataset 64x64 128x128 256x256 512x512 <ref type="figure">Figure 5</ref>. We show that the quality of reconstruction results are not affected much by the image resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Noisy views Results</head><p>Perturbed camera poses Results as <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28]</ref>, which includes 13 categories of objects from ShapeNet <ref type="bibr" target="#b2">[3]</ref>. Each object has 24 rendered images from different views at 64 ? 64 resolution. We use the same train/validate/test sets on the same dataset as in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b54">55]</ref>, and the standard reconstruction metric, i.e., 3D intersection over union (IoU) <ref type="bibr" target="#b27">[28]</ref> for quantitative comparisons. Network. Our network contains two parts as shown in <ref type="figure" target="#fig_2">Figure 7</ref>. The first part is an Encoder-Decoder network which takes images as input and outputs coarse SDF results. The second part is a refiner network to further improve the quality of the 3D reconstruction results. The network is trained on all the 3D shapes in the dataset simultaneously.</p><p>Loss Function. In addition to the energy function as shown in Section 5.1 containing the L 2 image-based loss L img and the SDF loss L reg ensuring the SDF values represent a valid signed distance function, we also add a geometry loss L geo that regularizes the finite difference Laplacian of the predicted SDFs to obtain smooth outputs. Furthermore, we use a narrow band technique to control the effects of the SDF and Laplacian losses since we care more about these losses locally around the surfaces. Also, the SDFloss cannot be enforced everywhere on the discrete grid due to singularities (e.g., the medial axis of the shape forms a sharp crease) in the continuous SDF. The narrow-band considers the SDF-loss only close to the surface, avoiding SDF discretization issues elsewhere in the volume. We use   a distance-based binary mask M to zero them out further away from the zero level-set. The mask is defined as</p><formula xml:id="formula_2">M = ||SDF || ? ? ? voxelSize,<label>(3)</label></formula><p>where ? is a hyperparameter to define the width of the narrow band. We currently set it to be 1.6, which is determined experimentally. The final loss function is a weighted sum of the three losses with weights ? 1 = ? 2 = 0.02,</p><formula xml:id="formula_3">L = L img + M ? (? 1 L reg + ? 2 L geo ).<label>(4)</label></formula><p>Training Process. We first train the Encoder-Decoder part of the network alone based on the three loss terms. Then we fix the encoder and decoder and train the refiner network on the same three loss terms to get refined SDF shapes. In the end, we train all the three parts, i.e., encoder, decoder, and refiner together to further improve the results. We do not use the multi-resolution approach.</p><p>Qualitative Evaluation. <ref type="figure">Figure 8</ref> shows that our method can reconstruct detailed objects and accurately recover complicated topologies. In contrast, SoftRasterizer <ref type="bibr" target="#b27">[28]</ref> relies on a template mesh with spherical topology and it cannot capture the complex topology of the chairs.</p><p>Quantitative Evaluation. We compare our method with the state-of-the-art <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28]</ref> in terms of 3D IoU scores in <ref type="table" target="#tab_2">Table 2</ref>. Our method can reconstruct shapes with finer details in the 13 categories. In addition, the IoU numbers show that our results achieve higher accuracy, where our scores surpass other approaches in most of the categories. A comparison to Chen et al. <ref type="bibr" target="#b4">[5]</ref> is omitted because they use different data preprocessing than the other methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion and Limitations</head><p>As a main advantage, SDFs can represent arbitrary topologies, in contrast to triangle meshes that are restricted to the topology of a template. In contrast to point clouds, SDFs inherently represent continuous watertight surfaces. We demonstrated applications of our approach in multiview shape reconstruction, and single view 3D reconstruction using deep learning. Our experimental results showed that we can more robustly perform multi-view reconstruction than a state-of-the-art point-based differentiable renderer. In addition, we achieve state-of-the-art results on single view 3D reconstruction with deep learning models.</p><p>In our multi-view 3D reconstruction approach, our current shading model is not sufficient to perform inverse rendering from real images taken with a camera. For example, we currently do not include effects such as shadows, interreflections, texture, non-diffuse surfaces, or complex illumination. In contrast to rasterization-based differentiable renderers, our ray tracing-based renderer could be extended to include all such effects. A disadvantage of our deep learning approach is that we output a discrete SDF on a 3D grid. Instead, we could learn a continuous signed distance function represented by a deep network like in DeepSDF <ref type="bibr" target="#b40">[41]</ref>. This would be more memory efficient, but it might be computationally too expensive for unsupervised 3D reconstruction with differentiable rendering, since it would require to evaluate the network for each ray marching step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We proposed a novel approach to differentiable rendering using signed distance functions to represent watertight 3D geometry. Our rendering algorithm is based on sphere tracing, but we observe that only the local shading computation needs to be differentiable in our framework, which makes the approach computationally more efficient and allows for straightforward integration into deep learning frameworks. We demonstrate applications in multi-view 3D reconstruction and unsupervised single-view 3D recon-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image</head><p>Rendered Views Input Image Rendered Views</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>SoftRas <ref type="bibr" target="#b27">[28]</ref> GT Ours SoftRas <ref type="bibr" target="#b27">[28]</ref> GT Ours SoftRas <ref type="bibr" target="#b27">[28]</ref> GT Ours SoftRas <ref type="bibr" target="#b27">[28]</ref> GT Ours SoftRas[28] <ref type="figure">Figure 8</ref>. Single-view reconstruction results for airplanes, chairs, and benches. struction using deep neural networks. Our experimental results illustrate the advantages over geometry representations such as point clouds and meshes. In particular, we report the state-of-the-art results in shape reconstruction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>We use 26 input views in our multi-view reconstruction experiments as shown here for the bunny.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 6 .</head><label>6</label><figDesc>Experimental results with noisy data. All 26 input views or camera poses are perturbed with Gaussian noise (variance=0.03 for views and variance=0.01 for camera poses). The third column shows the view differences caused by perturbed camera poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 .</head><label>7</label><figDesc>Network structure for single-view SDF reconstruction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of the symmetric Hausdorff distance between ground truth and reconstructed meshes for torus, bunny and dragon. SMVS could not reconstruct torus and bunny because camera pose estimation failed.</figDesc><table><row><cell></cell><cell cols="2">DSS [53] SMVS [24, 48]</cell></row><row><cell>Torus</cell><cell>0.015637 0.035398</cell><cell>N/A</cell></row><row><cell cols="2">Bunny 0.026654 0.109432</cell><cell>N/A</cell></row><row><cell cols="2">Dragon 0.074723 0.179456</cell><cell>0.097816</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>.7143 0.7095 0.4990 0.5831 0.4126 0.6536 0.6322 0.6735 0.4829 0.7777 0.5645 0.6015 SoftRas (sil.)<ref type="bibr" target="#b27">[28]</ref> 0.6419 0.5080 0.7116 0.7697 0.5270 0.6156 0.4628 0.6654 0.6811 0.6878 0.4487 0.7895 0.5953 0.6234 SoftRas (full)<ref type="bibr" target="#b27">[28]</ref> 0.6670 0.5429 0.7382 0.7876 0.5470 0.6298 0.4580 0.6807 0.6702 0.7220 0.5325 0.8127 0.6145 0.6464 Ours 0.6874 0.6860 0.7735 0.8002 0.6436 0.6584 0.5150 0.6534 0.5553 0.7654 0.6288 0.8278 0.6244 0.6674</figDesc><table><row><cell>Category</cell><cell cols="2">Airplane Bench Cabinet</cell><cell>Car</cell><cell>Chair</cell><cell>Display Lamp Speaker</cell><cell>Rifle</cell><cell>Sofa</cell><cell>Table</cell><cell>Phone Vessel</cell><cell>Mean</cell></row><row><cell>NMR [21]</cell><cell>0.6172</cell><cell>0.4998 0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison of IoU with the state-of-the-art approaches [21, 28] on 13 ShapeNet categories.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Acknowledgements</head><p>This work builds on initial explorations of multi-view 3D reconstruction with differentiable rendering <ref type="bibr" target="#b43">[44]</ref> by Simone Raimondi and the last author. This project was supported by NSF IIS grant nr. #1813583. We also appreciate active discussion with Qian Zheng, Hui Huang, and Daniel Cohen-Or at Shenzhen University.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Representation learning and adversarial generation of 3D point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panos</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Diamanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno>abs/1707.02392</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Synthesizing robust adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Kwok</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsm?ssan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi-Xing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/1512.03012</idno>
	</analytic>
	<monogr>
		<title level="m">ShapeNet: An information-rich 3D model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqian</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fujun</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10820</idno>
		<title level="m">Kavita Bala, and Ioannis Gkioulekas. Inverse Transport Networks. arXiv eprints</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to predict 3d objects with an interpolation-based differentiable renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="9609" to="9619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning implicit fields for generative shape modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3D-R2N2: A unified approach for single and multi-view 3D object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A volumetric method for building complex models from range images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd annual conference on Computer graphics and interactive techniques -SIGGRAPH &apos;96</title>
		<meeting>the 23rd annual conference on Computer graphics and interactive techniques -SIGGRAPH &apos;96</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gradient flows for optimizing triangular mesh-based surfaces: Applications to 3D reconstruction problems dealing with visibility</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ama?l</forename><surname>Delaunoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Prados</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="100" to="123" />
			<date type="published" when="2011-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A point set generation network for 3D object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3D shape induction from 2D views of multiple objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matheus</forename><surname>Gadelha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="402" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Weakly supervised 3D reconstruction with adversarial constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">View Inter-Prediction GAN: Unsupervised representation learning for 3D shapes by learning global shape memories to support local view predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Shen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8376" to="8384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Y2Seq2Seq: Cross-modal representation learning for 3D shape and text by joint reconstruction and prediction of view and word sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Shen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="126" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Sphere Tracing: a geometric method for the antialiased ray tracing of implicit surfaces. The Visual Computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Hart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996-12" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="527" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to generate and reconstruct 3D meshes with only 2D supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th British Machine Vision Conference (BMVC 2018)</title>
		<meeting>the 29th British Machine Vision Conference (BMVC 2018)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Escaping Plato&apos;s Cave using adversarial training: 3d shape from unstructured 2d image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Henzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision 2019 (ICCV 2019)</title>
		<meeting>the International Conference on Computer Vision 2019 (ICCV 2019)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised learning of shape and pose with differentiable point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2807" to="2817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">GAL: Geometric adversarial loss for single-view 3D-object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised learning of 3D structure from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Ali Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4996" to="5004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural 3D mesh renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroharu</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">DeformNet: Free-form deformation network for 3D shape reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurenkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018-03" />
			<biblScope unit="page" from="858" to="866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Shading-aware multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Langguth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hadap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goesele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Differentiable monte carlo ray tracing through edge sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tzu-Mao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?do</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno>222:1-222:11</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH Asia)</title>
		<meeting>SIGGRAPH Asia)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adversarial geometry and lighting using a differentiable renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsueh-Ti Derek</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Nowrouzezahrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Jacobson</surname></persName>
		</author>
		<idno>abs/1808.02651</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Paparazzi: Surface editing by way of multi-view image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsueh-Ti Derek</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Jacobson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Soft Rasterizer: A differentiable renderer for image-based 3D reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weikai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">DIST: Rendering deep implicit signed distance function with differentiable sphere tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyou</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">OpenDR: An approximate differentiable renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">LIME: Live intrinsic material estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhimitra</forename><surname>Meka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Maximov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhoefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avishek</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Occupancy networks: Learning 3D reconstruction in function space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep Level Sets: Implicit surface representations for 3D shape inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Michalkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jhony</forename><forename type="middle">K</forename><surname>Pontes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominic</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahsa</forename><surname>Baktashmotlagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><forename type="middle">P</forename><surname>Eriksson</surname></persName>
		</author>
		<idno>abs/1901.06802</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">CAPNet: Continuous approximation projection for 3D point cloud reconstruction using 2d supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyanka</forename><surname>Kl Navaneet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Mandikal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Venkatesh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8819" to="8826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">KinectFusion: Real-time dense surface mapping and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Molyneaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th IEEE International Symposium on Mixed and Augmented Reality</title>
		<meeting><address><addrLine>Jamie Shotton, Steve Hodges, and Andrew Fitzgibbon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Pushmeet Kohli</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">RenderNet: A deep convolutional network for differentiable rendering from 3D shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thu</forename><surname>Nguyen-Phuoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Balaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Liang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Differentiable volumetric rendering: Learning implicit 3d representations without 3d supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Real-time 3D reconstruction at scale using voxel hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Stamminger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fedkiw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Piechor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Level Set Methods and Dynamic Implicit Surfaces. Applied Mechanics Reviews</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">End-to-end 6-DoF object pose estimation through differentiable rasterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Palazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bergamini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV) Workshops</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeong Joon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Lovegrove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepsdf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.05103</idno>
		<title level="m">Learning Continuous Signed Distance Functions for Shape Representation. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A PDE-Based Fast Local Level Set Method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danping</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Merriman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myungjoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Pix2Vex: Image-to-geometry reconstruction using a smooth differentiable renderer. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Bermano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Deussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen-Or</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Refining multi-view 3d reconstruction with differentiable rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Raimondi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A signal-processing framework for inverse rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;01</title>
		<meeting>the 28th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;01<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning detailed face reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Or-El</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="5553" to="5562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A comparison and evaluation of multi-view stereo reconstruction algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Diebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<date type="published" when="2006-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="519" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A new variational framework for multiview surface reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Semerjian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Wetzstein</surname></persName>
		</author>
		<title level="m">Scene Representation Networks: Continuous 3D-structure-aware neural scene representations. CoRR, abs</title>
		<imprint>
			<date type="published" when="1618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning category-specific deformable 3D models for object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="719" to="731" />
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multi-view supervision for single-view reconstruction via differentiable ray consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="209" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">First experiments with neural translation of informal to formal mathematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingxiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cezary</forename><surname>Kaliszyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Urban</surname></persName>
		</author>
		<idno>abs/1805.06502</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Differentiable surface splatting for point-based geometry processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felice</forename><surname>Serena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cengiz?ztireli</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (proceedings of ACM SIGGRAPH ASIA)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">MarrNet: 3D Shape Reconstruction via 2.5D Sketches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Perspective Transformer Nets: Learning single-view 3D object reconstruction without 3D supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1696" to="1704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Visual Object Networks: Image generation with disentangled 3D representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoutong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS 2018)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Surface splatting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Zwicker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeroen</forename><surname>Van Baar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;01</title>
		<meeting>the 28th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;01<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="371" to="378" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
