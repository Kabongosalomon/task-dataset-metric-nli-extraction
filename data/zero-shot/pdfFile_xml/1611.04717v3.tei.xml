<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><surname>Foote</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering and Computer Sciences</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Stooke</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering and Computer Sciences</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering and Computer Sciences</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering and Computer Sciences</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>De Turck</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Information Technology 4 OpenAI</orgName>
								<orgName type="institution">Ghent University -imec</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering and Computer Sciences</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Count-based exploration algorithms are known to perform near-optimally when used in conjunction with tabular reinforcement learning (RL) methods for solving small discrete Markov decision processes (MDPs). It is generally thought that count-based methods cannot be applied in high-dimensional state spaces, since most states will only occur once. Recent deep RL exploration strategies are able to deal with high-dimensional continuous state spaces through complex heuristics, often relying on optimism in the face of uncertainty or intrinsic motivation. In this work, we describe a surprising finding: a simple generalization of the classic count-based approach can reach near state-of-the-art performance on various highdimensional and/or continuous deep RL benchmarks. States are mapped to hash codes, which allows to count their occurrences with a hash table. These counts are then used to compute a reward bonus according to the classic count-based exploration theory. We find that simple hash functions can achieve surprisingly good results on many challenging tasks. Furthermore, we show that a domaindependent learned hash code may further improve these results. Detailed analysis reveals important aspects of a good hash function: 1) having appropriate granularity and 2) encoding information relevant to solving the MDP. This exploration strategy achieves near state-of-the-art performance on both continuous control tasks and Atari 2600 games, hence providing a simple yet powerful baseline for solving MDPs that require considerable exploration.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Reinforcement learning (RL) studies an agent acting in an initially unknown environment, learning through trial and error to maximize rewards. It is impossible for the agent to act near-optimally until it has sufficiently explored the environment and identified all of the opportunities for high reward, in all scenarios. A core challenge in RL is how to balance exploration-actively seeking out novel states and actions that might yield high rewards and lead to long-term gains; and exploitation-maximizing short-term rewards using the agent's current knowledge. While there are exploration techniques for finite MDPs that enjoy theoretical guarantees, there are no fully satisfying techniques for highdimensional state spaces; therefore, developing more general and robust exploration techniques is an active area of research.</p><p>Most of the recent state-of-the-art RL results have been obtained using simple exploration strategies such as uniform sampling <ref type="bibr" target="#b20">[21]</ref> and i.i.d./correlated Gaussian noise <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">30]</ref>. Although these heuristics are sufficient in tasks with well-shaped rewards, the sample complexity can grow exponentially (with state space size) in tasks with sparse rewards <ref type="bibr" target="#b24">[25]</ref>. Recently developed exploration strategies for deep RL have led to significantly improved performance on environments with sparse rewards. Bootstrapped DQN <ref type="bibr" target="#b23">[24]</ref> led to faster learning in a range of Atari 2600 games by training an ensemble of Q-functions. Intrinsic motivation methods using pseudo-counts achieve state-of-the-art performance on Montezuma's Revenge, an extremely challenging Atari 2600 game <ref type="bibr" target="#b3">[4]</ref>. Variational Information Maximizing Exploration (VIME, <ref type="bibr" target="#b12">[13]</ref>) encourages the agent to explore by acquiring information about environment dynamics, and performs well on various robotic locomotion problems with sparse rewards. However, we have not seen a very simple and fast method that can work across different domains.</p><p>Some of the classic, theoretically-justified exploration methods are based on counting state-action visitations, and turning this count into a bonus reward. In the bandit setting, the well-known UCB algorithm of <ref type="bibr" target="#b17">[18]</ref> chooses the action a t at time t that maximizesr(a t ) + 2 log t n(at) wherer(a t ) is the estimated reward, and n(a t ) is the number of times action a t was previously chosen. In the MDP setting, some of the algorithms have similar structure, for example, Model Based Interval Estimation-Exploration Bonus (MBIE-EB) of <ref type="bibr" target="#b33">[34]</ref> counts state-action pairs with a table n(s, a) and adding a bonus reward of the form ? ? n(s,a)</p><p>to encourage exploring less visited pairs. <ref type="bibr" target="#b15">[16]</ref> show that the inverse-square-root dependence is optimal. MBIE and related algorithms assume that the augmented MDP is solved analytically at each timestep, which is only practical for small finite state spaces.</p><p>This paper presents a simple approach for exploration, which extends classic counting-based methods to high-dimensional, continuous state spaces. We discretize the state space with a hash function and apply a bonus based on the state-visitation count. The hash function can be chosen to appropriately balance generalization across states, and distinguishing between states. We select problems from rllab <ref type="bibr" target="#b7">[8]</ref> and Atari 2600 <ref type="bibr" target="#b2">[3]</ref> featuring sparse rewards, and demonstrate near state-of-the-art performance on several games known to be hard for na?ve exploration strategies. The main strength of the presented approach is that it is fast, flexible and complementary to most existing RL algorithms.</p><p>In summary, this paper proposes a generalization of classic count-based exploration to highdimensional spaces through hashing (Section 2); demonstrates its effectiveness on challenging deep RL benchmark problems and analyzes key components of well-designed hash functions (Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notation</head><p>This paper assumes a finite-horizon discounted Markov decision process (MDP), defined by (S, A, P, r, ? 0 , ?, T ), in which S is the state space, A the action space, P a transition probability distribution, r : S ? A ? R a reward function, ? 0 an initial state distribution, ? ? (0, 1] a discount factor, and T the horizon. The goal of RL is to maximize the total expected discounted reward E ?,P T t=0 ? t r(s t , a t ) over a policy ?, which outputs a distribution over actions given a state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Count-Based Exploration via Static Hashing</head><p>Our approach discretizes the state space with a hash function ? : S ? Z. An exploration bonus r + : S ? R is added to the reward function, defined as</p><formula xml:id="formula_0">r + (s) = ? n(?(s)) ,<label>(1)</label></formula><p>where ? ? R ?0 is the bonus coefficient. Initially the counts n(?) are set to zero for the whole range of ?. For every state s t encountered at time step t, n(?(s t )) is increased by one. The agent is trained with rewards (r + r + ), while performance is evaluated as the sum of rewards without bonuses. Compute hash codes through any LSH method, e.g., for SimHash, ?(s m ) = sgn(Ag(s m )) <ref type="bibr" target="#b6">7</ref> Update the hash table counts ?m : 0 ? m ? M as n(?(s m )) ? n(?(s m )) + 1 <ref type="bibr" target="#b7">8</ref> Update the policy ? using rewards r(s m , a m ) + ? ? n(?(sm)) M m=0 with any RL algorithm Note that our approach is a departure from count-based exploration methods such as MBIE-EB since we use a state-space count n(s) rather than a state-action count n(s, a). State-action counts n(s, a) are investigated in the Supplementary Material, but no significant performance gains over state counting could be witnessed. A possible reason is that the policy itself is sufficiently random to try most actions at a novel state.</p><p>Clearly the performance of this method will strongly depend on the choice of hash function ?. One important choice we can make regards the granularity of the discretization: we would like for "distant" states to be be counted separately while "similar" states are merged. If desired, we can incorporate prior knowledge into the choice of ?, if there would be a set of salient state features which are known to be relevant. A short discussion on this matter is given in the Supplementary Material. Algorithm 1 summarizes our method. The main idea is to use locality-sensitive hashing (LSH) to convert continuous, high-dimensional data to discrete hash codes. LSH is a popular class of hash functions for querying nearest neighbors based on certain similarity metrics <ref type="bibr" target="#b1">[2]</ref>. A computationally efficient type of LSH is SimHash <ref type="bibr" target="#b5">[6]</ref>, which measures similarity by angular distance. SimHash retrieves a binary code of state s ? S as</p><formula xml:id="formula_1">?(s) = sgn(Ag(s)) ? {?1, 1} k ,<label>(2)</label></formula><p>where g : S ? R D is an optional preprocessing function and A is a k ? D matrix with i.i.d. entries drawn from a standard Gaussian distribution N (0, 1). The value for k controls the granularity: higher values lead to fewer collisions and are thus more likely to distinguish states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Count-Based Exploration via Learned Hashing</head><p>When the MDP states have a complex structure, as is the case with image observations, measuring their similarity directly in pixel space fails to provide the semantic similarity measure one would desire. Previous work in computer vision <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b35">36]</ref> introduce manually designed feature representations of images that are suitable for semantic tasks including detection and classification. More recent methods learn complex features directly from data by training convolutional neural networks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b30">31]</ref>. Considering these results, it may be difficult for a method such as SimHash to cluster states appropriately using only raw pixels.</p><p>Therefore, rather than using SimHash, we propose to use an autoencoder (AE) to learn meaningful hash codes in one of its hidden layers as a more advanced LSH method. This AE takes as input states s and contains one special dense layer comprised of D sigmoid functions. By rounding the sigmoid activations b(s) of this layer to their closest binary number b(s) ? {0, 1} D , any state s can be binarized. This is illustrated in <ref type="figure" target="#fig_9">Figure 1</ref> for a convolutional AE.</p><p>A problem with this architecture is that dissimilar inputs s i , s j can map to identical hash codes b(s i ) = b(s j ) , but the AE still reconstructs them perfectly. For example, if b(s i ) and b(s j ) have values 0.6 and 0.7 at a particular dimension, the difference can be exploited by deconvolutional layers in order to reconstruct s i and s j perfectly, although that dimension rounds to the same binary value. One can imagine replacing the bottleneck layer b(s) with the hash codes b(s) , but then gradients cannot be back-propagated through the rounding function. A solution is proposed by Gregor et al. <ref type="bibr" target="#b9">[10]</ref> and Salakhutdinov &amp; Hinton <ref type="bibr" target="#b27">[28]</ref> is to inject uniform noise U (?a, a) into the sigmoid  with any RL algorithm activations. By choosing uniform noise with a &gt; 1 4 , the AE is only capable of (always) reconstructing distinct state inputs s i = s j , if it has learned to spread the sigmoid outputs sufficiently far apart, |b(s i ) ? b(s j )| &gt; , in order to counteract the injected noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As such, the loss function over a set of collected states {s</head><formula xml:id="formula_2">i } N i=1 is defined as L {s n } N n=1 = ? 1 N N n=1 log p(s n ) ? ? K D i=1 min (1 ? b i (s n )) 2 , b i (s n ) 2 ,<label>(3)</label></formula><p>with p(s n ) the AE output. This objective function consists of a negative log-likelihood term and a term that pressures the binary code layer to take on binary values, scaled by ? ? R ?0 . The reasoning behind this latter term is that it might happen that for particular states, a certain sigmoid unit is never used. Therefore, its value might fluctuate around 1 2 , causing the corresponding bit in binary code b(s) to flip over the agent lifetime. Adding this second loss term ensures that an unused bit takes on an arbitrary binary value.</p><p>For Atari 2600 image inputs, since the pixel intensities are discrete values in the range [0, 255], we make use of a pixel-wise softmax output layer <ref type="bibr" target="#b36">[37]</ref> that shares weights between all pixels. The architectural details are described in the Supplementary Material and are depicted in <ref type="figure" target="#fig_9">Figure 1</ref>. Because the code dimension often needs to be large in order to correctly reconstruct the input, we apply a downsampling procedure to the resulting binary code b(s) , which can be done through random projection to a lower-dimensional space via SimHash as in Eq. (2).</p><p>On the one hand, it is important that the mapping from state to code needs to remain relatively consistent over time, which is nontrivial as the AE is constantly updated according to the latest data (Algorithm 2 line 8). A solution is to downsample the binary code to a very low dimension, or by slowing down the training process. On the other hand, the code has to remain relatively unique for states that are both distinct and close together on the image manifold. This is tackled both by the second term in Eq. (3) and by the saturating behavior of the sigmoid units. States already well represented by the AE tend to saturate the sigmoid activations, causing the resulting loss gradients to be close to zero, making the code less prone to change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>Classic count-based methods such as MBIE <ref type="bibr" target="#b32">[33]</ref>, MBIE-EB and <ref type="bibr" target="#b15">[16]</ref> solve an approximate Bellman equation as an inner loop before the agent takes an action <ref type="bibr" target="#b33">[34]</ref>. As such, bonus rewards are propagated immediately throughout the state-action space. In contrast, contemporary deep RL algorithms propagate the bonus signal based on rollouts collected from interacting with environments, with value-based <ref type="bibr" target="#b20">[21]</ref> or policy gradient-based <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30]</ref> methods, at limited speed. In addition, our proposed method is intended to work with contemporary deep RL algorithms, it differs from classical count-based method in that our method relies on visiting unseen states first, before the bonus reward can be assigned, making uninformed exploration strategies still a necessity at the beginning. Filling the gaps between our method and classic theories is an important direction of future research.</p><p>A related line of classical exploration methods is based on the idea of optimism in the face of uncertainty <ref type="bibr" target="#b4">[5]</ref> but not restricted to using counting to implement "optimism", e.g., R-Max <ref type="bibr" target="#b4">[5]</ref>, UCRL <ref type="bibr" target="#b13">[14]</ref>, and E 3 <ref type="bibr" target="#b14">[15]</ref>. These methods, similar to MBIE and MBIE-EB, have theoretical guarantees in tabular settings.</p><p>Bayesian RL methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b34">35]</ref>, which keep track of a distribution over MDPs, are an alternative to optimism-based methods. Extensions to continuous state space have been proposed by <ref type="bibr" target="#b26">[27]</ref> and <ref type="bibr" target="#b24">[25]</ref>.</p><p>Another type of exploration is curiosity-based exploration. These methods try to capture the agent's surprise about transition dynamics. As the agent tries to optimize for surprise, it naturally discovers novel states. We refer the reader to <ref type="bibr" target="#b28">[29]</ref> and <ref type="bibr" target="#b25">[26]</ref> for an extensive review on curiosity and intrinsic rewards.</p><p>Several exploration strategies for deep RL have been proposed to handle high-dimensional state space recently. <ref type="bibr" target="#b12">[13]</ref> propose VIME, in which information gain is measured in Bayesian neural networks modeling the MDP dynamics, which is used an exploration bonus. <ref type="bibr" target="#b31">[32]</ref> propose to use the prediction error of a learned dynamics model as an exploration bonus. Thompson sampling through bootstrapping is proposed by <ref type="bibr" target="#b23">[24]</ref>, using bootstrapped Q-functions.</p><p>The most related exploration strategy is proposed by <ref type="bibr" target="#b3">[4]</ref>, in which an exploration bonus is added inversely proportional to the square root of a pseudo-count quantity. A state pseudo-count is derived from its log-probability improvement according to a density model over the state space, which in the limit converges to the empirical count. Our method is similar to pseudo-count approach in the sense that both methods are performing approximate counting to have the necessary generalization over unseen states. The difference is that a density model has to be designed and learned to achieve good generalization for pseudo-count whereas in our case generalization is obtained by a wide range of simple hash functions (not necessarily SimHash). Another interesting connection is that our method also implies a density model ?(s) = n(?(s)) N over all visited states, where N is the total number of states visited. Another method similar to hashing is proposed by <ref type="bibr" target="#b0">[1]</ref>, which clusters states and counts cluster centers instead of the true states, but this method has yet to be tested on standard exploration benchmark problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Experiments were designed to investigate and answer the following research questions:</p><p>1. Can count-based exploration through hashing improve performance significantly across different domains? How does the proposed method compare to the current state of the art in exploration for deep RL?</p><p>2. What is the impact of learned or static state preprocessing on the overall performance when image observations are used?</p><p>To answer question 1, we run the proposed method on deep RL benchmarks (rllab and ALE) that feature sparse rewards, and compare it to other state-of-the-art algorithms. Question 2 is answered by trying out different image preprocessors on Atari 2600 games. Trust Region Policy Optimization (TRPO, <ref type="bibr" target="#b29">[30]</ref>) is chosen as the RL algorithm for all experiments, because it can handle both discrete and continuous action spaces, can conveniently ensure stable improvement in the policy performance, and is relatively insensitive to hyperparameter changes. The hyperparameters settings are reported in the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Continuous Control</head><p>The rllab benchmark <ref type="bibr" target="#b7">[8]</ref> consists of various control tasks to test deep RL algorithms. We selected several variants of the basic and locomotion tasks that use sparse rewards, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>, and adopt the experimental setup as defined in <ref type="bibr" target="#b12">[13]</ref>-a description can be found in the Supplementary Material. These tasks are all highly difficult to solve with na?ve exploration strategies, such as adding Gaussian noise to the actions.   <ref type="figure" target="#fig_2">Figure 3</ref> shows the results of TRPO (baseline), TRPO-SimHash, and VIME <ref type="bibr" target="#b12">[13]</ref> on the classic tasks MountainCar and CartPoleSwingup, the locomotion task HalfCheetah, and the hierarchical task SwimmerGather. Using count-based exploration with hashing is capable of reaching the goal in all environments (which corresponds to a nonzero return), while baseline TRPO with Gaussia n control noise fails completely. Although TRPO-SimHash picks up the sparse reward on HalfCheetah, it does not perform as well as VIME. In contrast, the performance of SimHash is comparable with VIME on MountainCar, while it outperforms VIME on SwimmerGather.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Arcade Learning Environment</head><p>The Arcade Learning Environment (ALE, <ref type="bibr" target="#b2">[3]</ref>), which consists of Atari 2600 video games, is an important benchmark for deep RL due to its high-dimensional state space and wide variety of games. In order to demonstrate the effectiveness of the proposed exploration strategy, six games are selected featuring long horizons while requiring significant exploration: Freeway, Frostbite, Gravitar, Montezuma's Revenge, Solaris, and Venture. The agent is trained for 500 iterations in all experiments, with each iteration consisting of 0.1 M steps (the TRPO batch size, corresponds to 0.4 M frames).</p><p>Policies and value functions are neural networks with identical architectures to <ref type="bibr" target="#b21">[22]</ref>. Although the policy and baseline take into account the previous four frames, the counting algorithm only looks at the latest frame. BASS To compare with the autoencoder-based learned hash code, we propose using Basic Abstraction of the ScreenShots (BASS, also called Basic; see <ref type="bibr" target="#b2">[3]</ref>) as a static preprocessing function g. BASS is a hand-designed feature transformation for images in Atari 2600 games. BASS builds on the following observations specific to Atari: 1) the game screen has a low resolution, 2) most objects are large and monochrome, and 3) winning depends mostly on knowing object locations and motions. We designed an adapted version of BASS 3 , that divides the RGB screen into square cells, computes the average intensity of each color channel inside a cell, and assigns the resulting values to bins that uniformly partition the intensity range </p><p>Afterwards, the resulting integer-valued feature tensor is converted to an integer hash code (?(s t ) in Line 6 of Algorithm 1). A BASS feature can be regarded as a miniature that efficiently encodes object locations, but remains invariant to negligible object motions. It is easy to implement and introduces little computation overhead. However, it is designed for generic Atari game images and may not capture the structure of each specific game very well.</p><p>We compare our results to double DQN <ref type="bibr" target="#b38">[39]</ref>, dueling network <ref type="bibr" target="#b39">[40]</ref>, A3C+ <ref type="bibr" target="#b3">[4]</ref>, double DQN with pseudo-counts <ref type="bibr" target="#b3">[4]</ref>, Gorila <ref type="bibr" target="#b22">[23]</ref>, and DQN Pop-Art <ref type="bibr" target="#b37">[38]</ref> on the "null op" metric 4 . We show training curves in <ref type="figure" target="#fig_4">Figure 4</ref> and summarize all results in <ref type="table" target="#tab_0">Table 1</ref>. Surprisingly, TRPO-pixel-SimHash already outperforms the baseline by a large margin and beats the previous best result on Frostbite. TRPO-BASS-SimHash achieves significant improvement over TRPO-pixel-SimHash on Montezuma's Revenge and Venture, where it captures object locations better than other methods. <ref type="bibr" target="#b4">5</ref> TRPO-AE-SimHash achieves near state-of-the-art performance on Freeway, Frostbite and Solaris.</p><p>As observed in <ref type="table" target="#tab_0">Table 1</ref>, preprocessing images with BASS or using a learned hash code through the AE leads to much better performance on Gravitar, Montezuma's Revenge and Venture. Therefore, a static or adaptive preprocessing step can be important for a good hash function.</p><p>In conclusion, our count-based exploration method is able to achieve remarkable performance gains even with simple hash functions like SimHash on the raw pixel space. If coupled with domaindependent state preprocessing techniques, it can sometimes achieve far better results.</p><p>A reason why our proposed method does not achieve state-of-the-art performance on all games is that TRPO does not reuse off-policy experience, in contrast to DQN-based algorithms <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b37">38]</ref>), and is hence less efficient in harnessing extremely sparse rewards. This explanation is corroborated by the experiments done in <ref type="bibr" target="#b3">[4]</ref>, in which A3C+ (an on-policy algorithm) scores much lower than DQN (an off-policy algorithm), while using the exact same exploration bonus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This paper demonstrates that a generalization of classical counting techniques through hashing is able to provide an appropriate signal for exploration, even in continuous and/or high-dimensional MDPs using function approximators, resulting in near state-of-the-art performance across benchmarks. It provides a simple yet powerful baseline for solving MDPs that require informed exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>Anonymous Author(s) Affiliation Address email  The autoencoder architecture was shown in <ref type="figure" target="#fig_9">Figure 1</ref>   bins, representing the pixel intensities. Moreover, label smoothing is applied to the different softmax <ref type="bibr" target="#b15">16</ref> bins, in which the log-probability of each of the bins is increased by 0.003, before normalizing. The 17 softmax weights are shared among each pixel. <ref type="bibr" target="#b17">18</ref> In addition, we apply counting Bloom filters <ref type="bibr" target="#b4">[5]</ref> to maintain a small hash table. Details can be found <ref type="bibr" target="#b18">19</ref> in Appendix 4. This section describes the continuous control environments used in the experiments. The tasks are <ref type="bibr" target="#b21">22</ref> implemented as described in <ref type="bibr" target="#b3">[4]</ref>, following the sparse reward adaptation of <ref type="bibr" target="#b5">[6]</ref>. The tasks have the a reward of +1 when the goal state is reached, namely escaping the valley from the right side. <ref type="bibr" target="#b27">28</ref> Therefore, the agent has to figure out how to swing up the pole in the absence of any initial external <ref type="bibr" target="#b28">29</ref> rewards. In HalfCheetah, the agent receives a reward of +1 when x body &gt; 5. As such, it has to figure <ref type="bibr" target="#b29">30</ref> out how to move forward without any initial external reward. The time horizon is set to T = 500 for 31 all tasks. <ref type="bibr" target="#b31">32</ref> 3 Analysis of Learned Binary Representation 33 <ref type="figure" target="#fig_9">Figure 1</ref> shows the downsampled codes learned by the autoencoder for several Atari 2600 games <ref type="bibr" target="#b33">34</ref> (Frostbite, Freeway, and Montezuma's Revenge). Each row depicts 50 consecutive frames (from 0 to 35 49, going from left to right, top to bottom). The pictures in the right column depict the binary codes <ref type="bibr" target="#b35">36</ref> that correspond with each of these frames (one frame per row). <ref type="figure" target="#fig_1">Figure 2</ref> shows the reconstructions <ref type="bibr" target="#b36">37</ref> of several subsequent images according to the autoencoder. Some binaries stay consistent across <ref type="bibr" target="#b37">38</ref> frames, and some appear to respond to specific objects or events. Although the precise meaning of 39 each binary number is not immediately obvious, the figure suggests that the learned hash code is a 40 reasonable abstraction of the game state. We experimented with directly building a hashing dictionary with keys ?(s) and values the state 43 counts, but observed an unnecessary increase in computation time. Our implementation converts the 44 integer hash codes into binary numbers and then into the "bytes" type in Python. The hash table is a 45 dictionary using those bytes as keys. However, an alternative technique called Count-Min Sketch <ref type="bibr" target="#b2">[3]</ref>, with a data structure identical to counting Bloom filters <ref type="bibr" target="#b4">[5]</ref>, can count with a fixed integer array and thus reduce computation 48 time. Specifically, let p 1 , . . . , p l be distinct large prime numbers and define ? j (s) = ?(s) mod p j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>49</head><p>The count of state s is returned as min 1?j?l n j ? j (s) . To increase the count of s, we increment 50 n j ? j (s) by 1 for all j. Intuitively, the method replaces ? by weaker hash functions, while it reduces 51 the probability of over-counting by reporting counts agreed by all such weaker hash functions. The</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>52</head><p>final hash code is represented as ? 1 (s), . . . , ? l (s) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>53</head><p>Throughout all experiments above, the prime numbers for the counting Bloom filter are 999931, 54 999953, 999959, 999961, 999979, and 999983, which we abbreviate as "6 M". In addition, we 55 experimented with 6 other prime numbers, each approximately 15 M, which we abbreviate as "90 M".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>56</head><p>As we can see in <ref type="figure" target="#fig_2">Figure 3</ref>, counting states with a dictionary or with Bloom filters lead to similar 57 performance, but the computation time of latter is lower. Moreover, there is little difference between 58 direct counting and using a very larger Count-Min sketch is designed to support memory-efficient counting without introducing too many 73 over-counts. It maintains a separate count n j for each hash function ? j defined as ? j (s) = ?(s) 74 mod p j , where p j is a large prime number. For simplicity, we may assume that p j ? p ?j and ? j 75 assigns s to any of 1, . . . , p with uniform probability. <ref type="bibr">76</ref> We now derive the probability of over-counting. Let s be a fixed data sample (not necessarily 77 inserted yet) and suppose a dataset D of N samples are inserted. We assume that p l N . Let 78 n := min 1?j?l n j ? j (s) be the count returned by the Bloom filter. We are interested in computing 79 Prob(n &gt; 0|s / ? D). Due to assumptions about ? j , we know n j (?(s)) ? Binomial N, 1 p .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>80</head><p>Therefore,</p><formula xml:id="formula_4">81 Prob(n &gt; 0|s / ? D) = Prob(n &gt; 0, s / ? D) Prob(s ? D) = Prob(n &gt; 0) ? Prob(s ? D) Prob(s / ? D) ? Prob(n &gt; 0) Prob(s / ? D) = l j=1 Prob(n j (? j (s)) &gt; 0) (1 ? 1/p l ) N = (1 ? (1 ? 1/p) N ) l (1 ? 1/p l ) N ? (1 ? e ?N/p ) l e ?N/p l ? (1 ? e ?N/p ) l .<label>(1)</label></formula><p>In particular, the probability of over-counting decays exponentially in l. We refer the readers to <ref type="bibr" target="#b2">[3]</ref> 82 for other properties of the Count-Min sketch.    The best granularity depends on both the hash function and the MDP. While adjusting granularity 98 parameter, we observed that it is important to lower the bonus coefficient as granularity is increased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>99</head><p>This is because a higher granularity is likely to cause lower state counts, leading to higher bonus 100 rewards that may overwhelm the true rewards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>101</head><p>Apart from the experimental results shown in <ref type="table" target="#tab_0">Table 1</ref> in the main text and    Montezuma's Revenge is widely known for its extremely sparse rewards and difficult exploration 120 <ref type="bibr" target="#b0">[1]</ref>. While our method does not outperform <ref type="bibr" target="#b0">[1]</ref> on this game, we investigate the reasons behind this 121 through various experiments. The experiment process below again demonstrates the importance of a 122 hash function having the correct granularity and encoding relevant information for solving the MDP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>123</head><p>Our first attempt is to use game RAM states instead of image observations as inputs to the policy, <ref type="bibr">124</ref> which leads to a game score of 2500 with TRPO-BASS-SimHash. Our second attempt is to manually 125 design a hash function that incorporates domain knowledge, called SmartHash, which uses an 126 integer-valued vector consisting of the agent's (x, y) location, room number and other useful RAM 127 information as the hash code. The best SmartHash agent is able to obtain a score of 3500. Still 128 the performance is not optimal. We observe that a slight change in the agent's coordinates does 129 not always result in a semantically distinct state, and thus the hash code may remain unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>130</head><p>Therefore we choose grid size s and replace the x coordinate by (x ? x min )/s (similarly for y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>131</head><p>The bonus coefficient is chosen as ? = 0.01 ? s to maintain the scale relative to the true reward 1 (see <ref type="table" target="#tab_0">132   Table 7</ref>). Finally, the best agent is able to obtain 6600 total rewards after training for 1000 iterations 133 (1000 M time steps), with a grid size s = 10.       </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Algorithm 2 :m=0 with policy ? 6 7 if j mod j update = 0 then 8 1 12</head><label>126781</label><figDesc>The autoencoder (AE) architecture for ALE; the solid block represents the dense sigmoidal binary code layer, after which noise U (?a, a) is injected. Count-based exploration using learned hash codes1 Define state preprocessor g : S ? {0, 1} D as the binary code resulting from the autoencoder (AE) 2 Initialize A ? R k?D with entries drawn i.i.d. from the standard Gaussian distribution N (0, 1) 3 Initialize a hash table with values n(?) ? 0 4 for each iteration j do 5 Collect a set of state-action samples {(s m , a m )} M Add the state samples {s m } M m=0 to a FIFO replay pool R Update the AE loss function in Eq. (3) using samples drawn from the replay pool {s n } N n=1 ? R, for example using stochastic gradient descent 9 Compute g(s m ) = b(s m ) , the D-dim rounded hash code for s m learned by the AE 10 Project g(s m ) to a lower dimension k via SimHash as ?(s m ) = sgn(Ag(s m )) 11 Update the hash table counts ?m : 0 ? m ? M as n(?(s m )) ? n(?(s m )) + Update the policy ? using rewards r(s m , a m ) +</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustrations of the rllab tasks used in the continuous control experiments, namely Moun-tainCar, CartPoleSwingup, SimmerGather, and HalfCheetah; taken from<ref type="bibr" target="#b7">[8]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Mean average return of different algorithms on rllab tasks with sparse rewards. The solid line represents the mean average return, while the shaded area represents one standard deviation, over 5 seeds for the baseline and SimHash (the baseline curves happen to overlap with the axis).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 (</head><label>2</label><figDesc>[0, 255]. Mathematically, let C be the cell size (width and height), B the number of bins, (i, j) cell location, (x, y) pixel location, and z the channel, then feature(i, j, z) = B 255C x,y)? cell(i,j) I(x, y, z) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Atari 2600 games: the solid line is the mean average undiscounted return per iteration, while the shaded areas represent the one standard deviation, over 5 seeds for the baseline, TRPOpixel-SimHash, and TRPO-BASS-SimHash, while over 3 seeds for TRPO-AE-SimHash.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>8 U</head><label>8</label><figDesc>of Section 2.3. Specifically, uniform noise (?a, a) with a = 0.3 is added to the sigmoid activations. The loss function Eq.(3) (in the main</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>20 2</head><label>20</label><figDesc>Description of the Adapted rllab Tasks21</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>23 following 24 A ? R 1 ;</head><label>23241</label><figDesc>state and action dimensions: CartPoleSwingup, S ? R 4 , A ? R; MountainCar S ? R 3 , HalfCheetah, S ? R 20 , A ? R 6 ; SwimmerGather, S ? R 33 , A ? R 2 . For the sparse 25 reward experiments, the tasks have been modified as follows. In CartPoleSwingup, the agent receives 26 a reward of +1 when cos(?) &gt; 0.8, with ? the pole angle. In MountainCar, the agent receives27</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>41 4 Counting</head><label>4</label><figDesc>Bloom Filter/Count-Min Sketch42</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>46Figure 1 :</head><label>1</label><figDesc>Frostbite, Freeway, and Montezuma's Revenge: subsequent frames (left) and corresponding code (right); the frames are ordered from left (starting with frame number 0) to right, top to bottom; the vertical axis in the right images correspond to the frame number.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>85</head><label></label><figDesc>While our proposed method is able to achieve remarkable results without requiring much tuning,86 the granularity of the hash function should be chosen wisely. Granularity plays a critical role in 87 count-based exploration, where the hash function should cluster states without under-generalizing 88 or over-generalizing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 2 :</head><label>2</label><figDesc>Freeway: subsequent frames and corresponding code (top); the frames are ordered from left (starting with frame number 0) to right, top to bottom; the vertical axis in the right images correspond to the frame number. Within each image, the left picture is the input frame, the middle picture the reconstruction, and the right picture, the reconstruction error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 3 :</head><label>3</label><figDesc>Statistics of TRPO-pixel-SimHash (k = 256) on Frostbite. Solid lines are the mean, while the shaded areas represent the one standard deviation. Results are derived from 10 random seeds. Direct counting with a dictionary uses 2.7 times more computations than counting Bloom filters (6 M or 90 M).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 4 :</head><label>4</label><figDesc>SmartHash results on Montezuma's Revenge (RAM observations): the solid line is the mean average undiscounted return per iteration, while the shaded areas represent the one standard deviation, over 5 seeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1 :</head><label>1</label><figDesc>Count-based exploration through static hashing, using SimHash 1 Define state preprocessor g : S ? R D 2 (In case of SimHash) Initialize A ? R k?D with entries drawn i.i.d. from the standard Gaussian distribution N (0, 1) 3 Initialize a hash table with values n(?) ? 0 4 for each iteration j do Collect a set of state-action samples {(s m , a m )}</figDesc><table><row><cell>6</cell><cell>M m=0 with policy ?</cell></row></table><note>5</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Atari 2600: average total reward after training for 50 M time steps. Boldface numbers indicate best results. Italic numbers are the best among our methods.</figDesc><table><row><cell></cell><cell cols="6">Freeway Frostbite Gravitar Montezuma Solaris Venture</cell></row><row><cell>TRPO (baseline)</cell><cell>16.5</cell><cell>2869</cell><cell>486</cell><cell>0</cell><cell>2758</cell><cell>121</cell></row><row><cell>TRPO-pixel-SimHash</cell><cell>31.6</cell><cell>4683</cell><cell>468</cell><cell>0</cell><cell>2897</cell><cell>263</cell></row><row><cell>TRPO-BASS-SimHash</cell><cell>28.4</cell><cell>3150</cell><cell>604</cell><cell>238</cell><cell>1201</cell><cell>616</cell></row><row><cell>TRPO-AE-SimHash</cell><cell>33.5</cell><cell>5214</cell><cell>482</cell><cell>75</cell><cell>4467</cell><cell>445</cell></row><row><cell>Double-DQN</cell><cell>33.3</cell><cell>1683</cell><cell>412</cell><cell>0</cell><cell>3068</cell><cell>98.0</cell></row><row><cell>Dueling network</cell><cell>0.0</cell><cell>4672</cell><cell>588</cell><cell>0</cell><cell>2251</cell><cell>497</cell></row><row><cell>Gorila</cell><cell>11.7</cell><cell>605</cell><cell>1054</cell><cell>4</cell><cell>N/A</cell><cell>1245</cell></row><row><cell>DQN Pop-Art</cell><cell>33.4</cell><cell>3469</cell><cell>483</cell><cell>0</cell><cell>4544</cell><cell>1172</cell></row><row><cell>A3C+</cell><cell>27.3</cell><cell>507</cell><cell>246</cell><cell>142</cell><cell>2175</cell><cell>0</cell></row><row><cell>pseudo-count</cell><cell>29.2</cell><cell>1450</cell><cell>-</cell><cell>3439</cell><cell>-</cell><cell>369</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell cols="2">1 Hyperparameter Settings</cell></row><row><cell></cell><cell>: TRPO hyperparameters for rllab experiments</cell></row><row><cell>Experiment TRPO batch size TRPO step size Discount factor ? Policy hidden units Baseline function Exploration bonus SimHash dimension</cell><cell>MountainCar CartPoleSwingUp HalfCheetah SwimmerGatherer 5k 5k 5k 50k 0.01 0.99 (32, 32) (32, ) (32, 32) (64, 32) linear linear linear MLP: 32 units ? = 0.01 k = 32</cell></row><row><cell cols="2">Hyperparameters for Atari 2600 experiments are summarized in Table 2 and 3. By default, all</cell></row></table><note>1 Throughout all experiments, we use Adam [8] for optimizing the baseline function and the autoen-2 coder. Hyperparameters for rllab experiments are summarized in Table 1. Here the policy takes3 a state s as input, and outputs a Gaussian distribution N (?(s), ? 2 ), where ?(s) is the output of a4 multi-layer perceptron (MLP) with tanh nonlinearity, and ? &gt; 0 is a state-independent parameter.56 convolutional layers are followed by ReLU nonlinearity.7</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>TRPO hyperparameters for Atari experiments with image input</figDesc><table><row><cell>Experiment TRPO batch size TRPO step size Discount factor # random seeds Input preprocessing Policy structure Baseline structure Exploration bonus Hashing parameters</cell><cell>TRPO-pixel-SimHash TRPO-BASS-SimHash 100k 0.01 0.995 5 5 grayscale; downsampled to 52 ? 52; each pixel rescaled to [?1, 1] TRPO-AE-SimHash 3 4 previous frames are concatenated to form the input state 16 conv filters of size 8 ? 8, stride 4 32 conv filters of size 4 ? 4, stride 2 fully-connect layer with 256 units linear transform and softmax to output action probabilities (use batch normalization[7] at every layer) (same as policy, except that the last layer is a single scalar) ? = 0.01 k = 256 cell size C = 20 b(s) size: 256 bits B = 20 bins downsampled to 64 bits</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>TRPO hyperparameters for Atari experiments with RAM input = 10, is updated every j update = 3 iterations. The architecture looks as follows: an<ref type="bibr" target="#b9">10</ref> input layer of size 52 ? 52, representing the image luminance is followed by 3 consecutive 6 ? 6<ref type="bibr" target="#b10">11</ref> convolutional layers with stride 2 and 96 filters feed into a fully connected layer of size 1024, which 12 connects to the binary code layer. This binary code layer feeds into a fully-connected layer of 1024</figDesc><table><row><cell>Experiment TRPO batch size TRPO step size Discount factor # random seeds Input preprocessing vector of length 128 in the range [0, 255]; downsampled to [?1, 1] TRPO-RAM-SimHash 100k 0.01 0.995 10 Policy structure MLP: (32, 32, number_of_actions), tanh Baseline structure MLP: (32, 32, 1), tanh Exploration bonus ? = 0.01 SimHash dimension k = 256</cell></row><row><cell>text), using ?</cell></row></table><note>13 units, connecting to a fully-connected layer of 2400 units. This layer feeds into 3 consecutive 6 ? 614 transposed convolutional layers of which the final one connects to a pixel-wise softmax layer with 64 15</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>table for Bloom filters, as the average bonus rewards are 59 almost the same, indicating the same degree of exploration-exploitation trade-off. On the other hand, 60 Bloom filters require a fixed table size, which may not be known beforehand. Suppose we have l functions ? j that independently assign each data sample63 to an integer between 1 and p uniformly at random. Initially 1, 2, . . . , p are marked as 0. Then every 64 s ? D is "inserted" through marking ? j (s) as 1 for all j. A new sample s is reported as a member of D only if ? j (s) are marked as 1 for all j. A bloom filter has zero false negative rate (any s ? D is</figDesc><table><row><cell>61</cell><cell></cell></row><row><cell cols="2">62 belongs to a dataset D. 66 Theory of Bloom Filters Bloom filters [2] are popular for determining whether a data sample s</cell></row><row><cell>67</cell><cell>reported a member), while the false positive rate (probability of reporting a nonmember as a member) decays exponentially in l.</cell></row><row><cell>72</cell><cell></cell></row></table><note>68 Though Bloom filters support data insertion, it does not allow data deletion. Counting Bloom filters69 [5] maintain a counter n(?) for each number between 1 and p. Inserting/deleting s corresponds70 to incrementing/decrementing n ? j (s) by 1 for all j. Similarly, s is considered a member if71 ?j : n ? j (s) = 0.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc>summarizes granularity parameters for our hash functions. InTable 589 we summarize the performance of TRPO-pixel-SimHash under different granularities. We choose90 Frostbite and Venture on which TRPO-pixel-SimHash outperforms the baseline, and choose as reward 91 bonus coefficient ? = 0.01 ? 256 k to keep average bonus rewards at approximately the same scale. 92 k = 16 only corresponds to 65536 distinct hash codes, which is insufficient to distinguish between 93 semantically distinct states and hence leads to worse performance. We observed that k = 512 tends 94 to capture trivial image details in Frostbite, leading the agent to believe that every state is new and 95 equally worth exploring. Similar results are observed while tuning the granularity parameters for</figDesc><table><row><cell>96</cell></row><row><cell>TRPO-BASS-SimHash and TRPO-AE-SimHash.</cell></row></table><note>97</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Granularity parameters of various hash functions</figDesc><table><row><cell>SimHash</cell><cell>k: size of the binary code</cell></row><row><cell>BASS</cell><cell>C: cell size</cell></row><row><cell></cell><cell>B: number of bins for each color channel</cell></row><row><cell>AE</cell><cell>k: downstream SimHash parameter</cell></row><row><cell></cell><cell>?: binarization parameter</cell></row><row><cell cols="2">SmartHash s: grid size agent (x, y) coordinates</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Average score at 50 M time steps achieved by TRPO-pixel-SimHash</figDesc><table><row><cell>k</cell><cell>16</cell><cell>64 128</cell><cell>256 512</cell></row><row><cell cols="4">Frostbite 3326 4029 3932 4683 1117</cell></row><row><cell>Venture</cell><cell cols="2">0 218 142</cell><cell>263 306</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5</head><label>5</label><figDesc>, additional 102 experiments have been performed to study several properties of our algorithm.To study the performance sensitivity to hyperparameter changes, we focus on evaluating TRPO-105 RAM-SimHash on the Atari 2600 game Frostbite, where the method has a clear advantage over the 106 baseline. Because the final scores can vary between different random seeds, we evaluated each set of 107 hyperparameters with 30 seeds. To reduce computation time and cost, RAM states are used insteadThe results are summarized inTable 6. Herein, k refers to the length of the binary code for hashing110 while ? is the multiplicative coefficient for the reward bonus, as defined in Section 2.2 of the main 111 text. This table demonstrates that most hyperparameter settings outperform the baseline (? = 0) 112 significantly. Moreover, the final scores show a clear pattern in response to changing hyperparameters.</figDesc><table><row><cell>103</cell><cell></cell></row><row><cell>104</cell><cell>5.2 Hyperparameter sensitivity</cell></row><row><cell>108</cell><cell></cell></row><row><cell></cell><cell>of image observations.</cell></row><row><cell>118</cell><cell></cell></row></table><note>109113 Small ?-values lead to insufficient exploration, while large ?-values cause the bonus rewards to114 overwhelm the true rewards. With a fixed k, the scores are roughly concave in ?, peaking at around115 0.2. Higher granularity k leads to better performance. Therefore, it can be concluded that the116 proposed exploration method is robust to hyperparameter changes in comparison to the baseline, and117 that the best parameter settings can be obtained from a relatively coarse-grained grid search.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>TRPO-RAM-SimHash performance robustness to hyperparameter changes on Frostbite</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>k</cell><cell cols="8">0 0.01 0.05 0.1 0.2 0.4 0.8 1.6</cell></row><row><cell cols="2">-397</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>64</cell><cell>-</cell><cell>879</cell><cell>2464</cell><cell>2243</cell><cell>2489</cell><cell>1587</cell><cell>1107</cell><cell>441</cell></row><row><cell>128</cell><cell>-</cell><cell>1475</cell><cell>4248</cell><cell>2801</cell><cell>3239</cell><cell>3621</cell><cell>1543</cell><cell>395</cell></row><row><cell>256</cell><cell>-</cell><cell>2583</cell><cell>4497</cell><cell>4437</cell><cell>7849</cell><cell>3516</cell><cell>2260</cell><cell>374</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table><row><cell cols="7">Average score at 50 M time steps achieved by TRPO-SmartHash on Montezuma's Revenge (RAM observations)</cell></row><row><cell>s</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>40</cell><cell>60</cell></row><row><cell cols="7">score 2598 2500 3533 3025 2500 1921</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Interpretation of particular RAM entries in Montezuma's Revenge</figDesc><table><row><cell></cell><cell cols="3">ID Group Meaning</cell></row><row><cell></cell><cell>3</cell><cell>room</cell><cell>room number</cell></row><row><cell></cell><cell>42</cell><cell>agent</cell><cell>x coordinate</cell></row><row><cell></cell><cell>43</cell><cell>agent</cell><cell>y coordinate</cell></row><row><cell></cell><cell>52</cell><cell>agent</cell><cell>orientation (left/right)</cell></row><row><cell></cell><cell cols="3">27 beams on/off</cell></row><row><cell></cell><cell cols="3">83 beams beam countdown (on: 0, off: 36 ? 0) 0 counter counts from 0 to 255 and repeats</cell></row><row><cell></cell><cell cols="3">55 counter death scene countdown</cell></row><row><cell></cell><cell cols="3">67 objects Doors, skull, and key in 1st room</cell></row><row><cell></cell><cell>47</cell><cell>skull</cell><cell>x coordinate (1st and 2nd room)</cell></row><row><cell>119</cell><cell cols="3">5.3 A Case Study of Montezuma's Revenge</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Performance comparison between state counting (left of the slash) and state-action counting (right of the slash) using TRPO-RAM-SimHash on Frostbite</figDesc><table><row><cell>?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8</head><label>8</label><figDesc>lists the semantic interpretation of certain RAM entries in Montezuma's Revenge. SmartHash, 135 as described in Section 5.3, makes use of RAM indices 3, 42, 43, 27, and 67. "Beam walls" are</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The original BASS exploits the fact that at most 128 colors can appear on the screen. Our adapted version does not make this assumption.<ref type="bibr" target="#b3">4</ref> The agent takes no action for a random number (within 30) of frames at the beginning of each episode.<ref type="bibr" target="#b4">5</ref> We provide videos of example game play and visualizations of the difference bewteen Pixel-SimHash and BASS-SimHash at https://www.youtube.com/playlist?list=PLAd-UMX6FkBQdLNWtY8nH1-pzYJA_1T55</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The bonus scaling is chosen by assuming all states are visited uniformly and the average bonus reward should remain the same for any grid size.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank our colleagues at Berkeley and OpenAI for insightful discussions. This research was funded in part by ONR through a PECASE award. Yan Duan was also supported by a Berkeley AI Research lab Fellowship and a Huawei Fellowship. Xi Chen was also supported by a Berkeley AI Research lab Fellowship. We gratefully acknowledge the support of the NSF through grant IIS-1619362 and of the ARC through a Laureate Fellowship (FL110100281) and through the ARC Centre of Excellence for Mathematical and Statistical Frontiers. Adam Stooke gratefully acknowledges funding from a Fannie and John Hertz Foundation fellowship. Rein Houthooft was supported by a Ph.D. Fellowship of the Research Foundation -Flanders (FWO).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Exploratory gradient boosting for reinforcement learning in complex domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Abel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04119</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandr</forename><surname>Andoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Indyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS)</title>
		<meeting>the 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="459" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The arcade learning environment: An evaluation platform for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yavar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="253" to="279" />
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unifying count-based exploration and intrinsic motivation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Georg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29 (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1471" to="1479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">R-max-a general polynomial time algorithm for near-optimal reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronen</forename><forename type="middle">I</forename><surname>Brafman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Tennenholtz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="213" to="231" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Similarity estimation techniques from rounding algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moses</forename><forename type="middle">S</forename><surname>Charikar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th Annual ACM Symposium on Theory of Computing (STOC)</title>
		<meeting>the 34th Annual ACM Symposium on Theory of Computing (STOC)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="380" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Benchmarking deep reinforcement learning for continous control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning (ICML)</title>
		<meeting>the 33rd International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1329" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Bayesian reinforcement learning: A survey. Foundations and Trends in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Tamar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="359" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards conceptual compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Besse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimenez</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Danilo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29 (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3549" to="3557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bayes-adaptive simulation-based search with value function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nicolas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (Advances in Neural Information Processing Systems (NIPS))</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="451" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">VIME: Variational information maximizing exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Turck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29 (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1109" to="1117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Near-optimal regret bounds for reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Jaksch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Ortner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1563" to="1600" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Near-optimal reinforcement learning in polynomial time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satinder</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="209" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Near-bayesian exploration in polynomial time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Machine Learning (ICML)</title>
		<meeting>the 26th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25 (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Asymptotically efficient adaptive allocation rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tze</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Robbins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="22" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nicolas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02971</idno>
		<title level="m">Continuous control with deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the 7th IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Volodymyr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Volodymyr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adria</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Puigdomenech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.01783</idno>
		<title level="m">Asynchronous methods for deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Praveen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blackwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alcicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cagdas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rory</forename><surname>Fearon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Maria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedavyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.04296</idno>
		<title level="m">Massively parallel methods for deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep exploration via bootstrapped DQN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29 (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4026" to="4034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generalization and exploration via randomized value functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning (ICML)</title>
		<meeting>the 33rd International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2377" to="2386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">What is intrinsic motivation? A typology of computational approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Yves</forename><surname>Oudeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Kaplan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neurorobotics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">PAC optimal exploration in continuous space Markov decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Pazis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Parr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the 27th AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semantic hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="969" to="978" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Formal theory of creativity, fun, and intrinsic motivation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Autonomous Mental Development</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="230" to="247" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Trust region policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sergey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Philipp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Incentivizing exploration in reinforcement learning with deep predictive models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradly</forename><forename type="middle">C</forename><surname>Stadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.00814</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A theoretical analysis of model-based interval estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">L</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Machine Learning (ICML)</title>
		<meeting>the 21st International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="856" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An analysis of model-based interval estimation for Markov decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">L</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1309" to="1331" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Planning to be surprised: Optimal Bayesian exploration in dynamic environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Artificial General Intelligence (AGI)</title>
		<meeting>the 4th International Conference on Artificial General Intelligence (AGI)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="41" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">DAISY: An efficient dense descriptor applied to wide-baseline stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Engin</forename><surname>Tola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="815" to="830" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning (ICML)</title>
		<meeting>the 33rd International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1747" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning functions across many orders of magnitudes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07714</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with double Q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the 30th AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dueling network architectures for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning (ICML)</title>
		<meeting>the 33rd International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
