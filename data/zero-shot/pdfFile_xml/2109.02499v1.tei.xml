<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiageng</forename><surname>Mao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minzhe</forename><surname>Niu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyue</forename><surname>Bai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
						</author>
						<title level="a" type="main">Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a flexible and high-performance framework, named Pyramid R-CNN, for two-stage 3D object detection from point clouds. Current approaches generally rely on the points or voxels of interest for RoI feature extraction on the second stage, but cannot effectively handle the sparsity and non-uniform distribution of those points, and this may result in failures in detecting objects that are far away.</p><p>To resolve the problems, we propose a novel second-stage module, named pyramid RoI head, to adaptively learn the features from the sparse points of interest. The pyramid RoI head consists of three key components. Firstly, we propose the RoI-grid Pyramid, which mitigates the sparsity problem by extensively collecting points of interest for each RoI in a pyramid manner. Secondly, we propose RoI-grid Attention, a new operation that can encode richer information from sparse points by incorporating conventional attentionbased and graph-based point operators into a unified formulation. Thirdly, we propose the Density-Aware Radius Prediction (DARP) module, which can adapt to different point density levels by dynamically adjusting the focusing range of RoIs. Combining the three components, our pyramid RoI head is robust to the sparse and imbalanced circumstances, and can be applied upon various 3D backbones to consistently boost the detection performance. Extensive experiments show that Pyramid R-CNN outperforms the state-of-the-art 3D detection models by a large margin on both the KITTI dataset and the Waymo Open dataset. arXiv:2109.02499v1 [cs.CV] 6 Sep 2021</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Pyramid R-CNN</head><p>In this section, we detail the design of Pyramid R-CNN, a general two-stage framework for 3D object detection. We first introduce the overall architecture in 3.1. Then we introduce three key components in the pyramid RoI head: RoI-grid Pyramid in 3.2, RoI-grid Attention in 3.3, and the Density-Aware Radius Prediction (DARP) module in 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overall Architecture</head><p>Here, we present a new two-stage framework for accurate and robust 3D object detection, named Pyramid R-CNN, as shown in <ref type="figure">Figure 2</ref>. The framework can be compatible with multiple backbones, e.g. the point-based backbone, the voxel-based backbone , or the point-voxel-based back-</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D object detection is a key component of perception systems for robotics and autonomous driving, aiming at detecting vehicles, pedestrians, and other objects with 3D point clouds as input. In this paper, we propose a general two-stage 3D detection framework, named Pyramid R-CNN, which can be applied on multiple 3D backbones to enhance the detection adaptability and performance.</p><p>Among the existing 3D detection frameworks, two-stage detection models <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b28">29]</ref>   stage 3D detectors <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30]</ref> with remarkable margins owing to the RoI refinement stage. Different from the 2D counterparts <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b1">2]</ref> which apply RoIPool <ref type="bibr" target="#b7">[8]</ref> or RoIAlign <ref type="bibr" target="#b10">[11]</ref> to crop dense feature maps on the second stage, the 3D detection models generally perform various RoI feature extraction operations on the Points of Interest. For example, Point R-CNN <ref type="bibr" target="#b29">[30]</ref> utilizes a pointbased backbone to generate 3D proposals, treats the points near the proposals as Points of Interest and applies Region Pooling on those sparse points for box refinement; Part-A 2 Net <ref type="bibr" target="#b30">[31]</ref> utilizes a voxel-based backbone for proposal generation, uses the upsampled voxel points as Points of Interest, and applies sparse convolutions on those voxel points for each RoI; PV-RCNN <ref type="bibr" target="#b27">[28]</ref> encodes the whole scene into a set of keypoints, and utilizes keypoints as Points of Interest for RoI-grid Pooling. Those Points of Interest originate from raw point clouds and contain rich fine-grained information, which is required for the RoI refinement stage.</p><p>However, the Points of Interest inevitably suffer from the sparsity and non-uniform distribution characteristics of input point clouds. As is demonstrated by the statistical results on the KITTI dataset <ref type="bibr" target="#b6">[7]</ref> in <ref type="figure" target="#fig_0">Figure 1</ref>: 1) Point clouds can be quite sparse in certain objects. More than 7% of total objects have less than 10 points, and their visualized shapes are mostly incomplete. Thus it is hard to identify their categories without enough context information.</p><p>2) The distribution of object points is extremely imbalanced. The number of object points ranges from less than 10 to more than 500 on KITTI, and current RoI operations cannot handle the im-balanced conditions effectively. 3) The number of Points of Interest only accounts for a small proportion of input points or voxels, e.g. 2k keypoints in <ref type="bibr" target="#b27">[28]</ref> relative to the 15k total input points, which exacerbates the above problems.</p><p>To overcome the above limitations, we propose Pyramid R-CNN, a general two-stage 3D detection framework that can effectively detect objects and adapt along with environmental changes. Our main contribution lies in the design of a novel RoI feature extraction head, named pyramid RoI head, which can be applied on multiple 3D backbones and Points of Interest. pyramid RoI head consists of three key components. Firstly, we propose RoI-grid Pyramid. Given the observation that Points of Interest inside RoIs are too sparse for object recognition, our RoI-grid Pyramid captures more Points of Interest outside RoIs while still maintaining fine-grained geometric details, by extending the standard one-level RoI-grid to a pyramid structure. Secondly, we propose RoI-grid Attention, an effective operation to extract RoI-grid features from Points of Interest. RoI-grid Attention leverages the advantages of the graphbased and attention-based point operators by combining those formulas into a unified formulation, and it can adapt to different sparsity situations by dynamically attending to the crucial Points of Interest near the RoIs. Thirdly, we propose the Density-Aware Radius Prediction (DARP) module, which can predict the feature extraction radius of each RoI, conditioning on the neighboring distribution of Points of Interest. Thus we can address the imbalanced distribution problem by adaptively adjusting the focusing range for each RoI. Combining all the above components, the pyramid RoI head shows adaptability to different point cloud sparsity levels and can accurately detect the 3D objects with only a few points. Our Pyramid R-CNN is compatible with the point-based <ref type="bibr" target="#b29">[30]</ref>, voxel-based <ref type="bibr" target="#b30">[31]</ref> and pointvoxel-based <ref type="bibr" target="#b27">[28]</ref> frameworks, and significantly boosts the detection accuracy.</p><p>We summarize our key contributions as follows: 1) We propose Pyramid R-CNN, a general two-stage framework that can be applied on multiple backbones for accurate and robust 3D object detection.</p><p>2) We propose the pyramid RoI head, which combines the RoI-grid Pyramid, RoI-grid Attention, and the Density-Aware Radius Prediction (DARP) module together to mitigate the sparsity and non-uniform distribution problems.</p><p>3) Pyramid R-CNN consistently outperforms the baselines, achieves 82.08% moderate car mAP on the KITTI dataset, and ranks 1 st among the LiDAR-only methods on the Waymo test leaderboard for vehicle detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Single-stage 3D Object Detection. Single-stage methods can be divided into 3 streams, i.e., point-based, voxel-based and pillar-based. The point-based single-stage detectors generally take the raw points as input, and apply set abstraction <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b20">21]</ref> to obtain the point features for box prediction. 3DSSD <ref type="bibr" target="#b38">[39]</ref> introduces Feature-FPS as a new sam-pling strategy for raw point clouds. Point-GNN <ref type="bibr" target="#b31">[32]</ref> proposes a graph operator to aggregate the points information for object detection. The voxel-based single-stage detectors typically rasterize point clouds into voxel-grids and then apply 2D and 3D CNN to generate 3D proposals. Vox-elNet <ref type="bibr" target="#b45">[46]</ref> divides points into voxels and leverages a 3D CNN to aggregate voxel features for proposal generation. SECOND <ref type="bibr" target="#b37">[38]</ref> improves the voxel feature learning process by introducing 3D sparse convolutions. CenterPoints <ref type="bibr" target="#b41">[42]</ref> proposes a center-based assignment that can be applied on feature maps for accurate location prediction. Pillar-based approaches generally transform point clouds into Bird-Eye-View (BEV) pillars and apply 2D CNNs for 3D object detection. PointPillars <ref type="bibr" target="#b13">[14]</ref> is the first work that introduces the pillar representation. Pillar-based network <ref type="bibr" target="#b35">[36]</ref> extends the idea by proposing the cylindrical view projection. Unlike the two-stage approaches, the single-stage methods cannot benefit from the fine-grained point information, which is crucial for accurate box prediction. Two-stage 3D object detection. Two-stage approaches can be divided into 3 streams, based on the representation of Points of Interest, i.e., point-based, voxel-based and point-voxel-based. Point-based approaches treat the sampled point clouds as Points of Interest. PointRCNN <ref type="bibr" target="#b29">[30]</ref> generates 3D proposals from raw point clouds and proposes Region Pooling to extract RoI features for the second stage refinement. STD <ref type="bibr" target="#b39">[40]</ref> proposes a sparse-to-dense strategy and uses the PointsPool operation for RoI refinement. Voxel-based methods use the voxel points from 3D CNNs as Points of Interest. Part-A 2 Net [31] applies 3D sparse convolutions on the upsampled voxel points for RoI refinement. Voxel R-CNN <ref type="bibr" target="#b4">[5]</ref> utilizes Voxel RoI Pooling to extract RoI features from voxels. Point-Voxel-based approaches use the keypoints that encode the whole scene as Points of Interest. PV-RCNN <ref type="bibr" target="#b27">[28]</ref> designs RoI-grid Pooling to aggregate keypoint features near RoIs. PV-RCNN++ <ref type="bibr" target="#b28">[29]</ref> proposes Vector-Pooling to efficiently collect the keypoint features from different orientations. Compared with the previous methods, our Pyramid R-CNN shows better performance and robustness, and is compatible with all the representations of Points of Interest. <ref type="figure">Figure 2</ref>. The overall architecture. Our Pyramid R-CNN can be plugged on diverse backbones (e.g. point-based, voxel-based and pointvoxel-based networks), which generate 3D proposals and Points of Interest (yellow points) on the stage-1. On the stage-2, we propose the pyramid RoI head that can be applied upon the 3D proposals and Points of Interest. In the pyramid RoI head, an RoI-grid Pyramid is first built to capture more context information. Then for each RoI-grid point (red point), a focusing radius r (red dashed circle) is learned by the Density-Aware Radius Prediction module. Finally, RoI-grid Attention is performed on the Points of Interest within r for box refinement.</p><p>bone. On the first stage, those backbones output 3D proposals and corresponding Points of Interest: e.g. point clouds near RoIs in <ref type="bibr" target="#b29">[30]</ref>, upsampled voxels in <ref type="bibr" target="#b30">[31]</ref>, and keypoints in <ref type="bibr" target="#b27">[28]</ref>. On the second stage, we propose a novel pyramid RoI head, which consists of three key components: the RoIgrid Pyramid, RoI-grid Attention, and the Density-Aware Radius Prediction (DARP) module. For each RoI, we first build an RoI-grid Pyramid, by gradually enlarging the size of the original RoI in each pyramid level, and the coordinates of RoI-grid points are determined by the enlarged RoI size and the grid size. In each pyramid level, the focusing radius r of the RoI-grid points is predicted from the global context vector through the Density-Aware Radius Prediction module. Then RoI-grid Attention parameterized by r is performed to aggregate the features of Points of Interest into the RoI-grids. Finally, the RoI-grid features are enhanced and fed into two individual heads for classification and regression. We will describe those key components in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">RoI-grid Pyramid</head><p>In this section, we present the RoI-grid Pyramid, a simple and effective module that captures rich context while still maintains internal structural information. Different from 2D feature pyramid <ref type="bibr" target="#b17">[18]</ref> which hierarchically encodes context information upon dense backbone features, our RoIgrid Pyramid is applied on each RoI by gradually placing the grid points out of RoIs in a pyramid manner. The idea behind this design is based on the observation that image features inside RoIs generally contain sufficient semantic contexts, while point clouds inside RoIs contain quite limited information since object points are naturally sparse and incomplete. Even though each point has a large receptive field, the sparse compositional 3D shapes inside RoIs are hard to be recognized. In the following parts we will introduce detailed formulations.</p><p>RoI feature extraction generally relies on an RoI-grid for each RoI, and RoI-grid points collect the features of adjacent pixels or neighboring Points of Interest in the 2D or 3D cases respectively. Supposing we have an RoI with W, L, H as width, length, and height and (x c , y c , z c ) as the bottom left corner, in standard RoI-grid representation, the (i, j, k) RoI-grid point location p ijk grid can be computed as:</p><formula xml:id="formula_0">p ijk grid = ( W N w , L N l , H N h )?(0.5+(i, j, k))+(x c , y c , z c ),<label>(1)</label></formula><p>where (N w , N l , N h ) are the grid sizes in three dimensions and all grid points are generated inside RoIs. Utilizing features only inside the RoIs works well in the 2D detection models, mainly owing to two facts: the input feature map is dense and the collected pixels have large receptive fields. However, the cases are different in 3D models. As is shown in <ref type="figure">Figure 3</ref>, the Points of Interest are naturally sparse and non-uniformly distributed inside the RoIs, and the object shape is extremely incomplete. Thus it is hard to accurately infer the sizes and categories of objects by solely collecting the features of few individual points and not referring to enough neighboring points information.</p><p>To resolve the above problems, we propose the RoI-grid Pyramid which balances the fine-grained and large context information. The detailed structure is in <ref type="figure">Figure 3</ref>. The key idea is to construct a pyramid grid structure that contains the RoI-grid points both inside and outside RoIs, so that the grid points inside RoIs can capture fine-grained shape structures for accurate box refinement, while the grid points outside RoIs can obtain large context information to identify incomplete objects. The grid points p ijk grid for a pyramid  <ref type="figure">Figure 3</ref>. Illustration of the RoI-grid Pyramid. Red points in (a) are the RoI-grid points, and different colors represent different pyramid levels in (b). In (c) and (d) red points are object points and blue points are context points captured by the RoI. Compared to the standard RoI-grid, our RoI-grid Pyramid can capture more context points while maintain fine-grained internal structures, and by looking at neighboring vehicle and traffic sign (blue context points) outside the RoI, the cluster of red object points is easier to be recognized as a car.</p><p>level can be computed as:</p><formula xml:id="formula_1">p ijk grid = ( ? w W N w , ? l L N l , ? h H N h ) ? (0.5 + (i, j, k)) + (x c , y c , z c ),<label>(2)</label></formula><p>where ? is the enlarging ratio of the original RoI size. ? starts from 1 at the bottom level for maintaining finegrained details, and becomes larger when the level goes higher to capture more context information. The grid size N is initialized with the same value as the original N at the bottom level and gets smaller at higher levels to save computational resources. For each pyramid level, features of grid points f grid are then aggregated by RoI-grid Attention from the features of Points of Interest. Finally, features of all pyramid levels are combined for boxes refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">RoI-grid Attention</head><p>In this section, we introduce RoI-grid Attention, a novel RoI feature extraction operation that combines the state-ofthe-art graph-based and attention-based point operators <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b43">44]</ref> into a unified framework, and RoI-grid Attention can serve as a better substitute for conventional poolingbased operations <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b28">29]</ref> in 3D detection models. We first discuss the formulas of pooling-based, graph-based and attention-based point operators, and then we derive the formulation of RoI-grid Attention.</p><p>Preliminary. Let p grid be the coordinate of an RoI-grid point, and p i , f i be the coordinate and the corresponding feature vector of the i th Points of Interest near p grid . RoI feature extraction operation aims to obtain the respective feature vector f grid of the RoI-grid point p grid , using the information of neighboring p i and f i .</p><p>Pooling-based Operators. The pooling-based operators are extensively applied for RoI feature extraction in most two-stage 3D detection models <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b28">29]</ref>. The neighboring feature f i and the relative location p i ? p grid first go through a MLP layer to obtain the transformed feature vector:</p><formula xml:id="formula_2">V i = M LP ([f i , p i ? p grid ]), where [?]</formula><p>is the concatenation function, and then a maxpooling operation is applied upon all the transformed features V to obtain the RoI-grid feature f pool grid :</p><formula xml:id="formula_3">f pool grid = maxpool i??(r) (V i ),<label>(3)</label></formula><p>where ?(r) means Points of Interest within the fixed radius r of the RoI-grid point p grid . The pooling-based operators only focus on the maximum channel response and this results in a loss of much semantic and geometric information.</p><p>Graph-based Operators. Graph-based operators can model the grid points and Points of Interest as a graph. The graph node i represents the transformed feature of f i : V i = M LP (f i ), and the edge Q i pos can be formulated as a linear projection of the location differences between two nodes: Q i pos = Linear(p i ? p grid ). For the graph node of a grid point p grid , the feature f graph grid is collected from adjacent nodes by a weighted combination operation. Following the same notations as Eq.3, the general formula can be represented as</p><formula xml:id="formula_4">f graph grid = i??(r) W (Q i pos ) V i ,<label>(4)</label></formula><p>where the function W (?) projects the graph edge embedding into the scalar or vector weight space, and denotes either the Hadamard product, dot product or scalar-vector product between learned weights and graph nodes.</p><p>Attention-based Operators. Attention-based operators can also be applied upon the grid points and Points of Interest. Q i pos in Eq.4 can be viewed as the query embedding from the grid point p grid to the point p i . V i is the value embedding obtained from the feature f i as Eq.4. The key embedding K i can be formulated as K i = Linear(f i ). Thus standard attention can be formulated as</p><formula xml:id="formula_5">f atten grid = i??(r) W (Q i pos K i ) V i .<label>(5)</label></formula><p>Additional normalization function, i.e. softmax, is applied in W (?). Recently proposed Point Transformer <ref type="bibr" target="#b43">[44]</ref> extending the idea of standard attention and the formula can be represented as</p><formula xml:id="formula_6">f tr grid = i??(r) W (K i + Q i pos ) (V i + Q i pos ). (6)</formula><p>RoI-grid Attention. In our approach, we analyze the structural similarity of Eq.4, Eq.5 and Eq.6. We find that those formulas have common basic elements and operators. Thus it's natural to merge those formulas into a unified framework with gated functions. We name this new formula RoI-grid Attention:</p><formula xml:id="formula_7">f grid = i??(r) W (? k K i + ? q Q i pos + ? qk Q i pos K i ) (V i + ? v Q i pos ),<label>(7)</label></formula><p>where ? * is a learnable gated function which can be implemented by a linear projection of the respective embedding with a sigmoid activation output. RoI-grid Attention is a generalized formulation combining graph-based and attention-based operations. We can derive the graph operator Eq.4 from Eq.7 when ? q , ? k , ? qk , ? v are 1, 0, 0, 0 respectively. Similarly, we can derive the standard attention Eq.5 when ? q , ? k , ? qk , ? v are 0, 0, 1, 0, or Point Transformer Eq.6 when ? q , ? k , ? qk , ? v are 1, 1, 0, 1.</p><p>RoI-grid Attention is a flexible and effective operation for RoI feature extraction. With the learnable gated functions, RoI-grid Attention is able to learn which point is significant to the RoI-grid points, from both the geometric information Q pos and the semantic information K, as well as their combinations Q pos K adaptively. With ? v , RoI-grid Attention can also learn to balance the ratio of geometric features Q pos and semantic features V used in feature aggregation. Compared with the pooling-based methods, only a few linear projection layers are added in RoI-grid Attention, which maintains the computational efficiency. Replacing pooling-based operators with RoI-grid Attention consistently boosts the detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Density-Aware Radius Prediction</head><p>In this section, we investigate the learning problem of the radius r, which determines the range ?(r) of neighboring Points of Interest that participate in the feature extraction process. The radius r is a hyper-parameter used in all the point operators in 3.3, and has to be determined by researchers in previous approaches. The fixed and predefined r cannot adapt to the density changes of point clouds, and may lead to empty spherical ranges if not set properly. In this paper, we make the prediction of r a fully-differentiable process and further propose the Density-Aware Radius Prediction (DARP) module, aiming at learning an adaptive neighborhood for RoI feature extraction. We first introduce the general formulation of RoI-grid Attention from a probabilistic perspective. Next, we propose a novel method to differentiate the learning of r. Finally, we introduce the design of the DARP module.</p><p>RoI-grid Attention is composed of two steps: first selects Points of Interest within the radius r, and next performs weighted combinations on those points. With the same notations in 3.3, we can reformulate the first step as sampling from a conditional distribution p(i|r):</p><formula xml:id="formula_8">p(i|r) = 0 ||p i ? p grid || 2 &gt; r 1 ||p i ? p grid || 2 ? r<label>(8)</label></formula><p>Then the second step can be represented as calculating the probabilistic expectation:</p><formula xml:id="formula_9">f grid = E i?p(i|r) [W i V i ],<label>(9)</label></formula><p>where</p><formula xml:id="formula_10">W i denotes W (? k K i + ? q Q i pos + ? qk Q i pos K i ) and V i denotes (V i + ? v Q i pos )</formula><p>with a slight abuse of notations. We propose a new probability distribution s(i|r) as a substitute for p(i|r), and s(i|r) should satisfy two requirements: i) s(i|r) should have similar characteristics as p(i|r), which means that most points sampled from s(i|r) should be inside r; ii) s(i|r) should also leave a few points outside r, mainly for the exploration of the surrounding environment. Thus we formulate the probability s(i|r) as:</p><formula xml:id="formula_11">s(i|r) = 1 ? sigmoid( ||p i ? p grid || 2 ? r ? ),<label>(10)</label></formula><p>where sigmoid(x) = (1 + e ?x ) ?1 and ? is the temperature which controls the decay rate of probability. With a small ? , s(i|r) is close to 1 when p i is inside r, and is close to 0 if outside, while near the spherical boundary the sampling probability s(i|r) is between 0 and 1. With s(i|r) as a smooth approximation to p(i|r), we want to compute the gradient of r from the approximated RoI-grid Attention:</p><formula xml:id="formula_12">? r f grid = ? r E i?s(i|r) [W i V i ].<label>(11)</label></formula><p>However, taking the derivative w.r.t. r is still infeasible, since we cannot directly calculate the gradient of a parameterized distribution. The reparameterization trick <ref type="bibr" target="#b11">[12]</ref> offers a possible solution to the problem. The key insight is sampling from a basic distribution and then move the original distribution parameters inside the expectation function as coefficients. The gradient of r can be computed as:</p><formula xml:id="formula_13">? r f grid = E i?U ( ) [? r [s(i, r) ? W i V i ]],<label>(12)</label></formula><p>where s(i, r) is the same as Eq.10, and the theoretical distribution U ( ) = 1 means that the sampling probability is 1 in the whole 3D space. In practical, considering the fact that s(i, r) is close to 0 when r, we apply an approximation and restrict the sampling range U ( ) within a sphere with a radius slightly larger than r, i.e. r + 5? in our experiments. This approximation reduces the computational overhead to the same level as vanilla RoI-grid Attention. Since s(i, r) is a differentiable function w.r.t. r, we are able to compute the gradient of r in a differential manner using Eq.12. The new formulation of RoI-grid Attention can be represented as</p><formula xml:id="formula_14">f grid = i?U ( ) W (? k K i + ? q Q i pos + ? qk Q i pos K i ) (V i + ? v Q i pos ) ? s(i, r).<label>(13)</label></formula><p>Compared with vanilla RoI-grid Attention in Eq.7, a slightly larger sampling range r + 5? is used and an coefficient s(i, r) is added into the original formula, which costs little additional resources. Although several approximations are applied, we found that they didn't hamper the training but boost the performance in our experiments. We further propose the DARP module based on Eq.13. For each pyramid level, a context embedding is obtained by summarizing the information of Points of Interest near this RoI, and then the embedding is utilized to predict the radius r for all grid points in this level. r is further transformed into an coefficient by s(i, r) and participates in the computation of RoI-grid Attention. Since the context embedding captures point cloud information, i.e. density, shape, etc., the predicted r is able to adapt to the environmental changes, and is more robust than the human-defined counterpart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate our Pyramid R-CNN on the commonly used Waymo Open dataset <ref type="bibr" target="#b32">[33]</ref> and the KITTI <ref type="bibr" target="#b6">[7]</ref> dataset. We first introduce the experimental settings in 4.1 and then compare our approach with previous state-of-the-art methods on the Waymo Open dataset in 4.2 and the KITTI dataset in 4.3. Finally, we conduct ablation studies to evaluate the efficacy of each component in 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Waymo Open Dataset. The Waymo Open Dataset contains 1000 sequences in total, including 798 sequences (around 158k point cloud samples) in the training set and 202 sequences (around 40k point cloud samples) in the validation set. The official evaluation metrics are standard 3D mean Average Precision (mAP) and mAP weighted by heading accuracy (mAPH). Both of the two metrics are based on an IoU threshold of 0.7 for vehicles and 0.5 for other categories. The testing samples are split in two ways. The first way is based on the distances of objects to the sensor: 0 ? 30m, 30 ? 50m and &gt; 50m. The second way is according to the difficulty levels: LEVEL 1 for boxes with more than five LiDAR points and LEVEL 2 for boxes with at least one LiDAR point. KITTI Dataset. The KITTI dataset contains 7481 training samples and 7518 test samples, and the training samples are further divided into the train split (3712 samples) and the val split (3769 samples). The official evaluation metric is mean Average Precision (mAP) with a rotated IoU threshold 0.7 for cars. On the test set mAP is calculated with 40 recall positions by the official server. The results on the val set are calculated with 11 recall positions for a fair comparison with other approaches.</p><p>We provide 3 architectures of Pyramid R-CNN, compatible with the point-based, the voxel-based and the pointvoxel-based backbone, respectively. We would like readers to refer to <ref type="bibr" target="#b33">[34]</ref> for the detailed design of those backbones. Pyramid-P. Pyramid R-CNN for Points is built upon the point-based method PointRCNN <ref type="bibr" target="#b29">[30]</ref>. In particular, we replace the Canonical 3D Box Refinement module of PointR-CNN, with our proposed pyramid RoI head in Pyramid R-CNN, and we still use the sampled points in <ref type="bibr" target="#b29">[30]</ref> as Points of Interest. The point cloud backbone and other configurations are kept the same for a fair comparison. Pyramid-V. Pyramid R-CNN for Voxels is built upon the voxel-based method Part-A 2 Net <ref type="bibr" target="#b30">[31]</ref>. Specifically, we replace the 3D sparse convolutional head of Part-A 2 Net, with our proposed pyramid RoI head in Pyramid R-CNN, and we still use the upsampled voxels as Points of Interest. The voxel-based backbone and other configurations are kept the same for a fair comparison. Pyramid-PV. Pyramid R-CNN for Point-Voxels is designed upon the point-voxel-based method PV-RCNN <ref type="bibr" target="#b27">[28]</ref>. In particular, we replace the RoI-grid Pooling module of PV-RCNN, with our proposed pyramid RoI head in Pyramid R-CNN, and we still use the keypoints as Points of Interest. The keypoints encoding process, the 3D sparse convolutional networks and other configurations are kept the same for a fair comparison. Implementation Details. Here we only introduce the architecture of Pyramid-PV on the Waymo Open dataset. The implementations of other models are similar and can be found in the supplementary materials. In RoI-grid Attention, the number of attention heads is set to 4 and each head contains 16 feature channels. In the DARP module, the context embedding is extracted from the neighboring Points of Interest within two spheres with the radius 2.4m and 4.8m. The temperature ? starts from 0.02 and exponentially de- We append another frame following <ref type="bibr" target="#b27">[28]</ref> and use a larger voxel backbone. cays to 0.0001 in the end. The RoI-grid Pyramid consists of 5 levels, with the number of grid points as 6 3 , 4 3 , 4 3 , 4 3 , 1 respectively, and for each pyramid level, a focusing radius r is predicted and shared across all the grid points in this level. The enlarging ratio ? w and ? l are set to 1, 1, 1.5, 2, 4 for the respective level of the RoI-grid Pyramid, and ? h is set to 1 in all pyramid levels. The maximum number of points that participate in RoI-grid Attention for each grid point is set to <ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32</ref> for the corresponding pyramid level.</p><p>Training and Inference Details. Our Pyramid R-CNN is trained from scratch with the ADAM optimizer. On the KITTI dataset, Pyramid-P, Pyramid-V and Pyramid-PV are trained with the same batch size 16, the learning rate 0.01, 0.01, 0.005 respectively for 80 epochs on 8 V100 GPUs. On the Waymo Open dataset, we uniformly sample 20% frames for training and use the full validation set for evaluation following <ref type="bibr" target="#b27">[28]</ref>. Pyramid-P, Pyramid-V and Pyramid-PV are trained with the same batch size 32, the learning rate 0.01 for 40 epochs. The cosine annealing learning rate strategy is adopted for the learning rate decay.</p><p>Other configurations are kept the same as the corresponding baselines <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b27">28]</ref> for a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparisons on the Waymo Open Dataset</head><p>We evaluate the performance of Pyramid R-CNN on the Waymo Open dataset. The validation results in <ref type="table" target="#tab_1">Table 1</ref> show that our Pyramid-P, Pyramid-V and Pyramid-PV significantly outperform the baseline methods with 2.0%, 4.1% and 6.0% mAP gain respectively, and achieves superior mAP on all difficulty levels and all distance ranges, which demonstrates the effectiveness and generalizability of our approach. It is worth noting that Pyramid-V surpasses PV-RCNN by 12.3% mAP in detecting objects that are &gt; 50m, which indicates the adaptability of our approach to the extremely sparse conditions. Our Pyramid-PV outperforms all the previous approaches with a remarkable margin, and achieves the new state-of-the-art performance 76.30% mAP and 67.23% mAP for the LEVEL 1 and LEVEL 2 difficulty. In table 2, our Pyramid-PV ? achieves 81.77% LEVEL 1 mAP, ranks 1 st on the Waymo vehicle detection leaderboard as of March 10th, 2021, and surpasses all the LiDAR-only approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparisons on the KITTI Dataset</head><p>We evaluate our Pyramid R-CNN on the KITTI dataset. The test results in <ref type="table">Table 3</ref> show that our Pyramid-P, Pyramid-V and Pyramid-PV consistently outperform the baseline methods with 4.66%, 2.79% and 0.65% mAP gain respectively on the moderate car class, and Pyramid-PV achieves 82.08% mAP, becoming the new state-of-theart. The validation results in <ref type="table">Table 4</ref> show that Pyramid-P, Pyramid-V and Pyramid-PV improve the baselines by 4.47%, 3.67% and 0.69% mAP on the moderate car class, and 1.06%, 0.07% and 0.14% mAP on the hard car class respectively. We note that the performance gains are mainly from the hard cases, which indicates the adaptability of our approach, and the observations on the KITTI dataset are consistent with those on the Waymo Open dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head><p>The effects of different components. As is shown in dius boosts the performance by 0.37% mAP, which demonstrates the efficacy of the DARP module. The effects of different pyramid configurations. As is shown in <ref type="table">Table 6</ref>, we found that the RoI-grid Pyramid with ? w , ? l &gt; 1 enhances the performance compared with the standard RoI-grid only with ? w , ? l = 1, mainly because placing some grid points outside RoIs encodes richer contexts. The total number of used grid points is 409, which is comparable to 432 grid points used in <ref type="bibr" target="#b27">[28]</ref>.</p><p>Inference speed analysis. We test the inference speed of different frameworks under a single V100 GPU with batch size 1, and obtain the average running speed of all samples in KITTI val split. <ref type="table">Table 7</ref> shows that our models maintain computational efficiency compared to the baselines, and the pyramid RoI head only adds little latency per frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present a general two-stage framework Pyramid R-CNN which can be applied upon diverse backbones. Our framework can handle the sparse and non-uniform distribution problems of point clouds by introducing the pyramid RoI head. For future work, we plan to optimize Pyramid R-CNN for efficient inference. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Approximation in Radius Prediction</head><p>In this section, we explain why s(i|r) is used to approximate p(i|r). As is shown in <ref type="figure" target="#fig_4">Figure 6</ref>, s(i|r) can be viewed as a soft approximation to p(i|r), and the sharpness of the curve s(i|r) is controlled by the temperature ? . When ? approaches 0, s(i|r) is more similar to p(i|r). In this paper, we set the initial ? as 0.02 for exploration, and gradually decrease ? to 0.0001 to obtain a better approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation of the DARP Module</head><p>In this section, we provide the detailed implementation of the Density-Aware Radius Prediction (DARP) module. Inspired by the design of Deformable Convolutions which utilize standard convolutions to predict the deformable offsets, we first use a fixed sphere to aggregate the context embedding and then use this embedding to predict the dynamic radius offset for all grid points in a pyramid level. In particular, for each pyramid level in an RoI-grid Pyramid, we use two spheres centered at the RoI with radius 2.4m and 4.8m for context aggregation. Then the aggregated context embedding is fed into a MLP to predict the dynamic radius offset ?r. A predefined radius r added by the dynamic offset ?r, i.e. r + ?r, is utilized to obtain the coefficient s(i, r + ?r) in Eq.10, and with s(i, r + ?r), the Points of Interest within r + ?r + 5? are selected as ?(r + ?r + 5? ) for the computation of RoI-grid Attention in Eq.13, where ? is the temperature. The predefined r in this paper is set to 0.8, 1.6, 2.4, 3.2, 6.4 for the respective pyramid level. We note that all grid points in a pyramid level share the same r + ?r, and the prediction of ?r adds little computational overhead. It is worth noting that we can easily extend this idea to the settings where each grid point has its individual predicted radius, or we can additionally predict centers for the predicted spheres. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Backbones of Pyramid R-CNN</head><p>In this section, we provide additional information for some backbones of Pyramid R-CNN. We note that other backbones that are not mentioned are directly referred from the official source-code repositories. Pyramid RoI head is kept the same upon all the backbones in this paper.</p><p>Pyramid-P. We re-implement the backbone of PointR-CNN on the Waymo Open dataset. Different from the original version on the KITTI dataset, we set the number of input sampled point clouds to 40k, and the number of downsampled points is set to 18024, 2048, 512, 128 for the respective layer. We note that this modification enlarges the number of kept points, since the number of input point clouds is larger compared with those on the KITTI dataset. Pyramid-P and our re-implemented PointRCNN share the same backbone configurations on the Waymo Open dataset.</p><p>Pyramid-PV ? . In Pyramid-PV ? we implement a larger backbone with the input voxel size as [0.2m, 0.2m, 0.2m]. The backbones of Pyramid-PV ? and vanilla Pyramid-PV (PV-RCNN) are shown in <ref type="figure" target="#fig_5">Figure 7</ref>. Our Pyramid R-CNN is compatible with a small backbone for a fair comparison with baseline methods, or a large backbone to further enhance the detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Qualitative Results</head><p>In this section, we provide the qualitative results on the KITTI dataset in <ref type="figure" target="#fig_6">Figure 8</ref>, and the Waymo Open dataset in <ref type="figure" target="#fig_7">Figure 9</ref>. The figures show that our proposed Pyramid R-CNN can accurately detect 3D objects which are far away and have only a few points.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Statistical results on the KITTI dataset. Blue bars denote the distribution of the number of object points. Orange bars denote the distribution of the number of points gathered by RoIs in Pyramid R-CNN. Our approach can mitigate the sparsity and imbalanced distribution problems of point clouds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>object/context points in (a) (d) object/context points in (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of RoI-grid Attention. RoI-grid Attention introduces learnable gated functions ? to dynamically select the attention components, and it provides a unified formulation that includes the conventional graph and attention operators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Illustration of dynamic radius predicted by the Density-Aware Radius Prediction module. For each RoI, an adaptive focusing radius is learned based on the sparsity conditions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Illustration of p(i|r) and s(i|r).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>3D Backbone of Pyramid-PV and Pyramid-PV ? .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Visualization of detection results on the KITTI dataset. Blue boxes are the ground truth boxes, and red boxes are the boxes predicted by Pyramid-PV.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Visualization of detection results on the Waymo Open dataset. Blue boxes are the ground truth boxes, and red boxes are the boxes predicted by Pyramid-PV.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>surpass most single-The Chinese University of Hong Kong 2 Huawei Noah's Ark Lab 3 HKUST 4 Sun Yat-Sen University</figDesc><table><row><cell>7% 8%</cell><cell>Points in objects Points gathered by RoIs of Pyramid R-CNN</cell></row><row><cell>3% 4% 5% 6% percentage</cell><cell></cell></row><row><cell>2%</cell><cell></cell></row><row><cell>1%</cell><cell></cell></row><row><cell>0%</cell><cell>0 50 100 150 200 250 300 350 400 450 500 the number of points</cell></row></table><note>1? Corresponding author: xdliang328@gmail.com</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison on the Waymo Open Dataset with 202 validation sequences for the vehicle detection. : re-implemented by ourselves with the official code.</figDesc><table><row><cell>Methods</cell><cell>LEVEL 1 3D mAP/mAPH</cell><cell>LEVEL 2 3D mAP/mAPH</cell><cell cols="3">LEVEL 1 3D mAP/mAPH by Distance 0-30m 30-50m 50m-Inf</cell></row><row><cell>PointPillars [14]</cell><cell>63.3/62.7</cell><cell>55.2/54.7</cell><cell>84.9/84.4</cell><cell>59.2/58.6</cell><cell>35.8/35.2</cell></row><row><cell>MVF [45]</cell><cell>62.93/-</cell><cell>-</cell><cell>86.30/-</cell><cell>60.02/-</cell><cell>36.02/-</cell></row><row><cell>Pillar-OD [36]</cell><cell>69.8/-</cell><cell>-</cell><cell>88.5/-</cell><cell>66.5/-</cell><cell>42.9/-</cell></row><row><cell>AFDet [6]</cell><cell>63.69/-</cell><cell>-</cell><cell>87.38/-</cell><cell>62.19/-</cell><cell>29.27/-</cell></row><row><cell>LaserNet [22]</cell><cell>52.1/50.1</cell><cell>-</cell><cell>70.9/68.7</cell><cell>52.9/51.4</cell><cell>29.6/28.6</cell></row><row><cell>CVCNet [3]</cell><cell>65.2/-</cell><cell>-</cell><cell>86.80/-</cell><cell>62.19/-</cell><cell>29.27/-</cell></row><row><cell>StarNet [23]</cell><cell>64.7/56.3</cell><cell>45.5/39.6</cell><cell>83.3/82.4</cell><cell>58.8/53.2</cell><cell>34.3/25.7</cell></row><row><cell>RCD [1]</cell><cell>69.0/68.5</cell><cell>-</cell><cell>87.2/86.8</cell><cell>66.5/66.1</cell><cell>44.5/44.0</cell></row><row><cell>Voxel R-CNN [5]</cell><cell>75.59/-</cell><cell>66.59/-</cell><cell>92.49/-</cell><cell>74.09/-</cell><cell>53.15/-</cell></row><row><cell>PointRCNN [30]</cell><cell>45.05/44.25</cell><cell>37.41/36.74</cell><cell>72.24/71.31</cell><cell>31.21/30.41</cell><cell>23.77/23.15</cell></row><row><cell>Pyramid-P (ours)</cell><cell>47.02/46.58</cell><cell>39.10/38.76</cell><cell>74.24/73.78</cell><cell>32.49/31.96</cell><cell>25.68/25.24</cell></row><row><cell>Part-A 2 Net [31]</cell><cell>71.69/71.16</cell><cell>64.21/63.70</cell><cell>91.83/91.37</cell><cell>69.99/69.37</cell><cell>46.26/45.41</cell></row><row><cell>Pyramid-V (ours)</cell><cell>75.83/75.29</cell><cell>66.77/66.28</cell><cell>92.63/92.20</cell><cell>74.46/73.84</cell><cell>53.40/52.44</cell></row><row><cell>PV-RCNN [28]</cell><cell>70.3/69.7</cell><cell>65.4/64.8</cell><cell>91.9/91.3</cell><cell>69.2/68.5</cell><cell>42.2/41.3</cell></row><row><cell>Pyramid-PV (ours)</cell><cell>76.30/75.68</cell><cell>67.23/66.68</cell><cell>92.67/92.20</cell><cell>74.91/74.21</cell><cell>54.54/53.45</cell></row><row><cell>Methods</cell><cell>LEVEL 1 3D mAP/mAPH</cell><cell>LEVEL 2 3D mAP/mAPH</cell><cell cols="3">LEVEL 1 3D mAP/mAPH by Distance 0-30m 30-50m 50m-Inf</cell></row><row><cell>CenterPoint [42]</cell><cell>81.05/80.59</cell><cell>73.42/72.99</cell><cell>92.52/92.13</cell><cell>79.94/79.43</cell><cell>61.06/60,42</cell></row><row><cell>PV-RCNN [28]</cell><cell>81.06/80.57</cell><cell>73.69/73.23</cell><cell>93.40/92.98</cell><cell>80.12/79.57</cell><cell>61.22/60.47</cell></row><row><cell>Pyramid-PV  ? (ours)</cell><cell>81.77/81.32</cell><cell>74.87/74.43</cell><cell>93.19/92.80</cell><cell>80.53/80.04</cell><cell>64.55/63.84</cell></row></table><note>Table 2. Performance comparison on the Waymo Open Dataset test leaderboard for the vehicle detection. : test submissions are the modified version of original architectures. ?:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Performance comparison on the KITTI test set with AP calculated by 40 recall positions for the car category. R+L denotes the methods that combines RGB data and point clouds. L denotes LiDAR-only approaches. Performance comparison on the KITTI val split with AP calculated by 11 recall positions for the car category.</figDesc><table><row><cell>Methods</cell><cell>Modality</cell><cell cols="2">AP 3D (%) Easy Mod. Hard</cell></row><row><cell>MV3D [4]</cell><cell>R+L</cell><cell cols="2">74.97 63.63 54.00</cell></row><row><cell>AVOD-FPN [13]</cell><cell>R+L</cell><cell cols="2">83.07 71.76 65.73</cell></row><row><cell>F-PointNet [25]</cell><cell>R+L</cell><cell cols="2">82.19 69.79 60.59</cell></row><row><cell>MMF [16]</cell><cell>R+L</cell><cell cols="2">88.40 77.43 70.22</cell></row><row><cell>3D-CVF [43]</cell><cell>R+L</cell><cell cols="2">89.20 80.05 73.11</cell></row><row><cell>CLOCs [24]</cell><cell>R+L</cell><cell cols="2">88.94 80.67 77.15</cell></row><row><cell>ContFuse [17]</cell><cell>R+L</cell><cell cols="2">83.68 68.78 61.67</cell></row><row><cell>VoxelNet [46]</cell><cell>L</cell><cell cols="2">77.47 65.11 57.73</cell></row><row><cell>PointPillars [14]</cell><cell>L</cell><cell cols="2">82.58 74.31 68.99</cell></row><row><cell>SECOND [38]</cell><cell>L</cell><cell cols="2">84.65 75.96 68.71</cell></row><row><cell>STD [40]</cell><cell>L</cell><cell cols="2">87.95 79.71 75.09</cell></row><row><cell>Patches [15]</cell><cell>L</cell><cell cols="2">88.67 77.20 71.82</cell></row><row><cell>3DSSD [39]</cell><cell>L</cell><cell cols="2">88.36 79.57 74.55</cell></row><row><cell>SA-SSD [10]</cell><cell>L</cell><cell cols="2">88.75 79.79 74.16</cell></row><row><cell>TANet [19]</cell><cell>L</cell><cell cols="2">85.94 75.76 68.32</cell></row><row><cell>Voxel R-CNN [5]</cell><cell>L</cell><cell cols="2">90.90 81.62 77.06</cell></row><row><cell>HVNet [41]</cell><cell>L</cell><cell cols="2">87.21 77.58 71.79</cell></row><row><cell>PointGNN [32]</cell><cell>L</cell><cell cols="2">88.33 79.47 72.29</cell></row><row><cell>PointRCNN [30]</cell><cell>L</cell><cell cols="2">86.96 75.64 70.70</cell></row><row><cell>Pyramid-P (ours)</cell><cell>L</cell><cell cols="2">87.03 80.30 76.48</cell></row><row><cell>Part-A 2 Net [31]</cell><cell>L</cell><cell cols="2">87.81 78.49 73.51</cell></row><row><cell>Pyramid-V (ours)</cell><cell>L</cell><cell cols="2">87.06 81.28 76.85</cell></row><row><cell>PV-RCNN [28]</cell><cell>L</cell><cell cols="2">90.25 81.43 76.82</cell></row><row><cell>Pyramid-PV (ours)</cell><cell>L</cell><cell cols="2">88.39 82.08 77.49</cell></row><row><cell>Methods</cell><cell>Easy</cell><cell>AP 3D (%) Mod.</cell><cell>Hard</cell></row><row><cell>PointRCNN [30]</cell><cell>88.88</cell><cell>78.63</cell><cell>77.38</cell></row><row><cell>Pyramid-P (ours)</cell><cell>88.47</cell><cell>83.10</cell><cell>78.44</cell></row><row><cell>Part-A 2 Net [31]</cell><cell>89.47</cell><cell>79.47</cell><cell>78.54</cell></row><row><cell>Pyramid-V (ours)</cell><cell>88.44</cell><cell>83.14</cell><cell>78.61</cell></row><row><cell>PV-RCNN [28]</cell><cell>89.35</cell><cell>83.69</cell><cell>78.70</cell></row><row><cell>Pyramid-PV (ours)</cell><cell>89.37</cell><cell>84.38</cell><cell>78.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 ,Table 5 .Table 7 .</head><label>557</label><figDesc>on the Waymo validation set, the RoI-grid Pyramid of the Pyramid-PV model improves over the baseline by 1.20% mAP, mainly because the RoI-grid Pyramid is able to capture large context information which benefits the detection of the hard cases. Based on the RoI-grid Pyramid, replacing RoI-grid Pooling with RoI-grid Attention boosts the performance by 0.51% mAP, which indicates that RoIgrid Attention is a more effective operation than RoI-grid Pooling. Using the adaptive radius r instead of the fixed ra-Methods R.P. D.A.R.P. R.A. LEVEL 1 mAP Effects of different components in Pyramid-PV on the Waymo dataset. R.P.: the RoI-grid Pyramid. D.A.R.P.: the Density-Aware Radius Prediction module. R.A.: RoI-grid Attention. : re-implemented by ourselves with the official code. Comparisons on the inference speeds of different detection models on the KITTI dataset.</figDesc><table><row><cell>PV-RCNN</cell><cell></cell><cell></cell><cell></cell><cell>70.30</cell></row><row><cell>PV-RCNN (a) (b) (c) (d)</cell><cell>? ? ? ?</cell><cell>? ?</cell><cell>? ?</cell><cell>74.06 75.26 75.63 75.77 76.30</cell></row><row><cell>Methods</cell><cell>grid size</cell><cell></cell><cell>? w , ? l</cell><cell>LEVEL 1 mAP</cell></row><row><cell>PV-RCNN</cell><cell>[6, 6]</cell><cell></cell><cell>[1, 1]</cell><cell>74.06</cell></row><row><cell>(a)</cell><cell>[6,4,4]</cell><cell></cell><cell>[1,1,2]</cell><cell>74.55</cell></row><row><cell>(b)</cell><cell>[6,4,4,4]</cell><cell></cell><cell>[1,1,2,4]</cell><cell>74.71</cell></row><row><cell>(c)</cell><cell cols="3">[6,4,4,4,1] [1,1,1.5,2,4]</cell><cell>75.26</cell></row><row><cell cols="5">Table 6. Effects of different RoI pyramids in Pyramid-PV on the</cell></row><row><cell cols="5">Waymo dataset. Each element in [?] stands for the respective pa-</cell></row><row><cell cols="2">rameter of a pyramid level.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell></cell><cell></cell><cell cols="2">Inference speed (Hz)</cell></row><row><cell cols="2">PointRCNN [30]</cell><cell></cell><cell></cell><cell>10.08</cell></row><row><cell cols="2">Pyramid-P (ours)</cell><cell></cell><cell></cell><cell>8.92</cell></row><row><cell cols="2">Part-A 2 Net [31]</cell><cell></cell><cell></cell><cell>11.75</cell></row><row><cell cols="2">Pyramid-V (ours)</cell><cell></cell><cell></cell><cell>9.68</cell></row><row><cell cols="2">PV-RCNN [28]</cell><cell></cell><cell></cell><cell>9.25</cell></row><row><cell cols="2">Pyramid-PV (ours)</cell><cell></cell><cell></cell><cell>7.86</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Range conditioned dilated convolutions for scale invariant 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.09927</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Every view counts: Cross-view consistency in 3d object detection with hybrid-cylindrical-spherical voxelization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernest</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1907" to="1915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Voxel r-cnn: Towards high performance voxel-based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15712</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runzhou</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuangzhuang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12671</idno>
		<title level="m">Afdet: Anchor free one stage 3d object detection</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset. The International</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Structure aware single-stage 3d object detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11873" to="11882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungwook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="12697" to="12705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Lehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Mitterecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hofmarcher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04093</idno>
		<title level="m">Bernhard Nessler, and Sepp Hochreiter. Patch refinement-localized 3d object detection</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-task multi-sensor fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7345" to="7353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep continuous fusion for multi-sensor 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="641" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tanet: Robust 3d object detection from point clouds with triple attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengteng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruolan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11677" to="11684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">One million scenes for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiageng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minzhe</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxue</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yamin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.11037</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Once dataset. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Interpolated convolutional networks for 3d point cloud understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiageng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1578" to="1587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lasernet: An efficient probabilistic 3d object detector for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Laddha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Kee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">K</forename><surname>Vallespi-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wellington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12677" to="12686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Caine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ouais</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.11069</idno>
		<title level="m">Targeted computation for object detection in point clouds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Clocs: Cameralidar object candidates fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayder</forename><surname>Radha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.00784</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgbd data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Point-net++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02413</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster r-cnn: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pv-rcnn: Pointvoxel feature set abstraction for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="10529" to="10538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Pv-rcnn++: Point-voxel feature set abstraction with local vector representation for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.00463</idno>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Point-gnn: Graph neural network for 3d object detection in a point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><surname>Rajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1711" to="1719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scalability in perception for autonomous driving: Waymo open dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xerxes</forename><surname>Dotiwalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Chouard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijaysai</forename><surname>Patnaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Caine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2446" to="2454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Openpcdet: An opensource toolbox for 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Openpcdet</forename><surname>Development Team</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/OpenPCDet,2020.6" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Pillar-based object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Solomon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.10323</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Transactions On Graphics (tog)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Point-based 3d single stage object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zetong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="11040" to="11048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Std: Sparse-to-dense 3d object detector for point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zetong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hvnet: Hybrid voxel network for lidar based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangjie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongyi</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1631" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11275</idno>
		<title level="m">Centerbased 3d object detection and tracking</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">3d-cvf: Generating joint camera and lidar features using cross-view spatial feature fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Hyeok</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yecheol</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><forename type="middle">Song</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun Won</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12636</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09164,2020.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">Point transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Jiquan Ngiam, and Vijay Vasudevan. End-to-end multi-view fusion for 3d object detection in lidar point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Guo</surname></persName>
		</author>
		<idno>PMLR, 2020. 7</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<biblScope unit="page" from="923" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
