<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">OTA: Optimal Transport Assignment for Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ge</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Waseda University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
							<email>liusongtao@megvii.com</email>
							<affiliation key="aff1">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
							<email>lizeming@megvii.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osamu</forename><surname>Yoshie</surname></persName>
							<email>yoshie@waseda.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Waseda University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
							<email>sunjian@megvii.com</email>
							<affiliation key="aff1">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">OTA: Optimal Transport Assignment for Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in label assignment in object detection mainly seek to independently define positive/negative training samples for each ground-truth (gt) object. In this paper, we innovatively revisit the label assignment from a global perspective and propose to formulate the assigning procedure as an Optimal Transport (OT) problem -a well-studied topic in Optimization Theory. Concretely, we define the unit transportation cost between each demander (anchor) and supplier (gt) pair as the weighted summation of their classification and regression losses. After formulation, finding the best assignment solution is converted to solve the optimal transport plan at minimal transportation costs, which can be solved via Sinkhorn-Knopp Iteration. On COCO, a single FCOS-ResNet-50 detector equipped with Optimal Transport Assignment (OTA) can reach 40.7% mAP under 1? scheduler, outperforming all other existing assigning methods. Extensive experiments conducted on COCO and CrowdHuman further validate the effectiveness of our proposed OTA, especially its superiority in crowd scenarios. The code is available at https: //github.com/Megvii-BaseDetection/OTA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Current CNN-based object detectors <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">36]</ref> perform a dense prediction manner by predicting the classification (cls) labels and regression (reg) offsets for a set of pre-defined anchors <ref type="bibr" target="#b0">1</ref> . To train the detector, defining cls and reg targets for each anchor is a necessary procedure, which is called label assignment in object detection.</p><p>Classical label assigning strategies commonly adopt predefined rules to match the ground-truth (gt) object or background for each anchor. For example, RetinaNet <ref type="bibr" target="#b20">[21]</ref> adopts Intersection-over-Union (IoU) as its thresholding criterion GT2 GT2 GT1 GT1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT BBoxes</head><p>A Set of Ambiguous Anchor Points <ref type="figure">Figure 1</ref>. An illustration of ambiguous anchor points in object detection. Red dots show some of the ambiguous anchors in two sample images. Currently, the assignment of these ambiguous anchors is heavily based on hand-crafted rules.</p><p>for pos/neg anchors division. Anchor-free detectors like FCOS <ref type="bibr" target="#b38">[38]</ref> treat the anchors within the center/bbox region of any gt object as the corresponding positives. Such static strategies ignore a fact that for objects with different sizes, shapes or occlusion condition, the appropriate positive/negative (pos/neg) division boundaries may vary. Motivated by this, many dynamic assignment strategies have been proposed. ATSS <ref type="bibr" target="#b47">[47]</ref> proposes to set the division boundary for each gt based on statistical characteristics. Other recent advances <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b15">16]</ref> suggest that the predicted confidence scores of each anchor could be a proper indicator to design dynamic assigning strategies, i.e., high confidence anchors can be easily learned by the networks and thus be assigned to the related gt, while anchors with uncertain predictions should be considered as negatives. Those strategies enable the detector to dynamically choose positive anchors for each individual gt object and achieve state-of-the-art performance.</p><p>However, independently assigning pos/neg samples for each gt without context could be sub-optimal, just like the lack of context may lead to improper prediction. When dealing with ambiguous anchors (i.e., anchors that are qualified as positive samples for multiple gts simultaneously as seen in <ref type="figure">Fig. 1.)</ref>, existing assignment strategies are heavily based on hand-crafted rules (e.g., Min Area <ref type="bibr" target="#b38">[38]</ref>, Max IoU <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b47">47]</ref>.). We argue that assigning ambiguous an-chors to any gt (or background) may introduce harmful gradients w.r.t. other gts. Hence the assignment for ambiguous anchors is non-trivial and requires further information beyond the local view. Thus a better assigning strategy should get rid of the convention of pursuing optimal assignment for each gt independently and turn to the ideology of global optimum, in other words, finding the global high confidence assignment for all gts in an image.</p><p>DeTR <ref type="bibr" target="#b2">[3]</ref> is the first work that attempts to consider label assignment from global view. It replaces the detection head with transformer layers <ref type="bibr" target="#b39">[39]</ref> and considers one-to-one assignment using the Hungarian algorithm that matches only one query for each gt with global minimum loss. However, for the CNN based detectors, as the networks often produce correlated scores to the neighboring regions around the object, each gt is assigned to many anchors (i.e., oneto-many), which also benefits to training efficiency. In this one-to-many manner, it remains intact to assign labels with a global view.</p><p>To achieve the global optimal assigning result under the one-to-many situation, we propose to formulate label assignment as an Optimal Transport (OT) problem -a special form of Linear Programming (LP) in Optimization Theory. Specifically, we define each gt as a supplier who supplies a certain number of labels, and define each anchor as a demander who needs one unit label. If an anchor receives sufficient amount of positive label from a certain gt, this anchor becomes one positive anchor for that gt . In this context, the number of positive labels each gt supplies can be interpreted as "how many positive anchors that gt needs for better convergence during the training process". The unit transportation cost between each anchor-gt pair is defined as the weighted summation of their pair-wise cls and reg losses. Furthermore, as being negative should also be considered for each anchor, we introduce another supplierbackground who supplies negative labels to make up the rest of labels in need. The cost between background and a certain anchor is defined as their pair-wise classification loss only. After formulation, finding the best assignment solution is converted to solve the optimal transport plan, which can be quickly and efficiently solved by the off-the-shelf Sinkhorn-Knopp Iteration <ref type="bibr" target="#b4">[5]</ref>. We name such an assigning strategy as Optimal Transport Assignment (OTA).</p><p>Comprehensive experiments are carried out on MS COCO <ref type="bibr" target="#b21">[22]</ref> benchmark, and significant improvements from OTA demonstrate its advantage. OTA also achieves the SOTA performance among one-stage detectors on a crowded pedestrian detection dataset named CrowdHuman <ref type="bibr" target="#b35">[35]</ref>, showing OTA's generalization ability on different detection benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Fixed Label Assignment</head><p>Determining which gt (or background) should each anchor been assigned to is a necessary procedure before training object detectors. Anchor-based detectors usually adopt IoU at a certain threshold as the assigning criterion. For example, RPN in Faster R-CNN <ref type="bibr" target="#b33">[33]</ref> uses 0.7 and 0.3 as the positive and negative thresholds, respectively. When training the R-CNN module, the IoU threshold for pos/neg division is changed to 0.5. IoU based label assignment is proved effective and soon been adopted by many Faster R-CNN's variants like <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b37">37]</ref>, as well as many one-stage detectors like <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>Recently, anchor-free detectors have drawn much attention because of their concision and high computational efficiency. Without anchor box, FCOS <ref type="bibr" target="#b38">[38]</ref>, Foveabox <ref type="bibr" target="#b16">[17]</ref> and their precursors <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b46">46]</ref> directly assign anchor points around the center of objects as positive samples, showing promising detection performance. Another stream of anchor-free detectors <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b3">4]</ref> view each object as a single or a set of key-points. They share distinct characteristics from other detectors, hence will not be further discussed in our paper.</p><p>Although detectors mentioned above are different in many aspects, as for label assignment, they all adopt a single fixed assigning criterion (e.g., a fixed region of the center area or IoU threshold) for objects of various sizes, shapes, and categories, etc, which may lead to sub-optimal assigning results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Dynamic Label Assignment</head><p>Many recent works try to make the label assigning procedure more adaptive, aiming to further improve the detection performance. Instead of using pre-defined anchors, GuidedAnchoring <ref type="bibr" target="#b40">[40]</ref> generates anchors based on an anchor-free mechanism to better fit the distribution of various objects. MetaAnchor <ref type="bibr" target="#b44">[44]</ref> proposes an anchor generation function to learn dynamic anchors from the arbitrary customized prior boxes. NoisyAnchors <ref type="bibr" target="#b18">[19]</ref> proposes softlabel and anchor re-weighting mechanisms based on classification and localization losses. FreeAnchor <ref type="bibr" target="#b48">[48]</ref> constructs top-k anchor candidates for each gt based on IoU and then proposes a detection-customized likelihood to perform pos/neg division within each candidate set. ATSS <ref type="bibr" target="#b47">[47]</ref> proposes an adaptive sample selection strategy that adopts mean+std of IoU values from a set of closest anchors for each gt as a pos/neg threshold. PAA <ref type="bibr" target="#b15">[16]</ref> assumes that the distribution of joint loss for positive and negative samples follows the Gaussian distribution. Hence it uses GMM to fit the distribution of positive and negative samples, and then use the center of positive sample distribution as the pos/neg division boundary. AutoAssign <ref type="bibr" target="#b51">[51]</ref> tackles label</p><formula xml:id="formula_0">!* ? ? ? GT 0 GT 1 GT 2 ? BG ? Transportation Cost Plane ? ? ? GT 0 GT 1 GT 2 ? BG ?</formula><p>Optimal Assigning Plan !* </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sinkhorn-Knopp Iteration</head><p>Assigning Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cost Matrix</head><p>Anchors across FPN Levels <ref type="figure">Figure 2</ref>. An illustration of Optimal Transport Assignment. Cost Matrix is composed of the pair-wise cls and reg losses between each anchor-gt pair. The goal of finding the best label assigning is converted to solve the best transporting plan which transports the labels from suppliers (i.e. GT and BG) to demanders (i.e. anchors) at a minimal transportation cost via Sinkhorn-Knopp Iteration.</p><p>assignment in a fully data-driven manner by automatically determine the positives/negatives in both spatial and scale dimensions. These methods explore the optimal assigning strategy for individual objects, while failing to consider context information from a global perspective. DeTR <ref type="bibr" target="#b2">[3]</ref> examines the idea of global optimal matching. But the Hungarian algorithm they adopted can only work in a one-to-one assignment manner. So far, for the CNN based detectors in one-tomany scenarios, a global optimal assigning strategy remains uncharted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we first revisit the definition of the Optimal Transport problem and then demonstrate how we formulate the label assignment in object detection into an OT problem. We also introduce two advanced designs which we suggest adopting to make the best use of OTA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Optimal Transport</head><p>The Optimal Transport (OT) describes the following problem: supposing there are m suppliers and n demanders in a certain area. The i-th supplier holds s i units of goods while the j-th demander needs d j units of goods. Transporting cost for each unit of good from supplier i to demander j is denoted by c ij . The goal of OT problem is to find a transportation plan ? * = {? i,j |i = 1, 2, ...m, j = 1, 2, ...n}, according to which all goods from suppliers can be transported to demanders at a minimal transportation cost:</p><formula xml:id="formula_1">min ? m i=1 n j=1 c ij ? ij . s.t. m i=1 ? ij = d j , n j=1 ? ij = s i , m i=1 s i = n j=1 d j , ? ij ? 0, i = 1, 2, ...m, j = 1, 2, ...n.<label>(1)</label></formula><p>This is a linear program which can be solved in polynomial time. In our case, however, the resulting linear program is large, involving the square of feature dimensions with anchors in all scales. We thus address this issue by a fast iterative solution, named Sinkhorn-Knopp <ref type="bibr" target="#b4">[5]</ref> (described in Appendix. A.1.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">OT for Label Assignment</head><p>In the context of object detection, supposing there are m gt targets and n anchors (across all FPN <ref type="bibr" target="#b19">[20]</ref> levels) for an input image I, we view each gt as a supplier who holds k units of positive labels (i.e., s i = k, i = 1, 2, ..., m), and each anchor as a demander who needs one unit of label (i.e., d j = 1, j = 1, 2, ..., n). The cost c f g for transporting one unit of positive label from gt i to anchor a j is defined as the weighted summation of their cls and reg losses:</p><formula xml:id="formula_2">c f g ij =L cls (P cls j (?), G cls i )+ ?L reg (P box j (?), G box i ),<label>(2)</label></formula><p>where ? stands for model's parameters. P cls j and P box j denote predicted cls score and bounding box for a j . G cls i and G box i denote ground truth class and bounding box for gt i. L cls and L reg stand for cross entropy loss and IoU Loss <ref type="bibr" target="#b46">[46]</ref>. One can also replace these two losses with Focal Loss <ref type="bibr" target="#b20">[21]</ref> and GIoU <ref type="bibr" target="#b34">[34]</ref>/SmoothL1 Loss <ref type="bibr" target="#b10">[11]</ref>. ? is the balanced coefficient.</p><p>Besides positive assigning, a large set of anchors are treated as negative samples during training. As the optimal transportation involves all anchors, we introduce another supplier -background, who only provides negative labels. In a standard OT problem, the total supply must be equal to the total demand. We thus set the number of negative labels that background can supply as n ? m ? k. The cost for transporting one unit of negative label from background to a j is defined as:</p><formula xml:id="formula_3">c bg j = L cls (P cls j (?), ?),<label>(3)</label></formula><p>where ? means the background class. Concatenating this c bg ? R 1?n to the last row of c f g ? R m?n , we can get the complete form of the cost matrix c ? R (m+1)?n . The supplying vector s should be correspondingly updated as:</p><formula xml:id="formula_4">s i = k, if i ? m n ? m ? k, if i = m + 1.<label>(4)</label></formula><p>As we already have the cost matrix c, supplying vector s ? R m+1 and demanding vector d ? R n , the optimal transportation plan ? * ? R (m+1)?n can be obtained by solving this OT problem via the off-the-shelf Sinkhorn-Knopp Iteration <ref type="bibr" target="#b4">[5]</ref>. After getting ? * , one can decode the corresponding label assigning solution by assigning each anchor to the supplier who transports the largest amount of labels to them. The subsequent processes (e.g., calculating losses based on assigning result, back-propagation) are exactly the same as in FCOS <ref type="bibr" target="#b38">[38]</ref> and ATSS <ref type="bibr" target="#b47">[47]</ref>. Noted that the optimization process of OT problem only contains some matrix multiplications which can be accelerated by GPU devices, hence OTA only increases the total training time by less than 20% and is totally cost-free in testing phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Advanced Designs</head><p>Center Prior. Previous works <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b48">48]</ref> only select positive anchors from the center region of objects with limited areas, called Center Prior. This is because they suffer from either a large number of ambiguous anchors or poor Algorithm 1 Optimal Transport Assignment (OTA) Input:</p><p>I is an input image A is a set of anchors G is the gt annotations for objects in image I ? is the regularization intensity in Sinkhorn-Knopp Iter.</p><p>T is the number of iterations in Sinkhorn-Knopp Iter. ? is the balanced coefficient in Eq. 2 Output:</p><p>? * is the optimal assigning plan </p><formula xml:id="formula_5">1: m ? |G|, n ? |A| 2: P cls , P box ? Forward(I,A) 3: s i (i = 1, 2, ..., m) ? Dynamic k Estimation 4: s m+1 ? n ? m i=1 s i 5: d j (j = 1, 2, ..., n) ?</formula><formula xml:id="formula_6">u t+1 , v t+1 ? SinkhornIter(c, u t , v t , s, d)</formula><p>15: compute optimal assigning plan ? * according to Eq. 11 <ref type="bibr">16:</ref> return ? * statistics in the subsequent process. Instead of relying on statistical characteristics, our OTA is based on global optimization methodology and thus is naturally resistant to these two issues. Theoretically, OTA can assign any anchor within the region of gts' boxes as a positive sample. However, for general detection datasets like COCO, we find the Center Prior still benefit the training of OTA. Forcing detectors focus on potential positive areas ( i.e., center areas) can help stabilize the training process, especially in the early stage of training, which will lead to a better final performance. Hence, we impose a Center Prior to the cost matrix. For each gt, we select r 2 closest anchors from each FPN level according to the center distance between anchors and gts 2 . As for anchors not in the r 2 closest list, their corresponding entries in the cost matrix c will be subject to an additional constant cost to reduce the possibility they are assigned as positive samples during the training stage. In Sec. 4, we will demonstrate that although OTA adopts a certain degree of Center Prior like other works <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b48">48]</ref> do, OTA consistently outperforms counterparts by a large margin when r is set to a large value (i.e., large number of potential positive anchors as well as more ambiguous anchors).</p><p>Dynamic k Estimation. Intuitively, the appropriate number of positive anchors for each gt (i.e., s i in Sec. 3.1) should be different and based on many factors like objects' sizes, scales, and occlusion conditions, etc. As it is hard to directly model a mapping function from these factors to the positive anchor's number, we propose a simple but effective method to roughly estimate the appropriate number of positive anchors for each gt based on the IoU values between predicted bounding boxes and gts. Specifically, for each gt, we select the top q predictions according to IoU values. These IoU values are summed up to represent this gt's estimated number of positive anchors. We name this method as Dynamic k Estimation. Such an estimation method is based on the following intuition: The appropriate number of positive anchors for a certain gt should be positively correlated with the number of anchors that well-regress this gt. In Sec. 4, we present a detailed comparison between the fixed k and Dynamic k Estimation strategies.</p><p>A toy visualization of OTA is shown in <ref type="figure">Fig. 2</ref>. We also describe the OTA's completed procedure including Center Prior and Dynamic k Estimation in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we conduct extensive experiments on MS COCO 2017 <ref type="bibr" target="#b21">[22]</ref> which contains about 118k, 5k and 20k images for train, val, and test-dev sets, respectively. For ablation studies, we train detectors on train set and report the performance on val set. Comparisons with other methods are conducted on test-dev set. We also compare OTA with other methods on CrowdHuman <ref type="bibr" target="#b35">[35]</ref> validation set to demonstrate the superiority of OTA in crowd scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>If not specified, we use ResNet-50 <ref type="bibr" target="#b12">[13]</ref> pre-trained on ImageNet <ref type="bibr" target="#b5">[6]</ref> with FPN <ref type="bibr" target="#b19">[20]</ref> as our default backbone. Most of experiments are trained with 90k iterations which is denoted as "1?". The initial learning rate is 0.01 and is decayed by a factor of 10 after 60k and 80k iterations. Minibatch size is set to 16. Following the common practice, the model is trained with SGD [1] on 8 GPUs.</p><p>OTA can be adopted in both anchor-based and anchorfree detectors, the following experiments are mainly conducted on FCOS <ref type="bibr" target="#b38">[38]</ref> because of its simplicity. We adopt Focal Loss and IoU Loss as L cls and L reg that make up the cost matrix. ? in Eq. 2 is set to 1.5. For back-propagation, the regression loss is replaced by GIoU Loss and is reweighted by a factor of 2. IoU Branch is first introduced in YOLOv1 <ref type="bibr" target="#b30">[30]</ref> and proved effective in modern one-stage object detectors by PAA <ref type="bibr" target="#b15">[16]</ref>. We also adopt IoU Branch </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Studies and Analysis</head><p>Effects of Individual Components. We verify the effectiveness of each component in our proposed methods. For fair comparisons, all detectors' regression losses are multiplied by 2, which is known as a useful trick to boost the AP at high IoU thresholds <ref type="bibr" target="#b27">[28]</ref>. As seen in <ref type="table" target="#tab_0">Table 1</ref>, when no auxiliary branch is adopted, OTA outperforms FCOS by 0.9% AP (39.2% v.s.38.3%). This gap almost remains the same after adding IoU branch to both of them (39.5% v.s. 40.3% and 38.8% v.s. 39.6% with or without center prior, respectively). Finally, dynamic k pushes AP to a new state-of-the-art 40.7%. In the whole paper, we emphasize that OTA can be applied to both anchorbased and anchor-free detectors. Hence we also adopt OTA on RetinaNet <ref type="bibr" target="#b20">[21]</ref> with only one square anchor perlocation across feature maps. As shown in <ref type="table" target="#tab_0">Table 1</ref>, the AP values of OTA-FCOS and OTA-RetinaNet are exactly the same, demonstrating OTA's applicability on both anchorbased and anchor-free detectors.</p><p>Effects of r. The values of radius r for Center Prior serve to control the number of candidate anchors for each gt. If adopting a small r, only anchors near objects' centers could be assigned as positives, helping the optimization process focus on regions that are more likely to be informative. As r increases, the number of candidates also quadratically increases, leading to potential instability in the optimization process. For example, when r is set to 3, 5 or 7, their corresponding numbers of candidate anchors are 45, 125 and 245 3 , respectively. We study behaviors of ATSS <ref type="bibr" target="#b47">[47]</ref>, ATSS PAA OTA PAA <ref type="bibr" target="#b15">[16]</ref>, and OTA under different values of r in <ref type="table" target="#tab_1">Table 2</ref>. OTA achieves the best performance (40.7% AP) when r is set to 5. When r is set to 3 as ATSS and PAA do, OTA also achieves 40.6% AP, indicating that most potential positive anchors are near the center of objects on COCO. While r is set to 7, the performance only slightly drops 0.3%, showing that OTA is insensitive to the hyper-parameter r.</p><p>Ambiguous Anchors Handling. Most existing dynamic label assigning methods <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b48">48]</ref>  OTA, and evaluate their corresponding performance under different r in <ref type="table" target="#tab_1">Table 2</ref>. Noted that the optimal assigning plan in OTA is continuous, hence we define anchor a j as an ambiguous anchor if max ? * j &lt; 0.9. <ref type="table" target="#tab_1">Table 2</ref> shows that for ATSS, the number of ambiguous anchors greatly increases as r varies from 3 to 7. Its performance correspondingly drops from 39.4% to 37.2%. For PAA, the number of ambiguous anchors is less sensitive to r, but its performance still drops 0.8%, indicating that Max IoU adopted by PAA is not an ideal prior to ambiguous anchors. In OTA, when multiple gts tend to transport positive labels to the same anchor, the OT algorithm will automatically resolve their conflicts based on the principle of minimum global costs. Hence the number of ambiguous anchor for OTA remains low and barely increases as r increases from 3 to 7. The corresponding performance is also stable.</p><p>Further, we manually assign the ambiguous anchors based on hand-crafted rules before performing OTA. In this case, OTA is only in charge of pos/neg samples division. <ref type="table" target="#tab_2">Table 3</ref> shows that such a combination of hand-crafted rules and OTA decreases the AP by 0.7% and 0.4%, respectively. Finally, we visualize some assigning results in <ref type="figure" target="#fig_2">Fig. 3</ref>. Red arrows and dashed ovals highlight the ambiguous regions (i.e., overlaps between different f gs or junctions between f gs and bg). Suffering from the lack of context and global information, ATSS and PAA perform poorly in such regions, leading to sub-optimal detection performances. Conversely, OTA assigns much less positive anchors in such regions, which we believe is a desired behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>AP AP 50 AP 75 Min Area <ref type="bibr" target="#b38">[38]</ref>  Effects of k. Before performing Sinkhorn-Knopp Iteration, we need to define how many positive labels can each gt supply. This value also represents how many anchors every gt needs for better convergence. A naive way is setting k to a constant value for all gts. We try different values of k from 1 to 20. As seen in <ref type="table" target="#tab_3">Table 4</ref>, among all different values, k=10 and k=12 achieve the best performances. As k increases from 10 to 20, the possibility that an anchor is suitable as a positive sample for two close targets at the same time also increases, but there is no obvious performance drop (0.2%) according to <ref type="table" target="#tab_3">Table 4</ref> which proves OTA's superiority in handling potential ambiguity. When k=1, OTA becomes a one-to-one assigning strategy, the same as in DeTR. The poor performance tells us that achieving competitive performance via one-to-one assignment under the 1? scheduler remains challenging, unless an auxiliary oneto-many supervision is added <ref type="bibr" target="#b41">[41]</ref>. Fixing k strategy assumes every gt has the same number of appropriate positive anchors. However, we believe that this number for each gt should vary and may be affected by many factors like objects' sizes, spatial attitudes, and occlusion conditions, etc. Hence we adopt the Dynamic k Estimation proposed in Sec 3.3 and compare its performance to the fixed k strategy. Results in <ref type="table" target="#tab_3">Table 4</ref> shows that dynamic k surpasses the best performance of fixed k by 0.4% AP, validating our point and the effectiveness of Dynamic k Estimation strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-art Methods.</head><p>We compare our final models with other state-of-the-art one-stage detectors on MS COCO test-dev. Following previous works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b38">38]</ref>, we randomly scale the shorter side of images in the range from 640 to 800. Besides, we double the total number of iterations to 180K with the learning rate change points scaled proportionally. Other settings are consistent with <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b38">38]</ref>.</p><p>As shown in <ref type="table">Table 5</ref>, our method with ResNet-101-FPN achieves 45.3% AP, outperforms all other methods with the same backbone including ATSS (43.6% AP), AutoAssign (44.5% AP) and PAA (44.6% AP). Noted that for PAA, we remove the score voting procedure for fair comparisons between different label assigning strategies. With ResNeXt-64x4d-101-FPN <ref type="bibr" target="#b43">[43]</ref>, the performance of OTA can be fur-ther improved to 47.0% AP. To demonstrate the compatibility of our method with other advanced technologies in object detection, we adopt Deformable Convolutional Networks (DCN) <ref type="bibr" target="#b54">[54]</ref> to ResNeXt backbones as well as the last convolution layer in the detection head. This improves our model's performance from 47.0% AP to 49.2% AP. Finally, with the multi-scale testing technique, our best model achieves 51.5% AP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Experiments on CrowdHuman</head><p>Object detection in crowded scenarios has raised more and more attention <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. Compared to dataset designed for general object detection like COCO, ambiguity happens more frequently in crowded dataset. Hence to demonstrate OTA's advantage on handling ambiguous anchors, it is necessary to conduct experiments on a crowded dataset -Crowdhuman <ref type="bibr" target="#b35">[35]</ref>. CrowdHuman contains 15000, 4370, and 5000 images in training, validation, and test set, respectively, with the average number of persons in an image 22.6. For all experiments, we train the detectors for 30 epochs (i.e., 2.5x) for better convergence. NMS threshold is set to 0.6. We adopt ResNet-50 <ref type="bibr" target="#b12">[13]</ref> as the default backbone in our experiments. Other settings are the same as our experiments on COCO. For evaluation, we follow the standard Caltech <ref type="bibr" target="#b6">[7]</ref> evaluation metric -MR, which stands for the Log-Average Missing Rate over false positives per image (FPPI) ranging in [10 ?2 , 10 0 ]. AP and Recall are also reported for reference. All evaluation results are reported on the CrowdHuman val subset.</p><p>As shown in <ref type="table">Table 6</ref>, RetinaNet and FCOS only achieve 58.8% and 55.0% MR respectively, which are far worse than two stage detectors like Faster R-CNN (with FPN), revealing the dilemma of one-stage detectors in crowd scenarios. Starting from FreeAnchor, the performances of onestage detectors gradually get improved by the dynamic label assigning strategies. ATSS achieves 49.5% MR, which is very close to the performance of Faster R-CNN (48.7% AP). Recent proposed LLA <ref type="bibr" target="#b9">[10]</ref> leverages loss-aware label assignment, which is similar to OTA and achieves 47.9% MR. However, our OTA takes a step forward by introducing global information into the label assignment, boosting MR to 46.6%. The AP and Recall of OTA also surpass other existing one-stage detectors by a clear margin.</p><p>Although PAA achieves competitive performance with OTA on COCO, it performs struggling on CrowdHuman. We conjecture that PAA needs clear pos/neg decision boundaries to help GMM learn better clusters. But in crowded scenarios, such clear boundaries may not exist because potential negative samples usually cover a sufficient amount of foreground areas, resulting in PAA's poor performance. Also, PAA performs per-gt's clustering, which heavily increases the training time on crowded datasets like CrowdHuman. Compared to PAA, OTA still shows promis-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Iteration</head><p>Backbone AP AP 50 AP 75 APs APm APl RetinaNet <ref type="bibr" target="#b20">[21]</ref> 135k ResNet-101 39. ing results, which demonstrates OTA's superiority on various detection benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose Optimal Transport Assignment (OTA) -an optimization theory based label assigning strat-egy. OTA formulates the label assigning procedure in object detection into an Optimal Transport problem, which aims to transport labels from ground-truth objects and backgrounds to anchors at minimal transporting costs. To determine the number of positive labels needed by each gt, we further propose a simple estimation strategy based on the IoU values between predicted bounding boxes and each gt. As shown in experiments, OTA achieves the new SOTA performance on MS COCO. Because OTA can well-handle the assignment of ambiguous anchors, it also outperforms all other one-stage detectors on CrowdHuman dataset by a large margin, demonstrating its strong generalization ability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>OnesInit 6: pairwise cls cost: c ij cls = FocalLoss(P cls j , G cls i ) 7: pairwise reg cost: c ij reg = IoULoss(P box j ,G box i ) 8: pairwise Center Prior cost: c cp ij ? (A j , G box i ) 9: bg cls cost: c bg cls = FocalLoss(P cls j , ?) 10: f g cost: c fg = c cls + ?c reg + c cp 11: compute final cost matrix c via concatenating c bg cls to the last row of c fg 12: v 0 , u 0 ? OnesInit 13: for t=0 to T do: 14:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Visualizations of assigning results. For PAA, the dots stand for geometric centers of positive anchor boxes. For ATSS and OTA, the dots stand for positive anchor points. Rectangles represent the gt bounding boxes. To clearly illustrate the differences between different assigning strategies, we set r to 5 for all methods. Only FPN layers with the largest number of positive anchors are shown for better visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Method Aux. Branch Center Dyn. k AP AP 50 AP 75 Ablation studies on each components in OTA. "Center" stands for Center Prior and Center Sampling for OTA and FCOS, respectively. Dyn.k is the abbreviation of our proposed Dynamic k Estimation strategy.</figDesc><table><row><cell></cell><cell>-</cell><cell>38.3 57.1 41.3</cell></row><row><cell>FCOS</cell><cell>CenterNess IoU</cell><cell>38.9 57.5 42.0 38.8 57.7 41.8</cell></row><row><cell></cell><cell>IoU</cell><cell>39.5 57.6 42.9</cell></row><row><cell></cell><cell>-</cell><cell>39.2 58.3 42.2</cell></row><row><cell>OTA</cell><cell>IoU</cell><cell>39.6 58.1 42.5</cell></row><row><cell>(FCOS)</cell><cell>IoU</cell><cell>40.3 58.6 43.7</cell></row><row><cell></cell><cell>IoU</cell><cell>40.7 58.4 44.3</cell></row><row><cell>OTA (RetinaNet)</cell><cell>IoU</cell><cell>40.7 58.6 44.1</cell></row></table><note>as a default component in our experiments. The top q in Sec. 3.3 is directly set to 20, as we find this set of param- eter values can consistently yield stable results in various situations.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>only conduct a small candidate set for each gt, because a large number of candidates brings trouble -when occlusion happens or several objects are close enough, an anchor may simultaneously be a qualified candidate for multiple gts. We define such anchors as ambiguous anchors. Previous methods mainly handle this ambiguity by introducing hand-crafted rules e.g.N amb. 2.1 15.9 36.3 0.5 0.8 1.2 0.2 0.2 0.3 AP 39.4 38.0 37.2 40.3 40.1 39.5 40.6 40.7 40.4 AP 50 57.5 56.7 55.8 58.9 58.4 57.5 58.7 58.4 58.3 AP 75 42.7 40.4 39.8 43.4 43.4 42.4 44.1 44.3 43.6 Performances of different label assigning strategies under different number of anchor candidates. N amb. denotes the average number of ambiguous anchors per-image calculated on COCO train set.</figDesc><table><row><cell cols="4">Method ATSS [47]</cell><cell></cell><cell>PAA [16]</cell><cell></cell><cell></cell><cell>OTA</cell><cell></cell></row><row><cell>r</cell><cell>3</cell><cell>5</cell><cell>7</cell><cell>3</cell><cell>5</cell><cell>7</cell><cell>3</cell><cell>5</cell><cell>7</cell></row></table><note>, Min Area [38], Max IoU [47, 16, 21] and Min Loss 4 . To illustrate OTA's superiority on ambiguous handling, We count the number of ambiguous anchors in ATSS, PAA and4 Assigning ambiguous anchor to the gt with the minimal loss.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>f.b. OTA 40.0 57.8 43.6 Max IoU [47] f.b. OTA 40.3 58.1 43.7 Min Loss f.b. Performance comparisons on ambiguity handling between OTA and other human-designed strategies on the COCO val set.. f.b. denotes "followed by".</figDesc><table><row><cell>OTA</cell><cell>40.3 57.9 43.6</cell></row><row><cell>OTA</cell><cell>40.7 58.4 44.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Analysis of different values of k and Dynamic k Estimation strategy on the COCO val set.</figDesc><table><row><cell>k</cell><cell cols="2">AP AP 50 AP 75 APs APm APl</cell></row><row><cell>1</cell><cell>36.5 55.4</cell><cell>38.8 21.4 39.7 46.2</cell></row><row><cell>5</cell><cell>39.5 58.1</cell><cell>42.7 23.1 43.0 50.6</cell></row><row><cell>8</cell><cell>39.8 58.4</cell><cell>42.9 22.7 43.6 51.5</cell></row><row><cell>10</cell><cell>40.3 58.6</cell><cell>43.7 23.4 44.2 52.1</cell></row><row><cell>12</cell><cell>40.3 58.6</cell><cell>43.6 23.2 44.2 51.9</cell></row><row><cell>15</cell><cell>40.2 58.4</cell><cell>43.6 23.2 44.1 51.9</cell></row><row><cell>20</cell><cell>40.1 58.2</cell><cell>43.6 23.5 44.0 52.8</cell></row><row><cell cols="2">Dyn. k 40.7 58.4</cell><cell>44.3 23.2 45.0 53.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>DCN 51.5 68.6 57.1 34.1 53.7 64.1 Performance comparison with state-of-the-art one-stage detectors on MS COCO 2017 test-dev set. * indicates the specific form of multi-scale testing that adopted in ATSS [47]. Performance comparison on the CrowdHuman validation set. All experiments are conducted under 2.5x scheduler.</figDesc><table><row><cell>1 59.1</cell><cell>42.3 21.8 42.7 50.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For anchor-free detectors like FCOS<ref type="bibr" target="#b38">[38]</ref>, the feature points can be viewed as shrunk anchor boxes. Hence in this paper, we collectively refer to anchor box and anchor point as "anchor".</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For anchor-based methods, the distances are measured between the geometric center of anchors and gts</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Total number of potential positive anchors equals to (r 2 * FPN Levels).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was partially supported by National Key R&amp;D Program of China (No. 2017YFA0700800), and Beijing Academy of Artificial Intelligence (BAAI).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A. Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Optimal Transport and Sinkhorn Iteration</head><p>To ensure the integrity of this paper, we briefly introduce the derivation of the Sinkhorn Iteration algorithm which we emphasize not our contributions and belongs to textbook knowledge.</p><p>The mathematical formula of the Optimal Transport problem is defined in Eq. 1. This is a linear program which can be solved in polynomial time. For dense detectors, however, the resulting linear program is large, involving the square of feature dimensions with anchors in all scales. This issue can be addressed by a fast iterative solution, which converts the optimization target in Eq. 1 into a non-linear but convex form with an entropic regularization term E added:</p><p>where E(? ij ) = ? ij (log ? ij ? 1). ? is a constant hyperparameter controlling the intensity of regularization term. According to Lagrange Multiplier Method, the constraint optimization target in Eq. 5 can be convert to a nonconstraint target:</p><p>where ? j (j = 1, 2, ...n) and ? i (i = 1, 2, ..., m) are Lagrange multipliers. By letting the derivatives of the optimization target equal to 0, the optimal plan ? * is resolved as:</p><p>Letting</p><p>, the following constraints can be enforced:</p><p>These two equations have to be satisfied simultaneously. One possible solution is to calculate v i and u j by repeating the following updating formulas sufficient steps:</p><p>The updating rule in Eq. 10 is also known as the Sinkhorn-Knopp Iteration. After repeating this iteration T times, the approximate optimal plan ? * can be obtained:</p><p>? and T are empirically set to 0.1 and 50. Please refer to our code for more details.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<meeting>COMPSTAT&apos;2010</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12872</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Reppoints v2: Verification meets regression for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08508</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bernt Schiele, and Pietro Perona. Pedestrian detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wojek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="304" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6569" to="6578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ps-rcnn: Detecting secondary human instances in a crowd via primary object suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osamu</forename><surname>Yoshie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osamu</forename><surname>Yoshie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.04307</idno>
		<title level="m">Lla: Loss-aware label assignment for dense pedestrian detection</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Densebox: Unifying landmark localization with end to end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafeng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.04874</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Nms by representative region: Towards crowded pedestrian detection by proposal pairing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osamu</forename><surname>Yoshie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10750" to="10759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Probabilistic anchor assignment with iou prediction for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee Seok</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08103</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Foveabox: Beyound anchor-based object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="7389" to="7398" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning from noisy anchors for one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengduo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="10588" to="10597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Receptive field block net for accurate and fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="385" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adaptive nms: Refining pedestrian detection in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6459" to="6468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning spatial fusion for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09516</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pay attention to them: deep reinforcement learning-based cascade object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="2544" to="2556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Libra r-cnn: Towards balanced learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="821" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Borderdet: Border feature for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="549" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<idno>2020. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Crowdhuman: A benchmark for detecting human in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00123</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Hongbin Sun, Jian Sun, and Nanning Zheng. Fine-grained dynamic head for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengkai</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.03519</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Rethinking learnable tree filter for generic feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengkai</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.03482</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Region proposal by guided anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2965" to="2974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">End-to-end object detection with fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.03544</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multi-scale positive sample refinement for few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="456" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Metaanchor: Learning to detect objects with customized anchors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="320" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Reppoints: Point set representation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9657" to="9666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unitbox: An advanced object detection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM international conference on Multimedia</title>
		<meeting>the 24th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="9759" to="9768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Freeanchor: Learning to match anchors for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Cross-domain object detection through coarse-to-fine feature adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangtao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13766" to="13775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Objects as points. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Autoassign: Differentiable label assignment for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengkai</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuhang</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03496</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Soft anchor-point object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12448</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Feature selective anchor-free module for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="840" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9308" to="9316" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
