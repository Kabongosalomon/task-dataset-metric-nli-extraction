<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CenterNet: Keypoint Triplets for Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
							<email>kaiwen.duan@vipl.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
							<email>qmhuang@ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<email>tian.qi1@huawei.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CenterNet: Keypoint Triplets for Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In object detection, keypoint-based approaches often suffer a large number of incorrect object bounding boxes, arguably due to the lack of an additional look into the cropped regions. This paper presents an efficient solution which explores the visual patterns within each cropped region with minimal costs. We build our framework upon a representative one-stage keypoint-based detector named Corner-Net. Our approach, named CenterNet, detects each object as a triplet, rather than a pair, of keypoints, which improves both precision and recall. Accordingly, we design two customized modules named cascade corner pooling and center pooling, which play the roles of enriching information collected by both top-left and bottom-right corners and providing more recognizable information at the central regions, respectively. On the MS-COCO dataset, CenterNet achieves an AP of 47.0%, which outperforms all existing one-stage detectors by at least 4.9%. Meanwhile, with a faster inference speed, CenterNet demonstrates quite comparable performance to the top-ranked two-stage detectors. Code is available at https://github.com/ Duankaiwen/CenterNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection has been significantly improved and advanced with the help of deep learning, especially convolutional neural networks <ref type="bibr" target="#b10">[11]</ref> (CNNs). In the current era, one of the most popular flowcharts is anchor-based <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33]</ref>, which placed a set of rectangles with pre-defined sizes, and regressed them to the desired place with the help of ground-truth objects. These approaches often need a large number of anchors to ensure a sufficiently high IoU (intersection over union) rate with the ground-truth objects, and the size and aspect ratio of each anchor box need to be manually designed. In addition, anchors are usually not * This work was done when the first author was interning at Huawei Noah's Ark Lab.  <ref type="figure">Figure 1</ref>: In the first row, we visualize the top 100 bounding boxes (according to the MS-COCO dataset standard) of CornerNet. Ground-truth and predicted objects are marked in blue and red, respectively. In the second row, we show that correct predictions can be determined by checking the central parts.</p><p>aligned with the ground-truth boxes, which is not conducive to the bounding box classification task.</p><p>To overcome the drawbacks of anchor-based approaches, a keypoint-based object detection pipeline named Corner-Net <ref type="bibr" target="#b19">[20]</ref> was proposed. It represented each object by a pair of corner keypoints, which bypassed the need of anchor boxes and achieved the state-of-the-art one-stage object detection accuracy. Nevertheless, the performance of Corner-Net is still restricted by its relatively weak ability of referring to the global information of an object. That is to say, since each object is constructed by a pair of corners, the algorithm is sensitive to detect the boundary of objects, meanwhile not being aware of which pairs of keypoints should be grouped into objects. Consequently, as shown in <ref type="figure">Figure 1</ref>, it often generates some incorrect bounding boxes, most of which could be easily filtered out with complementary information, e.g., the aspect ratio.</p><p>To address this issue, we equip CornerNet with an ability of perceiving the visual patterns within each proposed region, so that it can identify the correctness of each bounding box by itself. In this paper, we present a low-cost yet effective solution named CenterNet, which explores the central part of a proposal, i.e., the region that is close to the geometric center, with one extra keypoint. Our intuition is that, if a predicted bounding box has a high IoU with the groundtruth box, then the probability that the center keypoint in its central region is predicted as the same class is high, and vice versa. Thus, during inference, after a proposal is generated as a pair of corner keypoints, we determine if the proposal is indeed an object by checking if there is a center keypoint of the same class falling within its central region. The idea, as shown in <ref type="figure">Figure 1</ref>, is to use a triplet, instead of a pair, of keypoints to represent each object.</p><p>Accordingly, for better detecting center keypoints and corners, we propose two strategies to enrich center and corner information, respectively. The first strategy is named center pooling, which is used in the branch for predicting center keypoints. Center pooling helps the center keypoints obtain more recognizable visual patterns within objects, which makes it easier to perceive the central part of a proposal. We achieve this by getting out the max summed response in both horizontal and vertical directions of the center keypoint on a feature map for predicting center keypoints. The second strategy is named cascade corner pooling, which equips the original corner pooling module <ref type="bibr" target="#b19">[20]</ref> with the ability of perceiving internal information. We achieve this by getting out the max summed response in both boundary and internal directions of objects on a feature map for predicting corners. Empirically, we verify that such a two-directional pooling method is more stable, i.e., being more robust to feature-level noises, which contributes to the improvement of both precision and recall.</p><p>We evaluate the proposed CenterNet on the MS-COCO dataset <ref type="bibr" target="#b24">[25]</ref>, one of the most popular benchmarks for largescale object detection. CenterNet, with both center pooling and cascade corner pooling incorporated, reports an AP of 47.0% on the test-dev set, which outperforms all existing one-stage detectors by a large margin. With an average inference time of 270ms using a 52-layer hourglass backbone <ref type="bibr" target="#b28">[29]</ref> and 340ms using a 104-layer hourglass backbone <ref type="bibr" target="#b28">[29]</ref> per image, CenterNet is quite efficient yet closely matches the state-of-the-art performance of the other twostage detectors.</p><p>The remainder of this paper is organized as follows. Section 2 briefly reviews related work, and Section 3 details the proposed CenterNet. Experimental results are given in Section 4, followed by the conclusion in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Object detection involves locating and classifying the objects. In the deep learning era, powered by deep convolutional neural networks, object detection approaches can be roughly categorized into two main types of pipelines, namely, two-stage approaches and one-stage approaches.</p><p>Two-stage approaches divide the object detection task into two stages: extract RoIs, then classify and regress the RoIs. R-CNN <ref type="bibr" target="#b10">[11]</ref> uses a selective search method <ref type="bibr" target="#b41">[42]</ref> to locate RoIs in the input images and uses a DCN-based regionwise classifier to classify the RoIs independently. SPP-Net <ref type="bibr" target="#b12">[13]</ref> and Fast-RCNN <ref type="bibr" target="#b9">[10]</ref> improve R-CNN by extracting the RoIs from the feature maps. Faster-RCNN <ref type="bibr" target="#b32">[33]</ref> is allowed to be trained end to end by introducing RPN (region proposal network). RPN can generate RoIs by regressing the anchor boxes. Later, the anchor boxes are widely used in the object detection task. Mask-RCNN <ref type="bibr" target="#b11">[12]</ref> adds a mask prediction branch on the Faster-RCNN, which can detect objects and predict their masks at the same time. R-FCN <ref type="bibr" target="#b5">[6]</ref> replaces the fully connected layers with the position-sensitive score maps for better detecting objects. Cascade R-CNN <ref type="bibr" target="#b3">[4]</ref> addresses the problem of overfitting at training and quality mismatch at inference by training a sequence of detectors with increasing IoU thresholds. The keypoint-based object detection approaches <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b27">28]</ref> are proposed to avoid the disadvantages of using anchor boxes and bounding boxes regression. Other meaningful works are proposed for different problems in object detection, e.g., <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b20">21]</ref> focus on the architecture design, <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b43">44]</ref> focus on the contextual relationship, <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b2">3]</ref> focus on the multi-scale unification.</p><p>One-stage approaches remove the RoI extraction process and directly classify and regress the candidate anchor boxes.</p><p>YOLO <ref type="bibr" target="#b30">[31]</ref> uses fewer anchor boxes (divide the input image into an S ? S grid) to do regression and classification. YOLOv2 <ref type="bibr" target="#b31">[32]</ref> improves the performance by using more anchor boxes and a new bounding box regression method. SSD <ref type="bibr" target="#b26">[27]</ref> places anchor boxes densely over an input image and use features from different convolutional layers to regress and classify the anchor boxes. DSSD <ref type="bibr" target="#b7">[8]</ref> introduces a deconvolution module into SSD to combine lowlevel and high-level features. While R-SSD <ref type="bibr" target="#b16">[17]</ref> uses pooling and deconvolution operations in different feature layers to combine low-level and high-level features. RON <ref type="bibr" target="#b18">[19]</ref> proposes a reverse connection and an objectness prior to extract multiscale features effectively. RefineDet <ref type="bibr" target="#b44">[45]</ref> refines the locations and sizes of the anchor boxes for two times, which inherits the merits of both one-stage and two-stage approaches. CornerNet <ref type="bibr" target="#b19">[20]</ref> is another keypoint-based approach, which directly detects an object using a pair of corners. Although CornerNet achieves high performance, it still has more room to improve.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Baseline and Motivation</head><p>This paper uses CornerNet <ref type="bibr" target="#b19">[20]</ref> as the baseline. For detecting corners, CornerNet produces two heatmaps: a heatmap of top-left corners and a heatmap of bottom-right corners. The heatmaps represent the locations of keypoints of different categories and assigns a confidence score for each keypoint. Besides, it also predicts an embedding and a group of offsets for each corner. The embeddings are used to identify if two corners are from the same object. The offsets learn to remap the corners from the heatmaps to the input image. For generating object bounding boxes, top-k left-top corners and bottom-right corners are selected from the heatmaps according to their scores, respectively. Then, the distance of the embedding vectors of a pair of corners is calculated to determine if the paired corners belong to the same object. An object bounding box is generated if the distance is less than a threshold. The bounding box is assigned a confidence score, which equals to the average scores of the corner pair.</p><p>In <ref type="table" target="#tab_0">Table 1</ref>, we provide a deeper analysis of CornerNet. We count the FD 1 (false discovery) rate of CornerNet on the MS-COCO validation dataset, defined as the proportion of the incorrect bounding boxes. The quantitative re-sults demonstrate the incorrect bounding boxes account for a large proportion even at low IoU thresholds, e.g., Corner-Net obtains 32.7% FD rate at IoU = 0.05. This means in average, 32.7 out of every 100 object bounding boxes have IoU lower than 0.05 with the ground-truth. The small incorrect bounding boxes are even more, which achieves 60.3% FD rate. One of the possible reasons lies in that CornerNet cannot look into the regions inside the bounding boxes. To make CornerNet <ref type="bibr" target="#b19">[20]</ref> perceive the visual patterns in bounding boxes, one potential solution is to adapt CornerNet into a two-stage detector, which uses the RoI pooling <ref type="bibr" target="#b9">[10]</ref> to look into the visual patterns in bounding boxes. However, it is known that such a paradigm is computationally expensive.</p><p>In this paper, we propose a highly efficient alternative called CenterNet to explore the visual patterns within each bounding box. For detecting an object, our approach uses a triplet, rather than a pair, of keypoints. By doing so, our approach is still a one-stage detector, but partially inherits the functionality of RoI pooling. Our approach only pays attention to the center information, the cost of our approach is minimal. Meanwhile, we further introduce the visual patterns within objects into the keypoint detection process by using center pooling and cascade corner pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Object Detection as Keypoint Triplets</head><p>The overall network architecture is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. We represent each object by a center keypoint and a pair of corners. Specifically, we embed a heatmap for the center keypoints on the basis of CornerNet and predict the offsets of the center keypoints. Then, we use the method proposed in CornerNet <ref type="bibr" target="#b19">[20]</ref> to generate top-k bounding boxes. However, to effectively filter out the incorrect bounding boxes, we leverage the detected center keypoints and resort to the following procedure: (1) select top-k center keypoints according to their scores; (2) use the corresponding offsets to remap these center keypoints to the input image; (3) de-  fine a central region for each bounding box and check if the central region contains center keypoints. Note that the class labels of the checked center keypoints should be same as that of the bounding box; (4) if a center keypoint is detected in the central region, we will preserve the bounding box. The score of the bounding box will be replaced by the average scores of the three points, i.e., the top-left corner, the bottom-right corner and the center keypoint. If there are no center keypoints detected in its central region, the bounding box will be removed.</p><p>The size of the central region in the bounding box affects the detection results. For example, smaller central regions lead to a low recall rate for small bounding boxes, while larger central regions lead to a low precision for large bounding boxes. Therefore, we propose a scale-aware central region to adaptively fit the size of bounding boxes. The scale-aware central region tends to generate a relatively large central region for a small bounding box, while a relatively small central region for a large bounding box. Suppose we want to determine if a bounding box i needs to be preserved. Let tl x and tl y denote the coordinates of the topleft corner of i and br x and br y denote the coordinates of the bottom-right corner of i. Define a central region j. Let ctl x and ctl y denote the coordinates of the top-left corner of j and cbr x and cbr y denote the coordinates of the bottomright corner of j. Then tl x , tl y , br x , br y , ctl x , ctl y , cbr x and cbr y should satisfy the following relationship:</p><formula xml:id="formula_0">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ctlx =</formula><p>(n + 1)tlx + (n ? 1)brx 2n ctly = (n + 1)tly + (n ? 1)bry 2n cbrx = (n ? 1)tlx + (n + 1)brx 2n cbry = (n ? 1)tly + (n + 1)bry 2n <ref type="bibr" target="#b0">(1)</ref> where n is odd that determines the scale of the central re- gion j. In this paper, n is set to be 3 and 5 for the scales of bounding boxes less and greater than 150, respectively. <ref type="figure" target="#fig_2">Figure 3</ref> shows two central regions when n = 3 and n = 5, respectively. According to Equation <ref type="formula">(1)</ref>, we can determine a scale-aware central region, then we check if the central region contains center keypoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Enriching Center and Corner Information</head><p>Center pooling. The geometric centers of objects do not necessarily convey very recognizable visual patterns (e.g., the human head contains strong visual patterns, but the center keypoint is often in the middle of the human body). To address this issue, we propose center pooling to capture richer and more recognizable visual patterns. <ref type="figure" target="#fig_3">Figure 4</ref>(a) shows the principle of center pooling. The detailed process of center pooling is as follows: the backbone outputs a feature map, and to determine if a pixel in the feature map is a center keypoint, we need to find the maximum value in its both horizontal and vertical directions and add them together. By doing this, center pooling helps the better detection of center keypoints.</p><p>Cascade corner pooling. Corners are often outside the objects, which lacks local appearance features. Corner-Net <ref type="bibr" target="#b19">[20]</ref> uses corner pooling to address this issue. The principle of corner pooling is shown in <ref type="figure" target="#fig_3">Figure 4</ref>(b). Corner pooling aims to find the maximum values on the boundary directions so as to determine corners. However, it makes corners sensitive to the edges. To address this problem, we need to let corners "see" the visual patterns of objects. The principle of cascade corner pooling is presented in <ref type="figure" target="#fig_3">Figure 4</ref>(c). It first looks along a boundary to find a boundary maximum value, then looks inside along the location of the boundary maximum value 2 to find an internal maximum value, and finally, add the two maximum values together. By doing this, the corners obtain both the the boundary information and the visual patterns of objects. Both the center pooling and the cascade corner pooling can be easily achieved by combining the corner pooling <ref type="bibr" target="#b19">[20]</ref>   <ref type="figure">Figure 5</ref>: The structures of the center pooling module (a) and the cascade top corner pooling module (b). We achieve center pooling and the cascade corner pooling by combining the corner pooling at different directions.</p><p>at different directions. <ref type="figure">Figure 5(a)</ref> shows the structure of the center pooling module. To take a maximum value in a direction, e.g., the horizontal direction, we only need to connect the left pooling and the right pooling in series. <ref type="figure">Figure 5(b)</ref> shows the structure of a cascade top corner pooling module. Compared with the top corner pooling in CornerNet <ref type="bibr" target="#b19">[20]</ref>, we add a left corner pooling before the top corner pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training and Inference</head><p>Training. Our method is implemented in Pytorch <ref type="bibr" target="#b29">[30]</ref> and the network is trained from scratch. The resolution of the input image is 511 ? 511, leading to heatmaps of size 128?128. We use the data augmentation strategy presented in <ref type="bibr" target="#b19">[20]</ref> to train a robust model. Adam <ref type="bibr" target="#b17">[18]</ref> is used to optimize the training loss:</p><formula xml:id="formula_1">L = L co det + L ce det + ?L co pull + ?L co push + ? (L co off + L ce off ),<label>(2)</label></formula><p>where L co det and L ce det denote the focal losses, which are used to train the network to detect corners and center keypoints, respectively. L co pull is a "pull" loss for corners, which is used to minimize the distance of the embedding vectors that belongs to the same objects. L co push is a "push" loss for corners, which is used to maximize the distance of the embedding vectors that belongs to different objects. L co off and L ce off are 1 -losses <ref type="bibr" target="#b9">[10]</ref>, which are used to train the network to predict the offsets of corners and center keypoints, respectively. ?, ? and ? denote the weights for corresponding losses, which are set to 0.1, 0.1 and 1, respectively. L det , L pull , L push and L off are all defined in the CornerNet, we suggest to refer to <ref type="bibr" target="#b19">[20]</ref> for details. We train the CenterNet on 8 Tesla V100 (32GB) GPUs and use a batch size of 48. The maximum number of iterations is 480K. We use a learning rate of 2.5 ? 10 ?4 for the first 450K iterations and then continue training 30K iterations with a rate of 2.5 ? 10 ?5 .</p><p>Inference. Following <ref type="bibr" target="#b19">[20]</ref>, for the single-scale testing, we input both the original and horizontally flipped images with the original resolutions into the network. While for the multi-scale testing, we input both the original and horizontally flipped images with the resolutions of 0.6, 1, 1.2, 1.5 and 1.8. We select top 70 center keypoints, top 70 top-left corners and top 70 bottom-right corners from the heatmaps to detect the bounding boxes. We flip the bounding boxes detected in the horizontally flipped images and mix them into the original bounding boxes. Soft-nms <ref type="bibr" target="#b1">[2]</ref> is used to remove the redundant bounding boxes. We finally select top 100 bounding boxes according to their scores as the final detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset, Metrics and Baseline</head><p>We evaluate our method on the MS-COCO dataset <ref type="bibr" target="#b24">[25]</ref>. It contains 80 categories and more than 1.5 million object instances. The large number of small objects makes it a very challenging dataset. We use the 'trainval35k' set <ref type="bibr" target="#b14">[15]</ref> (i.e., 80K training images and 35K validation images) for training and test the results on the test-dev set. We use another 5K images in the validation set to perform ablation studies and visualization experiments.</p><p>MS-COCO dataset <ref type="bibr" target="#b24">[25]</ref> uses AP and AR metrics to characterize the performance of a detector. AP represents the average precision rate, which is computed over ten different IoU thresholds (i.e., 0.5 : 0.05 : 0.95) and all categories. It is considered the single most important metric on the MS-COCO dataset. AR represents the maximum recall rate, which is computed over a fixed number of detections (i.e., 1, 10 and 100 ) per image and averaged over all categories and the ten different IoU thresholds. Additionally, AP and AR can be used to evaluate the performance under different object scales, including small objects (area &lt; 32 2 ), medium objects (32 2 &lt; area &lt; 96 2 ) and large objects (area &gt; 96 2 ).</p><p>Our direct baseline is CornerNet <ref type="bibr" target="#b19">[20]</ref>. Following it, we use the stacked hourglass network <ref type="bibr" target="#b28">[29]</ref> with 52 and 104 layers as the backbone -the latter has two hourglass modules while the former has only one. All modifications on the hourglass architecture, made by <ref type="bibr" target="#b19">[20]</ref>, are preserved.  Meanwhile, it can be seen that the most contribution comes from the small objects. For instance, CenterNet511-52 improves the AP for small objects by 5.5% (singlescale) and by 6.4% (multi-scale). As for the backbone Hourglass-104, the improvements are 6.2% (single-scale) and by 8.1% (multi-scale), respectively. The benefit stems from the center information modeled by the center keypoints: the smaller the scale of an incorrect bounding box is, the lower probability a center keypoint can be detected in its central region. <ref type="figure" target="#fig_6">Figure 6</ref>(a) and <ref type="figure" target="#fig_6">Figure 6</ref>(b) show some qualitative comparisons, which demonstrate the effectiveness of CenterNet in reducing small incorrect bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparisons with State-of-the-art Detectors</head><p>CenterNet also leads to a large improvement for reducing medium and large incorrect bounding boxes. As Table 2 shows, CenterNet511-104 improves the single-scale testing AP by 4.7% (from 42.7% to 47.4%) and 3.5% (from 53.9% to 57.4%), respectively. <ref type="figure" target="#fig_6">Figure 6</ref>(c) and <ref type="figure" target="#fig_6">Figure 6(d)</ref> show some qualitative comparisons for reducing medium and large incorrect bounding boxes. It is worth noting that the AR is also significantly improved, with the best performance achieved with multi-scale testing. This is because our approach removes lots of incorrect bounding boxes, which is equivalent to improving the confidence of those bounding boxes with accurate locations but lower scores.</p><p>When comparing other one-stage approaches, CenterNet511-52 reports 41.6% single-scale testing AP. This achievement is already better than those using deeper models (e.g., RetinaNet800 <ref type="bibr" target="#b23">[24]</ref> and RefineDet <ref type="bibr" target="#b44">[45]</ref>). The best performance of CenterNet is AP 47.0%, dramatically surpassing all the published one-stage approaches to our best knowledge.</p><p>At last, one can observe that the performance of CenterNet is also competitive with the two-stage approaches, e.g., the single-scale testing AP of CenterNet511-52 is comparable to the two-stage approach Fitness R-CNN <ref type="bibr" target="#b40">[41]</ref> (41.6% vs. 41.8%) and that of CenterNet511-104 is comparable to D-RFCN + SNIP <ref type="bibr" target="#b37">[38]</ref> (44.9% vs. 45.7%), respectively. Nevertheless, it should be mentioned that twostage approaches usually use larger resolution input images (e.g., ? 1000 ? 600), which significantly improves the detection accuracy especially for small objects. The multiscale testing AP 47.0% achieved by CenterNet511-104 closely matches the state-of-the-art AP 47.4%, achieved by the two-stage detector PANet <ref type="bibr" target="#b25">[26]</ref>. We present some qualitative detection results in <ref type="figure">Figure 7</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Incorrect Bounding Box Reduction</head><p>The AP <ref type="bibr" target="#b24">[25]</ref> metric reflects how many high quality object bounding boxes (usually IoU 0.5) a network can predict, but cannot directly reflect how many incorrect object bounding boxes (usually IoU 0.5) a network generates. The FD rate is a suitable metric, which reflects the proportion of the incorrect bounding boxes. <ref type="table" target="#tab_6">Table 3</ref> shows the FD rates for CornerNet and CenterNet. CornerNet generates many incorrect bounding boxes even at IoU = 0.05 threshold, i.e., CornerNet511-52 and CornerNet511-104 obtain 35.2% and 32.7% FD rate, respectively. On the other hand, CornerNet generates more small incorrect bounding   boxes than medium and large incorrect bounding boxes, which reports 62.5% for CornerNet511-52 and 60.3% for CornerNet511-104, respectively. Our CenterNet decreases the FD rates at all criteria via exploring central regions.</p><p>For instance, CenterNet511-52 and CenterNet511-104 decrease FD 5 by both 4.5%. In addition, the FD rates for small bounding boxes decrease the most, which are 9.5% by CenterNet511-52 and 9.6% by CenterNet511-104, respectively. This is also the reason why the AP improvement for small objects is more prominent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Inference Speed</head><p>The proposed CenterNet explores the visual patterns within each proposed region with minimal costs. To ensure a fair comparison, we test the inference speed of both Cor-nerNet <ref type="bibr" target="#b19">[20]</ref> and CenterNet on a NVIDIA Tesla P100 GPU. We obtain that the average inference time of CornerNet511-104 is 300ms per image and that of CenterNet511-104 is 340ms. Meanwhile, using the Hourglass-52 backbone can speed up the inference speed. Our CenterNet511-52 takes an average of 270ms to process per image, which is faster and more accurate than CornerNet511-104.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>Our work has contributed three components, including central region exploration, center pooling and cascade corner pooling. To analyze the contribution of each individual component, an ablation study is given here. The baseline is CornerNet511-52 <ref type="bibr" target="#b19">[20]</ref>. We add the three components to the baseline one by one and follow the default parameter setting detailed in Section 4.1. The results are given in <ref type="table" target="#tab_8">Table 4</ref>.</p><p>Central region exploration. To understand the importance of the central region exploration (see CRE in the table), we add a center heatmap branch to the baseline and use a triplet of keypoints to detect bounding boxes. For the center keypoint detection, we only use conventional convolutions. As presented in the third row in <ref type="table" target="#tab_8">Table 4</ref>, we improve the AP by 2.3% (from 37.6% to 39.9%). However, we find that the improvement for the small objects (that is 4.6%) is more significant than that for other object scales. The improvement for large objects is almost negligible (from 52.2% to 52.3%). This is not surprising because, from a probabilistic point of view, the center keypoint for a small object is easier   <ref type="table">Table 5</ref>: Error analysis of center keypoints via using ground-truth. we replace the predicted center keypoints with the ground-truth values, the results suggest there is still room for improvement in detecting center keypoints.</p><p>to be located than that of a large object.</p><p>Center pooling. To demonstrate the effectiveness of proposed center pooling, we then add the center pooling module to the network (see CTP in the table). The fourth row in <ref type="table" target="#tab_8">Table 4</ref> shows that center pooling improves the AP by 0.9% (from 39.9% to 40.8%). Notably, with the help of center pooling, we improve the AP for large objects by 1.4% (from 52.2% to 53.6%), which is much higher than the improvement using conventional convolutions (i.e., 1.4% vs. 0.1%). It demonstrates that our center pooling is effective in detecting center keypoints of objects, especially for large objects. Our explanation is that center pooling can extract richer internal visual patterns, and larger objects contain more accessible internal visual patterns. <ref type="figure" target="#fig_6">Figure 6</ref>(e) shows the results of detecting center keypoints without/with center pooling. We can see the conventional convolution fails to locate the center keypoint for the cow, but with center pooling, it successfully locates the center keypoint.</p><p>Cascade corner pooling. We replace corner pooling <ref type="bibr" target="#b19">[20]</ref> with cascade corner pooling to detect corners (see CCP in the table). The second row in <ref type="table" target="#tab_8">Table 4</ref> shows the results that we test on the basis of CornerNet511-52. We find that cascade corner pooling improves the AP by 0.7% (from 37.6% to 38.3%). The last row shows the results that we test on the basis of CenterNet511-52, which improves the AP by 0.5% (from 40.8% to 41.3%). The results of the second row show there is almost no change in the AP for large objects (i.e., 52.2% vs. 52.2%), but the AR is improved by 1.8% (from 74.0% to 75.8%). This suggests that cascade corner pooling can "see" more objects due to the rich internal visual patterns in large objects, but too rich visual patterns may interfere with its perception for the boundary information, leading to many inaccurate bounding boxes. After equipping with our CenterNet, the inaccurate bound-ing boxes are effectively suppressed, which improves the AP for large objects by 2.2% (from 53.6% to 55.8%). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Error Analysis</head><p>The exploration of visual patterns within each bounding box depends on the center keypoints. In other words, once a center keypoint is missed, the proposed CenterNet would miss the visual patterns within the bounding box. To understand the importance of center keypoints, we replace the predicted center keypoints with the ground-truth values and evaluate performance on the MS-COCO validation dataset. <ref type="table">Table 5</ref> shows that using the ground-truth center keypoints improves the AP from 41.3% to 56.5% for CenterNet511-52 and from 44.8% to 58.1% for CenterNet511-104, respectively. APs for small, medium and large objects are improved by 15.5%, 16.5%, and 14.5% for CenterNet511-52 and 14.5%, 14.1%, and 13.3% for CenterNet511-104, respectively. This demonstrates that the detection of center keypoints is far from the bottleneck.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we propose CenterNet, which detects objects using a triplet, including one center keypoint and two corners. Our approach addresses the problem that Corner-Net lacks an additional look into the cropped regions by exploring the visual patterns within each proposed region with minimal costs. In fact, this is a common defect for all one-stage approaches. As one-stage approaches remove the RoI extraction process, they cannot pay attention to internal information within cropped regions.</p><p>An intuitive explanation of our contribution lies in that we equip a one-stage detector with the ability of two-stage approaches, with an efficient discriminator being added. We believe that our idea of adding an extra branch for the center keypoint can be potentially generalized to other existing one-stage approaches (e.g., SSD <ref type="bibr" target="#b26">[27]</ref>). Meanwhile, some advanced training strategies <ref type="bibr" target="#b45">[46]</ref> can be used for better performance. We leave as our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Architecture of CenterNet. A convolutional backbone network applies cascade corner pooling and center pooling to output two corner heatmaps and a center keypoint heatmap, respectively. Similar to CornerNet, a pair of detected corners and the similar embeddings are used to detect a potential bounding box. Then the detected center keypoints are used to determine the final bounding boxes. Method FD FD 5 FD 25 FD 50 FD S FD M FD L CornerNet 37.8 32.7 36.8 43.8 60.3 33.2 25.1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a) The central region when n = 3. (b) The central region when n = 5. The solid rectangles denote the predicted bounding boxes and the shaded regions denote the central regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>(a) Center pooling takes the maximum values in both horizontal and vertical directions. (b) Corner pooling only takes the maximum values in boundary directions. (c) Cascade corner pooling takes the maximum values in both boundary directions and internal directions of objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>(a) and (b) show the small incorrect bounding boxes are significantly reduced by modeling center information. (c) and (d) show that the center information works for reducing medium and large incorrect bounding boxes. (e) shows the results of detecting the center keypoints without/with the center pooling. (f) shows the results of detecting the corners with corner pooling and cascade corner pooling, respectively. The blue boxes above denote the ground-truth. The red boxes and dots denote the predicted bounding boxes and keypoints, respectively. Some qualitative detection results on the MS-COCO validation dataset. Only detections with scores higher than 0.5 are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig- ure 6</head><label>6</label><figDesc>(f) shows the result of detecting corners with corner pooling or cascade corner pooling. We can see that cascade corner pooling can successfully locate a pair of corners for the cat on the left while corner pooling cannot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>: False discovery rates (%) of CornerNet. The false</cell></row><row><cell>discovery rate reflects the distribution of incorrect bounding</cell></row><row><cell>boxes. The results suggest the incorrect bounding boxes</cell></row><row><cell>account for a large proportion.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 shows</head><label>2</label><figDesc>the comparison with the state-of-the-art detectors on the MS-COCO test-dev set.Compared with the baseline CornerNet<ref type="bibr" target="#b19">[20]</ref>, the proposed CenterNet achieves a remarkable improvement. For example, CenterNet511-52 (means that the resolution of input images is 511 ? 511 and the backbone is Hourglass-52) reports a single-scale testing AP of 41.6%, an improvement of 3.8% over 37.8%, and a multi-scale testing AP of 43.5%, an improvement of 4.1% over 39.4%, achieved by Corner-Net under the same setting. When using the deeper backbone (i.e., Hourglass-104), the AP improvement over Cor-nerNet are 4.4% (from 40.5% to 44.9%) and 4.9% (from 42.1% to 47.0%) under the single-scale and multi-scale testing, respectively. These results firmly demonstrate the effectiveness of CenterNet.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Train input</cell><cell>Test input</cell><cell>AP</cell><cell cols="11">AP50 AP75 APS APM APL AR1 AR10 AR100 ARS ARM ARL</cell></row><row><cell>Two-stage:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeNet [40]</cell><cell>ResNet-101 [14]</cell><cell>512?512</cell><cell>512?512</cell><cell>33.8</cell><cell>53.4</cell><cell>36.1</cell><cell>12.3</cell><cell>36.1</cell><cell cols="2">50.8 29.6</cell><cell>42.6</cell><cell>43.5</cell><cell>19.2</cell><cell>46.9</cell><cell>64.3</cell></row><row><cell>CoupleNet [47]</cell><cell>ResNet-101</cell><cell>ori.</cell><cell>ori.</cell><cell>34.4</cell><cell>54.8</cell><cell>37.2</cell><cell>13.4</cell><cell>38.1</cell><cell cols="2">50.8 30.0</cell><cell>45.0</cell><cell>46.4</cell><cell>20.7</cell><cell>53.1</cell><cell>68.5</cell></row><row><cell>Faster R-CNN by G-RMI [16]</cell><cell>Inception-ResNet-v2 [39]</cell><cell>? 1000?600</cell><cell>? 1000?600</cell><cell>34.7</cell><cell>55.5</cell><cell>36.7</cell><cell>13.5</cell><cell>38.1</cell><cell>52.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Faster R-CNN +++ [14]</cell><cell>ResNet-101</cell><cell>? 1000?600</cell><cell>? 1000?600</cell><cell>34.9</cell><cell>55.7</cell><cell>37.4</cell><cell>15.6</cell><cell>38.7</cell><cell>50.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Faster R-CNN w/ FPN [23]</cell><cell>ResNet-101</cell><cell>? 1000?600</cell><cell>? 1000?600</cell><cell>36.2</cell><cell>59.1</cell><cell>39.0</cell><cell>18.2</cell><cell>39.0</cell><cell>48.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Faster R-CNN w/ TDM [37]</cell><cell>Inception-ResNet-v2</cell><cell>-</cell><cell>-</cell><cell>36.8</cell><cell>57.7</cell><cell>39.2</cell><cell>16.2</cell><cell>39.8</cell><cell cols="2">52.1 31.6</cell><cell>49.3</cell><cell>51.9</cell><cell>28.1</cell><cell>56.6</cell><cell>71.1</cell></row><row><cell>D-FCN [7]</cell><cell>Aligned-Inception-ResNet</cell><cell>? 1000?600</cell><cell>? 1000?600</cell><cell>37.5</cell><cell>58.0</cell><cell>-</cell><cell>19.4</cell><cell>40.1</cell><cell>52.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Regionlets [43]</cell><cell>ResNet-101</cell><cell>? 1000?600</cell><cell>? 1000?600</cell><cell>39.3</cell><cell>59.8</cell><cell>-</cell><cell>21.7</cell><cell>43.7</cell><cell>50.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Mask R-CNN [12]</cell><cell>ResNeXt-101</cell><cell>? 1300?800</cell><cell>? 1300?800</cell><cell>39.8</cell><cell>62.3</cell><cell>43.4</cell><cell>22.1</cell><cell>43.2</cell><cell>51.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Soft-NMS [2]</cell><cell>Aligned-Inception-ResNet</cell><cell>? 1300?800</cell><cell>? 1300?800</cell><cell>40.9</cell><cell>62.8</cell><cell>-</cell><cell>23.3</cell><cell>43.6</cell><cell>53.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Fitness R-CNN [41]</cell><cell>ResNet-101</cell><cell>512?512</cell><cell>1024?1024</cell><cell>41.8</cell><cell>60.9</cell><cell>44.9</cell><cell>21.5</cell><cell>45.0</cell><cell>57.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Cascade R-CNN [4]</cell><cell>ResNet-101</cell><cell>-</cell><cell>-</cell><cell>42.8</cell><cell>62.1</cell><cell>46.3</cell><cell>23.7</cell><cell>45.5</cell><cell>55.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Grid R-CNN w/ FPN [28]</cell><cell>ResNeXt-101</cell><cell>? 1300?800</cell><cell>? 1300?800</cell><cell>43.2</cell><cell>63.0</cell><cell>46.6</cell><cell>25.1</cell><cell>46.5</cell><cell>55.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">D-RFCN + SNIP (multi-scale) [38] DPN-98 [5]</cell><cell cols="3">? 2000?1200 ? 2000?1200 45.7</cell><cell>67.3</cell><cell>51.1</cell><cell>29.3</cell><cell>48.8</cell><cell>57.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PANet (multi-scale) [26]</cell><cell>ResNeXt-101</cell><cell>? 1400?840</cell><cell>? 1400?840</cell><cell>47.4</cell><cell>67.2</cell><cell>51.8</cell><cell>30.1</cell><cell>51.7</cell><cell>60.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>One-stage:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>YOLOv2 [32]</cell><cell>DarkNet-19</cell><cell>544?544</cell><cell>544?544</cell><cell>21.6</cell><cell>44.0</cell><cell>19.2</cell><cell>5.0</cell><cell>22.4</cell><cell cols="2">35.5 20.7</cell><cell>31.6</cell><cell>33.3</cell><cell>9.8</cell><cell>36.5</cell><cell>54.4</cell></row><row><cell>DSOD300 [34]</cell><cell>DS/64-192-48-1</cell><cell>300?300</cell><cell>300?300</cell><cell>29.3</cell><cell>47.3</cell><cell>30.6</cell><cell>9.4</cell><cell>31.5</cell><cell cols="2">47.0 27.3</cell><cell>40.7</cell><cell>43.0</cell><cell>16.7</cell><cell>47.1</cell><cell>65.0</cell></row><row><cell>GRP-DSOD320 [35]</cell><cell>DS/64-192-48-1</cell><cell>320?320</cell><cell>320?320</cell><cell>30.0</cell><cell>47.9</cell><cell>31.8</cell><cell>10.9</cell><cell>33.6</cell><cell cols="2">46.3 28.0</cell><cell>42.1</cell><cell>44.5</cell><cell>18.8</cell><cell>49.1</cell><cell>65.0</cell></row><row><cell>SSD513 [27]</cell><cell>ResNet-101</cell><cell>513?513</cell><cell>513?513</cell><cell>31.2</cell><cell>50.4</cell><cell>33.3</cell><cell>10.2</cell><cell>34.5</cell><cell cols="2">49.8 28.3</cell><cell>42.1</cell><cell>44.4</cell><cell>17.6</cell><cell>49.2</cell><cell>65.8</cell></row><row><cell>DSSD513 [8]</cell><cell>ResNet-101</cell><cell>513?513</cell><cell>513?513</cell><cell>33.2</cell><cell>53.3</cell><cell>35.2</cell><cell>13.0</cell><cell>35.4</cell><cell cols="2">51.1 28.9</cell><cell>43.5</cell><cell>46.2</cell><cell>21.8</cell><cell>49.1</cell><cell>66.4</cell></row><row><cell>RefineDet512 (single-scale) [45]</cell><cell>ResNet-101</cell><cell>512?512</cell><cell>512?512</cell><cell>36.4</cell><cell>57.5</cell><cell>39.5</cell><cell>16.6</cell><cell>39.9</cell><cell>51.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CornerNet511 (single-scale) [20]</cell><cell>Hourglass-52</cell><cell>511?511</cell><cell>ori.</cell><cell>37.8</cell><cell>53.7</cell><cell>40.1</cell><cell>17.0</cell><cell>39.0</cell><cell cols="2">50.5 33.9</cell><cell>52.3</cell><cell>57.0</cell><cell>35.0</cell><cell>59.3</cell><cell>74.7</cell></row><row><cell>RetinaNet800 [24]</cell><cell>ResNet-101</cell><cell>800?800</cell><cell>800?800</cell><cell>39.1</cell><cell>59.1</cell><cell>42.3</cell><cell>21.8</cell><cell>42.7</cell><cell>50.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CornerNet511 (multi-scale) [20]</cell><cell>Hourglass-52</cell><cell>511?511</cell><cell>?1.5?</cell><cell>39.4</cell><cell>54.9</cell><cell>42.3</cell><cell>18.9</cell><cell>41.2</cell><cell cols="2">52.7 35.0</cell><cell>53.5</cell><cell>57.7</cell><cell>36.1</cell><cell>60.1</cell><cell>75.1</cell></row><row><cell>CornerNet511 (single-scale) [20]</cell><cell>Hourglass-104</cell><cell>511?511</cell><cell>ori.</cell><cell>40.5</cell><cell>56.5</cell><cell>43.1</cell><cell>19.4</cell><cell>42.7</cell><cell cols="2">53.9 35.3</cell><cell>54.3</cell><cell>59.1</cell><cell>37.4</cell><cell>61.9</cell><cell>76.9</cell></row><row><cell>RefineDet512 (multi-scale) [45]</cell><cell>ResNet-101</cell><cell>512?512</cell><cell>?2.25?</cell><cell>41.8</cell><cell>62.9</cell><cell>45.7</cell><cell>25.6</cell><cell>45.1</cell><cell>54.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CornerNet511 (multi-scale) [20]</cell><cell>Hourglass-104</cell><cell>511?511</cell><cell>?1.5?</cell><cell>42.1</cell><cell>57.8</cell><cell>45.3</cell><cell>20.8</cell><cell>44.8</cell><cell cols="2">56.7 36.4</cell><cell>55.7</cell><cell>60.0</cell><cell>38.5</cell><cell>62.7</cell><cell>77.4</cell></row><row><cell>CenterNet511 (single-scale)</cell><cell>Hourglass-52</cell><cell>511?511</cell><cell>ori.</cell><cell>41.6</cell><cell>59.4</cell><cell>44.2</cell><cell>22.5</cell><cell>43.1</cell><cell cols="2">54.1 34.8</cell><cell>55.7</cell><cell>60.1</cell><cell>38.6</cell><cell>63.3</cell><cell>76.9</cell></row><row><cell>CenterNet511 (single-scale)</cell><cell>Hourglass-104</cell><cell>511?511</cell><cell>ori.</cell><cell>44.9</cell><cell>62.4</cell><cell>48.1</cell><cell>25.6</cell><cell>47.4</cell><cell cols="2">57.4 36.1</cell><cell>58.4</cell><cell>63.3</cell><cell>41.3</cell><cell>67.1</cell><cell>80.2</cell></row><row><cell>CenterNet511 (multi-scale)</cell><cell>Hourglass-52</cell><cell>511?511</cell><cell>?1.8?</cell><cell>43.5</cell><cell>61.3</cell><cell>46.7</cell><cell>25.3</cell><cell>45.3</cell><cell cols="2">55.0 36.0</cell><cell>57.2</cell><cell>61.3</cell><cell>41.4</cell><cell>64.0</cell><cell>76.3</cell></row><row><cell>CenterNet511 (multi-scale)</cell><cell>Hourglass-104</cell><cell>511?511</cell><cell>?1.8?</cell><cell>47.0</cell><cell>64.5</cell><cell>50.7</cell><cell>28.9</cell><cell>49.9</cell><cell cols="2">58.9 37.5</cell><cell>60.3</cell><cell>64.8</cell><cell>45.1</cell><cell>68.3</cell><cell>79.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Performance comparison (%) with the state-of-the-art methods on the MS-COCO test-dev dataset. CenterNet outperforms all existing one-stage detectors by a large margin and ranks among the top of state-of-the-art two-stage detectors.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Method FD FD 5 FD 25 FD 50 FD S FD M FD L</figDesc><table><row><cell>CornerNet511-52</cell><cell>40.4 35.2 39.4</cell><cell>46.7 62.5 36.9 28.0</cell></row><row><cell>CenterNet511-52</cell><cell>35.1 30.7 34.2</cell><cell>40.8 53.0 31.3 24.4</cell></row><row><cell cols="2">CornerNet511-104 37.8 32.7 36.8</cell><cell>43.8 60.3 33.2 25.1</cell></row><row><cell cols="2">CenterNet511-104 32.4 28.2 31.6</cell><cell>37.5 50.7 27.1 23.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Comparison of false discovery rates (%) of Cor-</cell></row><row><cell>nerNet and CenterNet on the MS-COCO validation dataset.</cell></row><row><cell>The results suggest CenterNet avoids a large number of</cell></row><row><cell>incorrect bounding boxes, especially for small incorrect</cell></row><row><cell>bounding boxes.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on the major components of CenterNet511-52 on the MS-COCO validation dataset. The CRE denotes central region exploration, the CTP denotes center pooling, and the CCP denotes cascade corner pooling.</figDesc><table><row><cell>Method</cell><cell>AP</cell><cell cols="5">AP50 AP75 APS APM APL</cell></row><row><cell>CenterNet511-52 w/o GT</cell><cell>41.3</cell><cell>59.2</cell><cell>43.9</cell><cell>23.6</cell><cell>43.8</cell><cell>55.8</cell></row><row><cell>CenterNet511-52 w/ GT</cell><cell>56.5</cell><cell>78.3</cell><cell>61.4</cell><cell>39.1</cell><cell>60.3</cell><cell>70.3</cell></row><row><cell cols="2">CenterNet511-104 w/o GT 44.8</cell><cell>62.4</cell><cell>48.2</cell><cell>25.9</cell><cell>48.9</cell><cell>58.8</cell></row><row><cell>CenterNet511-104 w/ GT</cell><cell>58.1</cell><cell>78.4</cell><cell>63.9</cell><cell>40.4</cell><cell>63.0</cell><cell>72.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">FD = 1 ? AP, where AP denotes the average precision at IoU = [0.05 : 0.05 : 0.5] on the MS-COCO dataset. Also, FD i = 1 ? AP i , where AP i denotes the average precision at IoU = i/100, FD scale = 1 ? AP scale , where scale = {small, medium, large}, denotes the scale of object.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For the topmost, leftmost, bottommost and rightmost boundary, look vertically towards the bottom, horizontally towards the right, vertically towards the top and horizontally towards the left, respectively.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2874" to="2883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Softnms-improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5561" to="5569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="354" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dual path networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4467" to="4475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<title level="m">Dssd: Deconvolutional single shot detector</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Object detection via a multiregion and semantic segmentation-aware cnn model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1134" to="1142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Diagnosing error in object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chodpathumwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="340" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7310" to="7311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Enhancement of ssd by concatenating feature maps for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer science</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ron: Reverse connection with objectness prior networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5936" to="5944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision</title>
		<meeting>the European conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Me r-cnn: Multi-expert r-cnn for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01069</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Scale-aware trident networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.01892</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Grid r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dsod: Learning deeply supervised object detectors from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1919" to="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning object detectors from scratch with gated recurrent feature pyramids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00886</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Contextual priming and feedback for faster r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="330" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Beyond skip connections: Top-down modulation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.06851</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An analysis of scale invariance in object detection snip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3578" to="3587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Denet: Scalable realtime object detection with directed sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tychsen-Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="428" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improving object localization with fitness nms and bounded iou loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tychsen-Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6877" to="6885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Selective search for object recognition. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="154" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep regionlets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision</title>
		<meeting>the European conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="798" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Gated bi-directional cnn for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="354" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Single-shot refinement neural network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4203" to="4212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Scratchdet: Training single-shot object detectors from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Couplenet: Coupling global structure with local parts for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4126" to="4134" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
