<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">dyngraph2vec: Capturing Network Dynamics using Dynamic Graph Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Palash</forename><surname>Goyal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute</orgName>
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<addrLine>4676 Admiralty Way, Suite 1001. Marina del Rey</addrLine>
									<postCode>90292</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rokka</forename><surname>Sujit</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California-Irvine Irvine</orgName>
								<address>
									<postCode>92697</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chhetri</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California-Irvine Irvine</orgName>
								<address>
									<postCode>92697</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arquimedes</forename><surname>Canedo</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Siemens Corporate Technology</orgName>
								<address>
									<addrLine>755 College Rd E</addrLine>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">dyngraph2vec: Capturing Network Dynamics using Dynamic Graph Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph embedding techniques</term>
					<term>Graph embedding applications</term>
					<term>Python Graph Embedding Methods GEM Library</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning graph representations is a fundamental task aimed at capturing various properties of graphs in vector space. The most recent methods learn such representations for static networks. However, real-world networks evolve over time and have varying dynamics. Capturing such evolution is key to predicting the properties of unseen networks. To understand how the network dynamics affect the prediction performance, we propose an embedding approach which learns the structure of evolution in dynamic graphs and can predict unseen links with higher precision. Our model, dyngraph2vec, learns the temporal transitions in the network using a deep architecture composed of dense and recurrent layers. We motivate the need for capturing dynamics for the prediction on a toy data set created using stochastic block models. We then demonstrate the efficacy of dyngraph2vec over existing state-ofthe-art methods on two real-world data sets. We observe that learning dynamics can improve the quality of embedding and yield better performance in link prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Understanding and analyzing graphs is an essential topic that has been widely studied over the past decades. Many realworld problems can be formulated as link predictions in graphs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. For example, link prediction in an author collaboration network <ref type="bibr" target="#b0">[1]</ref> can be used to predict potential future author collaboration. Similarly, new connections between proteins can be discovered using protein interaction networks <ref type="bibr" target="#b4">[5]</ref>, and new friendships can be predicted using social networks <ref type="bibr" target="#b5">[6]</ref>. Recent work on obtaining such predictions use graph representation learning. These methods represent each node in the network with a fixed dimensional embedding and map link prediction in the network space to the nearest neighbor search in the embedding space <ref type="bibr" target="#b6">[7]</ref>. It has been shown that such techniques can outperform traditional link prediction methods on graphs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>Existing works on graph representation learning primarily focus on static graphs of two types: (i) aggregated, consisting of all edges until time T ; and (ii) snapshot, which comprise of edges at the current time step t. These models learn latent representations of the static graph and use them to predict missing links <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14]</ref>. However, real networks often have complex dynamics which govern their evolution. As an illustration, consider the social network shown in <ref type="figure" target="#fig_0">Figure 1</ref>. In this example, user A moves from one friend to another in such a way that only a friend of a friend is followed and making sure not to befriend an old friend. Methods based on static networks can only observe the network at time t + 1 and cannot ascertain if A will befriend B or D in the next time step. Instead, observing multiple snapshots can capture the network dynamics and predict A's connection to D with high certainty.</p><p>In this work, we aim to capture the underlying network dynamics of evolution. Given temporal snapshots of graphs, our goal is to learn a representation of nodes at each time step while capturing the dynamics such that we can predict their fu-ture connections. Learning such representations is a challenging task. Firstly, the temporal patterns may exist over varying period lengths. For example, in <ref type="figure" target="#fig_0">Figure 1</ref>, user A may hold to each friend for a varying k length. Secondly, different vertices may have different patterns. In <ref type="figure" target="#fig_0">Figure 1</ref>, user A may break ties with friends whereas other users continue with their ties. Capturing such variations is extremely challenging. Existing research builds upon simplified assumptions to overcome these challenges. Methods including DynamicTriad <ref type="bibr" target="#b14">[15]</ref>, Dyn-GEM <ref type="bibr" target="#b15">[16]</ref> and TIMERS <ref type="bibr" target="#b16">[17]</ref> assume that the patterns are of short duration (length 2) and only consider the previous time step graph to predict new links. Furthermore, DynGEM and TIMERS make the assumption that the changes are smooth and use a regularization to disallow rapid changes.</p><p>In this work, we present a model which overcomes the above challenges. dyngraph2vec uses multiple non-linear layers to learn structural patterns in each network. Furthermore, it uses recurrent layers to learn the temporal transitions in the network. The look back parameter in the recurrent layers controls the length of temporal patterns learned. We focus our experiments on the task of link prediction. We compare dyngraph2vec with the state-of-the-art algorithms for dynamic graph embedding and show its performance on several real-world networks including collaboration networks and social networks. Our experiments show that using a deep model with recurrent layers can capture temporal dynamics of the networks and significantly outperform the state-of-the-art methods on link prediction. We emphasize that our work is targeted towards link prediction and not node classification. Furthermore, our algorithm works on both aggregated and snapshot temporal graphs.</p><p>Overall, our paper makes the following contributions:</p><p>1. We propose dyngraph2vec, a dynamic graph embedding model which captures temporal dynamics. 2. We demonstrate that capturing network dynamics can significantly improve the performance on link prediction. 3. We present variations of our model to show the key advantages and differences. 4. We publish a library, DynamicGEM 1 , implementing the variations of our model and state-of-the-art dynamic embedding approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Graph representation learning techniques can be broadly divided into two categories: (i) static graph embedding, which represents each node in the graph with a single vector; and (ii) dynamic graph embedding, which considers multiple snapshots of a graph and obtains a time series of vectors for each node. Most analysis has been done on static graph embedding. Recently, however, some works have been devoted to studying dynamic graph embedding. 1 https://github.com/palash1992/DynamicGEM</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Static Graph Embedding</head><p>Methods to represent nodes of a graph typically aim to preserve certain properties of the original graph in the embedding space. Based on this observation, methods can be divided into (i) distance preserving, and (ii) structure preserving. Distance preserving methods devise objective functions such that the distance between nodes in the original graph and the embedding space have similar rankings. For example, Laplacian Eigenmaps <ref type="bibr" target="#b17">[18]</ref> minimizes the sum of the distance between the embeddings of neighboring nodes under the constraints of translational invariance, thus keeping the nodes close in the embedding space. Similarly, Graph Factorization <ref type="bibr" target="#b9">[10]</ref> approximates the edge weight with the dot product of the nodes' embeddings, thus preserving distance in the inner product space. Recent methods have gone further to preserve higher order distances. Higher Order Proximity Embedding (HOPE) <ref type="bibr" target="#b8">[9]</ref> uses multiple higher-order functions to compute a similarity matrix from a graph's adjacency matrix and uses Singular Value Decomposition (SVD) to learn the representation. GraRep <ref type="bibr" target="#b11">[12]</ref> considers the node transition matrix and its higher powers to construct a similarity matrix.</p><p>On the other hand, structure-preserving methods aim to preserve the roles of individual nodes in the graph. node2vec <ref type="bibr" target="#b7">[8]</ref> uses a combination of breadth-first search and depth-first search to find nodes similar to a node in terms of distance and role. Recently, deep learning methods to learn network representations have been proposed. These methods inherently preserve the higher order graph properties including distance and structure. SDNE <ref type="bibr" target="#b18">[19]</ref>, DNGR <ref type="bibr" target="#b19">[20]</ref> and VGAE <ref type="bibr" target="#b20">[21]</ref> use deep autoencoders for this purpose. Some other recent approaches use graph convolutional networks to learn inherent graph structure <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Dynamic Graph Embedding</head><p>Embedding dynamic graphs is an emerging topic still under investigation. Some methods have been proposed to extend static graph embedding approaches by adding regularization <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b16">17]</ref>. DynGEM <ref type="bibr" target="#b25">[26]</ref> uses the learned embedding from previous time step graphs to initialize the current time step embedding. Although it does not explicitly use regularization, such initialization implicitly keeps the new embedding close to the previous. DynamicTriad <ref type="bibr" target="#b14">[15]</ref> relaxes the temporal smoothness assumption but only considers patterns spanning two-time steps. TIMERS <ref type="bibr" target="#b16">[17]</ref> incrementally updates the embedding using incremental Singular Value Decomposition (SVD) and reruns SVD when the error increases above a threshold.</p><p>DYLINK2VEC <ref type="bibr" target="#b26">[27]</ref> learns embedding of links (node pairs instead of nodes) and uses temporal functions to learn patterns over time.</p><p>Link embedding renders the method non-scalable for graphs with high density. Our model uses recurrent layers to learn temporal patterns over long sequences of graphs and multiple fully connected layer to capture intricate patterns at each time step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Dynamic Link Prediction</head><p>Several methods have been proposed on dynamic link prediction without emphasis on graph embedding. Many of these methods use probabilistic non-parametric approaches <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>. NonParam <ref type="bibr" target="#b27">[28]</ref> uses kernel functions to model the dynamics of individual node features influenced by the neighbor features. Another class of algorithms uses matrix and tensor factorizations to model link dynamics <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>. Further, many dynamic link prediction models have been proposed for specific applications including recommendation systems <ref type="bibr" target="#b31">[32]</ref> and attributed graphs <ref type="bibr" target="#b32">[33]</ref>. These methods often have assumptions about the inherent structure of the network and require node attributes. Our model, however, extends the traditional graph embedding framework to capture network dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Motivating Example</head><p>We consider a toy example to motivate the idea of capturing network dynamics. Consider an evolution of graph G, G = {G 1 , .., G T }, where G t represents the state of graph at time t. The initial graph G 1 is generated using the Stochastic Block Model <ref type="bibr" target="#b33">[34]</ref> with 2 communities (represented by colors indigo and yellow in <ref type="figure" target="#fig_3">Figure 3</ref>), each with 500 nodes (the figure shows a total of 50 nodes for ease of visualization). The in-block and cross-block probabilities are set to 0.1 and 0.01 respectively. The evolution pattern can be defined as a three-step process. In the first step (shown in <ref type="figure" target="#fig_3">Figure 3</ref>(a)), we randomly and uniformly select 10 nodes (colored red in <ref type="figure" target="#fig_3">Figure 3</ref> which shows 2 of these nodes) from the yellow community. In step two (shown in <ref type="figure" target="#fig_3">Figure 3</ref>(b)), we randomly add 30 edges between each of the selected nodes in step one and random nodes in the Indigo community. This is similar to having more than cross-block probability but less than in-block probability. In step three (shown in <ref type="figure" target="#fig_3">Figure 3</ref>(c)), the community membership of the nodes selected in step 2 is changed from yellow to indigo. Similarly, the edges (colored red in <ref type="figure" target="#fig_3">Figure 3</ref>) are either removed or added to reflect the cross-block and in-block connection probabilities. Then, for the next time step (shown in <ref type="figure" target="#fig_3">Figure 3</ref>(d)), the same three steps are repeated to evolve the graph. Informally, this can be interpreted as a two-step movement of users from one community to another by initially increasing friends in the other community and subsequently moving to it.</p><p>Our task is to learn the embeddings predictive of the change in community of the 10 nodes. <ref type="figure" target="#fig_2">Figure 2</ref> shows the results of the state-of-the-art dynamic graph embedding techniques (Dyn-GEM, optimalSVD, and DynamicTriad) and the three variations of our model: dyngraph2vecAE, dyngraph2vecRNN and dyn-graph2vecAERNN (see Methodology Section for the description of the methods). <ref type="figure" target="#fig_2">Figure 2</ref> shows the embeddings of nodes after the first step of evolution. The nodes selected for community shift are colored in red. We show the results for 4 runs of the model to ensure robustness. <ref type="figure" target="#fig_2">Figure 2</ref>(a) shows that Dyn-GEM brings the red nodes closer to the edge of the yellow community but does not move any of the nodes to the other community. Similarly, DynamicTriad results in <ref type="figure" target="#fig_2">Figure 2</ref>(c) show that it only shifts 1 to 4 nodes to its actual community in the next step. The optimalSVD method in <ref type="figure" target="#fig_2">Figure 2</ref>(b) is not able to shift any nodes. However, our dyngraph2vecAE and dyngraph2vecRNN, and dyngraph2vecAERNN (shown in <ref type="figure" target="#fig_2">Figure 2</ref>(d-f)) successfully capture the dynamics and move the embedding of most   of the 10 selected nodes to the indigo community, keeping the rest of the nodes intact. This shows that capturing dynamics is critical in understanding the evolution of networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head><p>In this section, we define the problem statement. We then explain multiple variations of deep learning models capable of capturing temporal patterns in dynamic graphs. Finally, we design the loss functions and optimization approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Problem Statement</head><p>Consider a weighted graph G(V, E), with V and E as the set of vertices and edges respectively. We denote the adjacency matrix of G by A, i.e. for an edge (i, j) ? E, A i j denotes its weight, else A i j = 0. An evolution of graph G is denoted as G = {G 1 , .., G T }, where G t represents the state of graph at time t.</p><p>We define our problem as follows: Given an evolution of graph G, G, we aim to represent each node v in a series of lowdimensional vector space y v 1 , . . . y v t , where y v t is the embedding of node v at time t, by learning mappings f t :</p><formula xml:id="formula_0">{V 1 , . . . , V t , E 1 , . . . E t } ? R d and y v i = f i (v 1 , . . . , v i , E 1 , . . . E i )</formula><p>such that y v i can capture temporal patterns required to predict y v i+1 . In other words, the embedding function at each time step uses information from graph evolution to capture network dynamics and can thus predict links with higher precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">dyngraph2vec</head><p>Our dyngraph2vec is a deep learning model that takes as input a set of previous graphs and generates as output the graph at the next time step, thus capturing highly non-linear interactions between vertices at each time step and across multiple time steps. Since the embedding values capture the temporal evolution of the links, it allows us to predict the next time step</p><formula xml:id="formula_1">Algorithm 1: dyngraph2vec Function dyngraph2vec (Graphs G = {G 1 , .., G T }, Dimension d, Look back lb) Generate adjacency matrices A from G; ? ? RandomInit(); Set F = {(A t u )} for each u ? V, for each t ? {1..t}; for iter = 1 . . . I do M ? getArchitectureInput(F , lb); Choose L based on the architecture used; grad ? ?L/??; ? ? UpdateGradient(?, grad); Y ? EncoderForwardPass(G, ?); return Y</formula><p>graph link. The model learns the network embedding at time step t by optimizing the following loss function:</p><formula xml:id="formula_2">L t+l = (? t+l+1 ? A t+l+1 ) B 2 F , = ( f (A t , . . . , A t+l ) ? A t+l+1 ) B 2 F .<label>(1)</label></formula><p>Here we penalize the incorrect reconstruction of edges at time t+l+1 by using the embedding at time step t+l. Minimizing this loss function enforces the parameters to be tuned such that it can capture evolving patterns relations between nodes to predict the edges at a future time step. The embedding at time step t +d is a function of the graphs at time steps t, t+1, . . . , t+l where l is the temporal look back. We use a weighting matrix B to weight the reconstruction of observed edges higher than unobserved links as traditionally used in the literature <ref type="bibr" target="#b18">[19]</ref>. Here, B i j = ? for (i, j) ? E t+l+1 , else 1, where ? is a hyperparameter controlling the weight of penalizing observed edges. Note that represents elementwise product. We propose three variations of our model based on the architecture of deep learning models as shown in <ref type="figure" target="#fig_4">Figure 4</ref>: (i) dyngraph2vecAE, (ii) dyngraph2vecRNN, and (iii) dyn-graph2vecAERNN. Our three methods differ in the formulation of the function f (.). dyngraph2vecAE extends the autoencoders to the dynamic setting in a straightforward manner. Therefore, we overcome the limitations of capturing temporal information and the high number of model parameters through a newly proposed dyngraph2vecRNN and dyngraph2vecAERNN.</p><p>A simple way to extend the autoencoders traditionally used to embed static graphs <ref type="bibr" target="#b18">[19]</ref> to temporal graphs is to add the information about previous l graphs as input to the autoencoder. This model (dyngraph2vecAE) thus uses multiple fully connected layers to model the interconnection of nodes within and across time. Concretely, for a node u with neighborhood vector set u 1..t = [a u t , . . . , a u t+l ], the hidden representation of the first layer is learned as:</p><formula xml:id="formula_3">y (1) u t = f a (W (1) AE u 1..t + b (1) ),<label>(2)</label></formula><p>where f a is the activation function, W (1) AE ? R d (1) ?nl and d <ref type="bibr" target="#b0">(1)</ref> , n and l are the dimensions of representation learned by the first layer, number of nodes in the graph, and look back, respec- tively. The representation of the k th layer is defined as:</p><formula xml:id="formula_4">y (k) u t = f a (W (k) AE y (k?1) u t + b (k) ).<label>(3)</label></formula><p>Note that dyngraph2vecAE has O(nld <ref type="bibr" target="#b0">(1)</ref> ) parameters. As most real-world graphs are sparse, learning the parameters can be challenging.</p><p>To reduce the number of model parameters and achieve a more efficient temporal learning, we propose dyngraph2vecRNN and dyngraph2vecAERNN. In dyn-graph2vecRNN we use sparsely connected Long Short Term Memory (LSTM) networks to learn the embedding. LSTM is a type of Recurrent Neural Network (RNN) capable of handling long-term dependency problems. In dynamic graphs, there can be long-term dependencies which may not be captured by fully connected auto-encoders. The hidden state representation of a single LSTM network is defined as:</p><formula xml:id="formula_5">y (1) u t = o (1) u t * tanh(C (1) u t ) (4a) o (1) u t = ? u t (W (1) RNN [y (1) u t?1 , u 1..t ] + b (1) o ) (4b) C (1) u t = f (1) u t * C (1) u t?1 + i (1) u t * C (1) u t (4c) C (1) u t = tanh(W (1) C .[y (1) u t?1 , u 1..t + b (1) c ]) (4d) i (1) u t = ?(W (1) i .[y (1) u t?1 , u 1..t ] + b (1) i ) (4e) f (1) u t = ?(W (1) f .[y (1) u t?1 , u 1..t + b (1) f ])<label>(4f)</label></formula><p>where C u t represents the cell states of LSTM, f u t is the value to trigger the forget gate, o u t is the value to trigger the output gate, i u t represents the value to trigger the update gate of the LSTM, C u t represents the new estimated candidate state, and b represents the biases. There can be l LSTM networks connected in the first layer, where the cell states and hidden representation are passed in a chain from t ? l to t LSTM networks. The representation of the k th layer is then given as follows:</p><formula xml:id="formula_6">y (k) u t = o (k) u t * tanh(C (k) u t ) (5a) o (k) u t = ? u t (W (k) RNN [y (k) u t?1 , y (k?1) u t ] + b (k) o )<label>(5b)</label></formula><p>The problem with passing the sparse neighbourhood vector u 1..t = [a u t , . . . , a u t+l ] of node u to the LSTM network is that the LSTM model parameters (such as the number of memory cells, number of input units, output units, etc.) needed to learn a low dimension representation become large. Rather, the LSTM network may be able to better learn the temporal representation if the sparse neighbourhood vector is reduced to a low dimension representation. To achieve this, we propose a variation of dyngraph2vec model called dyngraph2vecAERNN. In dyn-graph2vecAERNN instead of passing the sparse neighbourhood vector, we use a fully connected encoder to initially acquire low dimensional hidden representation given as follows:</p><formula xml:id="formula_7">y (p) u t = f a (W (p) AERNN y (p?1) u t + b (p) ).<label>(6)</label></formula><p>where p represents the output layer of the fully connected encoder. This representation is then passed to the LSTM networks.</p><formula xml:id="formula_8">y (p+1) u t = o (p+1) u t * tanh(C (p+1) u t ) (7a) o (p+1) u t = ? u t (W (p+1) AERNN [y (p+1) u t?1 , y (p) u t ] + b (p+1) o )<label>(7b)</label></formula><p>Then the hidden representation generated by the LSTM network is passed to a fully connected decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Optimization</head><p>We optimize the loss function defined above to get the optimal model parameters. By applying the gradient with respect to the decoder weights on equation 1, we get:</p><formula xml:id="formula_9">?L t ?W (K) * = [2(? t+1 ? A t+1 ) B][ ? f a (Y (K?1) W (K) * + b (K) ) ?W (K) * ],</formula><p>where W (K) * is the weight matrix of the penultimate layer for all the three models. For each individual model, we back propagate the gradients based on the neural units to get the derivatives for all previous layers. For the LSTM based dyngraph2vec models, back propagation through time is performed to update the weights of the LSTM networks.</p><p>After obtaining the derivatives, we optimize the model using stochastic gradient descent (SGD) <ref type="bibr" target="#b34">[35]</ref> with Adaptive Moment Estimation (Adam) <ref type="bibr" target="#b35">[36]</ref>. The algorithm is specified in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we describe the data sets used and establish the baselines for comparison. Furthermore, we define the evaluation metrics for our experiments and parameter settings. All the experiments were performed on a 64 bit Ubuntu 16.04.1 LTS system with Intel (R) Core (TM) i9-7900X CPU with 19 processors, 10 CPU cores, 3.30 GHz CPU clock frequency, 64 GB RAM, and two Nvidia Titan X, each with 12 GB memory. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>We conduct experiments on two real-world datasets and a synthetic dataset to evaluate our proposed algorithm. We assume that the proposed models are aware of all the nodes, and that no new nodes are introduced in subsequent time steps. Rather, the links between the existing nodes change with a certain temporal pattern. The datasets are summarized in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stochastic Block Model (SBM) -community diminishing:</head><p>In order to test the performance of various static and dynamic graph embedding algorithms, we generated synthetic SBM data with two communities and a total of 1000 nodes. The crossblock connectivity probability is 0.01 and in-block connectivity probability is set to 0.1. One of the communities is continuously diminished by migrating the 10-20 nodes to the other community. A total of 10 dynamic graphs are generated for the evaluation. Since SBM is a synthetic dataset, there is no notion of time steps..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hep-th [1]:</head><p>The first real-world data set used to test the dynamic graph embedding algorithms is the collaboration graph of authors in High Energy Physics Theory conference. The original data set contains abstracts of papers in High Energy Physics Theory conference in the period from January 1993 to April 2003. Hence, the resolution of the time step is one month. This graph is aggregated over the months. For our evaluation, we consider the last 50 snapshots of this dataset. From this dataset 2000 nodes are sampled for training and testing the proposed models.</p><p>Autonomous Systems (AS) <ref type="bibr" target="#b36">[37]</ref>: The second real-world dataset utilized is a communication network of who-talks-to-whom from the BGP (Border Gateway Protocol) logs. The dataset contains 733 instances spanning from November 8, 1997, to January 2, 2000. Hence, the resolution of time step for the AS dataset is one month. However, they are snapshots of each month instead of an aggregation as in Hep-th. For our evaluation, we consider a subset of this dataset which contains the last 50 snapshots. From this dataset 2000 nodes are sampled for training and testing the proposed models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Baselines</head><p>We compare our model with the following state-of-the-art static and dynamic graph embedding methods:</p><p>? Optimal Singular Value Decomposition (OptimalSVD) <ref type="bibr" target="#b37">[38]</ref>: It uses the singular value decomposition of the adjacency matrix or its variation (i.e., the transition matrix) to represent the individual nodes in the graph. The low rank SVD decomposition with largest d singular values are then used for graph structure matching, clustering, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Incremental</head><p>Singular Value Decomposition (IncSVD) <ref type="bibr" target="#b38">[39]</ref>: It utilizes a perturbation matrix which captures the changing dynamics of the graphs and performs additive modification on the SVD.</p><p>? Rerun Singular Value Decomposition (RerunSVD or TIMERS) <ref type="bibr" target="#b16">[17]</ref>: It utilizes the incremental SVD to get the dynamic graph embedding, however, it also uses a tolerance threshold to restart the optimal SVD calculation when the incremental graph embedding starts to deviate.</p><p>? Dynamic Embedding using Dynamic Triad Closure Process (dynamicTriad) <ref type="bibr" target="#b14">[15]</ref>: It utilizes the triadic closure process to generate a graph embedding that preserves structural and evolution patterns of the graph.</p><p>? Deep Embedding Method for Dynamic Graphs (dynGEM) <ref type="bibr" target="#b15">[16]</ref>: It utilizes deep auto-encoders to incrementally generate embedding of a dynamic graph at snapshot t by using only the snapshot at time t ? 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Evaluation Metrics</head><p>In our experiments, we evaluate our model on link prediction at time step t + 1 by using all graphs until the time step t . We use Mean Average Precision (MAP) as our metrics. precision@k is the fraction of correct predictions in the top k predictions. It is defined as P@k = . P@k values are used to test the top predictions made by the model. MAP values are more robust and average the predictions for all nodes. High MAP values imply that the model can make good predictions for most nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results and Analysis</head><p>In this section, we present the performance result of various models for link prediction on different datasets. We train the model on graphs from time step t to t + l where l is the lookback of the model, and predict the links of the graph at time step t + l + 1. The lookback l is a model hyperparameter. For an evolving graph with T steps, we perform the above prediction from T/2 to T and report the average MAP of link prediction. Furthermore, we also present the performance of models when an increasing length of the graph sequence are provided in the training data. Unless explicitly mentioned, for the models consisting of recurrent neural network, a lookback value of 3 is used for the training and testing purpose.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">SBM Dataset</head><p>The MAP values for various algorithms with SBM dataset with a diminishing community is shown in <ref type="figure" target="#fig_7">Figure 5</ref>. The MAP values shown are for link prediction with embedding sizes 64, 128 and 256. This figure shows that our methods dyn-graph2vecAE, dyngraph2vecRNN and dyngraph2vecAERNN all have higher MAP values compared to the rest of the baselines except for dynGEM. The dynGEM algorithm is able to have higher MAP values than all the algorithms. This is due to the fact that dynGEM also generates the embedding of the graph at snapshot t+1 using the graph at snapshot t. Since in our SBM dataset the node-migration criteria are introduced only one-time step earlier, the dynGEM node embedding technique is able to capture these dynamics. However, the proposed dyngraph2vec methods also achieve average MAP values within ?1.5% of the MAP values achieved by dynGEM. Notice that the MAP values of SVD based methods increase as the embedding size increases. However, this is not the case for dynTriad.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Hep-th Dataset</head><p>The link prediction results for the Hep-th dataset is shown in <ref type="figure" target="#fig_9">Figure 6</ref>. The proposed dyngraph2vec algorithms outperform all the other state-of-the-art static and dynamic algorithms. Among the proposed algorithms, dyngraph2vecAERNN has the highest MAP values, followed by dyngraph2vecRNN and dyngraph2vecAE, respectively. The dynamicTriad is able to perform better than the SVD based algorithms. Notice  that dynGEM is not able to have higher MAP values than the dyngraph2vec algorithms in the Hep-th dataset. Since dyn-graph2vec utilizes not only t ?1 but t ?l?1 time-steps to predict the link for the time-step t, it has higher performance compared to other state-of-the-art algorithms.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">AS Dataset</head><p>The MAP value for link prediction with various algorithms for the AS dataset is shown in <ref type="figure" target="#fig_11">Figure 7</ref>. dyngraph2vecAERNN outperforms all the state-of-the-art algorithms. The algorithm with the second highest MAP score is dyngraph2vecRNN. However, dyngraph2vecAE has a higher MAP only with a lower embedding of size 64. SVD methods are able to improve their MAP values by increasing the embedding size. However, they are not able to outperform the dyngraph2vec algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">MAP exploration</head><p>The summary of MAP values for different embedding sizes (64, 128 and 256) for different datasets is presented in <ref type="table" target="#tab_1">Table  2</ref>. The top three highest MAP values are highlighted in bold. For the synthetic SBM dataset, the top three algorithms with highest MAP values are dynGEM, dyngraph2VecAERNN, and dyngraph2vecRNN, respectively. Since the change pattern for the SBM dataset is introduced only at timestep t?1, dynGEM is able to better predict the links. The model architecture of dyn-GEM and dyngraph2vecAE are only different on what data are fed to train the model. In dyngraph2vecAE, we essentially feed more data depending on the size of the lookback. The lookback size increases the model complexity. Since the SBM dataset doesn't have temporal patterns evolving for more than one-time steps, the dyngraph2vec models are only able to achieve comparable but not better result compared to dynGEM. For the Hep-th dataset, the top three algorithm with highest MAP values are dyngraph2VecAERNN, dyngraph2VecRNN, and dyngraph2VecAE, respectively. In fact, compared to the state-of-the-art algorithm dynamicTriad, the proposed models dyngraph2VecAERNN(with lookback=8), dyngraph2VecRNN (with lookback=8)), and dyngraph2VecAE(with lookback=5) obtain ?105%, ?102%, and ?42% higher average MAP values, respectively.</p><p>For the AS dataset, the top three algorithm with highest MAP values are dyngraph2VecAERNN, dyngraph2VecRNN, and dyngraph2VecAE, respectively.</p><p>Compared to the state-of-the-art rerunSVD algorithm, the proposed models dyngraph2VecAERNN(with lookback=10), dyngraph2VecRNN (with lookback=10), and dyngraph2VecAE (with lookback=5) obtain ?137%, ?95%, and ?74% higher average MAP values, respectively.</p><p>These results show that the dyngraph2vec variants are able to capture the graph dynamics much better than most of the state-of-the-art algorithms in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Hyper-parameter Sensitivity: Lookback</head><p>One of the important parameters for time-series analysis is how much in the past the method looks to predict the future. To analyze the effect of look back on the MAP score we have trained the dyngraph2Vec algorithms with various look back values. The embedding dimension is fixed to 128. The look back size is varied from 1 to 10. We then tested the change in MAP values with the real word datasets AS and Hep-th.</p><p>Performance of dyngraph2Vec algorithms with various lookback values for the Hep-th dataset is presented in <ref type="figure" target="#fig_12">Figure 8</ref>. It can be noticed that increasing lookback values consistently increase the average MAP values. Moreover, it is interesting to notice that dyngraph2VecAE although has increased in performance until lookback size of 8, its performance is decreased for lookback value of 10. Since it does not have memory units to store the temporal patterns like the recurrent variations, it relies solely on the fully connected dense layers to encode to the pattern. This seems rather ineffective compared to the dyn-graph2VecRNN and dyngraph2vecAERNN for the Hep-th dataset. The highest MAP values achieved if by dyngraph2vecAERNN is 0.739 for the lookback size of 8. Similarly, the performance of the proposed models while changing the lookback size for AS dataset is presented in <ref type="figure" target="#fig_13">Figure 9</ref>. The average MAP values also increase with the increasing lookback size in the AS dataset. The highest MAP value of 0.3801 is again achieved by dyngraph2vecAERNN with the lookback size of 10. The dyngraph2vecAE model, initially, has comparable and sometimes even higher MAP value with respect to dyngraph2vecRNN. However, it can be noticed that for the lookback size of 10, the dyngraph2vecRNN outperforms dyngraph2vecAE model consisting of just the fully connected neural networks. In fact, the MAP value does not increase after the lookback size of 5 for dyngraph2vecAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6.">Length of training sequence versus MAP value</head><p>In this section, we present the impact of length of graph sequence supplied to the models during training on its performance. In order to conduct this experiment, the graph sequence provided as training data is increased one step at a time. Hence, we use graph sequence of length 1 to t ? [T, T + 1, T + 2, T + 3, . . . , T + n] to predict the links for graph at time step t ? [T + 1, T + 2, . . . , T + n + 1], where T ? lookback. The experiment is performed on Hep-th and AS dataset with a fixed lookback size of 8. The total sequence of data is 50 and it is split between 25 for training and 25 for testing. Hence, in the experiment the training data sequence increases from total of 25 sequence to 49 graph sequence. The results in <ref type="figure" target="#fig_0">Figure 10</ref> and 11 shows the average MAP values for predicting the links starting the graph sequence at 26 th to all the way to 50 th time-step. Where each time step represents a month.</p><p>The result of increasing the amount of graph sequence in training data for Hep-th dataset is shown in <ref type="figure" target="#fig_0">Figure 10</ref>. It can be noticed that for both the RNN and AERNN the increasing amount of graph sequence in the data does not drastically increase the MAP value. For, dyngraph2vecAE there is a slight increase in the MAP value towards the end. On the other hand, increasing the amount of graph sequence for the AS dataset during training gives a gradual improvement in link prediction performance in the testing phase. However, they start converging eventually after seeing 80% (total of 40 graph sequence) of the sequence data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head><p>Model Variation: It can be observed that among different proposed models, the recurrent variation was capable of achieving higher average MAP values. These architectures are efficient in learning short and long term temporal patterns and provide an edge in learning the temporal evolution of the graphs compared to the fully connected neural networks without recurrent units.</p><p>Dataset: We observe that depending on the dataset, the same model architecture provides different performance. Due to the nature of data, it may have different temporal patterns, periodic, semi-periodic, stationary, etc. Hence, to capture all these patterns, we found out that the models have to be tuned specifically to the dataset.</p><p>Sampling: One of the weakness of the proposed algorithms is that the model size (in terms of the number of weights to be trained) increases based on the size of the nodes considered during the training phase. To overcome this, the nodes have been sampled. Currently, we utilize uniform sampling of the nodes to mitigate this issue. However, we believe that a better sampling scheme that is aware of the graph properties may further improve its performance.</p><p>Large Lookbacks: While it is desirable to test large lookback values for learning the temporal evolution with the current hardware resources, we constantly ran into resource exhausted error with lookbacks greater than 10. (especially for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Future Work</head><p>Other Datasets: We have validated our algorithms with a synthetic dynamic SBM and two real-world datasets including Hep-th and AS. We leave the test on further datasets as future work.</p><p>Hyper-parameters: Currently, we provided the evaluation of the proposed algorithm with embedding size of 64, 128 and 256. We leave the exhaustive evaluation of the proposed algorithms for broader ranges of embedding size and look back size for future work.</p><p>Evaluation: We have demonstrated the effectiveness of the proposed algorithms for predicting the links of the next time step. However, in dynamic graph networks, there are various evaluations such as node classification that can be performed. We leave them as our future work.</p><p>Evolving communities: In real world graphs, communities often grow or shrink in terms of number of nodes per community, and in terms of total number of communities. Using inductive methods to handle such cases is an interesting future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Conclusion</head><p>This paper introduced dyngraph2vec, a model for capturing temporal patterns in dynamic networks. It learns the evolution patterns of individual nodes and provides an embedding capable of predicting future links with higher precision. We propose three variations of our model based on the architecture with varying capabilities. The experiments show that our model can capture temporal patterns on synthetic and real datasets and outperform state-of-the-art methods in link prediction. There are several directions for future work: (1) interpretability by extending the model to provide more insight into network dynamics and better understand temporal dynamics; (2) automatic hyperparameter optimization for higher accuracy; and (3) graph convolutions to learn from node attributes and reduce the number of parameters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>User A breaks ties with his friend at each time step and befriends a friend of a friend. Such temporal patterns require knowledge across multiple time steps for accurate prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Motivating example of network evolution -community shift.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Motivating example of network evolution -community shift (for clarity, only showing 50 of 500 nodes and 2 out 10 migrating nodes).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>dyngraph2vec architecture variations for dynamic graph embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>|E pred (k)?E gt | k, where E pred and E gt are the predicted and ground truth edges respectively. MAP averages the precision over all nodes. It can be written as i AP(i)|V|where AP(i) = k precision@k(i)?I{E pred i (k)?E gt i } |{k:E pred i (k)?E gt i }| and precision@k(i) = |E pred i (1:k)?E gt i | k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>MAP values for the SBM dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>MAP values for the Hep-th dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>MAP values for the AS dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>Mean MAP values for various lookback numbers for Hep-th dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 :</head><label>9</label><figDesc>Mean MAP values for various lookback numbers for AS dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 10 :</head><label>10</label><figDesc>MAP value with increasing amount of temporal graphs added in the training data for Hep-th dataset (lookback = 8).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 11 :</head><label>11</label><figDesc>MAP value with increasing amount of temporal graphs added in the training data for AS dataset (lookback = 8).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Dataset Statistics</figDesc><table><row><cell>Name</cell><cell>SBM</cell><cell>Hep-th</cell><cell>AS</cell></row><row><cell>Nodes n</cell><cell cols="2">1000 150-14446</cell><cell>7716</cell></row><row><cell>Edges m</cell><cell cols="3">56016 268-48274 487-26467</cell></row><row><cell>Time steps T</cell><cell>10</cell><cell>136</cell><cell>733</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Average MAP values over different embedding sizes.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Average MAP</cell></row><row><cell>Method</cell><cell>SBM</cell><cell>Hep-th</cell><cell>AS</cell></row><row><cell>IncrementalSVD</cell><cell>0.4421</cell><cell>0.2518</cell><cell>0.1452</cell></row><row><cell>rerunSVD</cell><cell>0.5474</cell><cell>0.2541</cell><cell>0.1607</cell></row><row><cell>optimalSVD</cell><cell>0.5831</cell><cell>0.2419</cell><cell>0.1152</cell></row><row><cell>dynamicTriad</cell><cell>0.1509</cell><cell>0.3606</cell><cell>0.0677</cell></row><row><cell>dynGEM</cell><cell>0.9648</cell><cell>0.2587</cell><cell>0.0975</cell></row><row><cell>dyngraph2vecAE (lb=3)</cell><cell>0.9500</cell><cell>0.3951</cell><cell>0.1825</cell></row><row><cell>dyngraph2vecAE (lb=5)</cell><cell>-</cell><cell>0.512</cell><cell>0.2800</cell></row><row><cell>dyngraph2vecRNN (lb=3)</cell><cell>0.9567</cell><cell>0.5451</cell><cell>0.2350</cell></row><row><cell>dyngraph2vecRNN</cell><cell>-</cell><cell cols="2">0.7290 (lb=8) 0.313 (lb=10)</cell></row><row><cell cols="2">dyngraph2vecAERNN (lb=3) 0.9581</cell><cell>0.5952</cell><cell>0.3274</cell></row><row><cell>dyngraph2vecAERNN</cell><cell>-</cell><cell cols="2">0.739 (lb=8) 0.3801 (lb=10)</cell></row><row><cell>lb = Lookback value</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ginsparg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Overview of the 2003 kdd cup</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Visualizing social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of social structure</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Network visualization and analysis of gene expression data using biolayout express3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Theocharidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Van Dongen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Enright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature protocols</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1535" to="1550" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recommending teammates with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sapienza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ferrara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th on Hypertext and Social Media</title>
		<meeting>the 29th on Hypertext and Social Media</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="57" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey of visualization tools for biological network analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-L</forename><surname>Wegener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biodata mining</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wasserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Faust</surname></persName>
		</author>
		<title level="m">Social network analysis: Methods and applications</title>
		<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="1994" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Graph embedding techniques, applications, and performance: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ferrara</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2018.03.022</idno>
		<ptr target="https://doi.org/10.1016/j.knosys.2018.03.022.URLhttp://www.sciencedirect.com/science/article/pii/S0950705118301540" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<title level="m">Proceedings of the 22nd International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
	<note>node2vec: Scalable feature learning for networks</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Asymmetric transitivity preserving graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGKDD</title>
		<meeting>of ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1105" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distributed large-scale natural graph factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayanamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Josifovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on World Wide Web</title>
		<meeting>the 22nd international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 20th international conference on Knowledge discovery and data mining</title>
		<meeting>20th international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<title level="m">Grarep: Learning graph representations with global structural information</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="891" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 24th International Conference on World Wide Web</title>
		<meeting>24th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Embedding networks with edge attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hosseinmardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ferrara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th on Hypertext and Social Media</title>
		<meeting>the 29th on Hypertext and Social Media</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="38" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Dynamic Network Embedding by Modelling Triadic Closure Process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kamra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11273</idno>
		<title level="m">Dyngem: Deep embedding method for dynamic graphs</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09541</idno>
		<title level="m">Timers: Error-bounded svd restart on dynamic networks</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>NIPS</publisher>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="585" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<title level="m">Proceedings of the 22nd International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
	<note>Structural deep network embedding</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep neural networks for learning graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1145" to="1152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<title level="m">Variational graph auto-encoders</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<title level="m">Semi-supervised classification with graph convolutional networks</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graphstructured data</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scalable temporal latent space inference for link prediction in dynamic social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Steeg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2765" to="2777" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dyngem: Deep embedding method for dynamic graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kamra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI International Workshop on Representation Learning for Graphs</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.05755</idno>
		<title level="m">Dylink2vec: Effective feature representation for link prediction in dynamic networks</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6394</idno>
		<title level="m">Nonparametric link prediction in dynamic networks</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning continuous-time bayesian networks in relational domains: A non-parametric approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Temporal link prediction using matrix and tensor factorizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Dunlavy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Acar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data (TKDD)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Graph regularized nonnegative matrix factorization for temporal link prediction in dynamic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Physica A: Statistical mechanics and its applications</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">496</biblScope>
			<biblScope unit="page" from="121" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A link prediction based approach for recommendation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Talasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jonnalagadda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S A</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 international conference on advances in computing, communications and informatics (ICACCI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2059" to="2062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Streaming link prediction on dynamic attributed networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Eleventh ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="369" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stochastic blockmodels for directed graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">Y</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">397</biblScope>
			<biblScope unit="page" from="8" to="19" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Neurocomputing: Foundations of research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<editor>JA Anderson and E. Rosenfeld</editor>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="page" from="696" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Graphs over time: densification laws, shrinking diameters and possible explanations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining</title>
		<meeting>the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="177" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Asymmetric transitivity preserving graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1105" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fast low-rank modifications of the thin singular value decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear algebra and its applications</title>
		<imprint>
			<biblScope unit="volume">415</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="30" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
