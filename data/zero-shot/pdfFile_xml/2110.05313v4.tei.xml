<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Source Separation via Bayesian Inference in the Latent Domain</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Mancusi</surname></persName>
							<email>mancusi@di.uniroma1.it</email>
							<affiliation key="aff0">
								<orgName type="institution">Sapienza University of Rome</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilian</forename><surname>Postolache</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sapienza University of Rome</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Mariani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sapienza University of Rome</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Fumero</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sapienza University of Rome</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Santilli</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sapienza University of Rome</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Cosmo</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Ca&apos; Foscari</orgName>
								<orgName type="institution">University of Venice</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Lugano</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodol?</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sapienza University of Rome</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Source Separation via Bayesian Inference in the Latent Domain</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: Signal separation</term>
					<term>Autoregressive generative models</term>
					<term>Bayesian inference</term>
					<term>Unsupervised learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State of the art audio source separation models rely on supervised data-driven approaches, which can be expensive in terms of labeling resources. On the other hand, approaches for training these models without any direct supervision are typically high-demanding in terms of memory and time requirements, and remain impractical to be used at inference time. We aim to tackle these limitations by proposing a simple yet effective unsupervised separation algorithm, which operates directly on a latent representation of time-domain signals. Our algorithm relies on deep Bayesian priors in the form of pre-trained autoregressive networks to model the probability distributions of each source. We leverage the low cardinality of the discrete latent space, trained with a novel loss term imposing a precise arithmetic structure on it, to perform exact Bayesian inference without relying on an approximation strategy. We validate our approach on the Slakh dataset [1], demonstrating results in line with state of the art supervised approaches while requiring fewer resources with respect to other unsupervised methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generative models have reached promising results in a wide range of domains, including audio, and can be used to solve different tasks in unsupervised learning. A relevant problem in the musical domain is the task of source separation of different instruments. Given the sequential nature of music and the high variability of rhythm, timbre and melody, autoregressive models <ref type="bibr" target="#b1">[2]</ref> represent a popular and effective choice to process data on such domain, showcasing high multi-modality in the modeled probability distributions. The widely adopted WaveNet autoregressive architecture <ref type="bibr" target="#b2">[3]</ref> works in the temporal domain. Given that audio signals are typically sampled at high frequencies (e.g. 44 kHz) for music, the choice of modeling the data distribution directly in the time domain leads to short contexts being captured by neural computations and quick saturation of memory. Nevertheless, existing unsupervised approaches for source separation operate in the time domain <ref type="bibr" target="#b3">[4]</ref>. In order to capture longer contexts and to reduce memory burden, different quantization schemes have been introduced for autoregressive models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, where chunks in time are mapped to sequences of latent tokens belonging to a small vocabulary. OpenAI's Jukebox <ref type="bibr" target="#b6">[7]</ref> follows this approach and excels as an architecture that can capture very long contexts, generating highly consistent tracks. Leveraging the useful properties of this architecture, we propose a novel approach to unsupervised source separation that works directly on quantized latent domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>* Equal contribution</head><p>Our contributions can be summarized as follows:</p><p>1. We perform source separation applying exact Bayesian inference directly in the latent domain, exploiting the relative small size of the latent dictionary. We do not rely on any approximation strategy, such as variational inference or Langevin dynamics.</p><p>2. We introduce LQ-VAE: a quantized autoencoder trained with a novel loss that imposes an algebraic structure on the discrete latent space. This allows us to alleviate noisy and distorted samples which arise from a vanilla quantization approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The problem of source separation has been classically tackled in an unsupervised fashion <ref type="bibr" target="#b7">[8]</ref>, where the sources to be separated from a mixture signal are unknown <ref type="bibr" target="#b8">[9]</ref>. With the advent of deep learning, most source separation tasks applied to musical data started relying on supervised learning, training models on data with known correspondence between sources. Recently, following the success of deep generative models, there has been a renewed interest in unsupervised methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Supervised source separation</head><p>Supervised source separation aims to map high dimensional observations of audio mixtures to a smaller dimensional space and apply, explicitly or implicitly, a mask to filter out the sources from the latent representation of the mixtures in a supervised way. Most of these works can be divided into frequency-domain or waveform-domain approaches. The former <ref type="bibr" target="#b9">[10]</ref> operate on the spectral representation of the input mixtures. This line of works has highly benefited from the incoming of deep learning techniques from simple fully connected networks <ref type="bibr" target="#b10">[11]</ref>, LSTM <ref type="bibr" target="#b11">[12]</ref>, and CNN coupled with recurrent approaches <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. Recent approaches such as <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b15">[16]</ref> hold the state of the art in music source separation over the dataset MUSDB18 <ref type="bibr" target="#b16">[17]</ref>, by respectively extending the conditional U-net architecture of <ref type="bibr" target="#b17">[18]</ref> to multi-source separation, and by exploiting multi-dilated convolution that applies different dilation factors in each layer to model different resolutions simultaneously. In contrast, waveform domain approaches process the mixtures directly in the time domain to overcome phase estimation, which is necessary when converting the signal from the frequency domain. The method of <ref type="bibr" target="#b18">[19]</ref> performs in line with the state of the art by extending a WaveNet-like architecture, coupled with an LSTM in the latent space. The main limitation of these state-of-the-art methods for audio source separation is that they require large amounts of fully separated, labeled data to perform the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Unsupervised source separation</head><p>Recent approaches in unsupervised source separation leverage self-supervised learning. A prominent baseline is MixIt <ref type="bibr" target="#b19">[20]</ref>, which trains a model by trying to separate sources from a mixture of mixtures. Although promising, such model suffers from the over-separation problem, where at test time a number of sources that is greater than those present in the mixture are estimated. As such, stems can be split across different output tracks. Generative approaches instead overcome this problem by imposing that a model should output an individual stem.</p><p>Closer to our work, <ref type="bibr" target="#b20">[21]</ref> proposes to leverage generative priors in the form of GANs trained on individual sources. They use projected gradient descent optimization to search in the source-specific latent spaces and effectively recover the constituent sources in the time domain. Although promising, GANs suffer from modal collapse, so their performance is limited in the musical domain, where variability is abundant. <ref type="bibr" target="#b3">[4]</ref> proposes to use Langevin dynamics on the global log-likelihood of the audio sequences to parallelize the sampling procedure of autoregressive models used as Bayesian priors. This approach produces good results but with a high computational cost due to the need of training distinct models for each noise level, and due to the costly optimization procedure in the time domain.</p><p>Differently, our inference procedure has much lower computational and memory requirements, allowing us to efficiently run the model on a single GPU. In addition, we can perform exact Bayesian inference without relying on an approximation scheme of the posterior (e.g., its score).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head><p>In this section we briefly introduce the background concepts necessary to understand our architecture, which builds upon <ref type="bibr" target="#b6">[7]</ref>. The overall architecture can be split into two parts: (i) a quantization module mapping the input sequences to a discrete latent space, and (ii) an autoregressive prior (one per source) which models the distribution of a given source in the discrete latent space. We point the reader to <ref type="bibr" target="#b6">[7]</ref> for a deeper understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Quantization module</head><p>Let us consider an input sequence x = x1, . . . , xT ? [?1, 1] T of length T , which represents a normalized waveform in the time domain. In order to be representative of an expressive portion of the audio sequence, T should be large. However, due to the complexity of modern neural architectures, choosing a large enough value of T is not always feasible. To reduce the dimensionality of the space one can leverage the VQ-VAE architecture <ref type="bibr" target="#b4">[5]</ref> to map large continuous sequences in the time domain to smaller sequences in a discrete latent domain. A VQ-VAE is composed of three blocks:</p><formula xml:id="formula_0">? A convolutional encoder E : [?1, 1] T ? R S?D , with S T ,</formula><p>where S is the length of the latent sequence and D denotes the number of channels;</p><formula xml:id="formula_1">? A bottleneck block B = BI ?BQ where BQ : R S?D ? C S ? R S?D</formula><p>is a vector quantizer, mapping the sequence of latent vectors h = h1, . . . , hS = E(x) into the sequence of nearest neighbors contained in a codebook C = {e k } K k=1 of learned latent codes, and BI : C S ? [K] S is an indexer mapping the codes e k 1 , . . . , e k S into the associated codebook indices z1 = k1, . . . , zS = kS. Note that since BI is bijective, the codes e k and their indices k are semantically equivalent, but we shall use the term 'codes' for the vectors in C and 'latent indices' for the associated integers;</p><formula xml:id="formula_2">? A decoder D : [K] S ? [?1, 1] T mapping the discrete sequence back into the time domain.</formula><p>The VQ-VAE is trained by minimizing the composite loss:</p><formula xml:id="formula_3">LVQ-VAE = Lrec + Lcodebook + ?Lcommit ,<label>(1)</label></formula><p>where:</p><formula xml:id="formula_4">Lrec = 1 T t xt ? D(zt) 2 2 (2) Lcodebook = 1 S s sg[hs] ? ez s 2 2 (3) Lcommit = 1 S s hs ? sg[ez s ] 2 2 ,<label>(4)</label></formula><p>where sg is the stop-gradient operator and ? is the commitment loss weight. The losses Lcodebook and Lcommit update the entries of the codebook C during the training procedure. In addition, we introduce a novel loss term Llin, described in Section 4.2, which imposes a precise algebraic structure on the latent space, facilitating the task of source separation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Latent autoregressive priors</head><p>Once the VQ-VAE is trained, time domain data x ? p data can be mapped to latent sequences z. Autoregressive priors p(z) = p(z1)p(z2|z1) . . . p(zS|zS?1, . . . , z1) can then be learned over the discrete domain. In this work, the autoregressive models are based on a deep scalable Transformer architecture as in <ref type="bibr" target="#b6">[7]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>The proposed algorithm is composed of two parts. A first separation phase in the latent domain, in which we sequentially sample from an exact posterior on discrete indices. A following rejection sampling procedure based on a (scaled) global posterior conditioned on the separation results, which we use to sort the proposed solutions and select the most promising one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Latent Bayesian source separation</head><p>Our task is to separate a mixture signal m = 1 2 x1 + 1 2 x2 into x1 ? p data are typically deep generative models and the likelihood p(m|x1, x2) is parameterized as p(m| 1 2 x1 + 1 2 x2). In this work, we follow the Bayesian approach but we work in the latent domain. After training the VQ-VAE on an arbitrary audio dataset (with samples lying also outside p data 1 and p data 2 ), we learn two latent autoregressive priors p1(z1) and p2(z2) over the two instrument classes. The priors do not require any correspondence between the sources, being trained in a completely unsupervised setting. We assume the two priors to be independent, i.e. p(z) = p(z1, z2) = p1(z1)p2(z2). Therefore, for each step s ? [S], we can  The random variable y = f (m) is a function of the mixture m. One can choose to model y in multiple ways; a naive approach is to choose f as the identity and set y = m, thus computing the likelihood function directly in the time domain. This approach, however, requires the decoding of at least 2K possible latent indices in order to locally compare the mixture m with the hypotheses z1,s and z2,s. Note that this corresponds to a lower bound, given that the convolutional nature of the decoder requires a larger past context to produce meaningful results. Differently, we propose to define y in the latent domain, setting y = BQ(E(m)) := mlatent. This approach is preferable since it does not require decoding the hypotheses at each step s, resulting in lower memory usage and computation time. Our method benefits from the choice of operating in the latent space, thanks to the relatively small size of the priors and the likelihood function domain (we choose K = 2048, as in <ref type="bibr" target="#b6">[7]</ref>). In addition, by exploiting the Transformer architecture, the prior distributions can be computed in parallel. For these reasons, evaluating and sampling from p(z1,s, z2,s|z1:s?1, y) at each s is computationally feasible and has O(K 2 ) memory complexity. <ref type="figure" target="#fig_2">See Figure 1</ref> for a visual description of the inference algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Latent likelihood via LQ-VAE</head><p>In this section we describe how we model the likelihood function and introduce the LQ-VAE model. Following <ref type="bibr" target="#b21">[22]</ref> we chose a ?-isotropic Gaussian likelihood, setting:</p><formula xml:id="formula_5">p (mlatent|z1,s, z2,s, z1:s?1) = = p (mlatent,s|z1, z2) = N mlatent,s BQ( 1 2 ez 1 + 1 2 ez 2 ), ? 2 I .<label>(5)</label></formula><p>The hyper-parameter ? balances the trade-off between the likelihood and the priors. Lower values promote the likelihood: the separated tracks combine perfectly with m, but may not sound like the instrument of the class they belong to. Instead, higher values of ? give importance to the priors: the separated tracks contain only sounds from the corresponding source distribution, but may not mix back to m (not resembling the sources). The logarithm of the likelihood is:</p><formula xml:id="formula_6">? 1 2? 2 mlatent,s ? BQ 1 2 ez 1 + 1 2 ez 2 2 2 .<label>(6)</label></formula><p>At each step s, we compare a variable term mlatent,s with a constant matrix BQ 1 2 ez 1 + 1 2 ez 2 representing all possible (scaled) sums over all codes in C. This term can be precomputed once and then reused during inference, saving additional computational resources.</p><p>We observed that performing separation with the likelihood in Eq. (5) using a VQ-VAE trained with the loss in Eq.</p><p>(1), results in disturbed and noisy outcomes. Such behavior is expected because the standard VQ-VAE does not impose any algebraic structure on the discrete domain; therefore, summing codes as in Eq. (5) does not lead to meaningful results. This problem can be lifted by enforcing a post-quantization linearization loss on the VQ-VAE:</p><formula xml:id="formula_7">L = LVQ-VAE + Llin ,<label>(7)</label></formula><p>where LVQ-VAE is defined as in Eq. (1) and  <ref type="bibr" target="#b26">[27]</ref> 0.59 1.31 2.63 -0.15 0.65 -1.02 <ref type="table">Table 1</ref>: SDR scores evaluated on Slakh2100 test set. All methods are unsupervised except those marked with ?. The rej attribute indicates that the solutions were obtained by the rejection sampling procedure with ? = 0. The scores are computed according to the implementation in <ref type="bibr" target="#b27">[28]</ref> .  <ref type="table">Table 2</ref>: Ablation study for rejection parameter ?.</p><formula xml:id="formula_8">Llin = 1 T t LQt ? QLt 2 2 (8) QLt = BQ 1 2 BQ (E (x1,t)) + 1 2 BQ (E (x2,t))<label>(9)</label></formula><formula xml:id="formula_9">LQt = BQ E 1 2 x1,t + 1 2 x2,t .<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Drums Piano Ours 0.68 3.66 Ours (rejection ? = 0) 0.08 2.75 GAN <ref type="bibr" target="#b19">[20]</ref> -3.16 -2.26 <ref type="table">Table 3</ref>: SDR table evaluated on the test set of <ref type="bibr" target="#b20">[21]</ref>.</p><p>Minimizing this loss pushes the quantized latent code representing a mixture of two arbitrary source signals (LQt term) to be equal to the sum of the quantized latent codes, corresponding to the single sources (QLt term), therefore enforcing the discrete codes to behave in an approximately linear way. We shall refer to the VQ-VAE trained as above, as a Linearly Quantized Variational Autoencoder (LQ-VAE). See <ref type="figure" target="#fig_3">Figure 2</ref> for a visual illustration of the LQ-VAE training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Rejection sampling</head><p>Given the low memory requirements of our method, at inference time we can sample in parallel multiple solutions {z (b) } B b=1 in the same batch. Autoregressive models tend to accumulate errors over the course of ancestral sampling, therefore the quality of the solutions varies across the batch. In order to select a solution, we look at the posterior prej(z|m) ? prej,1(z1)prej,2(z2)prej(m|z), conditioned by the sampling event. We obtain the priors prej,1 and prej,2 by normalizing p1 and p2 over the batch (computed by integrating over s during the inference). For numerical stability, we scale their logits by the length of the latent sequences S. The likelihood function prej(z|m) = N m 1 2 D(z1) + 1 2 D(z2), ? 2 rej I is computed directly in the time domain, with the decoding pass being executed only once at the end of the sampling procedure. The hyper-parameter ?rej plays a similar role to the ? used in Eq. <ref type="bibr" target="#b4">(5)</ref>. We can balance the likelihood and the priors by setting:</p><formula xml:id="formula_10">E b log prej(z (b) ) =? 1 2? 2 rej E b m? 1 2 (D(z (b) 1 ) + D(z (b) 2 )) 2 2</formula><p>and solving for ?rej. Albeit natural, this framework does not lead to the best selection. We performed an ablation study by weighting the contribution of the global likelihood with a scalar ? ? [0, 1] (using ? 2 rej = ?? 2 rej ) and the best empirical results are obtained when the global likelihood is not taken into account (? = 0), see <ref type="table">Table 2</ref>. We call this selection criterion priorbased rejection sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>We validate our approach on Slakh2100 [1]: a large musical source dataset containing mixed tracks separated into 34 instrument categories. We select tracks from the classes 'drum', 'bass' and 'guitar' coming from the training and test splits, subsampled at a frequency of 22kHz. We train the convolutional LQ-VAE over mixtures obtained by randomly mixing sources from the individual tracks of the training set. The LQ-VAE has a downsampling factor of T S = 64 and uses a dictionary of K = 2048 latent codes. After training the LQ-VAE, we train two autoregressive models, one per source, on latent codes extracted from ? 1200 tracks each. In all our separation experiments we fixed ? = 0.1 in Eq. (6). In <ref type="table">Table 1</ref> we compare our method with two state-of-the-art supervised approaches and different non-learning based unsupervised methods. To this end, we iterate on the test split of <ref type="bibr" target="#b0">[1]</ref> made up of about 150 different songs, and for each we extract 450 random chunks each of 3 seconds.</p><p>In order to strengthen our empirical evaluation, we show in <ref type="table">Table 3</ref> results of our model applied to a different validation data set in order to perform a comparison with the GAN model of <ref type="bibr" target="#b20">[21]</ref>. We evaluate both methods over the test dataset proposed in <ref type="bibr" target="#b20">[21]</ref>, consisting of 1000 mixtures of 1 second each. Each mixture combines a drum sample with a piano track randomly, thus independence in the test data is assumed, resulting in a more artificial setting with respect to the one present in Slakh2100. For <ref type="bibr" target="#b20">[21]</ref> we use the pre-trained model given by the authors while for our method we use the "drums" and "piano" priors trained on Slakh2100 thus showing the cross-dataset generalization capability of our model.</p><p>All our experiments are performed on a Nvidia RTX 3080 GPU with 16 GB of VRAM. With this GPU our method can sample a batch of 200 candidate solutions (100 for each instrument) simultaneously. The code to reproduce our experiments is available at https://github.com/michelemancusi/ LQVAE-separation. Interestingly, even if solutions selected by the rejection sampling algorithm have slightly lower metrics than supervised approaches, by individually selecting the best solution for each instrument we achieve performance in line with the state of the art (especially on 'bass' and 'drum' stems). This testifies the quality of our separation. Remarkably, our method employs 3 minutes on average for sampling a track of 3 seconds, compared to the more than 100 minutes of <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this work, we introduced a simple algorithm to perform exact Bayesian inference in the discrete latent domain. Our method allows to achieve good separation results while being much faster than other likelihood-based unsupervised approaches.</p><p>The main bottleneck of our method lies in the rejection sampling strategy. Future work will attempt to improve this aspect by investigating the design of more accurate learning-based rejection samplers. Other benefits could come from the adoption of multi-level VQ-VAEs <ref type="bibr" target="#b6">[7]</ref> or by leveraging deeper autoregressive priors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>In order to generate new time-domain examples, sequences of latent indices are sampled from p(z) via ancestral sampling and then mapped back to the time domain via the decoder of the VQ-VAE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 and x2 ? p data 2 , where p data 1 and p data 2 represent 1 (x1)p model 2 (model 1 , p model 2</head><label>12121212</label><figDesc>the distributions of each instrument class in the time domain. In a Bayesian framework, a candidate solution x = x1, x2 is distributed according to the posterior p(x1, x2|m) ? p model x2)p(m|x1, x2), where the priors p</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>In our method, two autoregressive priors T1 and T2 are trained on different instrument sources in the latent domain. At each step s they provide the joint prior p(zs). The prior is combined with a ?-isotropic Gaussian likelihood p(y = mlatent,s|zs) = N m latent,s BQ( 1 2 ez 1 + 1 2 ez 2 ), ? 2 I in order to compute the posterior p(zs|y = mlatent,s) from which new samples are drawn.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Training scheme of the LQ-VAE: reconstructionsx1, x2 are obtained from input pairs x1, x2 as in the VQ-VAE, leading to the loss LVQ-VAE (Eq. (1)). To this loss we add the postquantization linearization loss L lin(Eq. (8)), that is computed by matching time-domain sums with latent vector sums.compute the posterior distribution p(z1,s, z2,s|z1:s?1, y) ? p1(z1,s|z1,1:s?1)p2(z2,s|z2,1:s?1)p(y|z1,s, z2,s, z1:s?1).</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cutting music source separation some Slakh: A dataset to study the impact of training data quality and quantity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Manilow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seetharaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)</title>
		<meeting>IEEE Workshop on Applications of Signal essing to Audio and Acoustics (WASPAA)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The neural autoregressive distribution estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, ser. Proceedings of Machine Learning Research</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics, ser. Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Parallel and flexible sampling from autoregressive models via langevin dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jayaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thickstun</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generating diverse high-fidelity images with VQ-VAE-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Jukebox: A generative model for music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Independent Component Analysis, a new concept</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Comon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Static and dynamic source separation using nonnegative factorizations: A unified view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>F?votte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Mysore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mohammadiha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">One microphone source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 13, Papers from Neural Information Processing Systems (NIPS)</title>
		<meeting><address><addrLine>Denver, CO, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep neural network based instrument extraction from music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Giron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>South Brisbane, Queensland, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2015</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving music source separation based on deep neural networks through data augmentation and network blending</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Porcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Giron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enenkl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-03-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Denoising auto-encoder with recurrent skip connections and residual regression for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mmdenselstm: An efficient combination of convolutional and recurren</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv preprint</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Lasaft: Latent source attentive frequency transformation for conditioned source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">D3net: Densely connected multidilated densenet for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The MUSDB18 corpus for music separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rafii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>St?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Mimilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bittner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meseguer-Brocal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peeters</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01277</idno>
		<idno>arXiv: 1907.01277</idno>
		<title level="m">Conditioned-U-Net: Introducing a Control Mechanism in the U-Net for Multiple Source Separations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs, eess</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Music Source Separation in the Waveform Domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>D?fossez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.13254</idno>
		<idno>arXiv: 1911.13254</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs, eess, stat</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Unsupervised sound separation using mixture invariant training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hershey</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/28538c394c36e4d5ea8ff5ad60562a93-Paper.pdf" />
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3846" to="3857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Unsupervised audio source separation using generative priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Narayanaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Thiagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anirudh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Spanias</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Source separation with deep generative priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jayaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thickstun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
	<note>Virtual Event, ser. Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Singing-voice separation from monaural recordings using robust principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="57" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Independent component analysis: algorithms and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyv?rinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="411" to="430" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Harmonic/percussive separation using median filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fitzgerald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of DAFX</title>
		<meeting>of DAFX</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Repeating pattern extraction technique (repet): A simple method for music/voice separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rafii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on audio, speech, and language processing</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="73" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Music/voice separation using the 2d fourier transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seetharaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pishdadian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WAS-PAA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="36" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The 2018 signal separation evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>St?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Latent Variable Analysis and Signal Separation: 14th International Conference</title>
		<meeting><address><addrLine>Surrey, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="293" to="305" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
