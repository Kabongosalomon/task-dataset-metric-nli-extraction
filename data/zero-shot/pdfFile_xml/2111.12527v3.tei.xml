<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MorphMLP: An Efficient MLP-Like Backbone for Spatial-Temporal Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">Junhao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunchang</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">University of Chinese Academy of Sciences 5 Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Meitu, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashwat</forename><surname>Chandra</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luoqi</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Meitu, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MorphMLP: An Efficient MLP-Like Backbone for Spatial-Temporal Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>3 ShenZhen Key Lab of Computer Vision and Pattern Recognition, SIAT-SenseTime Joint Lab, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>MLP</term>
					<term>Video and Image Recognition</term>
					<term>Representation Learn- ing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, MLP-Like networks have been revived for image recognition. However, whether it is possible to build a generic MLP-Like architecture on video domain has not been explored, due to complex spatial-temporal modeling with large computation burden. To fill this gap, we present an efficient self-attention free backbone, namely MorphMLP, which flexibly leverages the concise Fully-Connected (FC) layer for video representation learning. Specifically, a MorphMLP block consists of two key layers in sequence, i.e., MorphFCs and MorphFCt, for spatial and temporal modeling respectively. MorphFCs can effectively capture core semantics in each frame, by progressive token interaction along both height and width dimensions. Alternatively, MorphFC t can adaptively learn long-term dependency over frames, by temporal token aggregation on each spatial location. With such multi-dimension and multiscale factorization, our MorphMLP block can achieve a great accuracycomputation balance. Finally, we evaluate our MorphMLP on a number of popular video benchmarks. Compared with the recent state-of-theart models, MorphMLP significantly reduces computation but with better accuracy, e.g., MorphMLP-S only uses 50% GFLOPs of VideoSwin-T but achieves 0.9% top-1 improvement on Kinetics400, under Ima-geNet1K pretraining. MorphMLP-B only uses 43% GFLOPs of MViT-B but achieves 2.4% top-1 improvement on SSV2, even though MorphMLP-B is pretrained on ImageNet1K while MViT-B is pretrained on Ki-netics400. Moreover, our method adapted to the image domain outperforms previous SOTA MLP-Like architectures. Code is available at https://github.com/MTLab/MorphMLP.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>: Visualization of spatial feature in 3rd layer. Left: Kinetics400 <ref type="bibr" target="#b5">[6]</ref>; Right: SthV2 <ref type="bibr" target="#b20">[21]</ref>.</p><p>ranging from image domain <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b65">65,</ref><ref type="bibr" target="#b74">73]</ref> to video domain <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b76">75]</ref>. However, recent studies have demonstrated that, self-attention maybe not critical and it can be replaced by simple Multiple Layer Perceptron (MLP) <ref type="bibr" target="#b55">[55]</ref>. Following this line, a number of MLP-Like architectures have been developed on image-domain tasks with promising results <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b73">72,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b55">55]</ref>.</p><p>A natural question is that, is it possible to design a generic MLP-Like architecture for video domain? Unfortunately, it has not been explored in the literature, to our best knowledge. Motivated by this fact, we analyze the main challenges of using MLP on spatial-temporal representation learning. First, from the spatial perspective, we find that the current MLP-Like models lack progressive understanding of semantic details. This is mainly because that, they often operate MLP globally on all the tokens in the space, while ignoring hierarchical learning of visual representation. For illustration, we visualize the feature map of the well-known MLP-like model (i.e., ViP <ref type="bibr" target="#b23">[24]</ref>) in <ref type="figure">Fig.1</ref>. Clearly, it suffers from difficulty in capturing key details, even in the shallow layer. Hence, how to discover semantics in each frame is important for designing spatial operation of MLP-like video backbone. Second, from the temporal perspective, the critical challenge is to learn long-range dependencies over frames. As shown in <ref type="figure" target="#fig_0">Fig.  2</ref>, the current video-based transformers can leverage self-attention to achieve this goal, but with huge computation cost. Hence, how to efficiently replace self-attention for long-range aggregation is important for designing temporal operation of MLP-like video backbone.</p><p>To tackles these challenges, we propose an effective and efficient MLP-like architecture, namely MorphMLP, for video representation learning. Specifically, it consists of two key layers, i.e., MorphFC s and MorphFC t , which leverage the concise FC operations on spatial and temporal modeling respectively. Our MorphFC s can effectively capture core semantics in the space, as shown in <ref type="figure">Fig. 1</ref>. The main reason is that, we gradually expand the receptive field of visual tokens along both height and width dimensions as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. Such progressive token design brings two advantages in spatial modeling, compared with the existing MLP-like models, e.g., ViP <ref type="bibr" target="#b23">[24]</ref>. First, it can learn hierarchical token interactions to discover the discriminative details, by operating FC from small to big spatial regions. Second, such small-to-big token construction can effectively reduce computation of FC operation for spatial modeling. Moreover, our MorphFC t can adaptively capture long-range dependencies over frames. Instead of exhausting token comparison in self-attention, we concatenate the features of each spatial location across all frames into a temporal chunk. In this way, each temporal chunk can be processed efficiently by FC, which adaptively aggregates token relations in the chunk to model temporal dependencies. Finally, we build up a MorphMLP block by arranging MorphFC s and MorphFC t in sequence, and stack these blocks into our generic MorphMLP backbone for video modeling. On one hand, such hierarchical manner can enlarge the cooperative power of MorphFC s and MorphFC t to learn complex spatial-temporal interactions in videos. On the other hand, such multi-scale and multi-dimension factorization allows our MorphMLP to achieve a preferable balance between accuracy and efficiency.</p><p>To our best knowledge, we are the first to build efficient MLP-Like architecture for video domain. Compared with the recent state-of-the-art video models, MorphMLP significantly reduces computation but with better accuracy.</p><p>We further apply our method to an image classification task on ImageNet-1K <ref type="bibr" target="#b11">[12]</ref>and a semantic segmentation task on ADE20K <ref type="bibr" target="#b77">[76]</ref>, by simply removing the temporal dimension of the video. Our method adapted to the image domain achieves competitive results compared to previous SOTA MLP-Like architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Self-Attention based backbones. Vision Transformer (ViT) <ref type="bibr" target="#b13">[14]</ref> firstly applies Transformer architecture to a sequence of image tokens. It utilizes multihead self-attention to capture long-range dependencies, thus achieving surprising results on image classification. Following works <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b69">68,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b74">73,</ref><ref type="bibr" target="#b65">65,</ref><ref type="bibr" target="#b50">51</ref>] make a series of breakthroughs to achieve state-of-art performance on several image tasks, i.e., semantic segmentation <ref type="bibr" target="#b70">[69,</ref><ref type="bibr" target="#b28">29]</ref> and object detection <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b78">77]</ref>. In video domain, a couple of woks <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b76">75,</ref><ref type="bibr" target="#b72">71,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b15">16]</ref> explore space-time self-attention to model spatial-temporal relation and achieve state-of-the-art performance. It seems that self-attention based architectures have been gradually dominating the computer vision community.</p><p>In this paper, we aim to explore a simple yet effective self-attention free architecture, which builds upon the FC layer to extract features. Our comparisons show that MorphMLP can achieve competitive results compared with Transformers not only in images but also in videos without self-attention layers. CNN based backbones. CNNs <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b71">70,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b32">33]</ref> have dominated vision tasks in the past few years. In image domain, beginning with AlexNet <ref type="bibr" target="#b30">[31]</ref>, more effective and deeper networks, VGG <ref type="bibr" target="#b49">[50]</ref>, GoogleNet <ref type="bibr" target="#b51">[52]</ref>, ResNet <ref type="bibr" target="#b22">[23]</ref>, DenseNet <ref type="bibr" target="#b26">[27]</ref> and EfficentNet <ref type="bibr" target="#b52">[53]</ref> are proposed and achieve great success in computer vision. In the video domain, several works <ref type="bibr" target="#b59">[59,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b18">19]</ref> explore how to utilize convolution to learn effective spatial-temporal representation. However, the typical spatial and temporal convolution are so local that they struggle to capture long-range information well even if stacked deeper. A series of works propose efficient modules (e.g., Non-local <ref type="bibr" target="#b66">[66]</ref>, Double Attention <ref type="bibr" target="#b8">[9]</ref>) to enhance local features via integrating long-range relation. The improvement of these methods can not be achieved without the supplement of self-attention layers.</p><p>In contrast, we propose the MorphMLP, which is self-attention free but not limited to capture local structure. The FC filter of MorphFC operates from small to big spatial regions. Meanwhile, the MorphFC t can capture long-term temporal information. MLP-Like based backbones. Recent works <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b73">72]</ref> try to replace selfattention layer with FC layer to explore the necessity of self-attention in Transformer architecture. But they suffer from dense parameters and computation. <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b54">54]</ref> apply FC layer along horizontal, vertical, and channel directions, respectively, in order to reduce the number of parameters and computation cost. However, the parameters of FC layer are still determined by the input resolution, so it is hard to handle different image scales. CycleMLP <ref type="bibr" target="#b7">[8]</ref> addresses such problem with padding, but it only focuses on global information, ignoring local inductive bias. Meanwhile, the ability of MLP-Like architecture for video modeling has not been explored.</p><p>On the contrary, our MorphMLP can cope with diverse scales via splitting the sequence of tokens into chunks. Furthermore, it is able to effectively capture local to global information by gradually expanding chunk length. More importantly, we are the first to build MLP-Like architecture on videos to explore its generalization ability as a new paradigm of versatile backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we present our MorphMLP. We first introduce the two critical components of MorphMLP, MorphFC s and MorphFC t . Then, we illustrate how to build efficient spatial-temporal MorphMLP block. Finally, the overall spatialtemporal network architecture and its adaption to image domain are provided. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MorphFC for Spatial Modeling</head><p>As discussed above, mining core semantics is critical to video recognition. Typical CNN and previous MLP-Like architectures only focus on either local or global information modeling thus they fail to do that. To tackle this challenge, we propose a novel MorphFC s layer that can hierarchical expand the receptive field of FC and make it operate from small to big regions. Our MorphFC s processes each frame of video independently in horizontal and vertical pathways. We take the horizontal one (blue chunks in <ref type="figure">Fig. 4</ref>) for example. Specifically, given one frame of input videos X ? R HW ?C that has been projected into a sequence of tokens, we first split X along horizontal direction. We set chunk length to L and thus obtain X i ? R L?C , where i ? {1, ..., HW/L}. Furthermore, to reduce computation cost, we also split each X i into multiple groups along channel dimension, where each group has D channels. Thus we get split chunks, and each single chunk is X k i ? R LD , where k ? {1, ..., C/D}. Next, we flatten each chunk into 1D vector and apply a FC weight matrix W ? R LD?LD to transform each chunk, yielding</p><formula xml:id="formula_0">Y k i = X k i W.<label>(1)</label></formula><p>After feature transformation, we reshape all chunks Y k i back to the original dimension Y ? R H?W ?C . The vertical way (green chunks in <ref type="figure">Fig. 4</ref>) does likewise except splitting the sequence of tokens along vertical direction. To make communication among groups along channel dimension, we also apply a FC layer to process each token individually. Finally, we get the output by element-wise summing horizontal, vertical, and channel features together. The chunks length L hierarchically increases as the network deepens, thereby enabling the FC filter to discover more core semantics progressively from small to big spatial region. Difference between our M orphF C s and convolution. (i) Typical convolution utilizes fixed small kernel size (e.g., 3?3), which only aggregates local context. On the contrary, the chunks lengths in MorphFC s hierarchically increase as the network deepens, which can model short-to-long range information progressively. (ii) Convolution uses sliding windows to obtain overlapping tokens, which requires cumbersome operations, including unfold, reshape and fold. In contrast, we simply reshape the feature map to obtain our chunks with nonoverlapping tokens. output, the convolution kernel of 1?3 window size needs to slide three times, and each 1?1 output is generated by the shared weight matrix W conv ? R 3?1 . In contrast, FC layer applies weight matrix W f c ? R 3?3 to the input yielding 1?3 ouput. Each 1?1 output is equivalent to being generated by non-shared weight matrix W ? R 3?1 , which brings more flexible spatial encoding than convolution. Comparisons with ViP <ref type="bibr" target="#b23">[24]</ref>. Our design is related to the well-known ViP designed for image domain, which also leverages the multi-branch features in spatial modeling. Hence, we further discuss the differences. (i) The FC filters of whole ViP network have the fixed size and receptive field, thus they only capture global information. On the contrary, our FC filters are morphable, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. In shallow layers, they have small size to model local structure, while in deeper layers, they gradually change to large size to model long-range information. Hence, ours can discover more detailed semantics by progressively operating FC from small to big spatial region. (ii)As shown in <ref type="figure" target="#fig_4">Fig. 8</ref>, at the network level, ours have hierarchical downsampling after each stage but ViP does not. (iii) As ViP paper said, ViP is hard to transfer to downstream tasks i.e. segmentation with spatial resolution 2048?512, since its filter size is always equal to the height/weight of features. But it is easy for ours, because the filter size is equal to pre-defined chunk size in the pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MorphFC t on Temporal Modeling</head><p>In addition to the horizontal and vertical pathways in MorphFC s , we introduce another temporal pathway MorphFC t . It aims at capturing long-term temporal information using the simple FC layer with low computation cost. Specifically, as shown in <ref type="figure" target="#fig_3">Fig. 6</ref>, given an input video clip tokens X ? R H?W ?T ?C , we first split X into a couple of groups along channel dimension (D channels in each group) to reduce computation cost and get X k ? R H?W ?T ?D , where k ? {1, ..., C/D}. For each spatial position s, we concatenate features across all frames into a chunk X k s ? R T D , where s ? {1, ..., HW }. Then we apply a F C matrix W ? R T D?T D , to transform temporal features and get</p><formula xml:id="formula_1">Y k s = X k s W.<label>(2)</label></formula><p>Finally, we reshape all chunks Y k s ? R T D back to original tokens dimension and output Y ? R H?W ?T ?C . In this way, the FC filter can simply aggregate token relations along time dimension in the chunk to model temporal dependencies.  Spatial-Temporal MorphMLP block. Based on the MorphFC s and MorphFC t , we propose a factorized spatial-temporal MorphMLP block in the video domain for efficient video representation learning. As shown in <ref type="figure">Fig. 7</ref>, our MorphMLP block contains MorphFC t , MorphFC s and MLP <ref type="bibr" target="#b62">[62]</ref> modules in a sequential order. On one hand, it is difficult for joint spatial-temporal optimization <ref type="bibr" target="#b2">[3]</ref>. On the other hand, factorizing spatial and temporal modeling is able to reduce the computation cost significantly. Therefore, we place temporal and spatial MorphFC s layers in the sequential style. The LN <ref type="bibr" target="#b1">[2]</ref> layer is applied before each module, and the standard residual connections are used after MorphFC t and MLP module. Instead of applying a standard residual connection <ref type="bibr" target="#b22">[23]</ref> after MorphFC s , we add a skip residual connection (red line) between the original input and output features from MorphFC s layer. We found that such a connection can make training more stable.</p><formula xml:id="formula_2">224?224 ? ?3 56?56(? 2 )? + 1?1(?1) ? - + = 14 0 = 28 2 = 28 -= 49 28?28(? 2 )? 0 14?14(? 2 )? 2 7?7(? 2 )? -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Network architecture</head><p>For video recognition, as shown in <ref type="figure" target="#fig_4">Fig. 8</ref>, we hierarchically stack spatial-temporal MorphMLP blocks to build up our network. Given an video sequence X ? R H?W ?T ?3 , taking H=W =224 for example, our MorphMLP backbone first performs patch embedding on the video clip and gets a sequence of tokens with dimension 56?56?T /2 ? C 1 . Then, we have four sequential stages and each of them contains a couple of MorphMLP blocks. The feature size remains unchanged as passing through layers inside the same stage. At the end of each stage excluding the last one, we expand the channel dimension and downsample the spatial resolution of features by ratio 2. Note that we set chunk lengths of MorphFC s to be 14, 28, 28, 49 for stage 1-4, respectively. Horizontal/vertical chunks with lengths 14, 28, 28, 49 of stage 1-4 can cover quarter, one, two, all rows/columns of feature maps of stage 1-4, re-spectively. In shallow layers, our network can learn detailed representation from the local spatial context in small chunk length, e.g., length 14 for 56?56?C 1 feature map. In deep layers, our network can capture long-range information from the global semantic context in considerable chunk length, e.g., length 49 for 7?7?C 4 feature map. With downsampling the spatial resolution and expanding chunk length as the network goes deeper, our MorphMLP is capable of discovering more core semantics progressively by operating the FC filter from small to big spatial regions.</p><p>We </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>In this section, we first examine the performance of MorphMLP and evaluate its spatiotemporal effectiveness on Kinetics-400 <ref type="bibr" target="#b5">[6]</ref>, and Something-Something V1&amp;V2 <ref type="bibr" target="#b20">[21]</ref> datasets. For fair comparisons and due to GPU resources limitation, we only report MorphMLP-S and B for video classification. Then we verify the effectiveness of its adaption to image domain, including ImageNet-1K <ref type="bibr" target="#b11">[12]</ref> image classification and ADE20K <ref type="bibr" target="#b77">[76]</ref> semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Video Classification on Kinetics-400</head><p>Settings. Kinetics-400 <ref type="bibr" target="#b5">[6]</ref> is a large-scale scene-related video benchmark. It contains around 240K training videos and about 20K validation videos in 400 classes. Our code heavily relies on PySlowFast <ref type="bibr" target="#b14">[15]</ref> repository and the training recipe mainly follows MViT <ref type="bibr" target="#b15">[16]</ref>. We directly load the parameters of MorphFC s pretrained on ImageNet and randomly initialize the parameters of MorphFC t in the video domain. We adopt a dense sampling strategy <ref type="bibr" target="#b66">[66]</ref> and AdamW optimizer to train the whole network. The warm-up epoch, total epoch, batch size, base learning rate, and weight decay are 10, 60, 64, 2e-4, and 0.05 respectively. We utilize the stochastic depth rates 0.1 and 0.3 for MorphMLP-S and B. Results. As shown in <ref type="table" target="#tab_1">Table 1</ref>, our method achieves outstanding performance with fewer computation costs. Compared with CNN models such as SlowFast <ref type="bibr" target="#b18">[19]</ref>, our MorphMLP requires 8? fewer GFLOPS but achieves 1.9% accuracy improvement (80.8% vs. 78.9%). With only ImageNet-1K pre-training, our method surpasses most of the self-attention based Transformer backbones with larger dataset pre-training. For example, compared with ViViT-L[1] pre-trained on ImageNet-21K, our MorphMLP obtains better performance with 20? fewer computations. When our model is scaled larger, the accuracy increases as well. Since the computation cost is relatively low, our method still has great potential for better performance. It demonstrates that our MorphMLP is a strong MLP-Like backbone for video recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Video Classification on Something-Something</head><p>Settings. Something-Something <ref type="bibr" target="#b20">[21]</ref> is another large-scale dataset, in which the temporal relationship modeling is critical for action understanding. It includes two versions, i.e., V1 and V2, both of which contain plentiful videos over 174 categories. We adopt the same training setting as used for Kinetics-400, except that a random horizontal flip is not applied. We utilize the sparse sampling strategy. The warm-up epoch, total epoch, batch size, base learning rate, and weight decay are 5, 50, 64, 4e-4, and 0.05, respectively. We set the stochastic depth rates to be 0.3 and 0.6 for Morph-S and B respectively.</p><p>Results. The comparison results on Something V2&amp;V1 are shown in <ref type="table" target="#tab_2">Table 2</ref> and <ref type="table" target="#tab_3">Table 3</ref> respectively. For SSV2, CNN architectures perform worse than Transformer architectures since they are limited to capturing local spatial and temporal information and struggle to model long-term dependencies. Transformer architectures can achieve better results, but they heavily rely on large-scale dataset  pre-training which requires high computation. Compared with CT-Net <ref type="bibr" target="#b31">[32]</ref>, our MorphMLP can reduce 2.5? computation but achieves 1.2% accuracy gain. Compared with the-state-of-art method MViT <ref type="bibr" target="#b15">[16]</ref>, which is pre-trained on large video dataset Kinetics-600, our MorphMLP only pre-trained on ImageNet-1K can obtain better performance (70.1% vs. 68.7%) with smaller GFLOPS (591G vs. 708G). For SSV1, our MorphMLP also achieves outstanding results. The superior results of our method on this dataset can be attributed to our unique progressively core semantics discovering manner and efficient spatialtemporal block design in MorpMLP. <ref type="table" target="#tab_6">Table 6</ref> and 7 can also demonstrate our point. Note that even if we do not add any complicated and unique temporal attention operation, our simple method can achieve such great performance. This indicates that our model can serve as a strong backbone for further improvement. <ref type="table">Table 4</ref>: ImageNet-1K results. As shown in (a), our method achieves the best performance among SOTA MLP-Like architectures. From (b), we can see that our MorphMLP also achieves the comparable results with SOTA self-attention based and hybrid models even with small computation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Image Classification on ImageNet-1K</head><p>Settings. We train our models from scratch on the ImageNet-1K dataset <ref type="bibr" target="#b11">[12]</ref>, which consists of 1.2M training images and 50K validation images from 1,000 categories. Our code is implemented based on DeiT <ref type="bibr" target="#b57">[57]</ref> repository, and we follow the same training strategy proposed in DeiT <ref type="bibr" target="#b57">[57]</ref>, including strong data augmentation and regularization. Stochastic depth rates are set to be 0.1, 0.1, 0.2, 0.3 for our 4 model variants. We adopt AdamW <ref type="bibr" target="#b42">[43]</ref> optimizer with cosine learning rate schedule <ref type="bibr" target="#b43">[44]</ref> for 300 epochs, while the first 20 epochs are used for linear warm-up <ref type="bibr" target="#b19">[20]</ref>. The total batch size, weight decay, and initial learning rate are set to 1024, 5?10 ?2 and 0.01 respectively.</p><p>Results. As shown in <ref type="table">Table 4a</ref>, our MorphMLP outperforms the state-of-the-art MLP-Like architectures. Compared with ViP-S <ref type="bibr" target="#b23">[24]</ref>, our method can get much higher accuracy (82.6% vs. 81.5%) with similar GFLOPS (7.0G vs. 6.9G). This demonstrates the effectiveness of our progressively short-to-long range pattern.</p><p>In <ref type="table">Table 4b</ref>, our MorphMLP can achieve competitive results with popular selfattention based models. Compared with other tiny models, e.g., Swin-T <ref type="bibr" target="#b39">[40]</ref>, our method can achieve better results (81.6% vs. 81.3%) with fewer parameters and GFLOPS (23M vs. 29M, 3.9G vs. 4.5G). As for larger settings, our method can achieve comparable result to Swin-B <ref type="bibr" target="#b39">[40]</ref> with fewer GFLOPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Semantic Segmentation on ADE20K</head><p>Settings. We conduct semantic segmentation experiments on ADE20K <ref type="bibr" target="#b77">[76]</ref>, which consists of 20K training images and 2K validation images over 150 semantic categories. Our code is based on mmsegmentation <ref type="bibr" target="#b9">[10]</ref> and we follow the experiment setting used in PVT <ref type="bibr" target="#b65">[65]</ref>. We simply apply Semantic FPN <ref type="bibr" target="#b29">[30]</ref> for fair comparisons, and all the backbones are pre-trained on ImageNet-1K. We adopt AdamW <ref type="bibr" target="#b42">[43]</ref> optimizer with cosine learning rate schedule <ref type="bibr" target="#b43">[44]</ref>, while the initial learning rate is 1e-4. The input images are randomly resized and cropped to 512?512 for training, and the shorter sides of images are set to 512 while testing.</p><p>Results. The results on ADE20k dataset are shown in <ref type="table" target="#tab_5">Table 5</ref>. Our MorphMLP outperforms ResNet <ref type="bibr" target="#b46">[47]</ref> and PVT <ref type="bibr" target="#b65">[65]</ref> significantly. Compared with Swin-T, our MorphMLP-T can achieve better mIoU with fewer parameters (26.4M vs. 31.9M).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>For <ref type="table" target="#tab_6">Table 6</ref>, 8, and 9, we train all the models based on MorphMLP-T for 100 epochs on ImageNet. To explore the variants of our spatial-temporal design, we adopt MorphMLP-S as the backbone on SSV1. Impact of chunk length. In the MorphMLP, we expand the chunk length gradually. The spatial resolutions of feature maps of Stages 1-4 are 56, 28, 14, 7, respectively. For the horizontal/vertical directions, chunk lengths 14, 28, 28, 49 in Stages1-4 can cover quarter, one, two and all rows/columns of the tokens, respectively, which can discover core semantics progressively by operating the FC filter from small to big spatial region. As shown in <ref type="table" target="#tab_6">Table 6</ref>, there are some alternative ways to set chunk length. The first line represents that MorphFC s in each stage covers one row of image/video  tokens, which only models global information. The second, third and fourth line utilize the small chunk length, which only captures local structure. The results show that our progressively expanding pattern can perform better than the solely local or global pattern. The reason is that, in the shallow layer, the original texture and shape information of the image/videos is relatively intact. Therefore, it is critical to capture detailed structures in the early stage. The features in the deep layers cover more semantic information, thus long-range relation modeling is significant. Note that the improvement brought by expanding chunk lengths on video is larger than image because such pattern is conducive to discovering more fine-grained semantics for many tiny movement actions.</p><p>It is also worth noting that since chunk sizes are equal to H, W of features in each stage if input is 224?224, 1st row is no 'Morph' (progressively discovering core semantics), but with hierarchical downsampling only. Last row is our final model (w/ both 'Morph' and same downsampling as the 1st row). Comparisons show benefits are from MorphFC design instead of hierarchical downsampling.</p><p>Detail designs of spatial-temporal MorphMLP block. We explore some alternative designs for our spatial-temporal MorphMLP block in the <ref type="table" target="#tab_7">Table 7</ref>. To begin with, in addition to applying MorphFC t and MorphFC s in a sequential way, we can add the features from MorphFC t and MorphFC s in parallel. As shown in <ref type="table" target="#tab_7">Table 7</ref>, the parallel way performs worse than the sequential way. We argue that it is more difficult for joint spatial and temporal optimization. Moreover, we explore different spatial-temporal orders and residual connections. Standard residual refers to applying a residual connection after each module in MorphMLP block of <ref type="figure">Fig. 7</ref>. Skip residual means that a connection is applied between input features of MorphMLP block and output features of the MorphFC s (red line in <ref type="figure">Figure 7</ref>). The results show that sequential temporal and spatial order with skip residual connection is the optimal setting. Comparisons with convolution. To compared with spatial convolution, we replace the MorphFC s layer with typical 3?3 and 7?7 convolution on image domain. As shown in <ref type="table" target="#tab_8">Table 8</ref>, our MorphFC s can outperform typical convolution by a large margin. This demonstrates that typical convolution is difficult to capture long-range information, which is crucial to the recognition problem. Furthermore, we adopt two 1D group convolutions along the horizontal and vertical direction, whose kernel sizes are exactly the same as our chunk lengths    in each stage. The results show that our method is much better than group conv in terms of speed and accuracy. This indicates the effectiveness of our MorphFC s . Moreover, we do comparisons between MorphFC t and typical temporal convolutions, i.e., 3?1?1 and 5?1?1. As shown in <ref type="table" target="#tab_1">Table 10</ref>, our MorphFC t outperforms typical temporal convolutions greatly. This is because that typical convolutions only focus on local temporal information aggregation. On the contrary, our MorphFC t is able to capture long-term temporal dependencies. Importance of different operations. We explore the importance of different operations in <ref type="table" target="#tab_9">Table 9</ref>. First, we evaluate the necessity of FC layers from three directions. It shows that each direction plays an important role. Second, we replace the 3?3 convolution with our MorphFC s layer in ResNet <ref type="bibr" target="#b22">[23]</ref>/R(2+1)D <ref type="bibr" target="#b61">[61]</ref> and the result shows that Transformer structure is more suitable for our MorphFC than the bottleneck block of CNN. Third, following the ViP <ref type="bibr" target="#b23">[24]</ref>, we utilize a weighted sum after three directions FC layers. Results show that weighted sum can bring a slight improvement (0.3%). Training speed. As shown in <ref type="table" target="#tab_1">Table 11</ref>, considering speed and accuracy tradeoff, our approach is more efficient for training with other SOTA video methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a self-attention free, MLP-Like backbone for video representation learning, named MorphMLP. MorphMLP is capable of progressively discovering core semantics and capturing long-term temporal information. To our best knowledge, we are the first to apply MLP-Like architecture in the video domain. The experiments demonstrate that such self-attention free models can be as strong as and even outperform self-attention based architectures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Our MorphMLP vs. other SOTA Transformers and CNNs for video classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Overview of progressive token construction in MorphMLP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :Fig. 5 :</head><label>45</label><figDesc>MorphFC s on the spatial dimension. Note that chunk length L hierarchically expands as network goes deeper. Comparison with the typical convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>(iii) As shown inFig. 5, given a 1?3 input, to get the 1?3 MorphFC t on the temporal dimension.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 :</head><label>8</label><figDesc>Architecture of our MorphMLP L means chuck length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>provide two model variants for the video recognition depending on the number of MorphMLP blocks in four stages: {3, 4, 9, 3} for MorphMLP-Small(S) and {4, 6, 15, 4} for MorphMLP-Base(B). The numbers of channels of four stages are {112, 224, 392, 784}. Adaption to Imgae domain. Additionally, for image-domain architecture, we simply exclude the temporal dimension and drop MorphFC t in the MorphMLP block. In addition to small and base settings, we provide two extra model variants for image domain, depending on the number of MorphMLP blocks in four stages, i.e., {3, 4, 7, 3} for MorphMLP-Tiny(T) and {4, 8, 18, 6} for MorphMLP-Large(L).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) Comparisons with MLP-Like models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparisons with the state-of-the-art on Kinetics-400<ref type="bibr" target="#b5">[6]</ref>. Our Mor-phMLP achieves outstanding results with much fewer computation costs. For example, compared with VideoSwin-T, our MorphMLP-S only requires 2? fewer GFLOPs but gets 0.9% accuracy improvement (79.7% vs. 78.8%).</figDesc><table><row><cell>Method</cell><cell>Pretrain</cell><cell>#Frame</cell><cell>GFLOPs</cell><cell>K400 Top-1</cell><cell>Top-5</cell></row><row><cell></cell><cell cols="3">Self-Attention Free -CNN</cell><cell></cell><cell></cell></row><row><cell>SlowFast R101[19]</cell><cell>-</cell><cell>(16+64)?3?10</cell><cell>6390</cell><cell>78.9</cell><cell>93.5</cell></row><row><cell>CorrNet-101[63]</cell><cell>-</cell><cell>32? 3?10</cell><cell>6720</cell><cell>79.2</cell><cell>-</cell></row><row><cell>ip-CSN[60]</cell><cell>Sports1M</cell><cell>32? 3?10</cell><cell>3264</cell><cell>79.2</cell><cell>93.8</cell></row><row><cell>X3D-XL[18]</cell><cell>-</cell><cell>16?3?10</cell><cell>1452</cell><cell>79.1</cell><cell>93.9</cell></row><row><cell>SmallBigEN [35]</cell><cell>IN-1K</cell><cell>(8+32)?3?4</cell><cell>5700</cell><cell>78.7</cell><cell>93.7</cell></row><row><cell>TDNEN [64]</cell><cell>IN-1K</cell><cell>(8+16)?3?10</cell><cell>5940</cell><cell>79.4</cell><cell>94.4</cell></row><row><cell>CT-NetEN [32]</cell><cell>IN-1K</cell><cell>(16+16)?3?4</cell><cell>2641</cell><cell>79.8</cell><cell>94.2</cell></row><row><cell></cell><cell cols="3">Self-Attention Based -Transformer</cell><cell></cell><cell></cell></row><row><cell>Timesformer-L[3]</cell><cell>IN-21K</cell><cell>96?3?1</cell><cell>7140</cell><cell>80.7</cell><cell>94.7</cell></row><row><cell>VidTr-L[75]</cell><cell>IN-21K</cell><cell>32?3?10</cell><cell>11760</cell><cell>79.1</cell><cell>93.9</cell></row><row><cell>ViViT-L[1]</cell><cell>IN-21K</cell><cell>16?3?4</cell><cell>17357</cell><cell>80.6</cell><cell>94.7</cell></row><row><cell>X-ViT[4]</cell><cell>IN-21K</cell><cell>16?3?1</cell><cell>850</cell><cell>80.2</cell><cell>94.7</cell></row><row><cell>Mformer[46]</cell><cell>IN-21K</cell><cell>32?3?10</cell><cell>11085</cell><cell>80.2</cell><cell>94.8</cell></row><row><cell>Mformer-L[46]</cell><cell>IN-21K</cell><cell>32?3?10</cell><cell>35550</cell><cell>80.2</cell><cell>94.8</cell></row><row><cell>MViT-B,16?4 [16]</cell><cell>-</cell><cell>16?1?5</cell><cell>355</cell><cell>78.4</cell><cell>93.5</cell></row><row><cell>MViT-B,32?3 [16]</cell><cell>-</cell><cell>32?1?5</cell><cell>850</cell><cell>80.2</cell><cell>94.4</cell></row><row><cell>VideoSwin-T[41]</cell><cell>IN-1K</cell><cell>32?3?4</cell><cell>1056</cell><cell>78.8</cell><cell>93.6</cell></row><row><cell>VideoSwin-B[41]</cell><cell>IN-1K</cell><cell>32?3?4</cell><cell>3384</cell><cell>80.6</cell><cell>94.6</cell></row><row><cell></cell><cell cols="3">Self-Attention Free -MLP-Like</cell><cell></cell><cell></cell></row><row><cell>MorphMLP-S</cell><cell>IN-1K</cell><cell>16?1?4</cell><cell>268</cell><cell>78.7</cell><cell>93.8</cell></row><row><cell>MorphMLP-S</cell><cell>IN-1K</cell><cell>32?1?4</cell><cell>532</cell><cell>79.7</cell><cell>94.2</cell></row><row><cell>MorphMLP-B</cell><cell>IN-1K</cell><cell>16?1?4</cell><cell>392</cell><cell>79.5</cell><cell>94.4</cell></row><row><cell>MorphMLP-B</cell><cell>IN-1K</cell><cell>32?1?4</cell><cell>788</cell><cell>80.8</cell><cell>94.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparisons with the SOTA on SSV2<ref type="bibr" target="#b20">[21]</ref>. Our MorphMLP outperforms previous sota Transformers and CNNs with IN-1K pretraining only.</figDesc><table><row><cell>Method</cell><cell>Pretrain</cell><cell>#Frame</cell><cell>GFLOPs</cell><cell cols="2">SSV2 Top-1 Top-5</cell></row><row><cell></cell><cell cols="2">Self-Attention Free -CNN</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SlowFast R50[19]</cell><cell>K400</cell><cell>(8+32)?3?1</cell><cell>197</cell><cell>61.7</cell><cell>46.6</cell></row><row><cell>TSM[38]</cell><cell>K400</cell><cell>16?3?2</cell><cell>374</cell><cell>63.4</cell><cell>88.5</cell></row><row><cell>STM[28]</cell><cell>IN-1K</cell><cell>16?3?10</cell><cell>1995</cell><cell>64.2</cell><cell>89.8</cell></row><row><cell>bLVNet[17]</cell><cell>IN-1K</cell><cell>32?3?10</cell><cell>3870</cell><cell>65.2</cell><cell>90.3</cell></row><row><cell>TEA[36]</cell><cell>IN-1K</cell><cell>16?3?10</cell><cell>2100</cell><cell>65.1</cell><cell>-</cell></row><row><cell>CT-Net[32]</cell><cell>IN-1K</cell><cell>16?3?2</cell><cell>450</cell><cell>65.9</cell><cell>90.1</cell></row><row><cell></cell><cell cols="3">Self-Attention Based -Transformer</cell><cell></cell><cell></cell></row><row><cell>Timesformer[3]</cell><cell>IN-21K</cell><cell>16?3?1</cell><cell>5109</cell><cell>62.5</cell><cell>-</cell></row><row><cell>VidTr-L[75]</cell><cell>IN-21K+K400</cell><cell>32?3?10</cell><cell>10530</cell><cell>60.2</cell><cell>-</cell></row><row><cell>ViViT-L[1]</cell><cell>IN-21K+K400</cell><cell>16?3?4</cell><cell>11892</cell><cell>65.4</cell><cell>89.8</cell></row><row><cell>X-ViT[4]</cell><cell>IN-21K</cell><cell>32?3?1</cell><cell>1269</cell><cell>65.4</cell><cell>90.7</cell></row><row><cell>Mformer[46]</cell><cell>IN-21K+K400</cell><cell>16?3?1</cell><cell>1110</cell><cell>66.5</cell><cell>90.1</cell></row><row><cell>Mformer-L[46]</cell><cell>IN-21K+K400</cell><cell>32?3?1</cell><cell>3555</cell><cell>68.1</cell><cell>91.2</cell></row><row><cell>MViT-B,16?4[16]</cell><cell>K400</cell><cell>16?3?1</cell><cell>510</cell><cell>67.1</cell><cell>90.8</cell></row><row><cell>MViT-B,32?3[16]</cell><cell>K400</cell><cell>32?3?1</cell><cell>1365</cell><cell>67.7</cell><cell>90.9</cell></row><row><cell>MViT-B-24,32?3[16]</cell><cell>K600</cell><cell>32?3?1</cell><cell>708</cell><cell>68.7</cell><cell>91.5</cell></row><row><cell></cell><cell cols="2">Self-Attention Free -MLP-like</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MorphMLP-S</cell><cell>IN-1K</cell><cell>16?3?1</cell><cell>201</cell><cell>67.1</cell><cell>90.9</cell></row><row><cell>MorphMLP-S</cell><cell>IN-1K</cell><cell>32?3?1</cell><cell>405</cell><cell>68.3</cell><cell>91.3</cell></row><row><cell>MorphMLP-B</cell><cell>IN-1K</cell><cell>16?3?1</cell><cell>294</cell><cell>67.6</cell><cell>91.3</cell></row><row><cell>MorphMLP-B</cell><cell>IN-1K</cell><cell>32?3?1</cell><cell>591</cell><cell>70.1</cell><cell>92.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparisons with the state-of-the-art on Something-Something V1<ref type="bibr" target="#b20">[21]</ref>.</figDesc><table><row><cell>Method</cell><cell>Pretrain</cell><cell>#Frame</cell><cell>GFLOPs</cell><cell cols="2">SSV1 Top-1 Top-5</cell></row><row><cell>I3D[67]</cell><cell>IN-1K+K400</cell><cell>32?3?2</cell><cell>918</cell><cell>41.6</cell><cell>72.2</cell></row><row><cell>NLI3D[67]</cell><cell>IN-1K+K400</cell><cell>32?3?2</cell><cell>1008</cell><cell>44.4</cell><cell>76.0</cell></row><row><cell>NLI3D+GCN[67]</cell><cell>IN-1K+K400</cell><cell>32?3?2</cell><cell>1818</cell><cell>46.1</cell><cell>76.8</cell></row><row><cell>TSM[38]</cell><cell>IN-1K+K400</cell><cell>16?1?1</cell><cell>65</cell><cell>47.2</cell><cell>77.1</cell></row><row><cell>SmallBig[35]</cell><cell>IN-1K</cell><cell>16?1?1</cell><cell>105</cell><cell>49.3</cell><cell>79.5</cell></row><row><cell>TEINet[42]</cell><cell>IN-1K</cell><cell>16?3?10</cell><cell>1980</cell><cell>51.0</cell><cell>-</cell></row><row><cell>TEA[36]</cell><cell>IN-1K</cell><cell>16?3?10</cell><cell>2100</cell><cell>52.3</cell><cell>81.9</cell></row><row><cell>CT-NET[32]</cell><cell>IN-1K</cell><cell>16?3?2</cell><cell>447</cell><cell>53.4</cell><cell>81.7</cell></row><row><cell>MorphMLP-S</cell><cell>IN-1K</cell><cell>16?1?1</cell><cell>67</cell><cell>50.6</cell><cell>78.0</cell></row><row><cell>MorphMLP-S</cell><cell>IN-1K</cell><cell>16?3?1</cell><cell>201</cell><cell>53.9</cell><cell>81.3</cell></row><row><cell>MorphMLP-B</cell><cell>IN-1K</cell><cell>16?3?1</cell><cell>294</cell><cell>55.5</cell><cell>82.4</cell></row><row><cell>MorphMLP-B</cell><cell>IN-1K</cell><cell>32?3?1</cell><cell>591</cell><cell>57.4</cell><cell>84.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Semantic segmentation with Semantic FPN [30] on ADE20K [76] val.</figDesc><table><row><cell>Method</cell><cell>Arch</cell><cell>#Param.(M)</cell><cell>mIoU</cell></row><row><cell>ResNet50[23]</cell><cell>CNN</cell><cell>28.5</cell><cell>36.7</cell></row><row><cell>PVT-S[65]</cell><cell>Trans</cell><cell>28.2</cell><cell>39.8</cell></row><row><cell>Swin-T[40]</cell><cell>Trans</cell><cell>31.9</cell><cell>41.5</cell></row><row><cell>GFNet-H-T[48]</cell><cell>FFN</cell><cell>26.6</cell><cell>41.0</cell></row><row><cell>CycleMLP-B2 [8]</cell><cell>MLP-Like</cell><cell>30.6</cell><cell>42.4</cell></row><row><cell>MorphMLP-T</cell><cell>MLP-Like</cell><cell>26.4</cell><cell>43.0</cell></row><row><cell>ResNet101[23]</cell><cell>CNN</cell><cell>47.5</cell><cell>38.8</cell></row><row><cell>ResNeXt101-32?4d[70]</cell><cell>CNN</cell><cell>47.1</cell><cell>39.7</cell></row><row><cell>PVT-M[65]</cell><cell>Trans</cell><cell>48.0</cell><cell>41.6</cell></row><row><cell>GFNet-H-S[48]</cell><cell>FFN</cell><cell>47.5</cell><cell>42.5</cell></row><row><cell>CycleMLP-B3[8]</cell><cell>MLP-Like</cell><cell>42.1</cell><cell>44.5</cell></row><row><cell>MorphMLP-S</cell><cell>MLP-Like</cell><cell>41.0</cell><cell>44.7</cell></row><row><cell>PVT-L[65]</cell><cell>Trans</cell><cell>65.1</cell><cell>42.1</cell></row><row><cell>Swin-S[40]</cell><cell>Trans</cell><cell>53.2</cell><cell>45.2</cell></row><row><cell>CycleMLP-B4 [8]</cell><cell>MLP-Like</cell><cell>55.6</cell><cell>45.1</cell></row><row><cell>MorphMLP-B</cell><cell>MLP-Like</cell><cell>59.3</cell><cell>45.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Impact of chunk length.</figDesc><table><row><cell cols="4">Stage1 Stage2 Stage3 Stage4</cell><cell cols="3">SSV1 ImageNet ADE20K Top-1 Top-1 mIoU</cell></row><row><cell>56</cell><cell>28</cell><cell>14</cell><cell>7</cell><cell>48.6</cell><cell>79.1</cell><cell>41.6</cell></row><row><cell>3</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>48.0</cell><cell>78.2</cell><cell>41.0</cell></row><row><cell>7</cell><cell>7</cell><cell>7</cell><cell>7</cell><cell>48.4</cell><cell>79.0</cell><cell>41.9</cell></row><row><cell>14</cell><cell>14</cell><cell>14</cell><cell>14</cell><cell>48.7</cell><cell>79.0</cell><cell>42.0</cell></row><row><cell>14</cell><cell>28</cell><cell>28</cell><cell>7</cell><cell>49.2</cell><cell>79.3</cell><cell>42.0</cell></row><row><cell>28</cell><cell>28</cell><cell>28</cell><cell>49</cell><cell>49.1</cell><cell>79.4</cell><cell>42.3</cell></row><row><cell>14</cell><cell>28</cell><cell>28</cell><cell>49</cell><cell>50.6</cell><cell>79.6</cell><cell>42.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Detail designs of spatialtemporal MorphMLP block.</figDesc><table><row><cell cols="2">Method Order</cell><cell>Standard Skip Residual Residual Top-1 SSV1</cell></row><row><cell>Parallel</cell><cell>T S</cell><cell>49.2</cell></row><row><cell></cell><cell>T+S</cell><cell>49.8</cell></row><row><cell>Sequential</cell><cell>S+T</cell><cell>50.2</cell></row><row><cell></cell><cell>T+S</cell><cell>50.6</cell></row><row><cell></cell><cell>S+T</cell><cell>31.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Spatial design of MorphMLP.</figDesc><table><row><cell>Operation</cell><cell cols="4">#Param. FLOPs Throughput ImageNet (M) (G) (images/s) Top-1</cell></row><row><cell>3?3 Conv</cell><cell>34.5</cell><cell>6.2</cell><cell>676</cell><cell>77.3</cell></row><row><cell>7?7 Conv</cell><cell>113</cell><cell>20.6</cell><cell>532</cell><cell>77.7</cell></row><row><cell cols="2">Group Conv 23.4</cell><cell>4.0</cell><cell>620</cell><cell>79.0</cell></row><row><cell>MorphFCs</cell><cell>23.4</cell><cell>4.0</cell><cell>734</cell><cell>79.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Different operations.</figDesc><table><row><cell>Dimension</cell><cell>Style</cell><cell>Weight Sum</cell><cell cols="2">SSV1 ImageNet Top-1 Top-1</cell></row><row><cell cols="2">H+W+C Transformer</cell><cell></cell><cell>50.6</cell><cell>79.6</cell></row><row><cell cols="2">H+W Transformer</cell><cell></cell><cell>49.4</cell><cell>78.5</cell></row><row><cell>H+W+C</cell><cell>CNN</cell><cell></cell><cell>47.2</cell><cell>77.2</cell></row><row><cell cols="2">H+W+C Transformer</cell><cell></cell><cell>50.2</cell><cell>79.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Temporal design.</figDesc><table><row><cell cols="4">Operation #Param.(M) FLOPs(G) SSV1</cell></row><row><cell>3?1?1 Conv</cell><cell>46.0</cell><cell>62.7</cell><cell>47.9</cell></row><row><cell>5?1?1 Conv</cell><cell>52.5</cell><cell>72.9</cell><cell>48.6</cell></row><row><cell>MorphFCt</cell><cell>47.0</cell><cell>66.4</cell><cell>50.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Training cost.</figDesc><table><row><cell cols="3">Video Model TFLOPs K400 Training Cost</cell></row><row><cell>SlowFast</cell><cell>1.11</cell><cell>71.0 30 epoch 444h</cell></row><row><cell cols="2">Timesformer 0.59</cell><cell>75.8 30 epoch 416h</cell></row><row><cell>Morph-S</cell><cell>0.27</cell><cell>77.0 30 epoch 408h</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This project is supported by the National Research Foundation, Singapore under its NRFF Award NRF-NRFF13-2021-0008, and Mike Zheng Shou's Start-Up Grant from NUS. David Junhao Zhang is supported by NUS IDS-ISEP scholarship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>ArXiv abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML) (2021) 2, 3, 7</title>
		<meeting>the International Conference on Machine Learning (ICML) (2021) 2, 3, 7</meeting>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Space-time mixing attention for video transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Perez-Rua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sudhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Quo vadis, action recognition? a new model and the kinetics dataset. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Quo vadis, action recognition? a new model and the kinetics dataset. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">CycleMLP: A MLPlike architecture for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A?2-nets: Double attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Contributors</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/mmsegmentation" />
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Cswin transformer: A general vision transformer backbone with cross-shaped windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno>ArXiv abs/2107.00652</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/slowfast(2020)8" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multiscale vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">More Is Less: Learning Efficient Video Representations by Temporal Aggregation Modules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pistoia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">X3d: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2020)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV) (2019) 4</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno>ArXiv abs/1706.02677</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The &quot;something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fr?nd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller-Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thurau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.13341</idno>
		<title level="m">Hire-mlp: Vision mlp via hierarchical rearrangement</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016) 4</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Vision permutator: A permutable mlp-like architecture for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>2022) 2, 4, 6</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>ArXiv abs/1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stm: Spatiotemporal and motion encoding for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Trseg: Transformer for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ct-net: Channel tensorization network for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Else-net: Elastic semantic network for continual action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hard-net: Hardness-aware discrimination network for 3d early activity prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Smallbignet: Integrating core and contextual views for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2020) 9</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Tea: Temporal excitation and aggregation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">AS-MLP: An axial shifted MLP architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pay attention to mlps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (2021)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Video swin transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>ArXiv abs/2106.13230 (2021) 2, 3</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Teinet: Towards an efficient architecture for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno>ArXiv abs/1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Video transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neimark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Asselmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Keeping your eye on the ball: Trajectory attention in video transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2020) 4</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Global filter networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>Mobilenetv2: Inverted residuals and linear bottlenecks</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Bottleneck transformers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.05422</idno>
		<title level="m">Sparse mlp for image recognition: Is self-attention really necessary? arXiv preprint</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">MLPmixer: An all-MLP architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (2021)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Resmlp: Feedforward networks for image classification with data-efficient training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J&amp;apos;egou</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>ArXiv abs/2105.03404 (2021) 2, 4</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J&amp;apos;egou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Going deeper with image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J&amp;apos;egou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Video classification with channelseparated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Video modeling with correlation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Tdn: Temporal difference networks for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<title level="m">Non-local neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Visual transformers: Token-based image representation and processing for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<idno>ArXiv abs/2006.03677</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Focal self-attention for local-global interactions in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">S2-mlp: Spatial-shift mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (2022)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (2022)</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E H</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno>abs/2101.11986</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Resnest: Split-attention networks</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Vidtr: Video transformer without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Marsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Semantic understanding of scenes through the ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
