<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multilingual Knowledge Graph Completion with Joint Relation and Entity Alignment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harkanwar</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Delhi</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prachi</forename><surname>Jain</surname></persName>
							<email>p6.jain@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Delhi</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mausam</surname></persName>
							<email>mausam@iitd.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Delhi</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
							<email>soumen@iitb.ac.in</email>
							<affiliation key="aff1">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Bombay</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multilingual Knowledge Graph Completion with Joint Relation and Entity Alignment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge Graph Completion (KGC) predicts missing facts in an incomplete Knowledge Graph. Almost all of existing KGC research is applicable to only one KG at a time, and in one language only. However, different language speakers may maintain separate KGs in their language and no individual KG is expected to be complete. Moreover, common entities or relations in these KGs have different surface forms and IDs, leading to ID proliferation. Entity alignment (EA) and relation alignment (RA) tasks resolve this by recognizing pairs of entity (relation) IDs in different KGs that represent the same entity (relation). This can further help prediction of missing facts, since knowledge from one KG is likely to benefit completion of another. High confidence predictions may also add valuable information for the alignment tasks.</p><p>In response, we study the novel task of jointly training multilingual KGC, relation alignment and entity alignment models. We present ALIGNKGC, which uses some seed alignments to jointly optimize all three of KGC, EA and RA losses. A key component of ALIGN-KGC is an embedding-based soft notion of asymmetric overlap defined on the (subject, object) set signatures of relations -this aids in better predicting relations that are equivalent to or implied by other relations. Extensive experiments with DBPedia in five languages establish the benefits of joint training for all tasks, achieving 10-32 MRR improvements of ALIGNKGC over a strong state-ofthe-art single-KGC system completion model over each monolingual KG . Further, ALIGN-KGC achieves reasonable gains in EA and RA tasks over a vanilla completion model over a KG that combines all facts without alignment, underscoring the value of joint training for these tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A knowledge graph (KG), also called as knowledge base (KB), has nodes representing entities and edges representing relations. Entities have unique canonical IDs. Relations also have canonical labels such as born-in or works-at, with associated IDs. A fact triple in a KG is of the form (subject entity, relation, object entity). A KG is usually associated with a human language. Each entity (or relation) ID is associated with one or more surface forms in the KG. E.g., the ID for the country USA may have aliases like "United States of America".</p><p>KGs are usually very incomplete, as curators struggle to keep up with the real world. KG completion (KGC) is thus a strongly motivated problem and studies the prediction of true facts unknown to the KG. While the problem is well-researched, with dozens of approaches explored over the last few years <ref type="bibr" target="#b1">(Bordes et al., 2013;</ref><ref type="bibr" target="#b7">Dettmers et al., 2018;</ref><ref type="bibr" target="#b16">Sun et al., 2019;</ref><ref type="bibr" target="#b18">Trouillon et al., 2016)</ref>, most KGC research is applicable to only one KG in one language at a time. However, different language speakers would maintain separate KGs in their own languages. Independent completion of each KG may not be optimal, since information from one KG will likely help completion of the other.</p><p>A second issue is that each KG will give a different ID, and often also the surface form, to the same entity, such as "Estados Unidos de Am?rica" for the USA entity in a Spanish KG. This leads to the problem of ID proliferation. A recent line of research around entity alignment (EA) across KGs in different languages attempts to assign a unique ID to all IDs representing the same entity <ref type="bibr" target="#b3">(Chen et al., 2017;</ref><ref type="bibr" target="#b13">Sun et al., 2017</ref><ref type="bibr" target="#b14">Sun et al., , 2018</ref><ref type="bibr" target="#b2">Cao et al., 2019;</ref><ref type="bibr" target="#b4">Chen et al., 2021;</ref><ref type="bibr" target="#b17">Tang et al., 2020)</ref>. A related task is relation alignment (RA), though relatively less attention has been given to this for multilingual KGs. We note that RA involves global evidence, because the decision to merge two relations in two languages can have far-reaching consequences to many facts in both KGs.</p><p>Our key contribution is to recognize and exploit the synergy between Multilingual KGC, EA and RA tasks. It is not surprising that such alignments will allow a KG an access to more facts and will lead to better completion. Similarly, if a fact can be predicted with high confidence in one KG that may give additional support to alignment of its constituent entities and relations.</p><p>In this paper, we present ALIGNKGC, a multitask system that learns to optimize for KGC, EA and RA jointly. At the heart of ALIGNKGC is the subject-object signature of each relation, which we represent as a bag of embeddings. These bags are compared for equivalence and implication between relations, and trained via an end-to-end training protocol for the multiple tasks.</p><p>We evaluate ALIGNKGC on slices of DBPedia in five languages. We compare it against a strong state-of-the-art single-KGC system trained over each monolingual KG Separately. Compared to this monolingual baseline, we find that ALIGN-KGC achieves substantial accuracy boost due to other KGs that get better aligned by our system, obtaining 10-32 pt MRR improvements across languages.For EA and RA, We compare it against a multilingual baseline trained over the single KG that has union of all (unaligned) facts. Joint training yields 22 pt HITS@1 gain on EA, and 26 pt HITS@1 gain in RA for frequent relations. Our code and data sets will be made publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Notation and preliminaries</head><p>Throughout, we use knowledge graph (KG) and knowledge base (KB), and similarly KBC and KGC interchangeably. A KG consists of entities E and relations (aka relation types) R. A KG instance is a triple (s, r, o) where s, o ? E and r ? R. These are all canonical IDs, but each ID is associated with aliases in one or more languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">KGC task</head><p>For any single KG, training data is provided as (s, r, o) triples. A test instance has the form (s, r, ?) or (?, r, o) where the system has to predict o or s.</p><p>Multiple correct values are possible. The evaluation protocol usually has the system rank candidate o's or s's and then measures MRR or hits@K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Alignment task</head><p>We consider a set L = [l 1 , l 2 ...] of languages.For simplicity of exposition, we consider two KGs called KG l and KG l . Here KG l represents the KG supported by the language l. An entity in this KG is called e l . A relation in this KG is called r l . For simplicity of exposition, we consider two KGs called KG l and KG l where l, l ? L.</p><p>Although we cast the alignment task as between KGs in two different languages, alignment between diverse KGs even in the same language (such as Wikipedia and IMDB) are considered in the same spirit. Furthermore, it is possible for multiple KGs to take the place of KG l to improve KGC in KG l .</p><p>An equivalence between entities e l and e l is specified as e l ? e l , also written as the triple (e l , ? , e l ). This induces a graph by adding some more edges with label '?' to KG l ? KG l . Similarly an equivalence between relations r l and r l is specified as r l ? r l ; this is not easily represented in the graph KG l ? KG l , however.</p><p>Other relations may be possible between relation pairs, such as r l =? r l , which means, for all s, o such that (s, r 1 , o) holds, so does (s, r 2 , o).</p><p>During training, a set of entity equivalences {(e n l , ?, e n l ) : n = 1, . . . , N } and a set of relation equivalences {(r m l , ?, r m l ) : m = 1, . . . , M } are revealed to the system. The goal of the system is to infer additional entity and relation equivalences. The system is usually called upon to produce a ranked list of equivalences, which is evaluated using HITS@K or MRR.</p><p>To achieve KGC enhanced with alignment, the system has to infer additional triples in either KG, making best use of the revealed equivalences. For the KG alignment goal, the system has to infer additional alignment triples between an entity or relation in KG l and an entity or relation in KG l .</p><p>3 Proposed methods</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline</head><p>Many KG embedding and KBC algorithms have been proposed in the last few years. ComplEx <ref type="bibr" target="#b18">(Trouillon et al., 2016)</ref> with all negative instances (no sampling) gives the best predictions <ref type="bibr" target="#b8">(Jain et al., 2020)</ref>, so we use it as our baseline KGC gadget. ComplEx defines a triple score as</p><formula xml:id="formula_0">f (s, r, o) = ( s, r, o ) ,<label>(1)</label></formula><p>where c is complex conjugate, ? ? ? is a 3-way elementwise inner product and (?) is the real part of a complex number. When applied to any one KG in isolation, we call this method KGCONE.</p><p>Perhaps the most straight-forward way to apply a KGC system is to compute KG l ? KG l , collapse node pairs specified as equivalent in {(e l , ?, e l )}, and rename all equivalent r l , r l relation IDs to a common new ID. A KGC system can work on the resulting KG, and this method is called KGC-UNION. Of course, this scheme does not impute any equivalences beyond what are explicitly provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Joint alignment and completion</head><p>For both textual and structured inputs, the problem of inferring a predicate or relation as entailed by another has been studied <ref type="bibr" target="#b10">(Lin and Pantel, 2001;</ref><ref type="bibr" target="#b0">Bhagat et al., 2007;</ref><ref type="bibr" target="#b12">Nakashole et al., 2012)</ref>. With that work as our point of departure, we ask: How similar are two relations r 1 , r 2 in one KG?</p><p>To build an estimate of similarity, we define the subject-object signature of a relation wrt a KG:</p><formula xml:id="formula_1">SO(r) = {(s, o) : (s, r, o) ? KG} (2)</formula><p>Here s, o are interpreted as canonical IDs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Jaccard similarity</head><p>Jaccard similarity can then be used as a standard symmetric belief that two relations are equivalent:</p><formula xml:id="formula_2">b(r 1 ? r 2 ) = | SO(r 1 ) ? SO(r 2 )| | SO(r 1 ) ? SO(r 2 )| .<label>(3)</label></formula><p>We add a threshold on these scores to reduce noise of false relation alignment signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Asymmetric subsumption</head><p>The issue with Jaccard similarity is that it can give a symmetric high score to relation pairs having asymmetric implications between them.</p><p>E.g., in the DBP5L data set, Jaccard similarity gives a large similarity score between locationCity and headquarter, or keyPerson and founders.</p><p>Therefore, we need a belief measure for one relation subsuming another, which we define as</p><formula xml:id="formula_3">b(r 1 =? r 2 ) = | SO(r 1 ) ? SO(r 2 )| | SO(r 1 )| ? [0, 1] (4) b(r 2 =? r 1 ) is defined likewise. Extending the logical statement (r 1 ? r 2 ) iff (r 1 =? r 2 ) ? (r 2 =? r 1 ) (5)</formula><p>to the fuzzy domain, we get b(r 1 ? r 2 ) = min {b(r 1 ?r 2 ), b(r 2 ?r 1 )} (6)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Soft subsumption and equivalence</head><p>In the above definitions involving SO, we assumed the (s, o) pairs were represented using canonical entity IDs. This may not be useful in the KG alignment task because canonical entity IDs will frequently not match.</p><p>Recall that the KBC system obtains embedding vectors e for each entity e in the KG. Reusing notation, we modify our earlier definition of subjectobject signature to</p><formula xml:id="formula_4">SO(r) = {(s, o) : (s, r, o) ? KG},<label>(7)</label></formula><p>i.e., where each element is the concatenation of the subject and object embedding vectors. Now consider one relation from each of two KGs to be aligned, viz., r l , r l . Suppose the (s, o) pairs of r l are indexed by i and pairs of r l are indexed by j. To generalize (6), we build a matrix A r l ,r l of pairwise cosine similarities:</p><formula xml:id="formula_5">A r l ,r l [i, j] = cos SO(r l )[i], SO(r l )[j]<label>(8)</label></formula><p>The continuous extension of SO(r l ) ? SO(r l ) involves solving a maximal bipartite matching problem using A r l ,r l as edge weights. Ideally, we should be able to backpropagate various KBC and alignment losses past the solution of the matching problem to the entity and relation embeddings. Gumbel-Sinkhorn matrix scaling <ref type="bibr" target="#b6">(Cuturi, 2013;</ref><ref type="bibr" target="#b11">Mena et al., 2018)</ref> can be used for this purpose, but it is computationally expensive at KG scales.</p><p>Here we use a computationally cheaper approximation: only if i is j's strongest partner and j is i's strongest partner, we choose edge (i, j) and accrue (toward the soft version of SO(r l ) ? SO(r l )) the score increment</p><formula xml:id="formula_6">? A r l ,r l [i, j] w + c ,<label>(9)</label></formula><p>where ? is the sigmoid nonlinearity, and w &gt; 0, b ? R are model parameters trained along with all embeddings. Summarizing, we estimate</p><formula xml:id="formula_7">| SO(r l ) ? SO(r l )| = i,j:p(i,j) ? A r l ,r l [i, j] w + c ,<label>(10)</label></formula><p>where the partner test indicator is written as</p><formula xml:id="formula_8">p(i, j) = i = argmax i A[i , j] j = argmax j A[i, j ]<label>(11)</label></formula><p>We continue to use (4) and (6) as defined with the modified continuous definition of intersection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Assembling a joint loss function</head><p>We start with the ComplEx KBC loss and then augment with loss terms corresponding to entity In our experiments, we found that retaining the full negative sets {s }, {o } is better than negative sampling, which we implemented using 1-N scoring <ref type="bibr" target="#b8">(Jain et al., 2020;</ref><ref type="bibr" target="#b7">Dettmers et al., 2018)</ref>.</p><p>We have no counterpart to the entity alignment loss <ref type="bibr" target="#b3">(Chen et al., 2017;</ref><ref type="bibr" target="#b14">Sun et al., 2018)</ref> because, for any pair e l ? e l , we force the two entities to share the same embedding vector. But we do introduce a novel relation alignment loss. If b(r l ? r l ) is large, but the relation embeddings r l and r l are very dissimilar, we should assess a loss. This naturally suggests the additional relation alignment loss term</p><formula xml:id="formula_9">L RA1 = c(r l ,r l ) b(r l ? r l ) r l ? r l 1 .<label>(15)</label></formula><p>Other forms are possible, like c(r l ,r l ) BCE b(r l ? r l ), ?(cos(r l , r l )) . <ref type="formula" target="#formula_0">(16)</ref> where the corresponding relation test indicator is written as c(r l , r l ) = r l ? argmax r l b(r l ? r l ) r l ? argmax r l b(r l ? r l ) (17)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">Joint loss</head><p>We put together all the above loss components and also a L2-regularizer L reg on entity and relation embeddings, each multiplied by tuned hyperparameters ?, ?:</p><formula xml:id="formula_10">L KGC + ?L reg + ?L RA1<label>(18)</label></formula><p>We initialize all elements of entity and relation embeddings to N (0, 0.05). Entity embeddings are not sensitive to alignment initially except those involved in seed alignment, hence we initialize the c = ?90 and w = 100 in (4). Thus, only very high initial cosine similarities contribute to equivalence score computation. As a form of curriculum, we let relation alignments get stable for few iterations and then make the equivalence scores trainable.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>Standard mono-lingual KGC datasets such as FB15k, FB15k-237, WN18, WN18RR, and Yago3-10, are not directly suited to evaluation of multilingual KGC task. Also, the focus on KG alignment is still relatively recent, with only a few suitable multi-lingual datasets that provide gold alignments between entity and relation pairs for training and evaluation. One such dataset is DBP5L, derived from DBPedia in five languages: English (En), Greek (El), Spanish (Es), Japanese (Ja) and French (Fr). Typically, about 40% of entities in one language are aligned to entities in the other language. We use the same 60-30-10 splits of the KG triples into train-dev-test folds as in <ref type="bibr" target="#b5">Chen et al. (2020)</ref> and combine the train set of all languages for training.We also pick half of seed enitity alignments randomly for training and rest is held for evaluation.</p><p>Because DBPedia uses a uniform relation vocabulary that is normalized across all languages, it cannot be directly used to test RA. To simulate that KBs in different languages come from different sources, we declare a unique ID for each relation in a language. This creates a testbed for the RA task. <ref type="table" target="#tab_2">Table 1</ref> lists the statistics of the five KGs in DBP5L. En is expectedly the most well-populated. <ref type="figure" target="#fig_0">Figure 2</ref> shows that 60% of the relation labels have associated string descriptions in only one of five languages mostly English in DBP-5L. However, these relation labels account for only 8% of fact triples. Meanwhile, almost 80% of fact triples have relations expressed in all five languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Salient statistics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance measures</head><p>We evaluate ALIGNKGC on DBP5L dataset, on three different tasks -KGC, EA and RA. As is common when evaluating the system for the task of KGC, we regard test instances (s, r, ?) as a task of ranking o (on the basis of scores computed in equations <ref type="formula" target="#formula_0">(12)</ref> and <ref type="formula" target="#formula_0">(13)</ref>), with gold o * known. We report MRR (Mean Reciprocal Rank) and the fraction of queries where o * is recalled within rank 1 and rank 10 (HITS). The filtered evaluation removes valid train or test tuples ranking above (s, r, o * ) for scoring purposes. To evaluate the system for EA, we use the test instance (e l , ?, ?) as a task of ranking e l , using the cosine distance between the entity embeddings of the language pair. We calculate Hits@1 and Hits@10 on the resulting rankings.</p><p>Similarly, to evaluate the system on the task of RA, we use the test instance (r l , ?, ?) as a task of ranking r l , using the cosine distance between the relation embeddings of the language pair. We calculate Hits@1 and Hits@10 on the resulting rankings.</p><p>Note that we use l and l' as placeholders for various language pairs we train/test on. <ref type="table" target="#tab_4">Table 3</ref> shows the KGC performance on the five languages of DBP-5L namely Greek, Japanese, French, Spanish and English. Though train and test set of each language KG have no overlap (KG l (train) ? KG l (test) = ?). But because we assign the same IDs for aligned entities during training, some form of overlap may exist in the combined training triples of all languages with test sets in any language. E.g., if (s l , r l , o l ) ? KG l (train) and (s l , r l , o l ) ? KG l (test), where s l ? s l , o l ? o l and r l ? r l (relation alignment may or may not be used).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">KGC performance</head><p>Therefore, rather than report metrics only on whole test-split, which can be misleading, we split our test set into two components: seen test set with triples/facts as shown above, and unseen test as the remaining triples. Qualitatively, performance on the seen split represents the capacity of the model to memorize known facts in a language and align them to another language, whereas the unseen split gives a true sense of a system's inference capability, since the fact has not been read in any language at train time. <ref type="table" target="#tab_4">Table 3</ref> shows the results of various ALIGNKGC variations compared to the baseline. KGCUNION being our multilingual baseline shows gains of combining the different language KGs over the monolingual baseline KGCONE. We find that each modification (Jaccard, Asymmetric scores and Soft Asymmetric scores) improves the results successively. Further analysis reveals that Jaccard signficantly helps with seen split, since it is able to learn similar embeddings for two aligned relations and entities, and yields high scores for seen tuple in a different language. Asymmetric computations help further as they remove false positives in RA that appear in the Symmetric Jaccard version.</p><p>Finally, making these asymmetric implication scores trainable learns better implications scores, which significantly improve entity alignment, which in turn gives better relation alignment. So we get best results on the both unseen and seen test splits for this version. <ref type="table" target="#tab_5">Table 4</ref> reports the performance of the various models on RA and EA tasks.We use entity and relation alignment results of KGCUNION as our baseline .We find that all ALIGNKGC variants outperform the baseline by vast margins. We notice that performance gain in RA are higher for highly frequent relations, and for less frequent relations, decreases slightly compared to Jaccard. Since a much larger fraction of triples (see <ref type="figure" target="#fig_0">Figure 2</ref>) express these highly frequent relations, it overall improves the performance of KGC significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Alignment performance</head><p>We also note that Jaccard and Asymmetric Jaccard models perform Entity alignment based on KGC loss only, since they do not have any trainable alignment loss. Since those models significantly improve the performance of EA, it provides evidence that KGC can result in better alignment. Learnable asymmetric model, of course, incentivizes better alignment too, leading to a further improvement in performance.    <ref type="bibr" target="#b8">(Jain et al., 2020;</ref><ref type="bibr" target="#b18">Trouillon et al., 2016)</ref>, ConvE <ref type="bibr" target="#b7">(Dettmers et al., 2018)</ref>, and RotatE <ref type="bibr" target="#b16">(Sun et al., 2019)</ref>. Almost all such systems are designed for a single KG, or are agnostic to the language used in entity and relation aliases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">KG alignment</head><p>Although more recent, interest in KG alignment is rapidly growing. MTransE <ref type="bibr" target="#b3">(Chen et al., 2017)</ref>, as the name suggests, uses TransE <ref type="bibr" target="#b1">(Bordes et al., 2013)</ref> on each KG separately, and adds a loss term that penalizes large distance between embeddings of equivalent entities. BootEA <ref type="bibr" target="#b14">(Sun et al., 2018)</ref> finds entity embeddings in their respective KGs and estimates a probability of equivalence by comparing their embeddings. This probability is then used for a semi-supervised bootstrapping of equivalent pairs. JAPE <ref type="bibr" target="#b13">(Sun et al., 2017)</ref> builds attribute-and network neighborhood-based embeddings of entities and combines them with EA constraints. MuGNN <ref type="bibr" target="#b2">(Cao et al., 2019</ref>) uses a graph neural network (GNN) to embed entities informed by entailment constraints of the form (s, r, o) =? (s, r , o)?s, o. These constraints appear to be manually provided and are the only KBC mechanism. Afterward, tied GNNs obtain node em-beddings which are compared to propose ? links. AliNet  is another GNN-based EA system. It uses an attention mechanism over larger node neighborhoods to build node representations. MultiKE <ref type="bibr" target="#b19">(Zhang et al., 2019)</ref> is perhaps the only work that treats entity and relation alignments at par and combines multiple views to make decisions. However, they use relation names rather than their structural subject-object summaries. JEANS <ref type="bibr" target="#b4">(Chen et al., 2021)</ref> is distinctive in that it links ('grounds') a text corpus to enities and uses TransE <ref type="bibr" target="#b1">(Bordes et al., 2013)</ref> for the KG, skip-grams for the text, with additional alignment constraints as usual. BERT-INT <ref type="bibr" target="#b17">(Tang et al., 2020</ref>) is another EA system that combines mBERT-obtained features from entity aliases and text descriptions with soft 1-hop graph neighborhood matching. Except MultiKE, most systems focus on EA, assuming RA is unnecessary, or already accomplished. <ref type="bibr" target="#b5">Chen et al. (2020)</ref> state so explicitly. At a high level, EA is a component of ALIGNKGC, and the precise EA method used is orthogonal to ALIGNKGC itself. alignment scores, underscoring the value of joint alignment and completion.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Fraction of relation labels and their fact tuples that are available in various number of languages between 1 and 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>alignment and relation alignment. Using f in Equation 1, ComplEx defines Pr(o|s, r) = e f (s,r,o) o e f (s,r,o ) , (12) Pr(s|o, r) = e f (s,r,o) s e f (s ,r,o) ,(13)and the log-likelihood KGC loss as L KGC =</figDesc><table><row><cell>(s,r,o)?KG</cell></row></table><note>? log Pr(o|s, r)?log Pr(s|o, r). (14)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Salient statistics of KGs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>KGC Performance of Models on Five languages</figDesc><table><row><cell></cell><cell cols="6">Relation Alignment(&lt;500) Relation Alignment(?500) Entity Alignment</cell></row><row><cell></cell><cell>Hits@1</cell><cell>Hits@3</cell><cell>Hits@1</cell><cell>Hits@3</cell><cell cols="2">Hits@1 Hits@10</cell></row><row><cell>KGCUNION</cell><cell>19.8</cell><cell>28.7</cell><cell>42.8</cell><cell>66.5</cell><cell>23.5</cell><cell>42.8</cell></row><row><cell>ALIGNKGC(Jaccard)</cell><cell>27.8</cell><cell>39.8</cell><cell>53.5</cell><cell>72.7</cell><cell>38.3</cell><cell>55.8</cell></row><row><cell>ALIGNKGC(Asymmetric)</cell><cell>27.2</cell><cell>38.7</cell><cell>66.9</cell><cell>75.0</cell><cell>40.7</cell><cell>57.4</cell></row><row><cell>ALIGNKGC(Soft Asymetric)</cell><cell>26.6</cell><cell>37.6</cell><cell>68.8</cell><cell>76.5</cell><cell>45.3</cell><cell>61.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Entity and relation alignment Performance of Models on Five languages</figDesc><table><row><cell>5 Related work</cell></row><row><cell>5.1 KBC</cell></row><row><cell>KBC through learning embeddings for KG artifacts</cell></row><row><cell>is a densely-populated research landscape. Among</cell></row><row><cell>the best performers are ComplEx</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">ConclusionWe have presented ALIGNKGC, a system that jointly learns to complete multiple KGs (KGC) and align their entities and relations. KGC and entity alignment were known tasks, but relation alignment was, to our knowledge, never integrated with them. In extensive experiments, ALIGNKGC significantly improves KGC accuracy, as well as</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">LEDIR: An unsupervised algorithm for learning directionality of inference rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Bhagat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="161" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-channel graph neural network for entity alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1140</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1452" to="1461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multilingual knowledge graph embeddings for cross-lingual knowledge alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1511" to="1517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cross-lingual entity alignment with incidental supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL Conference</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multilingual knowledge graph completion via ensemble knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changjun</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankith</forename><surname>Uppunda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Zaniolo</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.290</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3227" to="3238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2292" to="2300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Knowledge base completion: Baseline strikes back (again)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prachi</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sushant</forename><surname>Rathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/2005.00804</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">DIRT: Discovery of inference rules from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/pdf/10.1145/502512.502559</idno>
	</analytic>
	<monogr>
		<title level="m">SIGKDD Conference</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="323" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning latent permutations with gumbel-sinkhorn networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gonzalo</forename><surname>Mena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Linderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08665</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">PATTY: A taxonomy of relational patterns with semantic types</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ndapandula</forename><surname>Nakashole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1135" to="1145" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cross-lingual entity alignment via joint attributepreserving embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkai</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="628" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bootstrapping entity alignment with knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="4396" to="4402" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Knowledge graph alignment network with gated multi-hop neighborhood aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i01.5354</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="222" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10197</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">BERT-INT: A BERT-based interaction model for knowledge graph alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2020/439</idno>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3174" to="3180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Th?o</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-view knowledge graph embedding for entity alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019/754</idno>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5429" to="5435" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
