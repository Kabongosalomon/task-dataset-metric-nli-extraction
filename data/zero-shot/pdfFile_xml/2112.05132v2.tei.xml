<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spatio-temporal Relation Modeling for Few-shot Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Thatipelli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mohamed Bin Zayed University of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanath</forename><surname>Narayan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mohamed Bin Zayed University of Artificial Intelligence</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rao</forename><forename type="middle">Muhammad</forename><surname>Anwer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mohamed Bin Zayed University of Artificial Intelligence</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Aalto University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mohamed Bin Zayed University of Artificial Intelligence</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">CVL</orgName>
								<orgName type="institution">Link?ping University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">King Abdullah University of Science &amp; Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Spatio-temporal Relation Modeling for Few-shot Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel few-shot action recognition framework, STRM, which enhances class-specific feature discriminability while simultaneously learning higher-order temporal representations. The focus of our approach is a novel spatio-temporal enrichment module that aggregates spatial and temporal contexts with dedicated local patch-level and global frame-level feature enrichment sub-modules. Local patch-level enrichment captures the appearance-based characteristics of actions. On the other hand, global framelevel enrichment explicitly encodes the broad temporal context, thereby capturing the relevant object features over time. The resulting spatio-temporally enriched representations are then utilized to learn the relational matching between query and support action sub-sequences. We further introduce a query-class similarity classifier on the patchlevel enriched features to enhance class-specific feature discriminability by reinforcing the feature learning at different stages in the proposed framework. Experiments are performed on four few-shot action recognition benchmarks: Kinetics, SSv2, HMDB51 and UCF101. Our extensive ablation study reveals the benefits of the proposed contributions. Furthermore, our approach sets a new state-of-the-art on all four benchmarks. On the challenging SSv2 benchmark, our approach achieves an absolute gain of 3.5% in classification accuracy, as compared to the best existing method in the literature. Our code and models are available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Few-shot (FS) action recognition is a challenging computer vision problem, where the task is to classify an unlabelled query video into one of the action categories in the support set having limited samples per action class. The problem setting is particularly relevant for fine-grained action recognition <ref type="bibr" target="#b10">[11]</ref>, since it is challenging to collect sufficient labelled examples <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. Most existing FS action recognition methods typically search for either a single sup-port video <ref type="bibr" target="#b31">[32]</ref> or an average representation of a support class <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. However, these approaches utilize only framelevel representations and do not explicitly exploit video subsequences for temporal relationship modeling.</p><p>In the context of FS action recognition, modeling temporal relationships between a query video and limited support actions is a major challenge, since actions are typically performed at various speeds and occur at different time instants (temporal offsets). Further, video representations are desired to encode the relevant information from multiple sub-actions that constitute an action for enhanced matching between query and support videos. Moreover, an effective representation of spatial and temporal contexts of actions is crucial to distinguish fine-grained classes requiring temporal relational reasoning, where actions can be performed with different objects in various backgrounds, e.g., spilling something behind something.</p><p>The aforementioned problem of temporal relationship modeling is recently explored by Temporal-Relational CrossTransformers (TRX) <ref type="bibr" target="#b19">[20]</ref>, which compares the subsequences of query and support videos in a part-based manner to tackle the issue of varying speed and offsets of actions. Additionally, TRX models complex higher-order temporal relations by representing sub-sequences as tuples with different cardinalities. However, TRX struggles in the case of actions performed with different objects and background (see <ref type="figure" target="#fig_7">Fig. 1</ref>). This is likely due to not explicitly utilizing the available rich spatio-temporal contextual information during temporal relationship modeling. Furthermore, the tuple representations in TRX are fixed requiring a separate CrossTransformer <ref type="bibr" target="#b6">[7]</ref> branch per cardinality, which affects the model flexibility. Here, we set out to collectively address the above issues while modeling temporal relationships between query and limited support actions.</p><p>In this work, we argue that both local patch features in a frame and global frame features in a video are desirable cues to effectively enrich the encoding of spatial as well as temporal contextual information. Such feature enrichment improves class-specific discriminability, enabling focus on relevant objects and their corresponding motion in a video.  <ref type="figure" target="#fig_7">Figure 1</ref>. Example attention map visualizations obtained from the recently introduced TRX <ref type="bibr" target="#b19">[20]</ref> and our proposed STRM approach on four examples from the SSV2 and HMDB51 test set. The attention maps measure the activation magnitude of latent features. TRX struggles in the case of spatial and temporal context variations that are commonly encountered in actions performed with different objects and backgrounds, e.g., 5 th and 6 th frame from the left in (b), where the regions corresponding to actions are not emphasized. Similarly, while background region is also emphasized in 3 rd and 6 th frame from the left in (c), the action in the 2 nd and 3 rd frame from the left in (d) is not accurately captured due to the distractor motion from the moving hand of another person. Our STRM approach explicitly enhances class-specific feature discriminability through spatio-temporal context aggregation and intermediate latent feature classification. This leads to better matching between query and limited support action instances. Additional examples are presented in <ref type="figure" target="#fig_3">Fig. 5</ref> and Sec. B.</p><p>In addition, learning to classify feature representations at different stages is expected to reinforce the model to look for class-separable features, thereby further improving the class-specific discriminability. Moreover, this class-specific discriminability is attainable through a reduced set of cardinalities generated by the automatic learning of higher-order temporal relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions:</head><p>We introduce an FS action recognition framework that comprises spatio-temporal enrichment and temporal relationship modeling modules along with a query-class similarity classifier. The spatio-temporal enrichment module comprises local patch-level enrichment (PLE) and global frame-level enrichment (FLE) submodules. The PLE enriches local patch features with spatial context by attending to all patches in a frame, in a sampledependent manner, in order to capture the appearance-based similarities as well as dissimilarities among the action categories. On the other hand, the FLE enriches global frame features with temporal context by persistent relationship memory-based (sample-agnostic) aggregation that encompasses the entire receptive field in order to capture the relevant object motion in a video. The resulting enriched frame-level global representations are then utilized in the temporal relationship modeling (TRM) module to learn the temporal relations between query and support actions. Our TRM module does not rely on multiple cardinalities to model higher-order relations. Instead, it utilizes the spatio-temporal enrichment module to learn higher-order temporal representations at lower cardinalities. Moreover, we introduce a query-class similarity classifier that further enhances class-specific discriminability of the spatio-temporally enriched features by learning to classify representations from intermediate layer outputs. We conduct extensive experiments on four FS action recognition benchmarks: Kinetics <ref type="bibr" target="#b3">[4]</ref>, SSv2 <ref type="bibr" target="#b10">[11]</ref>, HMDB51, <ref type="bibr" target="#b13">[14]</ref> and UCF101 <ref type="bibr" target="#b23">[24]</ref>. Our extensive ablations show that both the proposed spatio-temporal enrichment and query-class similarity classifier enhance feature discriminability, leading to significant improvements over the baseline. The spatio-temporal enrichment module further enables the modeling of temporal relationships using a single cardinality. Our approach outperforms existing FS action recognition methods in the literature on all four benchmarks. On the challenging SSv2 benchmark, our approach achieves classification accuracy of 68.1% with an absolute gain of 3.5% over the recently introduced TRX <ref type="bibr" target="#b19">[20]</ref>, when employing the ResNet-50 backbone. <ref type="figure" target="#fig_7">Fig. 1</ref> shows a comparison of our approach with TRX, in terms of attention map visualizations, on examples from SSv2 and HMDB51.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>Problem Formulation: The goal of few-shot (FS) action recognition is to classify an unlabelled query video into one of the C action classes in the 'support set' comprising K labelled instances for each class that is unseen during training. To this end, let Q = {q 1 , ? ? ? , q L } denote a query video of L frames that is to be classified into a class c ? C. Moreover, let S c be the support set of K videos for an action class c with the k th video denoted as S c k = {s c k1 , ? ? ? , s c kL }. For simplicity, we represent each video as a sequence of uniformly sampled L frames. In this work, we follow an episodic training paradigm as in <ref type="bibr" target="#b15">[16]</ref>, where few-shot tasks are randomly sampled from the training set for learning the C-way K-shot classification task in each episode. Next, we describe the baseline FS action recognition framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Baseline FS Action Recognition Framework</head><p>In this work, we adopt as baseline the recently introduced Temporal-relational CrossTransformer (TRX) <ref type="bibr" target="#b19">[20]</ref> method, which has shown to achieve state-of-the-art performance on multiple action recognition benchmarks. The TRX classifies a query video by matching it with the actions occurring at different speeds and instants in the support class videos using CrossTransformers <ref type="bibr" target="#b6">[7]</ref>. First, for each sub-sequence in the query video, a query-specific class prototype is computed via an aggregation of all possible sub-sequences in the support videos of an action class. The aggregation weights are based on the cross-attention values between the query sub-sequence and support class sub-sequences. Afterwards, the distances between the embeddings of the subsequences of a query video and their corresponding queryspecific class prototypes are averaged to obtain the distance of the query to a class.</p><p>The TRX method introduces hand-crafted representations to capture the higher-order temporal relationships, where sub-sequences are represented by tuples of different cardinalities based on the number of frames used for encoding a sub-sequence. For instance, with e i ? R D as the i th frame representation, a sub-sequence between t i and t j can be represented as a pair (e i , e j ) ? R 2D , a triplet (e i , e k , e j ) ? R 3D , a quartet (e i , e k , e l , e j ) ? R 4D and so on, such that 1?i&lt;k&lt;l&lt;j?L. For a tuple t = (t 1 , ? ? ? , t ? ) of cardinality ? ? ?, let q t ? R D be a value embedding of query Q t = [e t1 ; ? ? ? ; e t? ] ? R ?D and p c t ? R D be the query-cardinality-specific class prototype, obtained by the attention-based aggregation of value embeddings of support tuples S c kt ? R ?D . Then, the distance between a query video Q and support set S c over multiple cardinalities is given by,</p><formula xml:id="formula_0">T(Q, S c ) = ??? 1 |? ? | t??? q t ? p c t ,<label>(1)</label></formula><p>where ? ? ={(t 1 , ? ? ? , t ? ) ? N ? : 1 ? t 1 &lt; ? ? ? &lt; t ? ? L} is the set of all possible tuples for cardinality ?. The distance T(?, ?) from a query video to its ground-truth class is minimized by employing a standard cross-entropy loss during training. For further details, we refer to <ref type="bibr" target="#b19">[20]</ref>.</p><p>Limitations: As discussed above, TRX performs temporal relationship modeling between the query and support action sub-sequences. However, this modeling struggles in the case of spatial context variation (appearance change of relevant objects in query and support videos) and associated variation in temporal context (aggregation of spatial context across frames). Such variations are typically encountered in case of fine-grained action categories (see <ref type="figure" target="#fig_7">Fig. 1</ref>). Furthermore, TRX jointly employs multiple CrossTransformers, one for each different cardinality, to model higher-order temporal relationships based on different hand-crafted temporal representations of sub-sequences. Consequently, this results in a less flexible model requiring dedicated branches for different cardinalities in addition to involving a manual model-search over different ? combinations to find the optimal ? * . Next, we present our proposed approach that aims to collectively treat the aforementioned issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed STRM Approach</head><p>Motivation: Here, we introduce our few-shot (FS) action recognition framework, STRM, which strives to enhance class-specific feature discriminability while simultaneously mitigating the flexibility issue. Feature Discriminability: Distinct from TRX that focuses solely on temporal relationship modeling, our approach emphasizes the importance of aggregating spatial and temporal context to effectively enrich the video sub-sequence representations before modeling the temporal relations. The local representation followed by learning rich spatial and temporal relationships enables enhanced feature discriminability, leading to an effective utilization of the limited samples available for FS action recognition. Model Flexibility: As discussed earlier, TRX employs hand-crafted higher-order temporal representations of different cardinalities, thereby requiring a search over multiple combinations. Instead, our approach learns to model higher-order relations at lower cardinalities with reduced inductive-bias, in turn improving the model flexibility.</p><p>To collectively address both the above issues, we introduce an enrichment mechanism that targets enhanced feature discriminability of individual frames at a local patchlevel (spatial) as well as the video itself at a global framelevel (temporal) while simultaneously learning higher-order temporal representations for improved flexibility.  Next, the frame-level enrichment (FLE) sub-module enhances the frame representations globally by encoding the temporal context from different frames in the video and outputs spatio-temporally enriched frame-level representations E ? R L?D . These representations E are input to a temporal relationship modeling (TRM) module, which classifies the query video by matching its sub-sequences with support actions. Additionally, classifying intermediate representations H by introducing a query-class similarity classifier reinforces the learning of corresponding class-level information at different stages and aids in further improving the overall feature discriminability. Our framework is learned jointly using standard cross-entropy loss terms L T M and L QC on the class predictions from the TRM module and query-class similarity classifier, respectively. Next, we present our proposed spatio-temporal enrichment module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overall Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Spatio-temporal Enrichment</head><p>The focus of our approach is the introduction of a spatiotemporal enrichment module that strives to enhance (i) local patch features spatially in an individual frame and (ii) global frame features temporally across frames in a video. The effective utilization of both spatial as well as temporal contextual information within a video enables improved class-specific feature discriminability before modeling the temporal relationships between query and support videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Enriching Local Patch Features</head><p>The patch features together in a frame encode its spatial information. Enhancing these features to encode the framelevel spatial context across all the patches in a frame is necessary to capture the appearance-based similarities as well as differences among the action classes. To this end, we introduce a patch-level enrichment (PLE) sub-module, which employs self-attention <ref type="bibr" target="#b27">[28]</ref> to let the patch features attend to themselves by aggregating the congruent patch contexts. The PLE sub-module is illustrated in <ref type="figure">Fig. 3</ref>. Let</p><formula xml:id="formula_1">x i ? R P 2 ?D denote the latent features of P 2 patches in frame q i (i ? [1, L]). Weights W 1 , W 2 , W 3 ? R D?D</formula><p>project these latent features to obtain query-key-value triplets, given by</p><formula xml:id="formula_2">x q i = x i W 1 , x k i = x i W 2 , x v i = x i W 3 .<label>(2)</label></formula><p>While the value embedding persists the current status of a patch p ? [1, P 2 ], the query and key vectors score the pairwise similarity between P 2 patches. These value embeddings are reweighted by corresponding normalized scores to obtain 'token-mixed' (attended) features ? i , given by</p><formula xml:id="formula_3">? i = ? x q i x k i ? D x v i + x i ,<label>(3)</label></formula><p>? <ref type="figure">Figure 3</ref>. Patch-level enrichment (PLE) sub-module. Latent features xi are projected by learnable weights W1, W2 and W3 to form query-key-value triplets (</p><formula xml:id="formula_4">x q i , x k i , x v i ).</formula><p>The value embeddings are re-weighted by the normalized pairwise scores between queries and keys, to obtain attended features ?i. A sub-network ?(?) refines these ?i to produce patch-level enriched features fi.</p><p>Here, example attention maps before (on the left) and after (on the right) patch-level enrichment are shown. Best viewed zoomed in.</p><p>where ? denotes softmax function. A sub-network ?(?) then point-wise refines these attended features ? i ? R P 2 ?D and outputs spatially enriched features f i ? R P 2 ?D , given by</p><formula xml:id="formula_5">f i = ?(? i ) + ? i ,<label>(4)</label></formula><p>leading to an improved aggregation of the appearance-based action context across patches in a frame (see <ref type="figure" target="#fig_3">Fig. 5</ref> row 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Enriching Global Frame Features</head><p>The local patch-level enrichment (PLE) described above aims to aggregate the spatial contexts locally within each frame of an action video. This enables focusing on relevant objects in a frame. However, it does not explicitly encode the temporal context and therefore struggles when encountered with object motion over time (see <ref type="figure" target="#fig_3">Fig. 5</ref>). Here, we proceed with the enrichment of temporal contexts globally across frames within a video by introducing a framelevel enrichment (FLE) sub-module comprising an MLPmixer <ref type="bibr" target="#b25">[26]</ref> layer. While self-attention is based on sampledependent (input-specific) mixing guided by pairwise similarities between the tokens, the token-mixing in MLPmixers assimilates the entire global receptive field through an input-independent and persistent relationship memory. Such a global assimilation of tokens enables the MLP-mixer to be better suited for enriching global frame representations. The FLE sub-module is shown in <ref type="figure">Fig. 4</ref>. For a frame q i , let h i ? R D denote the global representation obtained by spatially averaging the PLE output f i ? R P 2 ?D . The concatenated global representation H = [h 1 ; ? ? ? ; h L ] ? R L?D for the entire video is then processed by the FLE sub-module. First, the frame tokens are mixed through a two-layer MLP W t (?) that is shared across channels (feature dimensions). This is followed by the token refinement of the intermediate features H * by utilizing another twolayer MLP W r (?), which is shared across tokens. The two mixing operations in FLE are given by, where E ? R L?D is the enriched feature, W t1 , W t2 ? R L?L and W r1 , W r2 ? R D?D are the learnable weights for token-and channel-mixing, respectively. Here, ? denotes the ReLU non-linearity. In particular, the tokenmixing operation ensures that the frame representations interact together and imbibe the higher-order temporal relationships through the learnable weights W t1 and W t2 . As a result, the FLE sub-module enhances the frame representations h i temporally, with a global receptive field encompassing all the frames and produces temporally-enriched</p><formula xml:id="formula_6">H * = ?(H W t1 )W t2 + H ,<label>(5)</label></formula><formula xml:id="formula_7">E = ?(H * W r1 )W r2 + H * ,<label>(6)</label></formula><formula xml:id="formula_8">? R e L U R e L U ? ? Figure 4. Frame-level enrichment (FLE) sub-module.</formula><formula xml:id="formula_9">representations e i for i ? [1, L].</formula><p>The enriched frame-level global representations e i (i ? [1, L]) for the query and support videos are then input to the temporal relationship modeling (TRM) module, which models the temporal relationships between query and support actions. Within our framework, the TRM is a TRX (Eq. 1) built on a single cardinality ? = {2}, since our spatio-temporal enrichment module learns to model higherorder temporal representations without requiring multiple hand-crafted cardinality representations. Given the groundtruth labels y ? R C , our framework is then learned end-toend using the standard cross-entropy (CE) loss on the class probabilities? T M ? R C predicted by the TRM, given by</p><formula xml:id="formula_10">L T M = E[CE(? T M , y)].<label>(7)</label></formula><p>In summary, our spatio-temporal enrichment module leverages the advantages of local and global, sample-dependent and sample-agnostic enrichment mechanism to improve the aggregation of spatial as well as temporal contexts of actions. As a result, class-specific discriminative features are obtained along with the assimilation of higher-order temporal relationships in lower cardinality representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Query-class Similarity</head><p>As discussed above, the proposed framework comprising the feature extractor, spatio-temporal enrichment and temporal relationship modeling modules, is learned end-to-end with a CE loss on the output probabilities? T M . However, learning to classify query video representations from intermediate layer outputs reinforces the model to look for class- specific features at different stages in the pipeline. Consequently, such a multi-stage classification improves the feature discriminability, leading to better matching between query and support videos. To this end, we introduce a query-class similarity classifier on the patch-level enriched representations h i , i ? [1, L]. First, we obtain latent tuple representations l t = [h t1 ; ? ? ? ; h t? ] ? R ?D for tuples t = (t 1 , ? ? ? , t ? ) ? ? ? in a video. They are then projected by W cls ? R ?D?D to obtain z t = ?(W cls l t ), where ? is the ReLU non-linearity. Then, for each z Q t in a query video Q, its highest similarity among all tuples in the K support videos for an action class c is computed. These scores for all the tuples in Q are aggregated to obtain the query-class similarity M (Q, c) between the query and action c. With z c j representing a tuple j ? [1, K ? |? ? |] from the K support videos for an action c, the query-class similarity is given by</p><formula xml:id="formula_11">M (Q, c) = ??? 1 |? ? | t??? max j ?(z Q t , z c j ),<label>(8)</label></formula><p>where ?(?, ?) is a similarity function. Then, the C similarity scores are passed through softmax to obtain class probabilities? QC ? R C and trained with a CE loss given by</p><formula xml:id="formula_12">L QC = E[CE(? QC , y)].<label>(9)</label></formula><p>With ? as a hyper-weight, our STRM is trained using the joint formulation given by</p><formula xml:id="formula_13">L = L T M + ?L QC .<label>(10)</label></formula><p>Consequently, our proposed STRM, comprising a spatiotemporal enrichment module and an intermediate query-class similarity classifier, enhances feature discriminability (see <ref type="figure" target="#fig_3">Fig. 5</ref>) and leads to improved matching between queries and their support action classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets: Our approach is evaluated on four popular benchmarks: Something-Something V2 (SSv2) <ref type="bibr" target="#b10">[11]</ref>, Kinetics <ref type="bibr" target="#b3">[4]</ref>, HMDB51 <ref type="bibr" target="#b13">[14]</ref> and UCF101 <ref type="bibr" target="#b23">[24]</ref>. The SSv2 is crowdsourced, challenging and has actions requiring temporal reasoning. For SSv2, we use the split with 64/12/24 action classes in training/validation/testing, given by <ref type="bibr" target="#b2">[3]</ref>. A similar split with 64/12/24 action classes, as in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b34">35]</ref> is used for Kinetics. Furthermore, we evaluate on HMDB51 and UCF101 using the splits from <ref type="bibr" target="#b31">[32]</ref>. The standard 5-way 5shot evaluation is employed on all datasets and the average accuracy over 10,000 random test tasks is reported. Implementation Details: As in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20]</ref>, a ResNet-50 <ref type="bibr" target="#b11">[12]</ref>, pretrained on ImageNet <ref type="bibr" target="#b5">[6]</ref>, is used as the feature extractor for L = 8 uniformly sampled frames of a video. With D = 2,048, an adaptive maxpooling reduces the spatial resolution to P = 4. All the learnable weights matrices in PLE and FLE are implemented as fully-connected (FC) layers. The sub-network ?(?) in PLE is a 3-layer FC network with latent sizes set to 1,024. We set D = 1,024 for W cls . For the TRM, we employ ? = {2} in Eq. 1 and set D = 1,152, as in <ref type="bibr" target="#b19">[20]</ref>. The hyper-weight ? is set to 0.1.</p><p>While 75,000 randomly sampled training episodes are used for SSv2 dataset with a learning rate of 10 ?3 , the smaller datasets are trained with a 10 ?4 learning rate. Our STRM framework is trained end-to-end using an SGD optimizer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">State-of-the-art Comparison</head><p>Tab. 1 shows the state-of-the-art comparison on four benchmarks for the standard 5-way 5-shot action recognition task. For fairness, only the approaches employing a 2D backbone for extracting per-frame features are compared in Tab. 1. On Kinetics, the recent works of OTAM <ref type="bibr" target="#b2">[3]</ref> and TRX <ref type="bibr" target="#b19">[20]</ref> achieve comparable classification accuracies of 85.8 and 85.9%. Our STRM performs favorably against existing methods by achieving an improved performance of 86.5%. On the more challenging SSv2 dataset comprising actions requiring temporal relational reasoning, OTAM and HF-AR <ref type="bibr" target="#b14">[15]</ref> achieve 52.3% and 55.1%, while TRX obtains an accuracy of 64.6%, due to its temporal relationship modeling. Compared to the best existing approach of TRX, our STRM achieves a significant absolute gain of 3.5% on SSv2. Similarly, our STRM achieves improved performance on HMDB51 and UCF101, setting a new state-of-the-art on all four benchmarks. To further evaluate our contributions, we replace the ResNet-50 with ViT <ref type="bibr" target="#b7">[8]</ref> as the backbone. Even with this stronger backbone, our STRM outperforms TRX on all datasets. In addition, our STRM achieves gains of 1.5% and 1.9% over TRX on SSv2, when employing 3D ResNet-50 and MViT <ref type="bibr" target="#b8">[9]</ref> backbones. Note that the 3D ResNet-50 and MViT are pretrained on Kinetics400 <ref type="bibr" target="#b3">[4]</ref> and are not always compatible with few-shot action datasets due to possible overlap of pretraining classes with novel classes. The consistent improvement of our STRM emphasizes the efficacy of enhancing spatio-temporal features, by integrating local (sample-dependent) patch-level and global (sampleagnostic) frame-level enrichment along with a query-class similarity classifier, for FS action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>Impact of the proposed contributions: Here, we systematically analyse the impact of our spatio-temporal enrichment module along with the query-class classifier. Note that our spatio-temporal enrichment module comprises PLE and FLE sub-modules. <ref type="figure" target="#fig_4">Fig. 6</ref> (left) shows a performance comparison on SSv2, when integrating our two contributions (spatio-temporal enrichment module and the queryclass classifier) in the baseline TRM. Note that the baseline TRM is a TRX <ref type="bibr" target="#b19">[20]</ref> with cardinality ? = {2}. The baseline TRM achieves an FS action classification accuracy of 62.1% (red bar). Integrating our PLE in the baseline, for enriching the spatial context in the local patch-level features before temporal modeling, achieves an improved accuracy of 63.7% (orange bar). Similarly, enriching the temporal context alone in the global frame-level features through the integration of FLE (green bar) in TRM achieves a gain of 3.2%. Moreover, the joint integration of PLE and FLE (light blue bar) in the TRM further enhances the spatio-temporal contexts in the features, leading to an improved accuracy of 66.8%. Lastly, integrating the query-class classifier in our approach reinforces the learning of class-separable features at different stages and further enhances feature discriminability, thus, achieving a superior performance of 68.1%. The final STRM framework (blue bar) achieves an absolute gain of 6.0% over the baseline (red bar). Impact of varying cardinalities: Tab. 2 shows the impact of varying the cardinalities considered for modeling temporal relationships in our STRM. The comparison is shown for Kinetics and SSv2. The number of tuples present in corresponding cardinality combinations is also shown. We observe that our STRM achieves optimal performance even at lower cardinalities. In particular, our STRM achieves the best performance on both datasets with ? = {2}. In contrast, TRX employing hand-crafted higher-order temporal representations requires ? = {2, 3} to achieve its optimal performance of 64.6% on SSv2. Moreover, it is worth mentioning that our STRM is comparable to TRX in terms <ref type="table">Table 2</ref>. Impact of varying the cardinalities for temporal relationships in our STRM on Kinetics and SSv2. Here, we also show the number of tuples available in the corresponding cardinality combinations. Our STRM achieves best performance at a lower cardinality of ? = {2}, thereby mitigating the need of multiple TRM branches for different cardinalities. of compute, requiring only ? 4% additional FLOPs. The superior performance of our approach over TRX at lower cardinality is due to the enhanced feature discriminability achieved through the spatio-temporal feature enrichment and the learning of higher-order temporal representations caused by token-mixing in our FLE sub-module. Impact of varying tuples: <ref type="figure" target="#fig_4">Fig. 6 (right)</ref> shows the performance of our STRM approach on SSv2 when retaining different number of tuples for matching between query and support videos. We observe a marginal drop when the retained tuples are decreased. Moreover, even when retaining only 20% of the tuples at a lower cardinality (? = {2}), our STRM achieves an accuracy of 65.4% and performs favorably against 64.6% of TRX, which relies on all the tuples from multiple cardinalities (? = {2, 3}). This shows that our spatio-temporal enrichment module along with the query-class classifier enhances the feature discriminability while learning higher-order temporal representations in lower cardinalities itself. As a result, our STRM provides improved model flexibility, without requiring dedicated TRM branches for different cardinalities.</p><formula xml:id="formula_14">Cardinalities (?) {1} {2} {3} {4} {2, 3} {2, 4} {3, 4} {2, 3,</formula><p>Comparison with different number of support samples: <ref type="figure" target="#fig_5">Fig. 7</ref> compares STRM with the baseline and the TRX, when varying the number of support samples on SSv2. Here, we show K-shot (K ? 5 and 10) classification. Our STRM achieves consistent improvement in performance, compared to both TRM and TRX on all K-shot settings. Specifically, our STRM excels in the extreme one-shot case as well as the 10-shot setting, where it effectively leverages larger support sets. Additional results are provided in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Relation to Prior Art</head><p>Several works have investigated the few-shot (FS) problem for image classification <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10]</ref>, object detection <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b29">30]</ref>, and segmentation <ref type="bibr" target="#b16">[17]</ref>. While earlier approaches were either adaptation-based <ref type="bibr" target="#b17">[18]</ref>, generative <ref type="bibr" target="#b32">[33]</ref>, or metricbased <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29]</ref>, recent works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22]</ref> employ a combination of these. In the context of FS action recognition, <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref> employ memory networks for key-frame representations, whereas <ref type="bibr" target="#b1">[2]</ref> aligns variable length query and support videos. Differently, <ref type="bibr" target="#b2">[3]</ref> utilizes monotonic temporal ordering for enforcing temporal consistency between video pairs. The recent work of TRX <ref type="bibr" target="#b19">[20]</ref> focuses on modeling the temporal relationships by utilizing fixed higher-order temporal representations. Distinct from TRX, our STRM introduces a spatio-temporal enrichment module to produce spatiotemporally enriched features. The spatio-temporal enrichment module enriches features at local patch-level by employing a self-attention layer <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31]</ref> as well as global frame-level by utilizing an MLP-mixer layer <ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref>. Our spatio-temporal enrichment also enables learning higherorder temporal representations at lower cardinalities. The proposed spatio-temporal enrichment module performs local patch-level enrichment using a self-attention layer as well as global frame-level enrichment by integrating a MLP-mixer, in a FS action recognition framework. Furthermore, we introduce a query-class classifier for learning to classify feature representations from intermediate layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>We proposed a FS action recognition framework, STRM, comprising spatio-temporal enrichment and temporal relationship modeling (TRM) modules along with a queryclass similarity classifier. Our STRM leverages the advantages of combining local and global, sample-dependent and sample-agnostic enrichment mechanism for enhancing the spatio-temporal features, in addition to reinforcing classseparability of features at different stages. Consequently, this enhances the spatio-temporal feature discriminability and enables the learning of higher-order temporal relations even in lower cardinality representations. Our extensive ablations reveal the benefits of the proposed contributions, leading to state-of-the-art results on all benchmarks. A likely future direction, beyond the scope of the current work, is to broaden the few-shot action recognition capability to generalize across varying domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Quantitative Results</head><p>Impact of joint spatio-temporal enrichment: Tab. 3 shows the impact of replacing our patch-level enrichment (PLE) and frame-level enrichment (FLE) sub-modules in the proposed STRM framework with a joint spatio-temporal (Jnt-ST) enrichment sub-module on the SSv2 <ref type="bibr" target="#b10">[11]</ref> dataset. The performance of Baseline TRM is also shown for comparison. Jointly enriching all the spatiotemporal patches across the frames, as in Jnt-ST, does improve the performance over the baseline but with a 50% increase in FLOPs due to computing attention over all the spatio-temporal patches in a video. Although using two layers of Jnt-SA gains over the single layer variant, it requires twice the number of FLOPs than Baseline TRM. Our proposed approach of enriching patches locally with in a frame and then enriching the frames globally in a video requires only ? 4% additional FLOPs over the baseline and obtains superior performance. This shows the importance of proposed enrichment mechanism in our STRM framework. Impact of varying the enrichment mechanism: We present the impact of varying the enrichment mechanisms in our PLE and FLE sub-modules in Tab. 4 on the SSv2 dataset. It is worth mentioning that irrespective of the enrichment mechanism employed, integrating PLE and FLE sub-modules enhances the feature discriminability, leading to improved performance over Baseline TRM. However, we observe that employing an MLP-mixer <ref type="bibr" target="#b25">[26]</ref> for enriching patches locally with in a frame (PLE) or employing selfattention <ref type="bibr" target="#b27">[28]</ref> for enriching frames globally across frames in a video (FLE) results in sub-optimal performance. This is because self-attention enriches the tokens locally in a pairwise and sample-dependent manner and is likely to be less suited for enriching the frames at a global level. Similarly, the MLP-mixer is sample-agnostic and enriches the tokens globally through a persistent relationship memory while being less suitable for enriching the patches at a local level.</p><p>Thereby, employing self-attention for local patch-level enrichment and simultaneously an MLP-mixer for global frame-level enrichment achieves the best performance and achieves an absolute gain of 6.0% over baseline. These results emphasize the efficacy of enhancing spatio-temporal features by integrating local (sample-dependent) patchlevel and global (sample-agnostic) frame-level enrichment along with a query-class similarity classifier in our STRM for the task of FS action recognition. Impact of varying ?: <ref type="figure">Fig. 8</ref> shows the FS action recognition performance comparison for different values of ?, <ref type="table">Table 3</ref>. Impact of replacing our PLE and FLE sub-modules with joint spatio-temporal self-attention sub-module on SSv2. Enriching all the spatio-temporal patches jointly across frames, denoted by Jnt-ST (number of layers shown in parenthesis), improves over Baseline TRM. However, enriching patches spatially at a local level followed by enriching frames temporally at a global level in a hierarchical fashion, as in our STRM, obtains superior performance.</p><p>Baseline TRM Jnt-ST (1 l) Jnt-ST (2 l) Ours:STRM 62.1 64.7 65.8 68. <ref type="table" target="#tab_1">1   Table 4</ref>. Impact of varying the enrichment mechanism in PLE and FLE sub-modules of our STRM on SSv2. The enrichment mechanism at patch-level and frame-level are varied between self-attention and MLP-mixer based implementations. The performance of Baseline TRM without any PLE and FLE is also shown for comparison. Irrespective of the enrichment mechanism employed, integrating PLE and FLE sub-modules improves over the baseline performance. Employing either MLP-mixer for local patch-level enrichment or self-attention for global frame-level enrichment yields sub-optimal performance. The best performance is obtained by our STRM when self-attention based PLE and MLPmixer based FLE are integrated in the framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PLE FLE Accuracy</head><p>Baseline TRM --62. which is the weight factor for the query-class similarity classification loss in the proposed STRM framework. Setting ? high (&gt; 0.4) is likely to decrease the importance of the modeling temporal relationships between query and support actions in the TRM module during training and consequently leads to a drop in performance. Furthermore, we observe that employing this intermediate layer classification loss with a low weight (around 0.1) improves the performance and achieves the best results of 68.1% accuracy for FS action recognition on the SSv2 dataset.</p><p>Class-wise performance gains: <ref type="figure">Fig. 9</ref> shows the classwise gains obtained by the proposed STRM framework over Baseline TRM on the SSv2 dataset. We observe that our STRM achieves gains above 10% for classes such as Dropping something next to something, Showing something next to something, etc. Out of 24 action classes in the test set, our STRM achieves performance gains on 21 classes. These results show that enriching the features by encoding the spatio-temporal contexts aids in improving the feature discriminability, leading to improved FS action recognition performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Qualitative Results</head><p>Here, we present additional qualitative results w.r.t. tuple matching between query and support actions in <ref type="figure" target="#fig_7">Fig. 10</ref> to 14. In each example, a query video is shown on the top along with its ground-truth class name. Three query tuples of cardinality two are shown in red, green and blue. Their corresponding best matches in the support videos (of ground-truth action) obtained by Baseline TRM and our STRM are shown on the left and right, respectively. Generally, we observe that the best matches obtained by Baseline TRM do not encode the same representative features as in the corresponding query tuple. E.g., blue and red tuples in 4 th and 5 th support videos of <ref type="figure" target="#fig_7">Fig. 10</ref>, red and blue tuples in 1 st and 3 rd support videos of <ref type="figure" target="#fig_7">Fig. 11</ref>. These results show that hand-crafted temporal representations in Baseline TRM are likely to not encode classspecific spatio-temporal context at lower cardinalities. In contrast, our STRM obtains best matches that are highly representative of the corresponding query tuples and also encodes longer temporal variations. E.g., green and blue tuples in 4 th and 5 th support videos of <ref type="figure" target="#fig_7">Fig. 10</ref>, blue tuple in 5 th support video of <ref type="figure" target="#fig_7">Fig. 11</ref>. The improved tuple matching between query and support actions in STRM is due to the proposed spatio-temporal feature enrichment, comprising patch-level and frame-level enrichment, which enhances the feature discriminability and the learning of the higher-order temporal representations at lower cardinalities that improves the model flexibility. Furthermore, <ref type="figure" target="#fig_3">Fig. 15</ref> shows additional attention map visualizations on four example (novel) classes in the SSv2 dataset. Our STRM is able to emphasize the action-relevant objects in the video reasonably well. E.g., in <ref type="figure" target="#fig_3">Fig. 15(a)</ref>, remote is emphasized in frames 2, 3 and 7. Similarly, while the bag's position is emphasized in frames 6 and 7, the focus is on the table early on, which is required to reason out the Dropping Something next to Something action in <ref type="figure" target="#fig_3">Fig. 15(c)</ref>. We also observe that fine-grained novel actions with subtle 2D motion differences are harder to classify, e.g., Pretending to put Something behind Something vs. Pretending to put Something underneath Something. In general, our STRM learns to emphasize relevant spatio-temporal features that are discriminative, leading to improved FS action recognition performance.</p><p>In summary, the quantitative and qualitative results together emphasize the benefits of our proposed spatiotemporal enrichment module in enhancing feature discriminability and model flexibility, leading to improved few-shot action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Implementation Details</head><p>The input videos are rescaled to a height of 256 and L = 8 frames are uniformly sampled, as in <ref type="bibr" target="#b19">[20]</ref>. Random 224 ? 224 crops are used as augmentation during training. In contrast, only a centre crop is used during evaluation. We use the PyTorch <ref type="bibr" target="#b18">[19]</ref> library to train our STRM framework on four NVIDIA 2080Ti GPUs. Since only a single few-shot task can fit in the memory, the gradients are accumulated and backpropagated once every 16 iterations.  <ref type="figure" target="#fig_7">Figure 10</ref>. Qualitative comparison between Baseline TRM and our STRM w.r.t. tuple matches. Three query tuples of cardinality two are shown in red, green and blue for the query video at the top. Their corresponding best matches in the support videos (of ground-truth action) obtained by Baseline TRM and our STRM are shown on the left and right, respectively. The best matches for the blue and red tuples (4 th and 5 th support videos) in Baseline TRM do not encode the action completely and are less discriminative. We observe that our STRM is able to capture better matches with longer temporal variations (green and blue tuples in 4 th and 5 th support videos) due to the learned higher order temporal representations. See Sec. B for additional details.  <ref type="figure" target="#fig_7">Figure 11</ref>. Qualitative comparison between Baseline TRM and our STRM w.r.t. tuple matches. See <ref type="figure" target="#fig_7">Fig. 10</ref> and Sec. B for additional details. The best matches for red and blue query tuples obtained by STRM (4 th and 5 th support videos) are better representatives of the corresponding query tuples, in comparison to the best matches found by Baseline TRM (1 st and 3 rd support videos).  <ref type="figure" target="#fig_7">Figure 13</ref>. Qualitative comparison between Baseline TRM and our STRM w.r.t. tuple matches. See <ref type="figure" target="#fig_7">Fig. 10</ref> and Sec. B for additional details. The best matches found by Baseline TRM (2 nd support video) for the green and blue query tuples fail to encode the true motion occurring in the corresponding query tuples. This is mitigated in the best matches obtained by our STRM.  <ref type="figure" target="#fig_3">Figure 15</ref>. Attention map visualization for four example classes in SSv2. Our STRM learns to emphasize relevant spatio-temporal features that are discriminative, leading to improved FS action recognition performance. For instance, relevant objects for corresponding actions are emphasized: remote in frames 2, 3 and 7 in (a), gems in frames 4, 6, 7 and 8 in (d), respectively. Similarly, in (c), the focus on the the table early on shifts to the bag's position in frames 6 and 7, which is required to reason out the Dropping Something next to Something action.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>: Spilling Something behind Something (SSv2) ( ) Category: Pretending to put Something behind Something (SSv2) Video Frames ( ) Category: Run (HMDB51) ( ) Category: Pushup (HMDB51)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>illustrates our overall FS action recognition framework, STRM. The L video frames are passed through an image-feature extractor, which outputs D-dimensional frame features with a spatial resolution P ?P . The frame features are then spatially flattened to obtain x i ? R P 2 ?D , i ? [1, L], which are then input to our novel spatio-temporal enrichment module comprising patch-level and frame-levelPLE Video Feature Extractor</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FLEFigure 2 .</head><label>2</label><figDesc>entropy loss on query-class similarity logits PLE: Patch-level enrichment FLE: Frame-level enrichment LEGEND Proposed STRM architecture (Sec. 3.1). Spatially flattened D-dimensional features xi ? R P 2 ?D are extracted for video frames qi (i ? [1, L]). Here, P 2 is the number of patches. The features xi are input to a patch-level enrichment (PLE, Sec. 3.2.1) block, which attends to the spatial context across patches in a frame and outputs spatially enriched features fi ? R P 2 ?D . Next, global representations H ? R L?D are obtained by spatially averaging and temporally concatenating fi. These H are then input to a frame-level enrichment (FLE, Sec. 3.2.2) block, which models higher-order temporal representations by aggregating the temporal context of actions across frames in a video. The resulting spatio-temporally enriched features E ? R L?D of query and support videos are then input to the TRM, which models the temporal relationships between them. Moreover, a query-class similarity classifier (Sec. 3.3) on the global representations H reinforces the network to learn class-discriminative features at different stages. Our framework is learned jointly using LT M and LQC . enrichment sub-modules to obtain class-discriminative representations. The patch-level enrichment (PLE) sub-module enhances the patch features locally by attending to the spatial context in each frame and outputs spatially enriched features f i ? R P 2 ?D per frame. The f i 's are spatially averaged to obtain D-dimensional frame-level representations, which are then concatenated to form H ? R L?D .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>: Approaching Something with a Camera ( ) Category: Dropping Something onto Something The impact of progressively integrating our contributions one at a time, from top (baseline) to bottom. The comparison is shown in terms of attention map visualizations measuring the activation magnitude of latent features for two examples from the SSv2<ref type="bibr" target="#b10">[11]</ref> test set. The baseline (second row) struggles to accurately capture the spatial as well as temporal contextual information. The integration of PLE sub-module (third row), which explicitly encodes spatial context, enables focus on relevant objects in a frame (second and third frame from the left in (a)). The integration of FLE sub-module (fourth row) further encodes the temporal context by consistently capturing the relevant object over time. For instance, while the context in fourth and sixth frame from the left in (a) are missed by PLE due to object motion, it is captured by the introduction of FLE. Lastly, integrating the query-class classifier further improves the attention on objects, leading to enhanced feature discriminability, e.g., seventh and eighth frame from the left in (b) has improved attention on the object (book).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>(Left) Impact of integrating our contributions in the baseline on SSv2. Individually integrating our PLE (orange bar) and FLE (green bar) into the baseline TRM results in improved performance. The joint integration (light blue bar) of PLE and FLE in the baseline enriches spatio-temporal features, leading to superior performance. Lastly, integrating our query-class classifier further enhances the feature discriminability. Our final STRM (blue bar) obtains an absolute gain of 6.0% over baseline.(Right) Impact of varying #tuples in our STRM. Multiple trials and mean performance of STRM are denoted by ? ? and ? ?, respectively. Since the feature discriminability is enhanced due to spatiotemporal enrichment, even with only 20% tuples retained, STRM (? = {2}) performs favorably against TRX (denoted by ) using ? = {2, 3} and retaining all tuples. Best viewed zoomed in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Performance comparison when varying the number of support samples in the SSv2 dataset. We show the comparison of our STRM with both TRM and TRX. STRM achieves superior performance compared to both TRM and TRX on all settings, including the challenging one-shot case. Furthermore, STRM effectively leverages larger support set in the 10-shot settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .Figure 9 .</head><label>89</label><figDesc>Impact of varying ? on SSv2. A low weight for the query-class similarity classification loss yields the best performance for our STRM framework. Training with a large weight (&gt; 0.4) for this auxiliary classification loss decreases the importance of modeling temporal relationships in the TRM module and negatively affects the performance. Performance gains obtained by STRM over Baseline TRM on SSv2 test classes. Our STRM achieves improved performance over Baseline TRM on 21 out of 24 test action classes in SSv2. Best viewed zoomed in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>1 )</head><label>1</label><figDesc>Red(2, 4): 1st match: 5th video (1, 2), 2nd match: 5th video (0, 2), 3rd match: 5th video (2, 3) 2) Green(1, 3): 1st match: 4th video (0, 1), 2nd match: 5th video (1, 2), 3rd match: 5th video (1, 3) 3) Blue(2, 3): 1st match: 1st video (4, 5), 2nd match: 5th video (0, 1), 3rd match: 4th video (0, 1) Query video (Lifting up one end of something without letting it drop down) Support set best matches (Baseline) Support set best matches (STRM) 1) Red(2, 4): 1st match: 1st video (2, 4), 2nd match: 1st video (3, 4), 3rd match: 2nd video (2, 4) 2) Green(1, 3): 1st match: 4th video (0, 4), 2nd match: 4th video (2, 4), 3rd match: 4th video (0, 6) 3) Blue(2, 3): 1st match: 5th video (0, 6), 2nd match: 5th video (5, 6), 3rd match: 1st video (0, 4)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>1 )Figure 12 .</head><label>112</label><figDesc>Red(0, 4): 1st match: 1st video (4, 7), 2nd match: 1st video (4, 6), 3rd match: 1st video<ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b4">5)</ref> 2) Green(4, 5): 1st match: 1st video (6, 7), 2nd match: 1st video (5, 7), 3rd match: 1st video<ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b5">6)</ref> 3) Blue (5, 7): 1st match: 2nd video (2, 6), 2nd match: 1st video (6, 7) , 3rd match: 2nd video<ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b6">7)</ref> Query video (Pretending to put something behind something) Support set best matches (Baseline) Support set best matches (STRM) 1) Red(0, 7): 1st match: 1st video (2, 4), 2nd match: 2nd video (1, 4), 3rd match: 2nd video (0, 4)2) Green(4, 5): 1st match: 2nd video (0, 5), 2nd match: 2nd video (4, 5), 3rd match: 2nd video(3, 5)3) Blue(5, 7): 1st match: 3rd video (1, 4), 2nd match: 1st video (0, 4), 3rd match: 2nd video<ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b5">6)</ref> Qualitative comparison between Baseline TRM and our STRM w.r.t. tuple matches. SeeFig. 10and Sec. B for additional details. For the query tuple in green, the best match obtained by our STRM (2 nd support video) is a better representative, in comparison to the best match of Baseline TRM (1 st support video).1) Red(3, 5): 1st match: 2nd video (0, 1), 2nd match: 1st video (1, 7), 3rd match: 2nd video (0, 2) 2) Green(4, 5): 1st match: 2nd video (0, 2), 2nd match: 1st video (1, 7), 3rd match: 2nd video (0, 7) 3) Blue(2, 7): 1st match: 1st video (6, 7), 2nd match: 1st video (5, 7), 3rd match: 5th video (6, 7) Query video (Pulling something from left to right) Support set best matches (Baseline) Support set best matches (STRM) 1) Red(3, 5): 1st match: 3rd video (4, 5), 2nd match: 1st video (0, 4), 3rd match: 1st video (4, 5) 2) Green(4, 5): 1st match: 1st video (4, 6), 2nd match: 3rd video(4, 6), 3rd match: 2nd video (2, 4) 3) Blue(2, 7): 1st match: 5th video (4, 7), 2nd match: 1st video (5, 7), 3rd match: 5th video (4, 6)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>1 )Figure 14 .</head><label>114</label><figDesc>Red<ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b3">4)</ref>: 1st match: 3rd video<ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b5">6)</ref>, 2nd match: 3rd video(5, 7), 3rd match: 3rd video(6, 7)2) Green(0, 4): 1st match: 1st video (5, 6), 2nd match: 1st video(5, 7), 3rd match: 3rd video(6, 7)3) Blue(2, 7): 1st match: 3rd video (5, 7), 2nd match: 3rd video:<ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b5">6)</ref>, 3rd match: 1st video:<ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b4">5)</ref> Query video (Removing something, revealing something behind)Supportset best matches (Baseline) Support set best matches (STRM) 1) Red(3, 4): 1st match: 1st video (4, 6), 2nd match: 5th video(1, 4) , 3rd match: 5th video (0, 4) 2) Green(0, 4): 1st match: 5th video: (0, 4), 2nd match:5h video (1, 4), 3rd match: 5th video (0, 6) 3) Blue(2, 7): 1st match: 5th video (3, 6) , 2nd match: 5th video (3, 7), 3rd match: 5th video(3, 5) Qualitative comparison between Baseline TRM and our STRM w.r.t. tuple matches. SeeFig. 10and Sec. B for additional details. The Baseline TRM fails to obtain support tuples that are representative enough for the query tuples in red and green. Our STRM alleviates this issue and obtains good representative matches (1 st and 5 th support videos) since it enhances the feature disriminability through patch-level as well as frame-level enrichment and learns higher-order temporal representations.Time ( ) Pretending to put Something underneath Something (b) Pretending to put Something behind Something (c) Dropping Something next to Something (d) Pouring Something out of Something Time</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The L frame tokens hi of global representation H are first mixed through learnable weights Wt 1 and Wt 2 that are shared across the feature dimensions D, to obtain intermediate representations H * . This is then followed by individual token refinement using weights Wr 1 and Wr 2 that are shared across the L tokens, to obtain frame-level enriched features E. Best viewed zoomed in.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>State-of-the-art comparison on four FS action recognition datasets, in terms of classification accuracy. Our STRM outperforms existing FS action recognition methods on all four datasets. Importantly, for ResNet-50 backbone, STRM achieves an absolute gain of 3.5% over TRX<ref type="bibr" target="#b19">[20]</ref> on the challenging SSv2 that comprises actions requiring temporal relationship reasoning.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Kinetics</cell><cell>SSv2</cell><cell>HMDB</cell><cell>UCF</cell></row><row><cell>CMN-J [35]</cell><cell>ResNet-50</cell><cell>78.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TARN [2]</cell><cell>ResNet-50</cell><cell>78.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ARN [32]</cell><cell>ResNet-50</cell><cell>82.4</cell><cell>-</cell><cell>60.6</cell><cell>83.1</cell></row><row><cell>OTAM [3]</cell><cell>ResNet-50</cell><cell>85.8</cell><cell>52.3</cell><cell>-</cell><cell>-</cell></row><row><cell>HF-AR [15]</cell><cell>ResNet-50</cell><cell>-</cell><cell>55.1</cell><cell>62.2</cell><cell>86.4</cell></row><row><cell>TRX [20]</cell><cell>ResNet-50</cell><cell>85.9</cell><cell>64.6</cell><cell>75.6</cell><cell>96.1</cell></row><row><cell>Ours:STRM</cell><cell>ResNet-50</cell><cell>86.7</cell><cell>68.1</cell><cell>77.3</cell><cell>96.9</cell></row><row><cell>TRX [20]</cell><cell>ViT</cell><cell>90.6</cell><cell>67.3</cell><cell>79.7</cell><cell>97.1</cell></row><row><cell>Ours:STRM</cell><cell>ViT</cell><cell>91.2</cell><cell>70.2</cell><cell>81.3</cell><cell>98.1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was partially supported by VR starting grant (2016-05543), in addition to the compute support provided at the Swedish National Infrastructure for Computing (SNIC), partially funded by the Swedish Research Council through grant agreement 2018-05973.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In this appendix, we present additional quantitative (Sec. A) and qualitative (Sec. B) results of our proposed few-shot (FS) action recognition framework, STRM.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improved few-shot visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Bateni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaden</forename><surname>Masrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tarn: Temporal attentive relation network for fewshot and zero-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mina</forename><surname>Bishay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.09021</idno>
	</analytic>
	<monogr>
		<title level="m">Georgios Zoumpourlis, and Ioannis Patras</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Few-shot video classification via temporal alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yi</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Crosstransformers: spatially-aware few-shot transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.11498</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno>ICLR, 2021. 7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multiscale vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The &quot;something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Thurau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Bax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Repmet: Representative-based metric learning for classification and few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivan</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Amit Aides, Rogerio Feris, Raja Giryes, and Alex M Bronstein</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hmdb: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Few shot activity recognition using variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neeraj</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhansh</forename><surname>Narang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.08990</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">On episodes, prototypical networks, and few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steinar</forename><surname>Laenen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09831,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Crnet: Cross-reference networks for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weide</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Temporal-relational crosstransformers for few-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Masullo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tilo</forename><surname>Burghardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majid</forename><surname>Mirmehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Fast and flexible multi-task classification using conditional neural adaptive processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Requeima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05175</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Synthesizer: Rethinking self-attention for transformer models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Che</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01601</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03404</idno>
		<title level="m">Feedforward networks for image classification with data-efficient training</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Frustratingly simple few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06957</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pay less attention with lightweight and dynamic convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Few-shot action recognition with permutation-invariant attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Metagan: An adversarial approach to few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Compound memory networks for few-shot video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Label independent memory for semi-supervised few-shot video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
